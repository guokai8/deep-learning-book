# ç¬¬å…­ç« ï¼šå·ç§¯ç¥ç»ç½‘ç»œ (Convolutional Neural Networks)

## ğŸ“Œ ç« èŠ‚ç›®æ ‡
- ç†è§£å·ç§¯æ“ä½œçš„åŸç†
- æŒæ¡ CNN çš„å…³é”®æ¦‚å¿µï¼ˆå·ç§¯æ ¸ã€æ± åŒ–ã€æ„Ÿå—é‡ï¼‰
- å­¦ä¹ ç»å…¸ CNN æ¶æ„
- äº†è§£ CNN çš„ç‰¹æ€§å’Œä¼˜åŠ¿
- å®æˆ˜ï¼šå›¾åƒåˆ†ç±»å’Œç‰©ä½“æ£€æµ‹

---

## 6.1 ä¸ºä»€ä¹ˆéœ€è¦ CNNï¼Ÿ

### ğŸ¯ å…¨è¿æ¥ç½‘ç»œçš„é—®é¢˜

**å›¾åƒåˆ†ç±»ä»»åŠ¡**ï¼š28Ã—28 åƒç´ çš„æ‰‹å†™æ•°å­—

```
è¾“å…¥å±‚ï¼š28Ã—28 = 784 ä¸ªç¥ç»å…ƒ
éšè—å±‚ï¼š512 ä¸ªç¥ç»å…ƒ

æƒé‡æ•°é‡ï¼š784 Ã— 512 = 401,408 ä¸ªï¼
```

**é—®é¢˜**ï¼š
1. **å‚æ•°å¤ªå¤š**ï¼šå®¹æ˜“è¿‡æ‹Ÿåˆ
2. **è®¡ç®—é‡å¤§**ï¼šè®­ç»ƒæ…¢
3. **ç©ºé—´ç»“æ„ä¸¢å¤±**ï¼šç›¸é‚»åƒç´ çš„å…³ç³»è¢«å¿½è§†
4. **ä¸ç¨³å®š**ï¼šå¹³ç§»å›¾åƒä¼šå¾—åˆ°ä¸åŒç»“æœ

### ğŸ’¡ CNN çš„æ ¸å¿ƒæ€æƒ³

**è§‚å¯Ÿ**ï¼šå›¾åƒå…·æœ‰å±€éƒ¨ç»“æ„ç‰¹æ€§

```
ä¸€ä¸ªå°çš„å·ç§¯æ ¸å¯ä»¥æ£€æµ‹ï¼š
  - è¾¹ç¼˜
  - è§’
  - çº¹ç†
  - ...
```

**ç­–ç•¥**ï¼š
1. ç”¨å°å·ç§¯æ ¸æ‰«ææ•´ä¸ªå›¾åƒï¼ˆå‚æ•°å…±äº«ï¼‰
2. æå–å±€éƒ¨ç‰¹å¾
3. é€å±‚æŠ½è±¡ï¼ˆä»ä½çº§ç‰¹å¾åˆ°é«˜çº§è¯­ä¹‰ï¼‰

---

## 6.2 å·ç§¯æ“ä½œ (Convolution)

### ğŸ“ å•é€šé“å·ç§¯

**è¾“å…¥**ï¼š5Ã—5 çš„å›¾åƒ
```
1 0 1 0 1
0 1 0 1 0
1 0 1 0 1
0 1 0 1 0
1 0 1 0 1
```

**å·ç§¯æ ¸**ï¼š3Ã—3
```
1 0 -1
1 0 -1
1 0 -1
```

**å·ç§¯è¿‡ç¨‹**ï¼š

```
ç¬¬1ä¸ªä½ç½®ï¼š
  1 0 1       1 0 -1
  0 1 0   âŠ—   1 0 -1   = 1Ã—1 + 0Ã—0 + 1Ã—(-1) + 0Ã—1 + 1Ã—0 + 0Ã—(-1)
  1 0 1       1 0 -1     + 1Ã—1 + 0Ã—0 + 1Ã—(-1)
                       = 1 + 0 - 1 + 0 + 0 + 0 + 1 + 0 - 1 = 0
```

**è¾“å‡º**ï¼šç‰¹å¾å›¾ï¼ˆfeature mapï¼‰

```
0  2  0
2  0  2
0  2  0  (3Ã—3 çš„è¾“å‡º)
```

### ğŸ“ å¤šé€šé“å·ç§¯ (å½©è‰²å›¾åƒ)

**è¾“å…¥**ï¼š5Ã—5Ã—3 (RGB å›¾åƒ)
```
Ré€šé“ã€Gé€šé“ã€Bé€šé“
```

**å·ç§¯æ ¸**ï¼š3Ã—3Ã—3 (é’ˆå¯¹æ¯ä¸ªé€šé“å„æœ‰ä¸€ä¸ªæ ¸)

```
å·ç§¯è¿‡ç¨‹ï¼š
  å¯¹æ¯ä¸ªé€šé“åˆ†åˆ«åšå·ç§¯
  ç„¶åæ±‚å’Œ
```

### ğŸ“ æ•°å­¦è¡¨ç¤º

```
y[i,j] = Î£_m Î£_n x[i+m, j+n] Â· w[m,n] + b

å…¶ä¸­ï¼š
  x: è¾“å…¥
  w: å·ç§¯æ ¸æƒé‡
  b: åç½®
```

**æ•ˆç‡é«˜çš„åŸå› **ï¼š
- å‚æ•°å…±äº«ï¼šåŒä¸€ä¸ªå·ç§¯æ ¸ç”¨äºæ•´ä¸ªå›¾åƒ
- ç›¸æ¯”å…¨è¿æ¥ï¼šå‚æ•°é‡å¤§å¹…å‡å°‘

---

### ğŸ’» Python å®ç°

```python
import numpy as np

def convolve2d(image, kernel, padding=0, stride=1):
    """
    2D å·ç§¯

    å‚æ•°ï¼š
        image: è¾“å…¥ (H, W)
        kernel: å·ç§¯æ ¸ (K, K)
        padding: å¡«å……
        stride: æ­¥é•¿
    """
    H, W = image.shape
    K, _ = kernel.shape

    # åŠ  padding
    if padding > 0:
        image = np.pad(image, padding, mode='constant')

    # è¾“å‡ºå¤§å°
    H_out = (H + 2*padding - K) // stride + 1
    W_out = (W + 2*padding - K) // stride + 1

    # è¾“å‡ºç‰¹å¾å›¾
    output = np.zeros((H_out, W_out))

    # å·ç§¯æ“ä½œ
    for i in range(H_out):
        for j in range(W_out):
            # æå–åŒºåŸŸ
            region = image[i*stride:i*stride+K, j*stride:j*stride+K]
            # é€å…ƒç´ ç›¸ä¹˜åæ±‚å’Œ
            output[i, j] = np.sum(region * kernel)

    return output

# ç¤ºä¾‹
image = np.array([
    [1, 0, 1, 0, 1],
    [0, 1, 0, 1, 0],
    [1, 0, 1, 0, 1],
    [0, 1, 0, 1, 0],
    [1, 0, 1, 0, 1]
], dtype=float)

kernel = np.array([
    [1, 0, -1],
    [1, 0, -1],
    [1, 0, -1]
], dtype=float)

output = convolve2d(image, kernel, padding=0, stride=1)
print("å·ç§¯è¾“å‡ºï¼š")
print(output)
```

---

## 6.3 å…³é”®æ¦‚å¿µ

### ğŸ”¹ Padding (å¡«å……)

**é—®é¢˜**ï¼šå·ç§¯ä¼šå‡å°å›¾åƒå°ºå¯¸

```
è¾“å…¥ï¼š5Ã—5
å·ç§¯æ ¸ï¼š3Ã—3ï¼Œstride=1
è¾“å‡ºï¼š3Ã—3 (ç¼©å°äº†)
```

**è§£å†³**ï¼šåœ¨è¾¹ç•Œæ·»åŠ é›¶

```
ä¸ padding:           ä½¿ç”¨ padding=1:
1 0 1 0 1           0 0 0 0 0 0
0 1 0 1 0           0 1 0 1 0 1
1 0 1 0 1    â†’      0 0 1 0 1 0
0 1 0 1 0           0 1 0 1 0 1
1 0 1 0 1           0 0 1 0 1 0
                    0 0 0 0 0 0

è¾“å‡ºï¼š3Ã—3             è¾“å‡ºï¼š5Ã—5
```

**'Same' padding**ï¼š`padding = (kernel_size - 1) / 2`
- ä¿æŒè¾“å…¥è¾“å‡ºå°ºå¯¸ç›¸åŒ

**'Valid' padding**ï¼šæ—  padding
- è¾“å‡ºå°ºå¯¸ = (input_size - kernel_size) / stride + 1

---

### ğŸ”¹ Stride (æ­¥é•¿)

**ä¸€æ¬¡å·ç§¯æ ¸ç§»åŠ¨çš„è·ç¦»**

```
stride=1ï¼š
[â—]âš¬âš¬âš¬âš¬
âš¬â—âš¬âš¬
âš¬âš¬â—âš¬âš¬
âš¬âš¬âš¬â—âš¬
âš¬âš¬âš¬âš¬â—

stride=2ï¼š
[â—]âš¬[â—]âš¬[â—]
âš¬âš¬âš¬âš¬âš¬
[â—]âš¬[â—]âš¬[â—]
```

**è¾“å‡ºå°ºå¯¸è®¡ç®—**ï¼š

```
H_out = floor((H_in + 2Ã—padding - kernel_size) / stride) + 1
W_out = floor((W_in + 2Ã—padding - kernel_size) / stride) + 1
```

---

### ğŸ”¹ æ„Ÿå—é‡ (Receptive Field)

**å®šä¹‰**ï¼šè¾“å‡ºç‰¹å¾å›¾çš„ä¸€ä¸ªåƒç´ èƒ½"çœ‹åˆ°"çš„è¾“å…¥åŒºåŸŸå¤§å°

```
å•å±‚ 3Ã—3 å·ç§¯ï¼š
  æ„Ÿå—é‡ = 3Ã—3

ä¸¤å±‚ 3Ã—3 å·ç§¯ï¼š
  æ„Ÿå—é‡ = 5Ã—5

ä¸‰å±‚ 3Ã—3 å·ç§¯ï¼š
  æ„Ÿå—é‡ = 7Ã—7
```

**è®¡ç®—å…¬å¼**ï¼š

```
RF_l = RF_{l-1} + (kernel_size - 1) Ã— Î (stride_i)
```

**æ„ä¹‰**ï¼š
- æ·±å±‚ç¥ç»å…ƒèƒ½çœ‹åˆ°æ›´å¤§èŒƒå›´
- å¯ä»¥æ•è·æ›´é«˜çº§çš„ç‰¹å¾

---

### ğŸ”¹ æ± åŒ– (Pooling)

**ç›®çš„**ï¼šé™ä½ç‰¹å¾å›¾å°ºå¯¸ï¼Œå‡å°‘è®¡ç®—é‡

#### **Max Poolingï¼ˆæœ€å¤§æ± åŒ–ï¼‰**

```
è¾“å…¥ 4Ã—4:
1  3  2  4
5  6  7  8
9  10 11 12
13 14 15 16

Max Pooling 2Ã—2, stride=2:
[1  3]  [2  4]      6   8
[5  6]  [7  8]  â†’
                    14  16
[9  10] [11 12]
[13 14] [15 16]

è¾“å‡º 2Ã—2:
6  8
14 16
```

#### **Average Poolingï¼ˆå¹³å‡æ± åŒ–ï¼‰**

```
å¯¹æ¯ä¸ªåŒºåŸŸå–å¹³å‡å€¼
```

**ä»£ç **ï¼š

```python
def max_pool2d(input, pool_size=2, stride=2):
    """Max Pooling"""
    H, W = input.shape
    H_out = (H - pool_size) // stride + 1
    W_out = (W - pool_size) // stride + 1

    output = np.zeros((H_out, W_out))

    for i in range(H_out):
        for j in range(W_out):
            region = input[i*stride:i*stride+pool_size,
                          j*stride:j*stride+pool_size]
            output[i, j] = np.max(region)

    return output

# PyTorch
import torch.nn as nn
max_pool = nn.MaxPool2d(kernel_size=2, stride=2)
avg_pool = nn.AvgPool2d(kernel_size=2, stride=2)
```

**ç‰¹ç‚¹**ï¼š
- âœ… å‡å°‘å‚æ•°å’Œè®¡ç®—é‡
- âœ… å¢åŠ å¹³ç§»ä¸å˜æ€§
- âœ… æ‰©å¤§æ„Ÿå—é‡
- âŒ ä¸¢å¤±ä½ç½®ä¿¡æ¯

---

## 6.4 å…¸å‹ CNN æ¶æ„

### ğŸ—ï¸ åŸºæœ¬ç»“æ„

```
è¾“å…¥å›¾åƒ
    â†“
[å·ç§¯ â†’ ReLU â†’ æ± åŒ–] Ã— N å±‚
    â†“
å±•å¹³
    â†“
å…¨è¿æ¥å±‚ Ã— M å±‚
    â†“
Softmax è¾“å‡º
```

---

### ğŸ”¹ LeNet-5 (1998) - CNN çš„å¼€å±±ä¹‹ä½œ

**æ¶æ„**ï¼š

```
è¾“å…¥: 32Ã—32Ã—1 (ç°åº¦å›¾)
    â†“
Conv1: 6ä¸ª 5Ã—5 å·ç§¯æ ¸ â†’ 28Ã—28Ã—6
    â†“
AvgPool1: 2Ã—2 â†’ 14Ã—14Ã—6
    â†“
Conv2: 16ä¸ª 5Ã—5 å·ç§¯æ ¸ â†’ 10Ã—10Ã—16
    â†“
AvgPool2: 2Ã—2 â†’ 5Ã—5Ã—16
    â†“
Flatten â†’ 400
    â†“
FC1: 120
    â†“
FC2: 84
    â†“
FC3: 10 (è¾“å‡º)
```

**ä»£ç å®ç°**ï¼š

```python
import torch
import torch.nn as nn

class LeNet5(nn.Module):
    def __init__(self):
        super(LeNet5, self).__init__()

        # ç‰¹å¾æå–éƒ¨åˆ†
        self.conv1 = nn.Conv2d(1, 6, kernel_size=5, padding=2)  # 32â†’32
        self.pool1 = nn.AvgPool2d(kernel_size=2, stride=2)       # 32â†’16

        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)             # 16â†’12
        self.pool2 = nn.AvgPool2d(kernel_size=2, stride=2)       # 12â†’6

        # åˆ†ç±»éƒ¨åˆ†
        self.fc1 = nn.Linear(16 * 6 * 6, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x):
        # å·ç§¯å±‚
        x = torch.relu(self.conv1(x))
        x = self.pool1(x)

        x = torch.relu(self.conv2(x))
        x = self.pool2(x)

        # å±•å¹³
        x = x.view(-1, 16 * 6 * 6)

        # å…¨è¿æ¥å±‚
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = self.fc3(x)

        return x

model = LeNet5()
print(model)
```

---

### ğŸ”¹ AlexNet (2012) - æ·±åº¦å­¦ä¹ å¤å…´

**åˆ›æ–°ç‚¹**ï¼š
- ä½¿ç”¨ ReLU æ›¿ä»£ Sigmoid/Tanh
- ä½¿ç”¨ Dropout é˜²æ­¢è¿‡æ‹Ÿåˆ
- æ•°æ®å¢å¼º
- GPU åŠ é€Ÿè®­ç»ƒ
- æ›´æ·±çš„ç½‘ç»œï¼ˆ8å±‚ï¼‰

**æ¶æ„**ï¼š

```
è¾“å…¥: 224Ã—224Ã—3
    â†“
Conv1: 96ä¸ª 11Ã—11, stride=4 â†’ 55Ã—55Ã—96
MaxPool1: 3Ã—3, stride=2 â†’ 27Ã—27Ã—96
    â†“
Conv2: 256ä¸ª 5Ã—5 â†’ 27Ã—27Ã—256
MaxPool2: 3Ã—3, stride=2 â†’ 13Ã—13Ã—256
    â†“
Conv3: 384ä¸ª 3Ã—3 â†’ 13Ã—13Ã—384
Conv4: 384ä¸ª 3Ã—3 â†’ 13Ã—13Ã—384
Conv5: 256ä¸ª 3Ã—3 â†’ 13Ã—13Ã—256
MaxPool3: 3Ã—3, stride=2 â†’ 6Ã—6Ã—256
    â†“
FC1: 4096 + Dropout
FC2: 4096 + Dropout
FC3: 1000 (ImageNet)
```

**ä»£ç **ï¼š

```python
class AlexNet(nn.Module):
    def __init__(self, num_classes=1000):
        super(AlexNet, self).__init__()

        self.features = nn.Sequential(
            # Conv1
            nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=2),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=2),

            # Conv2
            nn.Conv2d(96, 256, kernel_size=5, padding=2),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=2),

            # Conv3
            nn.Conv2d(256, 384, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),

            # Conv4
            nn.Conv2d(384, 384, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),

            # Conv5
            nn.Conv2d(384, 256, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=2),
        )

        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))

        self.classifier = nn.Sequential(
            nn.Dropout(0.5),
            nn.Linear(256 * 6 * 6, 4096),
            nn.ReLU(inplace=True),

            nn.Dropout(0.5),
            nn.Linear(4096, 4096),
            nn.ReLU(inplace=True),

            nn.Linear(4096, num_classes),
        )

    def forward(self, x):
        x = self.features(x)
        x = self.avgpool(x)
        x = torch.flatten(x, 1)
        x = self.classifier(x)
        return x
```

---

### ğŸ”¹ VGGNet (2014) - æ›´æ·±æ›´è§„æ•´

**æ ¸å¿ƒæ€æƒ³**ï¼š
- åªä½¿ç”¨ 3Ã—3 å°å·ç§¯æ ¸
- ç½‘ç»œæ›´æ·±ï¼ˆ16-19å±‚ï¼‰
- ç»“æ„è§„æ•´ï¼Œæ˜“äºç†è§£

**ä¸ºä»€ä¹ˆ 3Ã—3ï¼Ÿ**

```
ä¸¤ä¸ª 3Ã—3 å·ç§¯ = ä¸€ä¸ª 5Ã—5 æ„Ÿå—é‡
ä¸‰ä¸ª 3Ã—3 å·ç§¯ = ä¸€ä¸ª 7Ã—7 æ„Ÿå—é‡

ä½†å‚æ•°æ›´å°‘ï¼š
  7Ã—7: 49 ä¸ªå‚æ•°
  3ä¸ª3Ã—3: 27 ä¸ªå‚æ•°
```

**VGG-16 æ¶æ„**ï¼š

```
è¾“å…¥: 224Ã—224Ã—3

Block 1:
  Conv3-64 Ã— 2
  MaxPool

Block 2:
  Conv3-128 Ã— 2
  MaxPool

Block 3:
  Conv3-256 Ã— 3
  MaxPool

Block 4:
  Conv3-512 Ã— 3
  MaxPool

Block 5:
  Conv3-512 Ã— 3
  MaxPool

FC: 4096 â†’ 4096 â†’ 1000
```

**ä»£ç **ï¼š

```python
class VGG16(nn.Module):
    def __init__(self, num_classes=1000):
        super(VGG16, self).__init__()

        self.features = nn.Sequential(
            # Block 1
            nn.Conv2d(3, 64, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),

            # Block 2
            nn.Conv2d(64, 128, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),

            # Block 3
            nn.Conv2d(128, 256, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),

            # Block 4
            nn.Conv2d(256, 512, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(512, 512, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(512, 512, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),

            # Block 5
            nn.Conv2d(512, 512, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(512, 512, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(512, 512, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
        )

        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))

        self.classifier = nn.Sequential(
            nn.Linear(512 * 7 * 7, 4096),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),

            nn.Linear(4096, 4096),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),

            nn.Linear(4096, num_classes),
        )

    def forward(self, x):
        x = self.features(x)
        x = self.avgpool(x)
        x = torch.flatten(x, 1)
        x = self.classifier(x)
        return x
```

---

### ğŸ”¹ ResNet (2015) - æ®‹å·®ç½‘ç»œ â­

**é—®é¢˜**ï¼šç½‘ç»œè¶Šæ·±ï¼Œæ€§èƒ½è¶Šå·®ï¼Ÿ

```
56å±‚ç½‘ç»œçš„è®­ç»ƒè¯¯å·® > 20å±‚ç½‘ç»œï¼Ÿ
è¿™ä¸æ˜¯è¿‡æ‹Ÿåˆï¼Œè€Œæ˜¯ä¼˜åŒ–é—®é¢˜ï¼
```

**è§£å†³**ï¼šæ®‹å·®è¿æ¥ï¼ˆSkip Connectionï¼‰

```
ä¼ ç»Ÿï¼š
  x â†’ Conv â†’ ReLU â†’ Conv â†’ ReLU â†’ output

ResNetï¼š
  x â†’ Conv â†’ ReLU â†’ Conv â”€â”¬â†’ ReLU â†’ output
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       (ç›´æ¥è¿æ¥)

è¾“å‡º = F(x) + x
```

**ä¼˜åŠ¿**ï¼š
- è§£å†³æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸
- å…è®¸è®­ç»ƒè¶…æ·±ç½‘ç»œï¼ˆ152å±‚ï¼Œç”šè‡³1000å±‚ï¼‰
- æ€§èƒ½æ˜¾è‘—æå‡

**Residual Block**ï¼š

```python
class ResidualBlock(nn.Module):
    def __init__(self, in_channels, out_channels, stride=1):
        super(ResidualBlock, self).__init__()

        # ä¸»è·¯å¾„
        self.conv1 = nn.Conv2d(in_channels, out_channels,
                               kernel_size=3, stride=stride, padding=1)
        self.bn1 = nn.BatchNorm2d(out_channels)

        self.conv2 = nn.Conv2d(out_channels, out_channels,
                               kernel_size=3, stride=1, padding=1)
        self.bn2 = nn.BatchNorm2d(out_channels)

        # æ·å¾„è¿æ¥
        self.shortcut = nn.Sequential()
        if stride != 1 or in_channels != out_channels:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_channels, out_channels,
                         kernel_size=1, stride=stride),
                nn.BatchNorm2d(out_channels)
            )

    def forward(self, x):
        # ä¸»è·¯å¾„
        out = self.conv1(x)
        out = self.bn1(out)
        out = torch.relu(out)

        out = self.conv2(out)
        out = self.bn2(out)

        # æ®‹å·®è¿æ¥
        out += self.shortcut(x)
        out = torch.relu(out)

        return out
```

**ResNet-34 æ¶æ„**ï¼š

```python
class ResNet34(nn.Module):
    def __init__(self, num_classes=1000):
        super(ResNet34, self).__init__()

        # åˆå§‹å±‚
        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3)
        self.bn1 = nn.BatchNorm2d(64)
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)

        # Residual blocks
        self.layer1 = self._make_layer(64, 64, 3, stride=1)
        self.layer2 = self._make_layer(64, 128, 4, stride=2)
        self.layer3 = self._make_layer(128, 256, 6, stride=2)
        self.layer4 = self._make_layer(256, 512, 3, stride=2)

        # åˆ†ç±»å±‚
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(512, num_classes)

    def _make_layer(self, in_channels, out_channels, num_blocks, stride):
        layers = []

        # ç¬¬ä¸€ä¸ª block å¯èƒ½æ”¹å˜å°ºå¯¸
        layers.append(ResidualBlock(in_channels, out_channels, stride))

        # å‰©ä½™çš„ blocks
        for _ in range(1, num_blocks):
            layers.append(ResidualBlock(out_channels, out_channels, 1))

        return nn.Sequential(*layers)

    def forward(self, x):
        # åˆå§‹å·ç§¯
        x = self.conv1(x)
        x = self.bn1(x)
        x = torch.relu(x)
        x = self.maxpool(x)

        # Residual blocks
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)

        # åˆ†ç±»
        x = self.avgpool(x)
        x = torch.flatten(x, 1)
        x = self.fc(x)

        return x
```

**ResNet å˜ç§**ï¼š

| æ¨¡å‹ | å±‚æ•° | Top-5 é”™è¯¯ç‡ (ImageNet) |
|------|------|----------------------|
| ResNet-18 | 18 | ~10% |
| ResNet-34 | 34 | ~8% |
| ResNet-50 | 50 | ~6.7% |
| ResNet-101 | 101 | ~6.4% |
| ResNet-152 | 152 | ~6.2% |

---

## 6.5 ç°ä»£ CNN æŠ€å·§

### ğŸ”¹ 1Ã—1 å·ç§¯

**ä½œç”¨**ï¼š
- æ”¹å˜é€šé“æ•°ï¼ˆé™ç»´/å‡ç»´ï¼‰
- å¢åŠ éçº¿æ€§
- å‚æ•°å°‘

```
è¾“å…¥: 56Ã—56Ã—192
1Ã—1 å·ç§¯ï¼Œ64ä¸ªæ ¸
è¾“å‡º: 56Ã—56Ã—64

ä½œç”¨ç±»ä¼¼å…¨è¿æ¥ï¼Œä½†ä¿æŒç©ºé—´ç»“æ„
```

```python
# é™ç»´ç¤ºä¾‹
nn.Conv2d(192, 64, kernel_size=1)  # 192é€šé“ â†’ 64é€šé“
```

---

### ğŸ”¹ å…¨å±€å¹³å‡æ± åŒ– (Global Average Pooling)

**æ›¿ä»£å…¨è¿æ¥å±‚**

```
ä¼ ç»Ÿï¼š
  7Ã—7Ã—512 â†’ Flatten â†’ FC(4096) â†’ FC(1000)
  å‚æ•°é‡ï¼š7Ã—7Ã—512Ã—4096 â‰ˆ 102M

GAPï¼š
  7Ã—7Ã—512 â†’ GAP â†’ 512 â†’ FC(1000)
  å‚æ•°é‡ï¼š512Ã—1000 â‰ˆ 512K
```

```python
# PyTorch
self.gap = nn.AdaptiveAvgPool2d((1, 1))  # è¾“å‡º 1Ã—1Ã—C

# ä½¿ç”¨
x = self.gap(x)  # (B, C, H, W) â†’ (B, C, 1, 1)
x = x.view(x.size(0), -1)  # (B, C)
```

**ä¼˜åŠ¿**ï¼š
- å‚æ•°å¤§å¹…å‡å°‘
- æ›´å¼ºçš„ç©ºé—´ä¸å˜æ€§
- å‡å°‘è¿‡æ‹Ÿåˆ

---

### ğŸ”¹ æ·±åº¦å¯åˆ†ç¦»å·ç§¯ (Depthwise Separable Convolution)

**MobileNet çš„æ ¸å¿ƒ**

**æ ‡å‡†å·ç§¯**ï¼š
```
è¾“å…¥: HÃ—WÃ—C_in
å·ç§¯æ ¸: KÃ—KÃ—C_inÃ—C_out
å‚æ•°é‡: KÃ—KÃ—C_inÃ—C_out
```

**æ·±åº¦å¯åˆ†ç¦»å·ç§¯**ï¼šåˆ†ä¸¤æ­¥

```
1. Depthwise å·ç§¯ï¼š
   æ¯ä¸ªè¾“å…¥é€šé“å•ç‹¬å·ç§¯
   å‚æ•°: KÃ—KÃ—C_in

2. Pointwise å·ç§¯ï¼š
   1Ã—1 å·ç§¯æ··åˆé€šé“
   å‚æ•°: 1Ã—1Ã—C_inÃ—C_out

æ€»å‚æ•°: KÃ—KÃ—C_in + C_inÃ—C_out
```

**å‚æ•°å‡å°‘æ¯”ä¾‹**ï¼š
```
(KÂ²Ã—C_in + C_inÃ—C_out) / (KÂ²Ã—C_inÃ—C_out)
= 1/C_out + 1/KÂ²
```

**ä»£ç **ï¼š

```python
class DepthwiseSeparableConv(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size=3):
        super().__init__()

        # Depthwise
        self.depthwise = nn.Conv2d(
            in_channels, in_channels,
            kernel_size=kernel_size,
            padding=kernel_size//2,
            groups=in_channels  # å…³é”®ï¼æ¯ç»„ä¸€ä¸ªé€šé“
        )

        # Pointwise
        self.pointwise = nn.Conv2d(
            in_channels, out_channels,
            kernel_size=1
        )

    def forward(self, x):
        x = self.depthwise(x)
        x = self.pointwise(x)
        return x
```

---

## 6.6 å®æˆ˜ï¼šCIFAR-10 å›¾åƒåˆ†ç±»

### ğŸ“‹ æ•°æ®é›†ä»‹ç»

```
CIFAR-10:
  - 60,000 å¼  32Ã—32 å½©è‰²å›¾åƒ
  - 10 ä¸ªç±»åˆ«ï¼ˆé£æœºã€æ±½è½¦ã€é¸Ÿ...ï¼‰
  - 50,000 è®­ç»ƒ + 10,000 æµ‹è¯•
```

### ğŸ’» å®Œæ•´å®ç°

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
import matplotlib.pyplot as plt
from tqdm import tqdm

# ==================== è¶…å‚æ•° ====================
BATCH_SIZE = 128
LEARNING_RATE = 0.001
EPOCHS = 50
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# ==================== æ•°æ®å¢å¼º ====================
transform_train = transforms.Compose([
    transforms.RandomCrop(32, padding=4),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465),
                        (0.2023, 0.1994, 0.2010))
])

transform_test = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465),
                        (0.2023, 0.1994, 0.2010))
])

# ==================== åŠ è½½æ•°æ® ====================
train_dataset = datasets.CIFAR10(
    root='./data', train=True, download=True, transform=transform_train
)
test_dataset = datasets.CIFAR10(
    root='./data', train=False, download=True, transform=transform_test
)

train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,
                          shuffle=True, num_workers=2)
test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,
                         shuffle=False, num_workers=2)

# ==================== å®šä¹‰æ¨¡å‹ ====================
class SimpleCNN(nn.Module):
    def __init__(self):
        super(SimpleCNN, self).__init__()

        # Block 1
        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)
        self.bn1 = nn.BatchNorm2d(64)
        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)
        self.bn2 = nn.BatchNorm2d(64)
        self.pool1 = nn.MaxPool2d(2, 2)
        self.dropout1 = nn.Dropout(0.2)

        # Block 2
        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
        self.bn3 = nn.BatchNorm2d(128)
        self.conv4 = nn.Conv2d(128, 128, kernel_size=3, padding=1)
        self.bn4 = nn.BatchNorm2d(128)
        self.pool2 = nn.MaxPool2d(2, 2)
        self.dropout2 = nn.Dropout(0.3)

        # Block 3
        self.conv5 = nn.Conv2d(128, 256, kernel_size=3, padding=1)
        self.bn5 = nn.BatchNorm2d(256)
        self.conv6 = nn.Conv2d(256, 256, kernel_size=3, padding=1)
        self.bn6 = nn.BatchNorm2d(256)
        self.pool3 = nn.MaxPool2d(2, 2)
        self.dropout3 = nn.Dropout(0.4)

        # å…¨è¿æ¥å±‚
        self.fc1 = nn.Linear(256 * 4 * 4, 512)
        self.dropout4 = nn.Dropout(0.5)
        self.fc2 = nn.Linear(512, 10)

    def forward(self, x):
        # Block 1
        x = torch.relu(self.bn1(self.conv1(x)))
        x = torch.relu(self.bn2(self.conv2(x)))
        x = self.pool1(x)
        x = self.dropout1(x)

        # Block 2
        x = torch.relu(self.bn3(self.conv3(x)))
        x = torch.relu(self.bn4(self.conv4(x)))
        x = self.pool2(x)
        x = self.dropout2(x)

        # Block 3
        x = torch.relu(self.bn5(self.conv5(x)))
        x = torch.relu(self.bn6(self.conv6(x)))
        x = self.pool3(x)
        x = self.dropout3(x)

        # å±•å¹³å’Œå…¨è¿æ¥
        x = x.view(-1, 256 * 4 * 4)
        x = torch.relu(self.fc1(x))
        x = self.dropout4(x)
        x = self.fc2(x)

        return x

model = SimpleCNN().to(DEVICE)

# æŸ¥çœ‹æ¨¡å‹ç»“æ„
from torchsummary import summary
summary(model, (3, 32, 32))

# ==================== æŸå¤±å’Œä¼˜åŒ–å™¨ ====================
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)
scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)

# ==================== è®­ç»ƒå‡½æ•° ====================
def train_epoch(model, train_loader, criterion, optimizer, device):
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0

    progress_bar = tqdm(train_loader, desc='Training')

    for images, labels in progress_bar:
        images, labels = images.to(device), labels.to(device)

        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
        _, predicted = outputs.max(1)
        total += labels.size(0)
        correct += predicted.eq(labels).sum().item()

        progress_bar.set_postfix({
            'loss': f'{running_loss/len(train_loader):.3f}',
            'acc': f'{100.*correct/total:.2f}%'
        })

    return running_loss / len(train_loader), 100. * correct / total

# ==================== æµ‹è¯•å‡½æ•° ====================
def test(model, test_loader, criterion, device):
    model.eval()
    test_loss = 0
    correct = 0
    total = 0

    with torch.no_grad():
        for images, labels in tqdm(test_loader, desc='Testing'):
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            loss = criterion(outputs, labels)

            test_loss += loss.item()
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()

    return test_loss / len(test_loader), 100. * correct / total

# ==================== è®­ç»ƒå¾ªç¯ ====================
train_losses, train_accs = [], []
test_losses, test_accs = [], []
best_acc = 0

for epoch in range(EPOCHS):
    print(f'\n=== Epoch {epoch+1}/{EPOCHS} ===')

    train_loss, train_acc = train_epoch(model, train_loader, criterion,
                                        optimizer, DEVICE)
    test_loss, test_acc = test(model, test_loader, criterion, DEVICE)

    train_losses.append(train_loss)
    train_accs.append(train_acc)
    test_losses.append(test_loss)
    test_accs.append(test_acc)

    print(f'Train Loss: {train_loss:.4f}, Acc: {train_acc:.2f}%')
    print(f'Test  Loss: {test_loss:.4f}, Acc: {test_acc:.2f}%')

    # ä¿å­˜æœ€ä½³æ¨¡å‹
    if test_acc > best_acc:
        best_acc = test_acc
        torch.save(model.state_dict(), 'best_model.pth')
        print(f'âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Acc: {best_acc:.2f}%)')

    scheduler.step()

# ==================== å¯è§†åŒ– ====================
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))

epochs_range = range(1, len(train_losses) + 1)

ax1.plot(epochs_range, train_losses, 'b-', label='Train Loss')
ax1.plot(epochs_range, test_losses, 'r-', label='Test Loss')
ax1.set_xlabel('Epoch')
ax1.set_ylabel('Loss')
ax1.set_title('Training and Test Loss')
ax1.legend()
ax1.grid(True, alpha=0.3)

ax2.plot(epochs_range, train_accs, 'b-', label='Train Acc')
ax2.plot(epochs_range, test_accs, 'r-', label='Test Acc')
ax2.set_xlabel('Epoch')
ax2.set_ylabel('Accuracy (%)')
ax2.set_title('Training and Test Accuracy')
ax2.legend()
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# ==================== æœ€ç»ˆè¯„ä¼° ====================
model.load_state_dict(torch.load('best_model.pth'))
_, final_acc = test(model, test_loader, criterion, DEVICE)
print(f'\næœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡: {final_acc:.2f}%')
```

---

## 6.7 å·ç§¯ç¥ç»ç½‘ç»œçš„å¯è§†åŒ–

### ğŸ¨ ç‰¹å¾å›¾å¯è§†åŒ–

```python
def visualize_feature_maps(model, image, device):
    """å¯è§†åŒ–ä¸­é—´å±‚çš„ç‰¹å¾å›¾"""
    model.eval()

    # æå–ç‰¹å¾å›¾çš„é’©å­
    activations = {}

    def get_activation(name):
        def hook(model, input, output):
            activations[name] = output.detach()
        return hook

    # æ³¨å†Œé’©å­
    model.conv1.register_forward_hook(get_activation('conv1'))
    model.conv3.register_forward_hook(get_activation('conv3'))
    model.conv5.register_forward_hook(get_activation('conv5'))

    # å‰å‘ä¼ æ’­
    with torch.no_grad():
        output = model(image.to(device))

    # å¯è§†åŒ–
    fig, axes = plt.subplots(3, 8, figsize=(16, 6))

    # Conv1 ç‰¹å¾å›¾
    feat1 = activations['conv1'][0]
    for i in range(8):
        ax = axes[0, i]
        ax.imshow(feat1[i].cpu().numpy(), cmap='gray')
        ax.set_title(f'Conv1-{i}')
        ax.axis('off')

    # Conv3 ç‰¹å¾å›¾
    feat3 = activations['conv3'][0]
    for i in range(8):
        ax = axes[1, i]
        ax.imshow(feat3[i].cpu().numpy(), cmap='gray')
        ax.set_title(f'Conv3-{i}')
        ax.axis('off')

    # Conv5 ç‰¹å¾å›¾
    feat5 = activations['conv5'][0]
    for i in range(8):
        ax = axes[2, i]
        ax.imshow(feat5[i].cpu().numpy(), cmap='gray')
        ax.set_title(f'Conv5-{i}')
        ax.axis('off')

    plt.tight_layout()
    plt.show()

# ä½¿ç”¨
test_image, _ = test_dataset[0]
test_image = test_image.unsqueeze(0)
visualize_feature_maps(model, test_image, DEVICE)
```

---

### ğŸ¯ å·ç§¯æ ¸å¯è§†åŒ–

```python
def visualize_kernels(model):
    """å¯è§†åŒ–ç¬¬ä¸€å±‚å·ç§¯æ ¸"""
    conv1_weight = model.conv1.weight.data

    fig, axes = plt.subplots(8, 8, figsize=(12, 12))

    for i in range(64):
        ax = axes[i // 8, i % 8]

        # å¹³å‡RGBä¸‰ä¸ªé€šé“
        kernel = conv1_weight[i].mean(dim=0)

        # æ ‡å‡†åŒ–åˆ° [0, 1]
        kernel = (kernel - kernel.min()) / (kernel.max() - kernel.min())

        ax.imshow(kernel.cpu().numpy(), cmap='gray')
        ax.set_title(f'Filter {i}')
        ax.axis('off')

    plt.tight_layout()
    plt.show()

# ä½¿ç”¨
visualize_kernels(model)
```

---

## 6.8 CNN vs ViTï¼ˆå±•æœ›ï¼‰

### ğŸ”„ ä» CNN åˆ° Vision Transformer

**CNN çš„å±€é™**ï¼š
- æ„Ÿå—é‡å±€é™ï¼ˆé€å±‚æ‰©å¤§ï¼‰
- ç©ºé—´å½’çº³åç½®å¼ºï¼ˆå¯èƒ½é™åˆ¶æ€§èƒ½ï¼‰
- éœ€è¦æ›´å¤šæ•°æ®

**Vision Transformer çš„ä¼˜åŠ¿**ï¼š
- å…¨å±€æ„Ÿå—é‡ï¼ˆä»ç¬¬ä¸€å±‚ï¼‰
- æ›´çµæ´»çš„ç‰¹å¾æå–
- æ‰©å±•æ€§å¥½

```
CNN æ¶æ„ï¼š
  å·ç§¯ â†’ å·ç§¯ â†’ å·ç§¯ â†’ ç‰¹å¾
  (é€å±‚èšåˆå±€éƒ¨ä¿¡æ¯)

ViT æ¶æ„ï¼š
  Patch Embedding â†’ Transformer â†’ ç‰¹å¾
  (ç›´æ¥æ•è·å…¨å±€å…³ç³»)
```

---

## ğŸ“ æœ¬ç« ä½œä¸š

### ä½œä¸š 1ï¼šä»é›¶å®ç°å·ç§¯

```python
# TODO:
# 1. å®ç°å‰å‘å·ç§¯ï¼ˆå·²æä¾›ï¼‰
# 2. å®ç°åå‘ä¼ æ’­ï¼šæ¢¯åº¦w.r.t è¾“å…¥ã€æƒé‡ã€åç½®
# 3. å®ç°æ± åŒ–å‰å‘å’Œåå‘
# 4. ç»„åˆæˆå®Œæ•´ CNN å±‚
# 5. åœ¨å°æ•°æ®é›†ä¸Šæµ‹è¯•
```

### ä½œä¸š 2ï¼šCNN æ¶æ„å¯¹æ¯”

åœ¨ CIFAR-10 ä¸Šå®ç°å¹¶å¯¹æ¯”ï¼š

```python
# 1. LeNet-5
# 2. ç®€å• CNN (3å±‚å·ç§¯)
# 3. VGG-16 (ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹)
# 4. ResNet-18 (ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹)

# è®°å½•ï¼š
#   - å‚æ•°æ•°é‡
#   - è®­ç»ƒæ—¶é—´
#   - æœ€ç»ˆå‡†ç¡®ç‡
#   - æ¨¡å‹å¤§å°

# åˆ†æä¼˜ç¼ºç‚¹
```

### ä½œä¸š 3ï¼šç‰¹å¾å¯è§†åŒ–

```python
# å¯¹è®­ç»ƒå¥½çš„ CNN æ¨¡å‹ï¼š

# 1. å¯è§†åŒ–ä¸åŒå±‚çš„ç‰¹å¾å›¾
# 2. ç»˜åˆ¶å·ç§¯æ ¸
# 3. å°è¯• DeconvNet æˆ– Grad-CAM è¿›è¡Œå¯è§†åŒ–
# 4. åˆ†æä¸åŒå±‚å­¦åˆ°äº†ä»€ä¹ˆ

# å†™ä¸€ä»½åˆ†ææŠ¥å‘Š
```

### ä½œä¸š 4ï¼šæ•°æ®å¢å¼ºå®éªŒ

```python
# åœ¨ CIFAR-10 ä¸Šæµ‹è¯•ä¸åŒçš„æ•°æ®å¢å¼ºæ–¹æ³•ï¼š

# 1. æ— å¢å¼º
# 2. éšæœºè£å‰ª + ç¿»è½¬
# 3. + é¢œè‰²æŠ–åŠ¨
# 4. + Cutout
# 5. + MixUp / CutMix

# è®°å½•æ€§èƒ½å·®å¼‚ï¼Œåˆ†ææ¯ç§å¢å¼ºçš„ä½œç”¨
```

---

## ğŸ”‘ æœ¬ç« å…³é”®æ¦‚å¿µ

| æ¦‚å¿µ | è¯´æ˜ |
|------|------|
| å·ç§¯ | ç‰¹å¾æå–çš„æ ¸å¿ƒæ“ä½œ |
| æ„Ÿå—é‡ | è¾“å‡ºèƒ½çœ‹åˆ°çš„è¾“å…¥åŒºåŸŸ |
| æ± åŒ– | é™ç»´å’Œç‰¹å¾èšåˆ |
| å¡«å…… | æ§åˆ¶è¾“å‡ºå°ºå¯¸ |
| æ­¥é•¿ | å·ç§¯æ ¸ç§»åŠ¨è·ç¦» |
| å‚æ•°å…±äº« | CNN çš„æ ¸å¿ƒä¼˜åŠ¿ |
| LeNet | CNN çš„å¼€åˆ›è€… |
| AlexNet | æ·±åº¦å­¦ä¹ å¤å…´ |
| VGG | è§„æ•´æ·±å±‚è®¾è®¡ |
| ResNet | æ®‹å·®è¿æ¥è§£å†³æ·±åº¦é—®é¢˜ |
| æ·±åº¦å¯åˆ†ç¦»å·ç§¯ | å‚æ•°é«˜æ•ˆè®¾è®¡ |

---

## ğŸ¯ åç»­ç« èŠ‚é¢„å‘Š

**ç¬¬ä¸ƒç« ï¼šå¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNN & LSTMï¼‰**
- åºåˆ—æ•°æ®å¤„ç†
- RNN çš„æ¢¯åº¦é—®é¢˜
- LSTM å’Œ GRU
- åŒå‘ RNN

**ç¬¬å…«ç« ï¼šAttention ä¸ Transformer**
- Self-Attention æœºåˆ¶
- Transformer æ¶æ„
- BERT å’Œ GPT

**ç¬¬ä¹ç« ï¼šè¿ç§»å­¦ä¹ ä¸å¾®è°ƒ**
- é¢„è®­ç»ƒæ¨¡å‹
- ç‰¹å¾æå–
- Fine-tuning ç­–ç•¥

---

è¿™æ˜¯ä¸€éƒ¨ç³»ç»Ÿã€è¯¦ç»†ä¸”æ˜“æ‡‚çš„æ·±åº¦å­¦ä¹ æ•™ç¨‹ï¼Œæ¶µç›–äº†ä»åŸºç¡€åˆ°è¿›é˜¶çš„å®Œæ•´å†…å®¹ã€‚æ¯ç« éƒ½åŒ…å«ï¼š

âœ… **ç†è®ºè®²è§£** - ç›´è§‚æ˜“æ‡‚ï¼Œé…å›¾è¯´æ˜
âœ… **æ•°å­¦æ¨å¯¼** - å…³é”®å…¬å¼è¯¦ç»†æ¨å¯¼
âœ… **ä»£ç å®ç°** - å®Œæ•´å¯è¿è¡Œçš„ç¤ºä¾‹
âœ… **å®æˆ˜é¡¹ç›®** - çœŸå®æ•°æ®é›†çš„ç«¯åˆ°ç«¯æµç¨‹
âœ… **ä½œä¸šç»ƒä¹ ** - å¸®åŠ©å·©å›ºçŸ¥è¯†
âœ… **å¯è§†åŒ–** - å¸®åŠ©ç†è§£å¤æ‚æ¦‚å¿µ


-----

