# ç¬¬äº”ç« ï¼šä¼˜åŒ–ç®—æ³•ä¸è®­ç»ƒæŠ€å·§ (Optimization & Training Tricks)

## ğŸ“Œ ç« èŠ‚ç›®æ ‡
- ç†è§£ä¸åŒæ¢¯åº¦ä¸‹é™å˜ç§çš„åŸç†
- æŒæ¡ç°ä»£ä¼˜åŒ–ç®—æ³•ï¼ˆMomentumã€Adamç­‰ï¼‰
- å­¦ä¹ æ­£åˆ™åŒ–æŠ€æœ¯é˜²æ­¢è¿‡æ‹Ÿåˆ
- äº†è§£æ‰¹å½’ä¸€åŒ–å’Œå­¦ä¹ ç‡è°ƒåº¦
- æŒæ¡æ·±åº¦å­¦ä¹ è®­ç»ƒçš„å®ç”¨æŠ€å·§

---

## 5.1 æ¢¯åº¦ä¸‹é™çš„å˜ç§

### ğŸ”„ ä¸‰ç§æ¢¯åº¦ä¸‹é™

#### **1. Batch Gradient Descent (æ‰¹é‡æ¢¯åº¦ä¸‹é™)**

**æ¯æ¬¡ä½¿ç”¨å…¨éƒ¨è®­ç»ƒæ•°æ®**

```python
for epoch in range(epochs):
    # ä½¿ç”¨æ‰€æœ‰æ•°æ®è®¡ç®—æ¢¯åº¦
    gradients = compute_gradients(X_train, y_train, parameters)
    parameters = update_parameters(parameters, gradients, lr)
```

**ä¼˜ç‚¹**ï¼š
- âœ… æ”¶æ•›ç¨³å®š
- âœ… å¯ä»¥åˆ©ç”¨çŸ©é˜µè¿ç®—åŠ é€Ÿ

**ç¼ºç‚¹**ï¼š
- âŒ æ•°æ®é‡å¤§æ—¶è®¡ç®—æ…¢
- âŒ å†…å­˜å ç”¨å¤§
- âŒ å®¹æ˜“é™·å…¥å±€éƒ¨æœ€ä¼˜

---

#### **2. Stochastic Gradient Descent (éšæœºæ¢¯åº¦ä¸‹é™ï¼ŒSGD)**

**æ¯æ¬¡ä½¿ç”¨ä¸€ä¸ªæ ·æœ¬**

```python
for epoch in range(epochs):
    # éšæœºæ‰“ä¹±æ•°æ®
    indices = np.random.permutation(len(X_train))

    for i in indices:
        # æ¯æ¬¡ç”¨ä¸€ä¸ªæ ·æœ¬
        gradients = compute_gradients(X_train[i:i+1], y_train[i:i+1], parameters)
        parameters = update_parameters(parameters, gradients, lr)
```

**ä¼˜ç‚¹**ï¼š
- âœ… æ›´æ–°é¢‘ç¹ï¼Œæ”¶æ•›å¿«
- âœ… å¯ä»¥é€ƒç¦»å±€éƒ¨æœ€ä¼˜
- âœ… å¯ä»¥åœ¨çº¿å­¦ä¹ 

**ç¼ºç‚¹**ï¼š
- âŒ æ³¢åŠ¨å¤§ï¼Œä¸ç¨³å®š
- âŒ éš¾ä»¥å¹¶è¡ŒåŒ–
- âŒ å¯èƒ½ä¸æ”¶æ•›

**Loss æ›²çº¿å¯¹æ¯”**ï¼š

```
Batch GD:        SGD:
Loss             Loss
 â†“                â†“
 |\                |  *
 | \               | * *
 |  \              |*   *
 |   \___          | *   *
 |_______â†’        |___*___â†’
   Epoch            Epoch
 (å¹³æ»‘ä¸‹é™)        (éœ‡è¡ä¸‹é™)
```

---

#### **3. Mini-batch Gradient Descent (å°æ‰¹é‡æ¢¯åº¦ä¸‹é™)** â­

**æ¯æ¬¡ä½¿ç”¨ä¸€å°æ‰¹æ•°æ®ï¼ˆé€šå¸¸ 32-256ï¼‰**

```python
batch_size = 64

for epoch in range(epochs):
    # éšæœºæ‰“ä¹±
    indices = np.random.permutation(len(X_train))

    # åˆ†æ‰¹å¤„ç†
    for i in range(0, len(X_train), batch_size):
        batch_indices = indices[i:i+batch_size]
        X_batch = X_train[batch_indices]
        y_batch = y_train[batch_indices]

        # è®¡ç®—æ¢¯åº¦å’Œæ›´æ–°
        gradients = compute_gradients(X_batch, y_batch, parameters)
        parameters = update_parameters(parameters, gradients, lr)
```

**ä¼˜ç‚¹**ï¼š
- âœ… å¹³è¡¡äº†é€Ÿåº¦å’Œç¨³å®šæ€§
- âœ… å¯ä»¥åˆ©ç”¨ GPU å¹¶è¡Œ
- âœ… æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›

**ç¼ºç‚¹**ï¼š
- éœ€è¦è°ƒæ•´ batch_size

**Batch Size é€‰æ‹©**ï¼š

```
å° batch (16-32):
  + æ³›åŒ–èƒ½åŠ›å¼º
  + é€‚åˆå°æ•°æ®é›†
  - è®­ç»ƒä¸ç¨³å®š
  - é€Ÿåº¦æ…¢

å¤§ batch (256-512):
  + è®­ç»ƒç¨³å®š
  + å……åˆ†åˆ©ç”¨ GPU
  + é€Ÿåº¦å¿«
  - å¯èƒ½è¿‡æ‹Ÿåˆ
  - æ³›åŒ–èƒ½åŠ›è¾ƒå¼±

å¸¸ç”¨: 32, 64, 128
```

---

### ğŸ’» å®ç° Mini-batch

```python
def create_mini_batches(X, y, batch_size):
    """
    åˆ›å»º mini-batches

    è¿”å›ï¼š
        mini_batches: åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ (X_batch, y_batch)
    """
    m = X.shape[0]
    mini_batches = []

    # éšæœºæ‰“ä¹±
    permutation = np.random.permutation(m)
    shuffled_X = X[permutation]
    shuffled_y = y[permutation]

    # åˆ†æ‰¹
    num_complete_batches = m // batch_size

    for k in range(num_complete_batches):
        X_batch = shuffled_X[k*batch_size:(k+1)*batch_size]
        y_batch = shuffled_y[k*batch_size:(k+1)*batch_size]
        mini_batches.append((X_batch, y_batch))

    # å¤„ç†å‰©ä½™çš„æ•°æ®
    if m % batch_size != 0:
        X_batch = shuffled_X[num_complete_batches*batch_size:]
        y_batch = shuffled_y[num_complete_batches*batch_size:]
        mini_batches.append((X_batch, y_batch))

    return mini_batches

# ä½¿ç”¨
def train_with_mini_batch(X, y, parameters, epochs, batch_size, learning_rate):
    """ä½¿ç”¨ mini-batch è®­ç»ƒ"""
    losses = []

    for epoch in range(epochs):
        epoch_loss = 0
        mini_batches = create_mini_batches(X, y, batch_size)

        for X_batch, y_batch in mini_batches:
            # å‰å‘ä¼ æ’­
            y_pred, cache = forward_propagation(X_batch, parameters)

            # è®¡ç®—æŸå¤±
            loss = compute_loss(y_pred, y_batch)
            epoch_loss += loss

            # åå‘ä¼ æ’­
            gradients = backward_propagation(X_batch, y_batch, parameters, cache)

            # æ›´æ–°å‚æ•°
            parameters = update_parameters(parameters, gradients, learning_rate)

        # å¹³å‡æŸå¤±
        avg_loss = epoch_loss / len(mini_batches)
        losses.append(avg_loss)

        if epoch % 100 == 0:
            print(f"Epoch {epoch}: Loss = {avg_loss:.6f}")

    return parameters, losses
```

---

## 5.2 åŠ¨é‡æ³• (Momentum)

### ğŸ¯ é—®é¢˜ï¼šæ¢¯åº¦ä¸‹é™çš„éœ‡è¡

```
Loss
 â†‘
 |     *
 |    * *
 |   *   *     â† å‚ç›´æ–¹å‘éœ‡è¡
 |  *     *
 | *       *
 |__________â†’
   å‚æ•°ç©ºé—´

ç†æƒ³ï¼šæ¨ªå‘å¿«é€Ÿå‰è¿›ï¼Œçºµå‘å‡å°‘éœ‡è¡
```

### ğŸ’¡ åŠ¨é‡æ³•åŸç†

**ç‰©ç†ç±»æ¯”**ï¼šæ»šä¸‹å±±çš„å°çƒ

```
å°çƒä¸ä¼šç«‹åˆ»æ”¹å˜æ–¹å‘
è€Œæ˜¯ç´¯ç§¯åŠ¨é‡ï¼Œå¹³æ»‘åœ°æ»šåŠ¨
```

**æ•°å­¦å…¬å¼**ï¼š

```
v_t = Î²Â·v_{t-1} + (1-Î²)Â·âˆ‡L_t

Î¸_t = Î¸_{t-1} - Î±Â·v_t

å…¶ä¸­ï¼š
  v_t: é€Ÿåº¦ï¼ˆåŠ¨é‡ï¼‰
  Î²: åŠ¨é‡ç³»æ•°ï¼ˆé€šå¸¸ 0.9ï¼‰
  âˆ‡L_t: å½“å‰æ¢¯åº¦
  Î±: å­¦ä¹ ç‡
```

**æŒ‡æ•°åŠ æƒç§»åŠ¨å¹³å‡**ï¼š

```
v_t = Î²Â·v_{t-1} + (1-Î²)Â·g_t
    = (1-Î²)Â·g_t + Î²Â·(1-Î²)Â·g_{t-1} + Î²Â²Â·(1-Î²)Â·g_{t-2} + ...

æƒé‡ï¼š
  g_t:   (1-Î²) = 0.1
  g_{t-1}: Î²(1-Î²) = 0.09
  g_{t-2}: Î²Â²(1-Î²) = 0.081
  ...

è¶Šè¿‘çš„æ¢¯åº¦æƒé‡è¶Šå¤§
```

### ğŸ“Š æ•ˆæœå¯¹æ¯”

```
ä¸ä½¿ç”¨åŠ¨é‡:        ä½¿ç”¨åŠ¨é‡:
    *                 â”€â”€â†’
   * *               â”€â”€â†’
  *   *             â”€â”€â†’
 *     *           â”€â”€â†’
*éœ‡è¡  *          å¹³æ»‘
```

### ğŸ’» å®ç°

```python
def initialize_momentum(parameters):
    """
    åˆå§‹åŒ–åŠ¨é‡

    è¿”å›ï¼š
        v: å­—å…¸ï¼Œä¸ parameters ç»“æ„ç›¸åŒï¼Œåˆå§‹åŒ–ä¸º 0
    """
    v = {}
    L = len(parameters) // 2  # W å’Œ b çš„å¯¹æ•°

    for l in range(1, L + 1):
        v[f'dW{l}'] = np.zeros_like(parameters[f'W{l}'])
        v[f'db{l}'] = np.zeros_like(parameters[f'b{l}'])

    return v

def update_parameters_with_momentum(parameters, gradients, v, learning_rate, beta=0.9):
    """
    ä½¿ç”¨åŠ¨é‡æ›´æ–°å‚æ•°

    å‚æ•°ï¼š
        parameters: å½“å‰å‚æ•°
        gradients: æ¢¯åº¦
        v: åŠ¨é‡
        learning_rate: å­¦ä¹ ç‡
        beta: åŠ¨é‡ç³»æ•°
    """
    L = len(parameters) // 2

    for l in range(1, L + 1):
        # æ›´æ–°åŠ¨é‡
        v[f'dW{l}'] = beta * v[f'dW{l}'] + (1 - beta) * gradients[f'dW{l}']
        v[f'db{l}'] = beta * v[f'db{l}'] + (1 - beta) * gradients[f'db{l}']

        # æ›´æ–°å‚æ•°
        parameters[f'W{l}'] -= learning_rate * v[f'dW{l}']
        parameters[f'b{l}'] -= learning_rate * v[f'db{l}']

    return parameters, v

# ä½¿ç”¨ç¤ºä¾‹
v = initialize_momentum(parameters)

for epoch in range(epochs):
    for X_batch, y_batch in mini_batches:
        # å‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­
        y_pred, cache = forward_propagation(X_batch, parameters)
        gradients = backward_propagation(X_batch, y_batch, parameters, cache)

        # ä½¿ç”¨åŠ¨é‡æ›´æ–°
        parameters, v = update_parameters_with_momentum(
            parameters, gradients, v, learning_rate, beta=0.9
        )
```

---

## 5.3 RMSprop (Root Mean Square Propagation)

### ğŸ¯ é—®é¢˜ï¼šä¸åŒå‚æ•°éœ€è¦ä¸åŒå­¦ä¹ ç‡

```
å‚æ•° wâ‚: æ¢¯åº¦èŒƒå›´ [-100, 100]  â† éœ€è¦å°å­¦ä¹ ç‡
å‚æ•° wâ‚‚: æ¢¯åº¦èŒƒå›´ [-0.01, 0.01] â† éœ€è¦å¤§å­¦ä¹ ç‡

å›ºå®šå­¦ä¹ ç‡æ— æ³•åŒæ—¶æ»¡è¶³
```

### ğŸ’¡ RMSprop åŸç†

**è‡ªé€‚åº”è°ƒæ•´æ¯ä¸ªå‚æ•°çš„å­¦ä¹ ç‡**

```
s_t = Î²Â·s_{t-1} + (1-Î²)Â·(âˆ‡L_t)Â²

Î¸_t = Î¸_{t-1} - Î±Â·âˆ‡L_t / âˆš(s_t + Îµ)

å…¶ä¸­ï¼š
  s_t: æ¢¯åº¦å¹³æ–¹çš„æŒ‡æ•°åŠ æƒç§»åŠ¨å¹³å‡
  Î²: è¡°å‡ç‡ï¼ˆé€šå¸¸ 0.999ï¼‰
  Îµ: é˜²æ­¢é™¤é›¶ï¼ˆé€šå¸¸ 10â»â¸ï¼‰
```

**ç›´è§‰**ï¼š
- æ¢¯åº¦å¤§ â†’ s å¤§ â†’ æ­¥é•¿å°ï¼ˆé™¤ä»¥å¤§æ•°ï¼‰
- æ¢¯åº¦å° â†’ s å° â†’ æ­¥é•¿å¤§ï¼ˆé™¤ä»¥å°æ•°ï¼‰

### ğŸ’» å®ç°

```python
def initialize_rmsprop(parameters):
    """åˆå§‹åŒ– RMSprop"""
    s = {}
    L = len(parameters) // 2

    for l in range(1, L + 1):
        s[f'dW{l}'] = np.zeros_like(parameters[f'W{l}'])
        s[f'db{l}'] = np.zeros_like(parameters[f'b{l}'])

    return s

def update_parameters_with_rmsprop(parameters, gradients, s, learning_rate,
                                   beta=0.999, epsilon=1e-8):
    """ä½¿ç”¨ RMSprop æ›´æ–°å‚æ•°"""
    L = len(parameters) // 2

    for l in range(1, L + 1):
        # æ›´æ–°å¹³æ–¹æ¢¯åº¦çš„ç§»åŠ¨å¹³å‡
        s[f'dW{l}'] = beta * s[f'dW{l}'] + (1 - beta) * gradients[f'dW{l}']**2
        s[f'db{l}'] = beta * s[f'db{l}'] + (1 - beta) * gradients[f'db{l}']**2

        # æ›´æ–°å‚æ•°
        parameters[f'W{l}'] -= learning_rate * gradients[f'dW{l}'] / (np.sqrt(s[f'dW{l}']) + epsilon)
        parameters[f'b{l}'] -= learning_rate * gradients[f'db{l}'] / (np.sqrt(s[f'db{l}']) + epsilon)

    return parameters, s
```

---

## 5.4 Adam (Adaptive Moment Estimation) â­

### ğŸ¯ Adam = Momentum + RMSprop

**ç»“åˆä¸¤è€…ä¼˜ç‚¹**ï¼š
- Momentumï¼šå¹³æ»‘æ¢¯åº¦æ–¹å‘
- RMSpropï¼šè‡ªé€‚åº”å­¦ä¹ ç‡

### ğŸ“ ç®—æ³•

```
åˆå§‹åŒ–ï¼š
  vâ‚€ = 0  (ä¸€é˜¶çŸ©ä¼°è®¡ï¼ŒåŠ¨é‡)
  sâ‚€ = 0  (äºŒé˜¶çŸ©ä¼°è®¡ï¼ŒRMSprop)

æ¯æ¬¡è¿­ä»£ï¼š
  1. è®¡ç®—æ¢¯åº¦ g_t = âˆ‡L_t

  2. æ›´æ–°åŠ¨é‡ï¼š
     v_t = Î²â‚Â·v_{t-1} + (1-Î²â‚)Â·g_t

  3. æ›´æ–°å¹³æ–¹æ¢¯åº¦ï¼š
     s_t = Î²â‚‚Â·s_{t-1} + (1-Î²â‚‚)Â·g_tÂ²

  4. åå·®ä¿®æ­£ï¼š
     vÌ‚_t = v_t / (1 - Î²â‚áµ—)
     Å_t = s_t / (1 - Î²â‚‚áµ—)

  5. æ›´æ–°å‚æ•°ï¼š
     Î¸_t = Î¸_{t-1} - Î±Â·vÌ‚_t / (âˆšÅ_t + Îµ)

é»˜è®¤è¶…å‚æ•°ï¼š
  Î± = 0.001
  Î²â‚ = 0.9
  Î²â‚‚ = 0.999
  Îµ = 10â»â¸
```

### ğŸ¤” ä¸ºä»€ä¹ˆéœ€è¦åå·®ä¿®æ­£ï¼Ÿ

```
åˆå§‹æ—¶ vâ‚€ = 0, sâ‚€ = 0

ç¬¬ä¸€æ­¥ï¼š
  vâ‚ = 0.9Â·0 + 0.1Â·gâ‚ = 0.1Â·gâ‚

é—®é¢˜ï¼švâ‚ è¿œå°äºçœŸå®æœŸæœ›ï¼
  (å› ä¸ºåˆå§‹åŒ–ä¸º 0ï¼Œæœ‰åå·®)

ä¿®æ­£ï¼š
  vÌ‚â‚ = vâ‚ / (1 - 0.9Â¹) = 0.1Â·gâ‚ / 0.1 = gâ‚  âœ“

éšç€ t å¢å¤§ï¼š
  (1 - Î²â‚áµ—) â†’ 1
  ä¿®æ­£æ•ˆæœé€æ¸æ¶ˆå¤±
```

### ğŸ’» å®Œæ•´å®ç°

```python
def initialize_adam(parameters):
    """
    åˆå§‹åŒ– Adam ä¼˜åŒ–å™¨

    è¿”å›ï¼š
        v: ä¸€é˜¶çŸ©ä¼°è®¡ï¼ˆåŠ¨é‡ï¼‰
        s: äºŒé˜¶çŸ©ä¼°è®¡ï¼ˆRMSpropï¼‰
    """
    v = {}
    s = {}
    L = len(parameters) // 2

    for l in range(1, L + 1):
        v[f'dW{l}'] = np.zeros_like(parameters[f'W{l}'])
        v[f'db{l}'] = np.zeros_like(parameters[f'b{l}'])
        s[f'dW{l}'] = np.zeros_like(parameters[f'W{l}'])
        s[f'db{l}'] = np.zeros_like(parameters[f'b{l}'])

    return v, s

def update_parameters_with_adam(parameters, gradients, v, s, t,
                                learning_rate=0.001,
                                beta1=0.9, beta2=0.999, epsilon=1e-8):
    """
    ä½¿ç”¨ Adam æ›´æ–°å‚æ•°

    å‚æ•°ï¼š
        t: å½“å‰è¿­ä»£æ¬¡æ•°ï¼ˆä» 1 å¼€å§‹ï¼‰
    """
    L = len(parameters) // 2
    v_corrected = {}
    s_corrected = {}

    for l in range(1, L + 1):
        # æ›´æ–°åŠ¨é‡
        v[f'dW{l}'] = beta1 * v[f'dW{l}'] + (1 - beta1) * gradients[f'dW{l}']
        v[f'db{l}'] = beta1 * v[f'db{l}'] + (1 - beta1) * gradients[f'db{l}']

        # æ›´æ–°å¹³æ–¹æ¢¯åº¦
        s[f'dW{l}'] = beta2 * s[f'dW{l}'] + (1 - beta2) * (gradients[f'dW{l}']**2)
        s[f'db{l}'] = beta2 * s[f'db{l}'] + (1 - beta2) * (gradients[f'db{l}']**2)

        # åå·®ä¿®æ­£
        v_corrected[f'dW{l}'] = v[f'dW{l}'] / (1 - beta1**t)
        v_corrected[f'db{l}'] = v[f'db{l}'] / (1 - beta1**t)
        s_corrected[f'dW{l}'] = s[f'dW{l}'] / (1 - beta2**t)
        s_corrected[f'db{l}'] = s[f'db{l}'] / (1 - beta2**t)

        # æ›´æ–°å‚æ•°
        parameters[f'W{l}'] -= learning_rate * v_corrected[f'dW{l}'] / (np.sqrt(s_corrected[f'dW{l}']) + epsilon)
        parameters[f'b{l}'] -= learning_rate * v_corrected[f'db{l}'] / (np.sqrt(s_corrected[f'db{l}']) + epsilon)

    return parameters, v, s

# ä½¿ç”¨ç¤ºä¾‹
def train_with_adam(X_train, y_train, layer_dims, epochs=1000, batch_size=64):
    """ä½¿ç”¨ Adam è®­ç»ƒç½‘ç»œ"""
    # åˆå§‹åŒ–å‚æ•°
    parameters = initialize_parameters(layer_dims)
    v, s = initialize_adam(parameters)

    losses = []
    t = 0  # å…¨å±€è¿­ä»£è®¡æ•°å™¨

    for epoch in range(epochs):
        mini_batches = create_mini_batches(X_train, y_train, batch_size)
        epoch_loss = 0

        for X_batch, y_batch in mini_batches:
            t += 1  # æ¯ä¸ª mini-batch å¢åŠ è®¡æ•°

            # å‰å‘ä¼ æ’­
            AL, caches = forward_propagation_deep(X_batch, parameters)

            # è®¡ç®—æŸå¤±
            loss = compute_loss(AL, y_batch)
            epoch_loss += loss

            # åå‘ä¼ æ’­
            gradients = backward_propagation_deep(AL, y_batch, caches)

            # Adam æ›´æ–°
            parameters, v, s = update_parameters_with_adam(
                parameters, gradients, v, s, t
            )

        avg_loss = epoch_loss / len(mini_batches)
        losses.append(avg_loss)

        if epoch % 100 == 0:
            print(f"Epoch {epoch}: Loss = {avg_loss:.6f}")

    return parameters, losses
```

---

## 5.5 ä¼˜åŒ–å™¨å¯¹æ¯”

### ğŸ“Š å¯è§†åŒ–å¯¹æ¯”

```python
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation

# å®šä¹‰ä¸€ä¸ªéå‡¸å‡½æ•°ï¼ˆç±»ä¼¼ Rosenbrockï¼‰
def f(x, y):
    return (1 - x)**2 + 100 * (y - x**2)**2

def grad_f(x, y):
    """è®¡ç®—æ¢¯åº¦"""
    dx = -2 * (1 - x) - 400 * x * (y - x**2)
    dy = 200 * (y - x**2)
    return np.array([dx, dy])

# åˆ›å»ºç­‰é«˜çº¿å›¾
x = np.linspace(-2, 2, 100)
y = np.linspace(-1, 3, 100)
X, Y = np.meshgrid(x, y)
Z = f(X, Y)

# æµ‹è¯•ä¸åŒä¼˜åŒ–å™¨
def sgd(pos, grad, lr=0.001):
    return pos - lr * grad

def momentum(pos, grad, v, beta=0.9, lr=0.001):
    v = beta * v + (1 - beta) * grad
    return pos - lr * v, v

def rmsprop(pos, grad, s, beta=0.999, lr=0.001, eps=1e-8):
    s = beta * s + (1 - beta) * grad**2
    return pos - lr * grad / (np.sqrt(s) + eps), s

def adam(pos, grad, v, s, t, beta1=0.9, beta2=0.999, lr=0.001, eps=1e-8):
    v = beta1 * v + (1 - beta1) * grad
    s = beta2 * s + (1 - beta2) * grad**2
    v_hat = v / (1 - beta1**t)
    s_hat = s / (1 - beta2**t)
    return pos - lr * v_hat / (np.sqrt(s_hat) + eps), v, s

# è¿è¡Œä¼˜åŒ–
def run_optimizer(optimizer_name, steps=200):
    pos = np.array([-1.5, 2.5])
    trajectory = [pos.copy()]

    if optimizer_name == 'SGD':
        for _ in range(steps):
            grad = grad_f(pos[0], pos[1])
            pos = sgd(pos, grad)
            trajectory.append(pos.copy())

    elif optimizer_name == 'Momentum':
        v = np.zeros(2)
        for _ in range(steps):
            grad = grad_f(pos[0], pos[1])
            pos, v = momentum(pos, grad, v)
            trajectory.append(pos.copy())

    elif optimizer_name == 'RMSprop':
        s = np.zeros(2)
        for _ in range(steps):
            grad = grad_f(pos[0], pos[1])
            pos, s = rmsprop(pos, grad, s)
            trajectory.append(pos.copy())

    elif optimizer_name == 'Adam':
        v = np.zeros(2)
        s = np.zeros(2)
        for t in range(1, steps + 1):
            grad = grad_f(pos[0], pos[1])
            pos, v, s = adam(pos, grad, v, s, t)
            trajectory.append(pos.copy())

    return np.array(trajectory)

# ç»˜åˆ¶å¯¹æ¯”å›¾
fig, axes = plt.subplots(2, 2, figsize=(14, 12))
optimizers = ['SGD', 'Momentum', 'RMSprop', 'Adam']

for ax, opt_name in zip(axes.flat, optimizers):
    ax.contour(X, Y, Z, levels=np.logspace(-1, 3, 20), cmap='viridis', alpha=0.3)

    trajectory = run_optimizer(opt_name, steps=200)
    ax.plot(trajectory[:, 0], trajectory[:, 1], 'r-', linewidth=2, alpha=0.7)
    ax.plot(trajectory[0, 0], trajectory[0, 1], 'go', markersize=10, label='èµ·ç‚¹')
    ax.plot(1, 1, 'r*', markersize=15, label='æœ€ä¼˜ç‚¹')

    ax.set_title(f'{opt_name} Optimizer', fontsize=14, fontweight='bold')
    ax.set_xlabel('x')
    ax.set_ylabel('y')
    ax.legend()
    ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

### ğŸ“ˆ æ€§èƒ½å¯¹æ¯”è¡¨

| ä¼˜åŒ–å™¨ | æ”¶æ•›é€Ÿåº¦ | ç¨³å®šæ€§ | å†…å­˜å¼€é”€ | è¶…å‚æ•°æ•æ„Ÿåº¦ | æ¨èåº¦ |
|--------|---------|--------|---------|------------|--------|
| SGD | â˜…â˜…â˜†â˜†â˜† | â˜…â˜…â˜†â˜†â˜† | â˜…â˜…â˜…â˜…â˜… | â˜…â˜…â˜…â˜…â˜† | â˜…â˜…â˜†â˜†â˜† |
| SGD+Momentum | â˜…â˜…â˜…â˜…â˜† | â˜…â˜…â˜…â˜…â˜† | â˜…â˜…â˜…â˜…â˜† | â˜…â˜…â˜…â˜†â˜† | â˜…â˜…â˜…â˜…â˜† |
| RMSprop | â˜…â˜…â˜…â˜…â˜† | â˜…â˜…â˜…â˜…â˜† | â˜…â˜…â˜…â˜…â˜† | â˜…â˜…â˜…â˜†â˜† | â˜…â˜…â˜…â˜…â˜† |
| Adam | â˜…â˜…â˜…â˜…â˜… | â˜…â˜…â˜…â˜…â˜… | â˜…â˜…â˜…â˜†â˜† | â˜…â˜…â˜…â˜…â˜… | â˜…â˜…â˜…â˜…â˜… |
| AdaGrad | â˜…â˜…â˜…â˜†â˜† | â˜…â˜…â˜…â˜†â˜† | â˜…â˜…â˜…â˜…â˜† | â˜…â˜…â˜…â˜…â˜† | â˜…â˜…â˜†â˜†â˜† |
| AdamW | â˜…â˜…â˜…â˜…â˜… | â˜…â˜…â˜…â˜…â˜… | â˜…â˜…â˜…â˜†â˜† | â˜…â˜…â˜…â˜…â˜… | â˜…â˜…â˜…â˜…â˜… |

### ğŸ’¡ é€‰æ‹©æŒ‡å—

```
é»˜è®¤é€‰æ‹©ï¼šAdam â­
  - å‡ ä¹é€‚ç”¨äºæ‰€æœ‰åœºæ™¯
  - ä¸éœ€è¦å¤ªå¤šè°ƒå‚
  - æ”¶æ•›å¿«ä¸”ç¨³å®š

éœ€è¦æœ€ä½³æ€§èƒ½ï¼šSGD + Momentum
  - è®­ç»ƒæ—¶é—´è¶³å¤Ÿæ—¶
  - é…åˆ Learning Rate Schedule
  - é€šå¸¸æ³›åŒ–èƒ½åŠ›æ›´å¥½

è®¡ç®—æœºè§†è§‰ï¼šSGD + Momentum
  - ResNet, VGG ç­‰ç»å…¸æ¨¡å‹
  - éœ€è¦ä»”ç»†è°ƒæ•´å­¦ä¹ ç‡

NLP / Transformerï¼šAdam / AdamW
  - BERT, GPT æ ‡é…
  - AdamW åŠ å…¥æƒé‡è¡°å‡

å†…å­˜å—é™ï¼šSGD
  - ä¸éœ€è¦é¢å¤–å­˜å‚¨åŠ¨é‡
```

---

## 5.6 å­¦ä¹ ç‡è°ƒåº¦ (Learning Rate Scheduling)

### ğŸ¯ ä¸ºä»€ä¹ˆéœ€è¦è°ƒæ•´å­¦ä¹ ç‡ï¼Ÿ

```
è®­ç»ƒåˆæœŸï¼š
  - ç¦»æœ€ä¼˜ç‚¹è¿œ
  - å¯ä»¥ç”¨å¤§å­¦ä¹ ç‡å¿«é€Ÿæ¥è¿‘

è®­ç»ƒåæœŸï¼š
  - æ¥è¿‘æœ€ä¼˜ç‚¹
  - éœ€è¦å°å­¦ä¹ ç‡ç²¾ç»†è°ƒæ•´

å›ºå®šå­¦ä¹ ç‡ï¼š
  å¤ªå¤§ â†’ éœ‡è¡ï¼Œä¸æ”¶æ•›
  å¤ªå° â†’ è®­ç»ƒæ…¢
```

### ğŸ“Š å¸¸è§è°ƒåº¦ç­–ç•¥

#### **1. Step Decay (é˜¶æ¢¯è¡°å‡)**

```
æ¯éš”å›ºå®š epochï¼Œå­¦ä¹ ç‡ä¹˜ä»¥è¡°å‡å› å­

lr_t = lr_0 Â· Î³^âŒŠepoch/step_sizeâŒ‹

ä¾‹ï¼š
  lr_0 = 0.1
  Î³ = 0.1
  step_size = 30

  epoch 0-29:  lr = 0.1
  epoch 30-59: lr = 0.01
  epoch 60-89: lr = 0.001
```

```python
def step_decay_schedule(epoch, lr, drop=0.5, epochs_drop=10):
    """é˜¶æ¢¯è¡°å‡"""
    return lr * (drop ** (epoch // epochs_drop))
```

#### **2. Exponential Decay (æŒ‡æ•°è¡°å‡)**

```
lr_t = lr_0 Â· e^(-Î»t)

æˆ–

lr_t = lr_0 Â· Î³^t
```

```python
def exponential_decay(epoch, lr_0, decay_rate=0.96):
    """æŒ‡æ•°è¡°å‡"""
    return lr_0 * np.exp(-decay_rate * epoch)
```

#### **3. Cosine Annealing (ä½™å¼¦é€€ç«)**

```
lr_t = lr_min + (lr_max - lr_min) Â· (1 + cos(Ï€t/T)) / 2

å¹³æ»‘ä¸‹é™ï¼Œå¸¸ç”¨äºè®­ç»ƒåæœŸ fine-tune
```

```python
def cosine_annealing(epoch, lr_max, lr_min, T_max):
    """ä½™å¼¦é€€ç«"""
    return lr_min + (lr_max - lr_min) * (1 + np.cos(np.pi * epoch / T_max)) / 2
```

#### **4. Warm-up + Cosine (ç°ä»£ Transformer æ ‡é…)**

```
Warm-up é˜¶æ®µï¼ˆå‰å‡ ä¸ª epochï¼‰ï¼š
  çº¿æ€§å¢åŠ å­¦ä¹ ç‡ 0 â†’ lr_max

ä¸»è®­ç»ƒé˜¶æ®µï¼š
  ä½™å¼¦é€€ç« lr_max â†’ lr_min
```

```python
def warmup_cosine_schedule(epoch, lr_max, warmup_epochs, total_epochs, lr_min=0):
    """Warm-up + Cosine"""
    if epoch < warmup_epochs:
        # Warm-up é˜¶æ®µ
        return lr_max * (epoch + 1) / warmup_epochs
    else:
        # Cosine é˜¶æ®µ
        progress = (epoch - warmup_epochs) / (total_epochs - warmup_epochs)
        return lr_min + (lr_max - lr_min) * 0.5 * (1 + np.cos(np.pi * progress))
```

#### **5. Reduce on Plateau (åŸºäºéªŒè¯é›†)**

```
ç›‘æ§éªŒè¯é›† Lossï¼š

å¦‚æœ N ä¸ª epoch æ²¡æœ‰æ”¹è¿›ï¼š
  lr = lr * factor

å¾ˆå®ç”¨ï¼
```

```python
class ReduceLROnPlateau:
    def __init__(self, lr_init, factor=0.1, patience=10, min_lr=1e-7):
        self.lr = lr_init
        self.factor = factor
        self.patience = patience
        self.min_lr = min_lr
        self.best_loss = float('inf')
        self.counter = 0

    def step(self, val_loss):
        """
        æ ¹æ®éªŒè¯é›† Loss è°ƒæ•´å­¦ä¹ ç‡
        """
        if val_loss < self.best_loss:
            self.best_loss = val_loss
            self.counter = 0
        else:
            self.counter += 1
            if self.counter >= self.patience:
                self.lr *= self.factor
                self.lr = max(self.lr, self.min_lr)
                self.counter = 0
                print(f"å­¦ä¹ ç‡å·²é™ä½åˆ° {self.lr:.6f}")

        return self.lr
```

### ğŸ“ˆ å¯è§†åŒ–å¯¹æ¯”

```python
import matplotlib.pyplot as plt
import numpy as np

epochs = 200
lr_0 = 0.1

# ç”Ÿæˆä¸åŒè°ƒåº¦çš„å­¦ä¹ ç‡
epochs_arr = np.arange(epochs)

lr_constant = np.ones(epochs) * lr_0
lr_step = np.array([step_decay_schedule(e, lr_0, drop=0.5, epochs_drop=50)
                    for e in epochs_arr])
lr_exp = np.array([exponential_decay(e, lr_0, decay_rate=0.02)
                   for e in epochs_arr])
lr_cosine = np.array([cosine_annealing(e, lr_0, 1e-5, 200)
                      for e in epochs_arr])
lr_warmup_cosine = np.array([warmup_cosine_schedule(e, lr_0, 10, 200, 1e-5)
                             for e in epochs_arr])

# ç»˜å›¾
plt.figure(figsize=(12, 6))
plt.semilogy(epochs_arr, lr_constant, label='Constant', linewidth=2)
plt.semilogy(epochs_arr, lr_step, label='Step Decay', linewidth=2)
plt.semilogy(epochs_arr, lr_exp, label='Exponential Decay', linewidth=2)
plt.semilogy(epochs_arr, lr_cosine, label='Cosine Annealing', linewidth=2)
plt.semilogy(epochs_arr, lr_warmup_cosine, label='Warm-up + Cosine', linewidth=2)

plt.xlabel('Epoch')
plt.ylabel('Learning Rate')
plt.title('Learning Rate Schedules Comparison')
plt.legend(fontsize=11)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
```

---

## 5.7 æ­£åˆ™åŒ–æŠ€æœ¯ (Regularization)

### ğŸ¯ é—®é¢˜ï¼šè¿‡æ‹Ÿåˆ

```
è®­ç»ƒé›† Loss: 0.01  âœ“
æµ‹è¯•é›† Loss: 0.5   âœ—

æ¨¡å‹è¿‡åº¦å­¦ä¹ äº†è®­ç»ƒæ•°æ®çš„å™ªå£°
```

### ğŸ”¹ L1 å’Œ L2 æ­£åˆ™åŒ–

**L2 æ­£åˆ™åŒ–ï¼ˆæƒé‡è¡°å‡ï¼‰**ï¼š

```
L_total = L_origin + Î»Â·(1/2)Â·Î£wÂ²

æ•ˆæœï¼šå€¾å‘äºè®©æƒé‡å˜å°
```

```python
# PyTorch ä¸­ç›´æ¥åœ¨ä¼˜åŒ–å™¨ä¸­æŒ‡å®š
optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.01)

# æˆ–æ‰‹åŠ¨æ·»åŠ åˆ° Loss
L_total = L_origin + weight_decay * sum(p**2 for p in model.parameters())
```

**L1 æ­£åˆ™åŒ–**ï¼š

```
L_total = L_origin + Î»Â·Î£|w|

æ•ˆæœï¼šè®©ä¸€äº›æƒé‡å˜æˆ 0ï¼ˆç¨€ç–æ€§ï¼‰
```

---

### ğŸ”¹ Dropout â­

**æ ¸å¿ƒæ€æƒ³**ï¼šè®­ç»ƒæ—¶éšæœº"å…³é—­"ä¸€äº›ç¥ç»å…ƒ

```
è®­ç»ƒæ—¶ (dropout = 0.5):
  xâ‚ â”€â”€ wâ‚ â”€â”€â”
  xâ‚‚ â”€â”€ âœ—   â”œâ”€ z  (éšæœºå…³é—­éƒ¨åˆ†è¿æ¥)
  xâ‚ƒ â”€â”€ wâ‚ƒ â”€â”€â”˜

é¢„æµ‹æ—¶ï¼š
  ä½¿ç”¨æ‰€æœ‰è¿æ¥ï¼Œä½†æƒé‡ä¹˜ä»¥ (1-p)
  æˆ–ä½¿ç”¨ inverted dropoutï¼Œè®­ç»ƒæ—¶å°±è°ƒæ•´
```

**Inverted Dropout** (æ¨è):

```
è®­ç»ƒæ—¶ï¼š
  a_dropped = a / (1 - p)  with probability (1-p)
              0             with probability p

é¢„æµ‹æ—¶ï¼š
  ä½¿ç”¨ a ç›´æ¥ï¼ˆä¸éœ€è¦è°ƒæ•´ï¼‰
```

**æ•ˆæœ**ï¼š
- é˜²æ­¢å…±é€‚åº”ï¼ˆco-adaptationï¼‰
- å‡å°‘è¿‡æ‹Ÿåˆ
- é›†æˆæ•ˆæœ

```python
# PyTorch
class NeuralNetworkWithDropout(nn.Module):
    def __init__(self, input_size, hidden_size, dropout_rate=0.5):
        super().__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.dropout1 = nn.Dropout(dropout_rate)
        self.fc2 = nn.Linear(hidden_size, hidden_size)
        self.dropout2 = nn.Dropout(dropout_rate)
        self.fc3 = nn.Linear(hidden_size, 10)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = self.dropout1(x)  # è®­ç»ƒæ—¶éšæœºå…³é—­ï¼Œé¢„æµ‹æ—¶è‡ªåŠ¨è°ƒæ•´

        x = torch.relu(self.fc2(x))
        x = self.dropout2(x)

        x = self.fc3(x)
        return x

# ä½¿ç”¨
model = NeuralNetworkWithDropout(784, 128, dropout_rate=0.5)
model.train()   # è®­ç»ƒæ¨¡å¼ï¼ŒDropout æœ‰æ•ˆ
model.eval()    # è¯„ä¼°æ¨¡å¼ï¼ŒDropout æ— æ•ˆ
```

### ğŸ“Š Dropout æ•ˆæœ

```
ä¸ä½¿ç”¨ Dropout:          ä½¿ç”¨ Dropout:
è®­ç»ƒLoss â†“              è®­ç»ƒLoss â†˜
æµ‹è¯•Loss â†—              æµ‹è¯•Loss â†˜

è¿‡æ‹Ÿåˆ                  æ³›åŒ–æ›´å¥½
```

---

### ğŸ”¹ Early Stopping

**æ€æƒ³**ï¼šç›‘æ§éªŒè¯é›†ï¼Œå½“éªŒè¯é›† Loss åœæ­¢æ”¹è¿›æ—¶åœæ­¢è®­ç»ƒ

```python
class EarlyStopping:
    def __init__(self, patience=10, min_delta=0, restore_best_weights=True):
        self.patience = patience
        self.min_delta = min_delta
        self.counter = 0
        self.best_loss = float('inf')
        self.restore_best_weights = restore_best_weights
        self.best_weights = None

    def __call__(self, val_loss, model):
        """
        è¿”å› True è¡¨ç¤ºåº”è¯¥åœæ­¢è®­ç»ƒ
        """
        if val_loss < self.best_loss - self.min_delta:
            self.best_loss = val_loss
            self.counter = 0
            if self.restore_best_weights:
                self.best_weights = model.state_dict().copy()
        else:
            self.counter += 1
            if self.counter >= self.patience:
                if self.restore_best_weights and self.best_weights is not None:
                    model.load_state_dict(self.best_weights)
                return True  # åœæ­¢è®­ç»ƒ
        return False

# ä½¿ç”¨
early_stopping = EarlyStopping(patience=10)

for epoch in range(num_epochs):
    # è®­ç»ƒ
    train_loss = train_one_epoch()

    # éªŒè¯
    val_loss = validate()

    # æ£€æŸ¥æ˜¯å¦åœæ­¢
    if early_stopping(val_loss, model):
        print(f"åœ¨ç¬¬ {epoch} ä¸ª epoch åœæ­¢")
        break
```

---

## 5.8 Batch Normalization (BN)

### ğŸ¯ ä¸ºä»€ä¹ˆéœ€è¦ BNï¼Ÿ

**é—®é¢˜ï¼šå†…éƒ¨åå˜é‡è½¬ç§» (Internal Covariate Shift)**

```
ç¬¬1å±‚çš„è¾“å‡ºå˜åŒ– â†’ ç¬¬2å±‚çš„è¾“å…¥åˆ†å¸ƒå˜åŒ–
â†’ ç¬¬2å±‚éœ€è¦ä¸æ–­é€‚åº” â†’ è®­ç»ƒå˜æ…¢
```

### ğŸ’¡ Batch Normalization åŸç†

**æ ‡å‡†åŒ–æ¯ä¸ª batch**ï¼š

```
å¯¹æ¯ä¸ªç‰¹å¾ï¼š
  1. è®¡ç®—å‡å€¼ï¼šÎ¼_B = (1/m)Â·Î£xáµ¢
  2. è®¡ç®—æ–¹å·®ï¼šÏƒ_BÂ² = (1/m)Â·Î£(xáµ¢ - Î¼_B)Â²
  3. æ ‡å‡†åŒ–ï¼šxÌ‚áµ¢ = (xáµ¢ - Î¼_B) / âˆš(Ïƒ_BÂ² + Îµ)
  4. å°ºåº¦å’Œå¹³ç§»ï¼šyáµ¢ = Î³Â·xÌ‚áµ¢ + Î²

å…¶ä¸­ Î³ å’Œ Î² æ˜¯å¯å­¦ä¹ çš„å‚æ•°
```

### ğŸ“ BN åœ¨å“ªé‡Œæ”¾ï¼Ÿ

```
å¸¸è§ä½ç½®ï¼š
  1. Linear â†’ BN â†’ Activation
  2. Linear â†’ Activation â†’ BN
  3. Conv â†’ BN â†’ ReLU (æ¨è)

ä¸€èˆ¬ï¼šBN åœ¨æ¿€æ´»å‡½æ•°ä¹‹å‰
```

### ğŸ’» PyTorch å®ç°

```python
class NeuralNetworkWithBN(nn.Module):
    def __init__(self, input_size, hidden_size):
        super().__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.bn1 = nn.BatchNorm1d(hidden_size)

        self.fc2 = nn.Linear(hidden_size, hidden_size)
        self.bn2 = nn.BatchNorm1d(hidden_size)

        self.fc3 = nn.Linear(hidden_size, 10)

    def forward(self, x):
        x = self.fc1(x)
        x = self.bn1(x)
        x = torch.relu(x)

        x = self.fc2(x)
        x = self.bn2(x)
        x = torch.relu(x)

        x = self.fc3(x)
        return x

# è®­ç»ƒæ—¶ BN ä½¿ç”¨ batch ç»Ÿè®¡
# æ¨ç†æ—¶ BN ä½¿ç”¨è¿è¡Œå‡å€¼å’Œæ–¹å·®
model.train()    # ä½¿ç”¨ batch ç»Ÿè®¡
model.eval()     # ä½¿ç”¨è¿è¡Œç»Ÿè®¡
```

### âœ… Batch Normalization çš„ä¼˜ç‚¹

1. **åŠ é€Ÿæ”¶æ•›**ï¼šå‡å°‘å†…éƒ¨åå˜é‡è½¬ç§»
2. **å…è®¸æ›´å¤§å­¦ä¹ ç‡**ï¼šæ›´ç¨³å®šçš„æ¢¯åº¦
3. **å‡å°‘åˆå§‹åŒ–æ•æ„Ÿæ€§**ï¼šå¯¹åˆå§‹åŒ–ä¸æ•æ„Ÿ
4. **è½»å¾®çš„æ­£åˆ™åŒ–æ•ˆæœ**
5. **ç®€åŒ–åç»­ç½‘ç»œ**ï¼šå¯ä»¥ç§»é™¤ Dropout

### âš ï¸ BN çš„ç¼ºç‚¹å’Œé™åˆ¶

```
é—®é¢˜ï¼š
  - Batch size å¤ªå°æ—¶æ•ˆæœå·®
  - è®­ç»ƒå’Œæ¨ç†æ—¶è¡Œä¸ºä¸åŒ
  - åœ¨ RNN ä¸­ä½¿ç”¨å›°éš¾

è§£å†³ï¼š
  - Layer Normalization (LN)
  - Group Normalization (GN)
  - Instance Normalization (IN)
```

---

## 5.9 å®æˆ˜ï¼šå®Œæ•´è®­ç»ƒæµç¨‹

### ğŸ’» PyTorch å®Œæ•´ç¤ºä¾‹

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
from tqdm import tqdm

# ==================== è¶…å‚æ•° ====================
BATCH_SIZE = 128
LEARNING_RATE = 0.001
WEIGHT_DECAY = 1e-5
EPOCHS = 50
DROPOUT_RATE = 0.3
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# ==================== æ•°æ®åŠ è½½ ====================
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.1307,), (0.3081,))
])

train_dataset = datasets.MNIST(root='./data', train=True,
                               download=True, transform=transform)
val_dataset = datasets.MNIST(root='./data', train=False,
                             download=True, transform=transform)

train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,
                          shuffle=True, num_workers=2)
val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE,
                        shuffle=False, num_workers=2)

# ==================== å®šä¹‰æ¨¡å‹ ====================
class ImprovedNN(nn.Module):
    def __init__(self, dropout_rate=0.3):
        super().__init__()

        # ç¬¬1å±‚
        self.fc1 = nn.Linear(28*28, 512)
        self.bn1 = nn.BatchNorm1d(512)
        self.dropout1 = nn.Dropout(dropout_rate)

        # ç¬¬2å±‚
        self.fc2 = nn.Linear(512, 256)
        self.bn2 = nn.BatchNorm1d(256)
        self.dropout2 = nn.Dropout(dropout_rate)

        # ç¬¬3å±‚
        self.fc3 = nn.Linear(256, 128)
        self.bn3 = nn.BatchNorm1d(128)
        self.dropout3 = nn.Dropout(dropout_rate)

        # è¾“å‡ºå±‚
        self.fc4 = nn.Linear(128, 10)

    def forward(self, x):
        # å±•å¹³
        x = x.view(-1, 28*28)

        # ç¬¬1å±‚
        x = self.fc1(x)
        x = self.bn1(x)
        x = torch.relu(x)
        x = self.dropout1(x)

        # ç¬¬2å±‚
        x = self.fc2(x)
        x = self.bn2(x)
        x = torch.relu(x)
        x = self.dropout2(x)

        # ç¬¬3å±‚
        x = self.fc3(x)
        x = self.bn3(x)
        x = torch.relu(x)
        x = self.dropout3(x)

        # è¾“å‡ºå±‚
        x = self.fc4(x)
        return x

model = ImprovedNN(dropout_rate=DROPOUT_RATE).to(DEVICE)

# ==================== æŸå¤±å’Œä¼˜åŒ–å™¨ ====================
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(),
                       lr=LEARNING_RATE,
                       weight_decay=WEIGHT_DECAY)

# å­¦ä¹ ç‡è°ƒåº¦
scheduler = optim.lr_scheduler.CosineAnnealingLR(
    optimizer, T_max=EPOCHS, eta_min=1e-6
)

# Early Stopping
early_stopping = EarlyStopping(patience=10)

# ==================== è®­ç»ƒå‡½æ•° ====================
def train_epoch(model, train_loader, criterion, optimizer, device):
    """è®­ç»ƒä¸€ä¸ª epoch"""
    model.train()
    total_loss = 0
    correct = 0
    total = 0

    progress_bar = tqdm(train_loader, desc='Training')

    for images, labels in progress_bar:
        images, labels = images.to(device), labels.to(device)

        # å‰å‘ä¼ æ’­
        outputs = model(images)
        loss = criterion(outputs, labels)

        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        # ç»Ÿè®¡
        total_loss += loss.item()
        _, predicted = outputs.max(1)
        correct += predicted.eq(labels).sum().item()
        total += labels.size(0)

        # æ›´æ–°è¿›åº¦æ¡
        accuracy = 100. * correct / total
        progress_bar.set_postfix({
            'loss': f'{total_loss/(total):.3f}',
            'acc': f'{accuracy:.2f}%'
        })

    avg_loss = total_loss / len(train_loader)
    accuracy = 100. * correct / total

    return avg_loss, accuracy

# ==================== éªŒè¯å‡½æ•° ====================
def validate(model, val_loader, criterion, device):
    """éªŒè¯"""
    model.eval()
    total_loss = 0
    correct = 0
    total = 0

    with torch.no_grad():
        for images, labels in tqdm(val_loader, desc='Validation'):
            images, labels = images.to(device), labels.to(device)

            outputs = model(images)
            loss = criterion(outputs, labels)

            total_loss += loss.item()
            _, predicted = outputs.max(1)
            correct += predicted.eq(labels).sum().item()
            total += labels.size(0)

    avg_loss = total_loss / len(val_loader)
    accuracy = 100. * correct / total

    return avg_loss, accuracy

# ==================== è®­ç»ƒå¾ªç¯ ====================
train_losses = []
train_accs = []
val_losses = []
val_accs = []

for epoch in range(EPOCHS):
    print(f'\n=== Epoch {epoch+1}/{EPOCHS} ===')
    print(f'å­¦ä¹ ç‡: {optimizer.param_groups[0]["lr"]:.6f}')

    # è®­ç»ƒ
    train_loss, train_acc = train_epoch(
        model, train_loader, criterion, optimizer, DEVICE
    )
    train_losses.append(train_loss)
    train_accs.append(train_acc)

    # éªŒè¯
    val_loss, val_acc = validate(model, val_loader, criterion, DEVICE)
    val_losses.append(val_loss)
    val_accs.append(val_acc)

    print(f'è®­ç»ƒ: Loss={train_loss:.4f}, Acc={train_acc:.2f}%')
    print(f'éªŒè¯: Loss={val_loss:.4f}, Acc={val_acc:.2f}%')

    # å­¦ä¹ ç‡è°ƒåº¦
    scheduler.step()

    # Early Stopping
    if early_stopping(val_loss, model):
        print(f'\nåœ¨ç¬¬ {epoch+1} ä¸ª epoch åœæ­¢è®­ç»ƒï¼ˆéªŒè¯é›†æ— æ”¹è¿›ï¼‰')
        break

# ==================== å¯è§†åŒ– ====================
import matplotlib.pyplot as plt

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))

epochs_range = range(1, len(train_losses) + 1)

ax1.plot(epochs_range, train_losses, label='Train Loss', marker='o')
ax1.plot(epochs_range, val_losses, label='Val Loss', marker='o')
ax1.set_xlabel('Epoch')
ax1.set_ylabel('Loss')
ax1.set_title('Loss Over Epochs')
ax1.legend()
ax1.grid(True, alpha=0.3)

ax2.plot(epochs_range, train_accs, label='Train Acc', marker='o')
ax2.plot(epochs_range, val_accs, label='Val Acc', marker='o')
ax2.set_xlabel('Epoch')
ax2.set_ylabel('Accuracy (%)')
ax2.set_title('Accuracy Over Epochs')
ax2.legend()
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# ==================== æœ€ç»ˆè¯„ä¼° ====================
print('\n' + '='*50)
print('æœ€ç»ˆç»“æœï¼š')
print(f'æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {max(val_accs):.2f}% (Epoch {val_accs.index(max(val_accs))+1})')
print('='*50)
```

---

## ğŸ“ æœ¬ç« ä½œä¸š

### ä½œä¸š 1ï¼šä¼˜åŒ–å™¨å¯¹æ¯”

å®ç°ä»¥ä¸‹ä¼˜åŒ–å™¨ï¼Œåœ¨ MNIST ä¸Šå¯¹æ¯”ï¼š

```python
# TODO:
# 1. SGD
# 2. SGD + Momentum
# 3. RMSprop
# 4. Adam

# è®°å½•ï¼š
#   - è¾¾åˆ° 95% å‡†ç¡®ç‡éœ€è¦çš„ epoch æ•°
#   - æœ€ç»ˆå‡†ç¡®ç‡
#   - è®­ç»ƒæ—¶é—´
#   - Loss æ›²çº¿å¹³æ»‘åº¦

# ç»˜åˆ¶å¯¹æ¯”å›¾è¡¨
```

### ä½œä¸š 2ï¼šå­¦ä¹ ç‡è°ƒåº¦å®éªŒ

```python
# åœ¨åŒä¸€ä¸ªæ¨¡å‹ä¸Šå¯¹æ¯”ä¸åŒçš„å­¦ä¹ ç‡è°ƒåº¦ï¼š

# 1. å›ºå®šå­¦ä¹ ç‡
# 2. Step Decay
# 3. Exponential Decay
# 4. Cosine Annealing
# 5. Warm-up + Cosine

# åˆ†æï¼š
#   - æœ€ç»ˆå‡†ç¡®ç‡
#   - æ”¶æ•›é€Ÿåº¦
#   - è®­ç»ƒç¨³å®šæ€§
```

### ä½œä¸š 3ï¼šæ­£åˆ™åŒ–æŠ€æœ¯å¯¹æ¯”

```python
# åœ¨ç›¸åŒæ¶æ„ä¸Šå¯¹æ¯”ï¼š

# 1. æ— æ­£åˆ™åŒ–
# 2. L2 æ­£åˆ™åŒ– (Î»=0.001, 0.01, 0.1)
# 3. Dropout (p=0.3, 0.5)
# 4. Batch Normalization
# 5. ç»„åˆï¼ˆBN + Dropoutï¼‰

# è§‚å¯Ÿï¼š
#   - è®­ç»ƒé›† vs æµ‹è¯•é›†æ€§èƒ½å·®è·
#   - è¿‡æ‹Ÿåˆæƒ…å†µ
#   - æ¯ç§æ–¹æ³•çš„å½±å“
```

### ä½œä¸š 4ï¼šå®Œæ•´é¡¹ç›®

åœ¨ CIFAR-10 æ•°æ®é›†ä¸Šæ„å»ºå®Œæ•´çš„è®­ç»ƒæµç¨‹

**è¦æ±‚**ï¼š
1. å®ç°ä¸€ä¸ª 4-5 å±‚çš„æ·±åº¦ç½‘ç»œ
2. ä½¿ç”¨ Batch Normalization
3. ä½¿ç”¨ Dropout
4. é€‰æ‹©åˆé€‚çš„ä¼˜åŒ–å™¨ï¼ˆAdam æˆ– SGD+Momentumï¼‰
5. å®ç°å­¦ä¹ ç‡è°ƒåº¦
6. å®ç° Early Stopping
7. è®°å½•è®­ç»ƒè¿‡ç¨‹å’Œè¯„ä¼°ç»“æœ
8. å¯è§†åŒ–è®­ç»ƒæ›²çº¿
9. åˆ†ææ¨¡å‹æ€§èƒ½å’Œæ”¹è¿›æ–¹å‘

---

## ğŸ”‘ æœ¬ç« å…³é”®æ¦‚å¿µ

| æ¦‚å¿µ | è¯´æ˜ |
|------|------|
| Mini-batch GD | æ‰¹é‡æ¢¯åº¦ä¸‹é™çš„æŠ˜ä¸­ |
| Momentum | ä½¿ç”¨å†å²æ¢¯åº¦åŠ é€Ÿæ”¶æ•› |
| RMSprop | è‡ªé€‚åº”å­¦ä¹ ç‡ |
| Adam | Momentum + RMSprop çš„ç»„åˆ |
| Learning Rate Schedule | åŠ¨æ€è°ƒæ•´å­¦ä¹ ç‡ |
| Batch Normalization | æ ‡å‡†åŒ–ä¸­é—´å±‚è¾“å‡º |
| Dropout | éšæœºç¦ç”¨ç¥ç»å…ƒé˜²æ­¢è¿‡æ‹Ÿåˆ |
| L1/L2 æ­£åˆ™åŒ– | æƒ©ç½šå¤§æƒé‡ |
| Early Stopping | ç›‘æ§éªŒè¯é›†æå‰åœæ­¢ |
| Weight Decay | æƒé‡è¡°å‡ï¼ˆç­‰åŒ L2ï¼‰ |

---

## ğŸ¯ ä¸‹ä¸€ç« é¢„å‘Š

**ç¬¬å…­ç« ï¼šå·ç§¯ç¥ç»ç½‘ç»œ (Convolutional Neural Networks)**
- å·ç§¯æ“ä½œçš„åŸç†
- æ„Ÿå—é‡å’Œå‚æ•°å…±äº«
- æ± åŒ–å’Œç‰¹å¾å›¾
- ç»å…¸ CNN æ¶æ„ï¼ˆLeNet, AlexNet, VGG, ResNetï¼‰
- å®æˆ˜ï¼šå›¾åƒåˆ†ç±»

---
