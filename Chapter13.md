# ç¬¬åä¸‰ç« ï¼šå¤§è¯­è¨€æ¨¡å‹æ—¶ä»£

## ğŸ“Œ ç« èŠ‚ç›®æ ‡
- ç†è§£å¤§è¯­è¨€æ¨¡å‹ (LLM) çš„æ ¸å¿ƒåŸç†
- æŒæ¡ Prompt Engineering æŠ€å·§
- å­¦ä¹  In-Context Learning å’Œ Few-Shot Learning
- äº†è§£ LLM çš„å¾®è°ƒæ–¹æ³•ï¼ˆLoRA, PEFTï¼‰
- æ¢ç´¢ LLM çš„åº”ç”¨å’Œæœªæ¥æ–¹å‘

---

## 13.1 ä» GPT åˆ° ChatGPTï¼šå¤§è¯­è¨€æ¨¡å‹çš„æ¼”è¿›

### ğŸŒŸ å…³é”®é‡Œç¨‹ç¢‘

```
2017: Transformer (Attention Is All You Need)
      â†“
2018: GPT-1 (117M å‚æ•°)
      BERT (340M å‚æ•°)
      â†“
2019: GPT-2 (1.5B å‚æ•°)
      T5, BART, XLNet
      â†“
2020: GPT-3 (175B å‚æ•°) ğŸ‘‘
      - Few-shot learning
      - In-context learning
      â†“
2022: ChatGPT (GPT-3.5 + RLHF)
      InstructGPT
      â†“
2023: GPT-4 (å¤šæ¨¡æ€)
      Claude, LLaMA, PaLM
      â†“
2024: Gemini, Claude 3
      å¼€æºæ¨¡å‹çˆ†å‘
```

---

### ğŸ“ æ ¸å¿ƒèƒ½åŠ›çš„æ¶Œç°

**è§„æ¨¡å®šå¾‹ (Scaling Laws)**ï¼š

```
æ€§èƒ½ âˆ log(æ¨¡å‹å¤§å° Ã— æ•°æ®é‡ Ã— è®¡ç®—é‡)

æ¶Œç°èƒ½åŠ› (Emergent Abilities):
  - å°‘æ ·æœ¬å­¦ä¹ 
  - æŒ‡ä»¤éµå¾ª
  - æ€ç»´é“¾æ¨ç†
  - ä»£ç ç”Ÿæˆ
  - å¤šæ­¥æ¨ç†
```

---

## 13.2 LLM çš„æ¶æ„åŸç†

### ğŸ—ï¸ Transformer å›é¡¾

```
è¾“å…¥ Token
    â†“
[Embedding + Positional Encoding]
    â†“
[Transformer Block] Ã—N
  - Multi-Head Attention
  - Feed-Forward Network
  - Layer Normalization
  - Residual Connection
    â†“
[Language Model Head]
    â†“
ä¸‹ä¸€ä¸ª Token çš„æ¦‚ç‡åˆ†å¸ƒ
```

### ğŸ”¹ GPT æ¶æ„ç‰¹ç‚¹

**åªç”¨ Decoderï¼ˆè‡ªå›å½’ï¼‰**ï¼š

```python
class GPTBlock(nn.Module):
    """GPT Transformer å—"""

    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):
        super().__init__()

        # Causal Self-Attentionï¼ˆåªçœ‹å‰æ–‡ï¼‰
        self.attn = MultiHeadAttention(d_model, n_heads, dropout, causal=True)

        # Feed-Forward
        self.ff = nn.Sequential(
            nn.Linear(d_model, d_ff),
            nn.GELU(),
            nn.Linear(d_ff, d_model),
            nn.Dropout(dropout)
        )

        # Layer Norm
        self.ln1 = nn.LayerNorm(d_model)
        self.ln2 = nn.LayerNorm(d_model)

        self.dropout = nn.Dropout(dropout)

    def forward(self, x, mask=None):
        # Pre-LN æ¶æ„
        x = x + self.attn(self.ln1(x), mask)
        x = x + self.ff(self.ln2(x))
        return x

class GPTModel(nn.Module):
    """ç®€åŒ–çš„ GPT æ¨¡å‹"""

    def __init__(self, vocab_size, d_model=768, n_heads=12,
                 n_layers=12, d_ff=3072, max_len=1024, dropout=0.1):
        super().__init__()

        # Token Embedding
        self.token_emb = nn.Embedding(vocab_size, d_model)

        # Positional Embeddingï¼ˆå¯å­¦ä¹ ï¼‰
        self.pos_emb = nn.Embedding(max_len, d_model)

        # Transformer Blocks
        self.blocks = nn.ModuleList([
            GPTBlock(d_model, n_heads, d_ff, dropout)
            for _ in range(n_layers)
        ])

        # Final Layer Norm
        self.ln_f = nn.LayerNorm(d_model)

        # Language Model Head
        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)

        # æƒé‡å…±äº«ï¼ˆembedding å’Œ lm_headï¼‰
        self.lm_head.weight = self.token_emb.weight

        self.dropout = nn.Dropout(dropout)

    def forward(self, input_ids, labels=None):
        """
        å‚æ•°:
            input_ids: (batch, seq_len)
            labels: (batch, seq_len) å¯é€‰ï¼Œç”¨äºè®­ç»ƒ
        """
        batch_size, seq_len = input_ids.shape

        # Embedding
        token_embeddings = self.token_emb(input_ids)  # (B, T, D)

        # Positional Embedding
        positions = torch.arange(0, seq_len, device=input_ids.device)
        position_embeddings = self.pos_emb(positions)  # (T, D)

        x = self.dropout(token_embeddings + position_embeddings)

        # Causal Maskï¼ˆä¸‹ä¸‰è§’çŸ©é˜µï¼‰
        mask = torch.tril(torch.ones(seq_len, seq_len, device=input_ids.device))
        mask = mask.view(1, 1, seq_len, seq_len)

        # Transformer Blocks
        for block in self.blocks:
            x = block(x, mask)

        x = self.ln_f(x)

        # Logits
        logits = self.lm_head(x)  # (B, T, vocab_size)

        # è®¡ç®—æŸå¤±ï¼ˆå¦‚æœæä¾›äº†æ ‡ç­¾ï¼‰
        loss = None
        if labels is not None:
            # ç§»ä½ï¼šé¢„æµ‹ä¸‹ä¸€ä¸ª token
            shift_logits = logits[:, :-1, :].contiguous()
            shift_labels = labels[:, 1:].contiguous()

            loss = F.cross_entropy(
                shift_logits.view(-1, shift_logits.size(-1)),
                shift_labels.view(-1),
                ignore_index=-100
            )

        return logits, loss

    def generate(self, input_ids, max_new_tokens=100,
                temperature=1.0, top_k=None, top_p=None):
        """
        è‡ªå›å½’ç”Ÿæˆ

        å‚æ•°:
            input_ids: (batch, seq_len) è¾“å…¥åºåˆ—
            max_new_tokens: ç”Ÿæˆçš„æœ€å¤§ token æ•°
            temperature: æ¸©åº¦å‚æ•°ï¼ˆæ§åˆ¶éšæœºæ€§ï¼‰
            top_k: Top-K é‡‡æ ·
            top_p: Nucleus (Top-P) é‡‡æ ·
        """
        for _ in range(max_new_tokens):
            # æˆªæ–­åˆ°æœ€å¤§é•¿åº¦ï¼ˆé¿å…è¶…è¿‡ä½ç½®ç¼–ç ï¼‰
            input_ids_cond = input_ids if input_ids.size(1) <= 1024 else input_ids[:, -1024:]

            # å‰å‘ä¼ æ’­
            logits, _ = self.forward(input_ids_cond)

            # è·å–æœ€åä¸€ä¸ªä½ç½®çš„ logits
            logits = logits[:, -1, :] / temperature

            # Top-K é‡‡æ ·
            if top_k is not None:
                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))
                logits[logits < v[:, [-1]]] = -float('Inf')

            # Top-P (Nucleus) é‡‡æ ·
            if top_p is not None:
                sorted_logits, sorted_indices = torch.sort(logits, descending=True)
                cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)

                # ç§»é™¤ç´¯ç§¯æ¦‚ç‡è¶…è¿‡ top_p çš„ tokens
                sorted_indices_to_remove = cumulative_probs > top_p
                sorted_indices_to_remove[:, 1:] = sorted_indices_to_remove[:, :-1].clone()
                sorted_indices_to_remove[:, 0] = 0

                for i in range(logits.size(0)):
                    indices_to_remove = sorted_indices[i, sorted_indices_to_remove[i]]
                    logits[i, indices_to_remove] = -float('Inf')

            # é‡‡æ ·
            probs = F.softmax(logits, dim=-1)
            next_token = torch.multinomial(probs, num_samples=1)

            # æ‹¼æ¥
            input_ids = torch.cat([input_ids, next_token], dim=1)

        return input_ids
```

---

## 13.3 é¢„è®­ç»ƒï¼šè‡ªç›‘ç£å­¦ä¹ 

### ğŸ¯ é¢„è®­ç»ƒä»»åŠ¡

```
è¯­è¨€æ¨¡å‹ç›®æ ‡ï¼š
  ç»™å®šä¸Šæ–‡ xâ‚, xâ‚‚, ..., x_{t-1}ï¼Œé¢„æµ‹ x_t

  P(x_t | xâ‚, ..., x_{t-1})

æŸå¤±å‡½æ•°ï¼š
  L = -âˆ‘_t log P(x_t | xâ‚, ..., x_{t-1})
```

### ğŸ’» é¢„è®­ç»ƒæµç¨‹

```python
def train_gpt(model, dataloader, num_epochs=10):
    """é¢„è®­ç»ƒ GPT"""

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.to(device)

    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4,
                                  betas=(0.9, 0.95), weight_decay=0.1)

    # å­¦ä¹ ç‡è°ƒåº¦ï¼ˆwarmup + cosine decayï¼‰
    def get_lr(step, warmup_steps=2000, max_steps=100000):
        if step < warmup_steps:
            return step / warmup_steps
        return 0.5 * (1 + np.cos(np.pi * (step - warmup_steps) / (max_steps - warmup_steps)))

    scheduler = torch.optim.lr_scheduler.LambdaLR(
        optimizer, lr_lambda=lambda step: get_lr(step)
    )

    step = 0
    for epoch in range(num_epochs):
        model.train()
        total_loss = 0

        for batch in dataloader:
            input_ids = batch['input_ids'].to(device)
            labels = input_ids.clone()

            # å‰å‘ä¼ æ’­
            logits, loss = model(input_ids, labels)

            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()

            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

            optimizer.step()
            scheduler.step()

            total_loss += loss.item()
            step += 1

            if step % 100 == 0:
                print(f'Step {step}: Loss = {loss.item():.4f}, '
                      f'LR = {scheduler.get_last_lr()[0]:.6f}')

        avg_loss = total_loss / len(dataloader)
        print(f'Epoch {epoch+1}: Avg Loss = {avg_loss:.4f}')

    return model
```

---

## 13.4 Prompt Engineering

### ğŸ¨ ä»€ä¹ˆæ˜¯ Promptï¼Ÿ

```
Prompt = ç»™æ¨¡å‹çš„æŒ‡ä»¤/ç¤ºä¾‹

ä¾‹ï¼š
  è¾“å…¥ï¼š"å°†ä¸‹é¢çš„è‹±æ–‡ç¿»è¯‘æˆä¸­æ–‡ï¼š\nHello, world!"
  æ¨¡å‹ï¼šæ ¹æ® prompt ç†è§£ä»»åŠ¡ï¼Œç”Ÿæˆç¿»è¯‘
```

### ğŸ“Š Prompt è®¾è®¡åŸåˆ™

#### **1. æ¸…æ™°æ˜ç¡®**

```
âŒ å·®çš„ Prompt:
  "å…³äº AI"

âœ… å¥½çš„ Prompt:
  "è¯·ç”¨ 200 å­—ä»‹ç»äººå·¥æ™ºèƒ½çš„å®šä¹‰ã€å‘å±•å†ç¨‹å’Œä¸»è¦åº”ç”¨é¢†åŸŸã€‚"
```

#### **2. æä¾›ä¸Šä¸‹æ–‡**

```
âŒ æ— ä¸Šä¸‹æ–‡:
  "è¿™ä¸ªæ€ä¹ˆæ ·ï¼Ÿ"

âœ… æœ‰ä¸Šä¸‹æ–‡:
  "æˆ‘æ­£åœ¨å†™ä¸€ç¯‡å…³äºæ°”å€™å˜åŒ–çš„æ–‡ç« ã€‚ä»¥ä¸‹æ˜¯è‰ç¨¿çš„ç¬¬ä¸€æ®µï¼š
  [æ®µè½å†…å®¹]
  è¯·è¯„ä»·è¿™æ®µå†…å®¹çš„é€»è¾‘æ€§å’Œè¯´æœåŠ›ã€‚"
```

#### **3. ä½¿ç”¨ç¤ºä¾‹ï¼ˆFew-shotï¼‰**

```
Zero-shotï¼ˆæ— ç¤ºä¾‹ï¼‰:
  "æƒ…æ„Ÿåˆ†ç±»ï¼šè¿™éƒ¨ç”µå½±å¾ˆå¥½çœ‹"

Few-shotï¼ˆæœ‰ç¤ºä¾‹ï¼‰:
  """
  æƒ…æ„Ÿåˆ†ç±»ä»»åŠ¡ï¼š

  ç¤ºä¾‹ï¼š
  æ–‡æœ¬: "è¿™å®¶é¤å…å¤ªå·®äº†" â†’ è´Ÿé¢
  æ–‡æœ¬: "æœåŠ¡æ€åº¦å¾ˆå¥½" â†’ æ­£é¢
  æ–‡æœ¬: "è¿˜è¡Œå§" â†’ ä¸­æ€§

  ç°åœ¨åˆ†ç±»ï¼š
  æ–‡æœ¬: "è¿™éƒ¨ç”µå½±å¾ˆå¥½çœ‹" â†’
  """
```

#### **4. æ€ç»´é“¾ï¼ˆChain-of-Thoughtï¼‰**

```
æ™®é€š Prompt:
  "Roger æœ‰ 5 ä¸ªç½‘çƒã€‚ä»–åˆä¹°äº† 2 ç½ç½‘çƒï¼Œæ¯ç½ 3 ä¸ªçƒã€‚
   ä»–ç°åœ¨æœ‰å¤šå°‘ä¸ªç½‘çƒï¼Ÿ"

CoT Prompt:
  "Roger æœ‰ 5 ä¸ªç½‘çƒã€‚ä»–åˆä¹°äº† 2 ç½ç½‘çƒï¼Œæ¯ç½ 3 ä¸ªçƒã€‚
   ä»–ç°åœ¨æœ‰å¤šå°‘ä¸ªç½‘çƒï¼Ÿ

   è®©æˆ‘ä»¬ä¸€æ­¥æ­¥æ€è€ƒï¼š
   1. Roger æœ€åˆæœ‰ 5 ä¸ªç½‘çƒ
   2. ä»–ä¹°äº† 2 ç½ï¼Œæ¯ç½ 3 ä¸ªï¼Œæ‰€ä»¥ä¹°äº† 2Ã—3=6 ä¸ª
   3. æ€»å…±ï¼š5+6=11 ä¸ª

   ç­”æ¡ˆï¼š11 ä¸ªç½‘çƒ"
```

---

### ğŸ’» Prompt æ¨¡æ¿ç¤ºä¾‹

```python
class PromptTemplate:
    """Prompt æ¨¡æ¿ç®¡ç†"""

    def __init__(self):
        self.templates = {
            'translation': """
Translate the following {source_lang} text to {target_lang}:

Text: {text}

Translation:""",

            'summarization': """
Summarize the following text in {num_sentences} sentences:

{text}

Summary:""",

            'classification': """
Classify the sentiment of the following text as Positive, Negative, or Neutral.

Examples:
{examples}

Text: {text}
Sentiment:""",

            'qa': """
Answer the following question based on the context.

Context: {context}

Question: {question}

Answer:""",

            'cot_reasoning': """
Question: {question}

Let's think step by step:
""",
        }

    def format(self, template_name, **kwargs):
        """æ ¼å¼åŒ–æ¨¡æ¿"""
        template = self.templates[template_name]
        return template.format(**kwargs)

# ==================== ä½¿ç”¨ç¤ºä¾‹ ====================

prompt_template = PromptTemplate()

# ç¿»è¯‘
translation_prompt = prompt_template.format(
    'translation',
    source_lang='English',
    target_lang='Chinese',
    text='Hello, how are you?'
)

# Few-shot åˆ†ç±»
examples = """
Text: "This movie is amazing!" â†’ Positive
Text: "Waste of time." â†’ Negative
Text: "It's okay." â†’ Neutral"""

classification_prompt = prompt_template.format(
    'classification',
    examples=examples,
    text='I love this product!'
)

print(classification_prompt)
```

---

### ğŸ”§ é«˜çº§ Prompt æŠ€å·§

#### **1. è§’è‰²æ‰®æ¼” (Role-Playing)**

```
"ä½ æ˜¯ä¸€ä½èµ„æ·±çš„ Python ç¨‹åºå‘˜ã€‚è¯·å¸®æˆ‘ä¼˜åŒ–ä»¥ä¸‹ä»£ç ï¼š
[ä»£ç ]"
```

#### **2. çº¦æŸæ¡ä»¶**

```
"ç”¨ç®€å•æ˜“æ‡‚çš„è¯­è¨€ï¼ˆé€‚åˆ 10 å²å„¿ç«¥ç†è§£ï¼‰è§£é‡Šä»€ä¹ˆæ˜¯ç¥ç»ç½‘ç»œã€‚
è¦æ±‚ï¼š
- ä¸è¶…è¿‡ 3 æ®µ
- ä½¿ç”¨æ—¥å¸¸ç”Ÿæ´»çš„æ¯”å–»
- é¿å…ä¸“ä¸šæœ¯è¯­"
```

#### **3. è¾“å‡ºæ ¼å¼**

```
"åˆ†æä»¥ä¸‹äº§å“è¯„è®ºï¼Œä»¥ JSON æ ¼å¼è¾“å‡ºï¼š
{
  "sentiment": "positive/negative/neutral",
  "key_points": ["point1", "point2"],
  "rating": 1-5
}

è¯„è®ºï¼š[è¯„è®ºå†…å®¹]"
```

#### **4. è‡ªæˆ‘ä¸€è‡´æ€§ (Self-Consistency)**

```
å¤šæ¬¡ç”Ÿæˆç­”æ¡ˆï¼Œé€‰æ‹©æœ€ä¸€è‡´çš„ç»“æœ
```

```python
def self_consistency_generate(model, tokenizer, prompt, n=5):
    """è‡ªæˆ‘ä¸€è‡´æ€§ç”Ÿæˆ"""

    answers = []

    for _ in range(n):
        # ç”Ÿæˆç­”æ¡ˆï¼ˆå¸¦éšæœºæ€§ï¼‰
        inputs = tokenizer(prompt, return_tensors='pt')
        outputs = model.generate(
            inputs['input_ids'],
            max_new_tokens=100,
            temperature=0.7,
            do_sample=True
        )

        answer = tokenizer.decode(outputs[0], skip_special_tokens=True)
        answers.append(answer)

    # é€‰æ‹©æœ€å¸¸è§çš„ç­”æ¡ˆ
    from collections import Counter
    most_common = Counter(answers).most_common(1)[0][0]

    return most_common, answers
```

---

## 13.5 In-Context Learning

### ğŸ¯ æ ¸å¿ƒæ¦‚å¿µ

```
In-Context Learning:
  åœ¨è¾“å…¥ä¸­æä¾›ç¤ºä¾‹ï¼Œæ— éœ€æ›´æ–°å‚æ•°

å…³é”®ç‰¹æ€§ï¼š
  âœ“ æ— éœ€æ¢¯åº¦æ›´æ–°
  âœ“ å³æ—¶é€‚åº”æ–°ä»»åŠ¡
  âœ“ çµæ´»æ€§é«˜
```

### ğŸ“Š Few-Shot Learning ç¤ºä¾‹

```python
class FewShotLearner:
    """Few-Shot Learning åŒ…è£…å™¨"""

    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer

    def create_few_shot_prompt(self, task, examples, query):
        """
        åˆ›å»º Few-Shot Prompt

        å‚æ•°:
            task: ä»»åŠ¡æè¿°
            examples: [(input, output), ...]
            query: æŸ¥è¯¢è¾“å…¥
        """
        prompt = f"{task}\n\n"

        # æ·»åŠ ç¤ºä¾‹
        for i, (inp, out) in enumerate(examples, 1):
            prompt += f"Example {i}:\n"
            prompt += f"Input: {inp}\n"
            prompt += f"Output: {out}\n\n"

        # æ·»åŠ æŸ¥è¯¢
        prompt += f"Now solve:\n"
        prompt += f"Input: {query}\n"
        prompt += f"Output:"

        return prompt

    def predict(self, task, examples, query, max_tokens=100):
        """Few-Shot é¢„æµ‹"""

        # åˆ›å»º prompt
        prompt = self.create_few_shot_prompt(task, examples, query)

        # ç”Ÿæˆ
        inputs = self.tokenizer(prompt, return_tensors='pt')
        outputs = self.model.generate(
            inputs['input_ids'],
            max_new_tokens=max_tokens,
            temperature=0.7,
            top_p=0.9
        )

        # è§£ç 
        result = self.tokenizer.decode(outputs[0], skip_special_tokens=True)

        # æå–è¾“å‡ºéƒ¨åˆ†
        result = result.split("Output:")[-1].strip()

        return result

# ==================== ä½¿ç”¨ç¤ºä¾‹ ====================

# æƒ…æ„Ÿåˆ†ç±» Few-Shot
task = "Classify the sentiment of the text as Positive or Negative."

examples = [
    ("I love this product!", "Positive"),
    ("Terrible experience.", "Negative"),
    ("Best purchase ever!", "Positive"),
    ("Complete waste of money.", "Negative"),
]

query = "This exceeded my expectations."

learner = FewShotLearner(model, tokenizer)
result = learner.predict(task, examples, query)

print(f"Query: {query}")
print(f"Prediction: {result}")
```

---

### ğŸ”¹ ç¤ºä¾‹é€‰æ‹©ç­–ç•¥

```python
def select_diverse_examples(example_pool, query, n=5, method='semantic'):
    """
    é€‰æ‹©å¤šæ ·åŒ–çš„ç¤ºä¾‹

    æ–¹æ³•:
        - random: éšæœºé€‰æ‹©
        - semantic: åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦
        - diverse: æœ€å¤§åŒ–å¤šæ ·æ€§
    """

    if method == 'random':
        return random.sample(example_pool, n)

    elif method == 'semantic':
        from sentence_transformers import SentenceTransformer

        # åŠ è½½å¥å­ç¼–ç å™¨
        encoder = SentenceTransformer('all-MiniLM-L6-v2')

        # ç¼–ç æŸ¥è¯¢å’Œç¤ºä¾‹
        query_emb = encoder.encode([query])[0]
        example_embs = encoder.encode([ex[0] for ex in example_pool])

        # è®¡ç®—ç›¸ä¼¼åº¦
        from sklearn.metrics.pairwise import cosine_similarity
        similarities = cosine_similarity([query_emb], example_embs)[0]

        # é€‰æ‹©æœ€ç›¸ä¼¼çš„
        top_indices = np.argsort(similarities)[-n:][::-1]
        return [example_pool[i] for i in top_indices]

    elif method == 'diverse':
        # k-means èšç±»é€‰æ‹©å¤šæ ·åŒ–ç¤ºä¾‹
        from sklearn.cluster import KMeans
        from sentence_transformers import SentenceTransformer

        encoder = SentenceTransformer('all-MiniLM-L6-v2')
        example_embs = encoder.encode([ex[0] for ex in example_pool])

        # èšç±»
        kmeans = KMeans(n_clusters=n, random_state=42)
        kmeans.fit(example_embs)

        # ä»æ¯ä¸ªç°‡é€‰æ‹©æœ€æ¥è¿‘ä¸­å¿ƒçš„ç¤ºä¾‹
        selected = []
        for i in range(n):
            cluster_indices = np.where(kmeans.labels_ == i)[0]
            center = kmeans.cluster_centers_[i]

            # æ‰¾æœ€æ¥è¿‘ä¸­å¿ƒçš„
            distances = np.linalg.norm(example_embs[cluster_indices] - center, axis=1)
            closest_idx = cluster_indices[np.argmin(distances)]
            selected.append(example_pool[closest_idx])

        return selected
```

---

## 13.6 æŒ‡ä»¤å¾®è°ƒ (Instruction Tuning)

### ğŸ¯ ä» GPT-3 åˆ° InstructGPT

```
é—®é¢˜ï¼š
  GPT-3 è™½ç„¶å¼ºå¤§ï¼Œä½†ä¸æ€»æ˜¯éµå¾ªç”¨æˆ·æŒ‡ä»¤

è§£å†³ï¼š
  Instruction Tuning + RLHF

æµç¨‹ï¼š
  1. æ”¶é›†æŒ‡ä»¤-å“åº”æ•°æ®
  2. ç›‘ç£å¾®è°ƒ (SFT)
  3. æ”¶é›†äººç±»åå¥½æ•°æ®
  4. è®­ç»ƒå¥–åŠ±æ¨¡å‹ (RM)
  5. å¼ºåŒ–å­¦ä¹ å¾®è°ƒ (PPO)
```

### ğŸ’» ç›‘ç£å¾®è°ƒ (SFT)

```python
def instruction_tuning(model, instruction_dataset, num_epochs=3):
    """
    æŒ‡ä»¤å¾®è°ƒ

    æ•°æ®æ ¼å¼:
    {
        "instruction": "å°†ä»¥ä¸‹è‹±æ–‡ç¿»è¯‘æˆä¸­æ–‡",
        "input": "Hello, world!",
        "output": "ä½ å¥½ï¼Œä¸–ç•Œï¼"
    }
    """

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.to(device)

    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)

    for epoch in range(num_epochs):
        model.train()
        total_loss = 0

        for batch in instruction_dataset:
            # æ ¼å¼åŒ–ä¸º prompt
            prompts = []
            for item in batch:
                prompt = f"### Instruction:\n{item['instruction']}\n\n"
                if item.get('input'):
                    prompt += f"### Input:\n{item['input']}\n\n"
                prompt += f"### Response:\n{item['output']}"
                prompts.append(prompt)

            # Tokenize
            inputs = tokenizer(prompts, return_tensors='pt',
                             padding=True, truncation=True)
            inputs = {k: v.to(device) for k, v in inputs.items()}

            # å‰å‘ä¼ æ’­
            outputs = model(**inputs, labels=inputs['input_ids'])
            loss = outputs.loss

            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            total_loss += loss.item()

        print(f'Epoch {epoch+1}: Loss = {total_loss/len(instruction_dataset):.4f}')

    return model
```

---

### ğŸ”¹ RLHF (Reinforcement Learning from Human Feedback)

```python
class RewardModel(nn.Module):
    """å¥–åŠ±æ¨¡å‹"""

    def __init__(self, base_model):
        super().__init__()
        self.base_model = base_model

        # å¥–åŠ±å¤´
        self.reward_head = nn.Linear(base_model.config.hidden_size, 1)

    def forward(self, input_ids, attention_mask=None):
        # è·å–æœ€åä¸€ä¸ª token çš„éšè—çŠ¶æ€
        outputs = self.base_model(input_ids, attention_mask=attention_mask)
        last_hidden = outputs.last_hidden_state[:, -1, :]

        # é¢„æµ‹å¥–åŠ±
        reward = self.reward_head(last_hidden)

        return reward

def train_reward_model(model, comparison_dataset):
    """
    è®­ç»ƒå¥–åŠ±æ¨¡å‹

    æ•°æ®æ ¼å¼ï¼š(prompt, response_A, response_B, preference)
    preference: 0 è¡¨ç¤º A æ›´å¥½ï¼Œ1 è¡¨ç¤º B æ›´å¥½
    """

    optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)

    for epoch in range(num_epochs):
        for batch in comparison_dataset:
            prompts, responses_A, responses_B, preferences = batch

            # è®¡ç®—å¥–åŠ±
            rewards_A = model(responses_A)
            rewards_B = model(responses_B)

            # æŸå¤±ï¼šåå¥½çš„å“åº”åº”è¯¥æœ‰æ›´é«˜å¥–åŠ±
            loss = -torch.log(torch.sigmoid(
                (rewards_A - rewards_B) * (2 * preferences - 1)
            )).mean()

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

    return model

def rlhf_training(policy_model, reward_model, prompts):
    """
    ä½¿ç”¨ PPO è¿›è¡Œ RLHF

    ç®€åŒ–ç‰ˆæœ¬ï¼ˆå®é™…å®ç°æ›´å¤æ‚ï¼‰
    """

    optimizer = torch.optim.Adam(policy_model.parameters(), lr=1e-6)

    for iteration in range(num_iterations):
        for prompt in prompts:
            # ç”Ÿæˆå“åº”
            with torch.no_grad():
                response = policy_model.generate(prompt)

            # è®¡ç®—å¥–åŠ±
            reward = reward_model(response)

            # PPO æ›´æ–°ï¼ˆç®€åŒ–ï¼‰
            # å®é™…éœ€è¦ï¼šold_log_probs, advantages, clip_epsilon ç­‰
            log_probs = policy_model.compute_log_probs(prompt, response)
            policy_loss = -(log_probs * reward).mean()

            # KL æ•£åº¦æƒ©ç½šï¼ˆé˜²æ­¢åç¦»å¤ªè¿œï¼‰
            ref_log_probs = ref_model.compute_log_probs(prompt, response)
            kl_penalty = (log_probs - ref_log_probs).mean()

            loss = policy_loss + 0.1 * kl_penalty

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
```

---

## 13.7 é«˜æ•ˆå¾®è°ƒï¼šLoRA å’Œ PEFT

### ğŸ¯ ä¸ºä»€ä¹ˆéœ€è¦é«˜æ•ˆå¾®è°ƒï¼Ÿ

```
é—®é¢˜ï¼š
  å¤§æ¨¡å‹å…¨å‚æ•°å¾®è°ƒæˆæœ¬é«˜æ˜‚
  - GPT-3 (175B å‚æ•°)
  - éœ€è¦æ•°ç™¾ GB æ˜¾å­˜
  - è®­ç»ƒæ—¶é—´é•¿

è§£å†³ï¼š
  Parameter-Efficient Fine-Tuning (PEFT)
  - åªè®­ç»ƒå°‘é‡å‚æ•°
  - ä¿æŒæ€§èƒ½
```

---

### ğŸ”¹ LoRA (Low-Rank Adaptation)

**æ ¸å¿ƒæ€æƒ³**ï¼šä½ç§©åˆ†è§£

```
åŸå§‹æƒé‡æ›´æ–°ï¼š
  W' = W + Î”W

LoRAï¼š
  W' = W + BA

  å…¶ä¸­ B âˆˆ â„^(dÃ—r), A âˆˆ â„^(rÃ—k), r << min(d, k)

å‚æ•°é‡ï¼š
  åŸå§‹ï¼šd Ã— k
  LoRAï¼šr Ã— (d + k)  ï¼ˆå‡å°‘ >90%ï¼‰
```

```python
class LoRALayer(nn.Module):
    """LoRA å±‚"""

    def __init__(self, in_features, out_features, rank=8, alpha=16):
        super().__init__()

        self.rank = rank
        self.alpha = alpha

        # ä½ç§©çŸ©é˜µ
        self.lora_A = nn.Parameter(torch.zeros(rank, in_features))
        self.lora_B = nn.Parameter(torch.zeros(out_features, rank))

        # åˆå§‹åŒ–
        nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))
        nn.init.zeros_(self.lora_B)

        # ç¼©æ”¾å› å­
        self.scaling = alpha / rank

    def forward(self, x):
        # LoRA è·¯å¾„ï¼šx @ A^T @ B^T
        lora_out = (x @ self.lora_A.T) @ self.lora_B.T
        return lora_out * self.scaling

class LoRALinear(nn.Module):
    """å¸¦ LoRA çš„çº¿æ€§å±‚"""

    def __init__(self, linear_layer, rank=8, alpha=16):
        super().__init__()

        # å†»ç»“åŸå§‹æƒé‡
        self.linear = linear_layer
        for param in self.linear.parameters():
            param.requires_grad = False

        # æ·»åŠ  LoRA
        self.lora = LoRALayer(
            linear_layer.in_features,
            linear_layer.out_features,
            rank, alpha
        )

    def forward(self, x):
        # åŸå§‹è¾“å‡º + LoRA å¢é‡
        return self.linear(x) + self.lora(x)

# ==================== åº”ç”¨ LoRA ====================

def apply_lora_to_model(model, rank=8, alpha=16, target_modules=['q_proj', 'v_proj']):
    """
    ä¸ºæ¨¡å‹æ·»åŠ  LoRA

    é€šå¸¸åªå¯¹ attention çš„ Q, V çŸ©é˜µæ·»åŠ  LoRA
    """

    for name, module in model.named_modules():
        # æ£€æŸ¥æ˜¯å¦æ˜¯ç›®æ ‡æ¨¡å—
        if any(target in name for target in target_modules):
            if isinstance(module, nn.Linear):
                # æ›¿æ¢ä¸º LoRA ç‰ˆæœ¬
                parent_name = '.'.join(name.split('.')[:-1])
                child_name = name.split('.')[-1]

                parent_module = model.get_submodule(parent_name)
                lora_linear = LoRALinear(module, rank, alpha)

                setattr(parent_module, child_name, lora_linear)

                print(f"Applied LoRA to {name}")

    return model

# ==================== è®­ç»ƒ LoRA ====================

def train_with_lora(model, dataloader, num_epochs=3):
    """ä½¿ç”¨ LoRA å¾®è°ƒ"""

    # åº”ç”¨ LoRA
    model = apply_lora_to_model(model)

    # åªä¼˜åŒ– LoRA å‚æ•°
    lora_params = [p for n, p in model.named_parameters() if 'lora' in n]
    optimizer = torch.optim.AdamW(lora_params, lr=1e-4)

    print(f"Total params: {sum(p.numel() for p in model.parameters()):,}")
    print(f"Trainable params: {sum(p.numel() for p in lora_params):,}")

    # è®­ç»ƒå¾ªç¯ï¼ˆä¸æ™®é€šå¾®è°ƒç›¸åŒï¼‰
    for epoch in range(num_epochs):
        # ... è®­ç»ƒä»£ç 
        pass

    return model
```

---

### ğŸ”¹ å…¶ä»– PEFT æ–¹æ³•

#### **Adapter Tuning**

```python
class Adapter(nn.Module):
    """Adapter æ¨¡å—"""

    def __init__(self, hidden_size, bottleneck_size=64):
        super().__init__()

        self.down_proj = nn.Linear(hidden_size, bottleneck_size)
        self.up_proj = nn.Linear(bottleneck_size, hidden_size)
        self.activation = nn.ReLU()

    def forward(self, x):
        residual = x
        x = self.down_proj(x)
        x = self.activation(x)
        x = self.up_proj(x)
        return x + residual  # æ®‹å·®è¿æ¥
```

#### **Prefix Tuning**

```python
class PrefixTuning(nn.Module):
    """Prefix Tuning"""

    def __init__(self, num_prefix_tokens, d_model):
        super().__init__()

        # å¯å­¦ä¹ çš„ prefix embeddings
        self.prefix_embeddings = nn.Parameter(
            torch.randn(num_prefix_tokens, d_model)
        )

    def forward(self, input_embeddings):
        batch_size = input_embeddings.size(0)

        # æ‰©å±• prefix åˆ° batch
        prefix = self.prefix_embeddings.unsqueeze(0).expand(batch_size, -1, -1)

        # æ‹¼æ¥åˆ°è¾“å…¥å‰é¢
        return torch.cat([prefix, input_embeddings], dim=1)
```

---

## 13.8 LLM åº”ç”¨èŒƒå¼

### ğŸ”¹ æ£€ç´¢å¢å¼ºç”Ÿæˆ (RAG)

```
RAG = Retrieval + Generation

æµç¨‹ï¼š
  1. æ£€ç´¢ç›¸å…³æ–‡æ¡£
  2. å°†æ–‡æ¡£ä½œä¸ºä¸Šä¸‹æ–‡
  3. ç”Ÿæˆç­”æ¡ˆ
```

```python
class RAGSystem:
    """æ£€ç´¢å¢å¼ºç”Ÿæˆç³»ç»Ÿ"""

    def __init__(self, llm, retriever, top_k=3):
        self.llm = llm
        self.retriever = retriever
        self.top_k = top_k

    def answer_question(self, question, knowledge_base):
        """
        åŸºäºçŸ¥è¯†åº“å›ç­”é—®é¢˜

        å‚æ•°:
            question: ç”¨æˆ·é—®é¢˜
            knowledge_base: æ–‡æ¡£åˆ—è¡¨
        """
        # 1. æ£€ç´¢ç›¸å…³æ–‡æ¡£
        relevant_docs = self.retriever.retrieve(
            question, knowledge_base, top_k=self.top_k
        )

        # 2. æ„å»º prompt
        context = "\n\n".join([
            f"Document {i+1}:\n{doc}"
            for i, doc in enumerate(relevant_docs)
        ])

        prompt = f"""Answer the question based on the context below.

Context:
{context}

Question: {question}

Answer:"""

        # 3. ç”Ÿæˆç­”æ¡ˆ
        answer = self.llm.generate(prompt)

        return answer, relevant_docs

class SimpleRetriever:
    """ç®€å•çš„åŸºäºåµŒå…¥çš„æ£€ç´¢å™¨"""

    def __init__(self, encoder_model='all-MiniLM-L6-v2'):
        from sentence_transformers import SentenceTransformer
        self.encoder = SentenceTransformer(encoder_model)

    def retrieve(self, query, documents, top_k=3):
        """æ£€ç´¢æœ€ç›¸å…³çš„æ–‡æ¡£"""

        # ç¼–ç 
        query_emb = self.encoder.encode([query])[0]
        doc_embs = self.encoder.encode(documents)

        # è®¡ç®—ç›¸ä¼¼åº¦
        from sklearn.metrics.pairwise import cosine_similarity
        similarities = cosine_similarity([query_emb], doc_embs)[0]

        # è¿”å› top-k
        top_indices = np.argsort(similarities)[-top_k:][::-1]

        return [documents[i] for i in top_indices]
```

---

### ğŸ”¹ Agent ç³»ç»Ÿ

```python
class LLMAgent:
    """åŸºäº LLM çš„ Agent"""

    def __init__(self, llm, tools):
        self.llm = llm
        self.tools = {tool.name: tool for tool in tools}

    def run(self, task, max_steps=5):
        """
        æ‰§è¡Œä»»åŠ¡

        æµç¨‹ï¼š
        1. æ€è€ƒä¸‹ä¸€æ­¥
        2. é€‰æ‹©å·¥å…·
        3. æ‰§è¡ŒåŠ¨ä½œ
        4. è§‚å¯Ÿç»“æœ
        5. é‡å¤ç›´åˆ°å®Œæˆ
        """

        history = []

        for step in range(max_steps):
            # æ„å»º prompt
            prompt = self._build_agent_prompt(task, history)

            # LLM å†³ç­–
            response = self.llm.generate(prompt)

            # è§£æåŠ¨ä½œ
            action = self._parse_action(response)

            if action['type'] == 'FINISH':
                return action['answer']

            # æ‰§è¡Œå·¥å…·
            tool_name = action['tool']
            tool_input = action['input']

            if tool_name in self.tools:
                observation = self.tools[tool_name].run(tool_input)
            else:
                observation = f"Tool {tool_name} not found."

            # è®°å½•
            history.append({
                'thought': action.get('tho