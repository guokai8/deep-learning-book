# ç¬¬ä¸ƒç« ï¼šå¾ªç¯ç¥ç»ç½‘ç»œ (Recurrent Neural Networks)

## ğŸ“Œ ç« èŠ‚ç›®æ ‡
- ç†è§£åºåˆ—æ•°æ®å’Œå¾ªç¯ç»“æ„
- æŒæ¡åŸºæœ¬ RNN åŠå…¶æ¢¯åº¦é—®é¢˜
- æ·±å…¥å­¦ä¹  LSTM å’Œ GRU çš„è®¾è®¡
- äº†è§£åŒå‘ RNN å’Œå¤šå±‚ RNN
- å®æˆ˜ï¼šæ–‡æœ¬åˆ†ç±»ã€æƒ…æ„Ÿåˆ†æã€åºåˆ—é¢„æµ‹

---

## 7.1 ä¸ºä»€ä¹ˆéœ€è¦ RNNï¼Ÿ

### ğŸ¯ åºåˆ—æ•°æ®çš„ç‰¹æ€§

**ä»€ä¹ˆæ˜¯åºåˆ—æ•°æ®ï¼Ÿ**

```
æ™®é€šæ•°æ®ï¼ˆç‹¬ç«‹ï¼‰ï¼š
  å›¾ç‰‡ã€æˆ¿ä»·ã€åŒ»å­¦è¯Šæ–­
  æ¯ä¸ªæ ·æœ¬æ˜¯ç‹¬ç«‹çš„

åºåˆ—æ•°æ®ï¼ˆæœ‰ä¾èµ–ï¼‰ï¼š
  æ–‡æœ¬ï¼šä»Šå¤©å¤©æ°”â†’å¾ˆå¥½â†’é€‚åˆâ†’å‡ºå»
  è¯­éŸ³ï¼šéŸ³é¢‘å¸§ tâ‚, tâ‚‚, ..., tâ‚™
  æ—¶é—´åºåˆ—ï¼šè‚¡ç¥¨ä»·æ ¼ã€æ¸©åº¦è®°å½•

ç‰¹æ€§ï¼š
  âœ“ é•¿åº¦å¯å˜
  âœ“ å‰åæœ‰ä¾èµ–å…³ç³»
  âœ“ é¡ºåºå¾ˆé‡è¦
```

### âŒ CNN å’Œ FC çš„å±€é™

**å…¨è¿æ¥ç½‘ç»œ**ï¼š
- å›ºå®šè¾“å…¥å¤§å°
- å¿½è§†åºåˆ—é¡ºåº
- æ— æ³•å¤„ç†å¯å˜é•¿åº¦

**CNN**ï¼š
- è™½ç„¶æœ‰å±€éƒ¨è¿æ¥ï¼Œä½†æ„Ÿå—é‡æœ‰é™
- éœ€è¦å¾ˆå¤šå±‚æ‰èƒ½æ•è·é•¿è·ç¦»ä¾èµ–
- ä¸å¤Ÿè‡ªç„¶

### âœ… RNN çš„ä¼˜åŠ¿

```
è®¾è®¡ç”¨äºåºåˆ—æ•°æ®ï¼š
  âœ“ å¯å˜é•¿åº¦è¾“å…¥
  âœ“ å¾ªç¯ç»“æ„ä¿ç•™åºåˆ—ä¿¡æ¯
  âœ“ å‚æ•°å…±äº«ï¼ˆæ‰€æœ‰æ—¶é—´æ­¥å…±ç”¨ï¼‰
  âœ“ å¯ä»¥å»ºæ¨¡é•¿æœŸä¾èµ–ï¼ˆç†è®ºä¸Šï¼‰
```

---

## 7.2 åŸºæœ¬ RNN (Vanilla RNN)

### ğŸ”„ RNN çš„å¾ªç¯ç»“æ„

**å±•å¼€è§†å›¾**ï¼š

```
yâ‚      yâ‚‚      yâ‚ƒ      yâ‚„
â†‘       â†‘       â†‘       â†‘
hâ‚      hâ‚‚      hâ‚ƒ      hâ‚„
â†‘       â†‘       â†‘       â†‘
xâ‚  â†’  hâ‚  â†’  hâ‚‚  â†’  hâ‚ƒ  â†’  hâ‚„
        â†“       â†“       â†“
        (å¾ªç¯)

éšè—çŠ¶æ€ä½œä¸ºä¿¡æ¯è½½ä½“ä¼ é€’
```

**æŠ˜å è§†å›¾**ï¼ˆå‚æ•°å…±äº«ï¼‰ï¼š

```
      x(t)
        â†“
    [U]  [W]  [V]
      â†“    â†“    â†“
   h(t-1) â†’ RNN â†’ h(t) â†’ y(t)
            å•å…ƒ
```

### ğŸ“ RNN è®¡ç®—

**å•æ—¶åˆ»è®¡ç®—**ï¼š

```
h(t) = tanh(UÂ·x(t) + WÂ·h(t-1) + b)
y(t) = VÂ·h(t) + c

æˆ–ç”¨æ¿€æ´»å‡½æ•° Ïƒï¼š
h(t) = Ïƒ(UÂ·x(t) + WÂ·h(t-1) + b)
```

**ç¬¦å·è¯´æ˜**ï¼š
- `x(t)`: t æ—¶åˆ»çš„è¾“å…¥
- `h(t)`: t æ—¶åˆ»çš„éšè—çŠ¶æ€
- `y(t)`: t æ—¶åˆ»çš„è¾“å‡º
- `U`: è¾“å…¥åˆ°éšè—çš„æƒé‡
- `W`: éšè—åˆ°éšè—çš„æƒé‡ï¼ˆå¾ªç¯ï¼‰
- `V`: éšè—åˆ°è¾“å‡ºçš„æƒé‡
- `b, c`: åç½®

### ğŸ’» ä»é›¶å®ç° RNN

```python
import numpy as np

class VanillaRNN:
    def __init__(self, input_size, hidden_size, output_size,
                 learning_rate=0.01):
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.output_size = output_size
        self.learning_rate = learning_rate

        # åˆå§‹åŒ–æƒé‡
        self.U = np.random.randn(hidden_size, input_size) * 0.01
        self.W = np.random.randn(hidden_size, hidden_size) * 0.01
        self.V = np.random.randn(output_size, hidden_size) * 0.01
        self.b = np.zeros((hidden_size, 1))
        self.c = np.zeros((output_size, 1))

    def forward(self, X):
        """
        å‰å‘ä¼ æ’­

        å‚æ•°ï¼š
            X: åºåˆ—è¾“å…¥ [(seq_len, input_size), ...]

        è¿”å›ï¼š
            Y: è¾“å‡ºåºåˆ—
            cache: ç”¨äºåå‘ä¼ æ’­çš„ä¸­é—´å€¼
        """
        seq_len = len(X)

        # åˆå§‹åŒ–éšè—çŠ¶æ€
        h = np.zeros((self.hidden_size, 1))

        # å­˜å‚¨ä¸­é—´å€¼
        cache = {
            'X': X,
            'h': [h],  # åŒ…æ‹¬åˆå§‹ h
            'z': [],
            'y': []
        }

        # å‰å‘ä¼ æ’­
        for t in range(seq_len):
            # éšè—çŠ¶æ€è®¡ç®—
            z = self.U @ X[t] + self.W @ h + self.b
            h = np.tanh(z)

            # è¾“å‡ºè®¡ç®—
            y = self.V @ h + self.c

            # ä¿å­˜ä¸­é—´å€¼
            cache['z'].append(z)
            cache['h'].append(h)
            cache['y'].append(y)

        return np.array(cache['y']), cache

    def backward(self, dY, cache):
        """
        åå‘ä¼ æ’­ï¼ˆBPTTï¼‰

        å‚æ•°ï¼š
            dY: è¾“å‡ºæ¢¯åº¦ [(output_size, 1), ...]
            cache: å‰å‘ä¼ æ’­çš„ä¸­é—´å€¼
        """
        seq_len = len(dY)

        # åˆå§‹åŒ–æ¢¯åº¦
        dU = np.zeros_like(self.U)
        dW = np.zeros_like(self.W)
        dV = np.zeros_like(self.V)
        db = np.zeros_like(self.b)
        dc = np.zeros_like(self.c)

        # åˆå§‹éšè—çŠ¶æ€æ¢¯åº¦
        dh_next = np.zeros((self.hidden_size, 1))

        # åå‘éå†æ—¶é—´æ­¥
        for t in reversed(range(seq_len)):
            # è¾“å‡ºå±‚æ¢¯åº¦
            dV += dY[t] @ cache['h'][t+1].T
            dc += dY[t]

            # éšè—å±‚æ¢¯åº¦
            dh = self.V.T @ dY[t] + dh_next

            # tanh çš„æ¢¯åº¦
            dz = dh * (1 - np.tanh(cache['z'][t])**2)

            # æƒé‡æ¢¯åº¦
            dU += dz @ cache['X'][t].T
            dW += dz @ cache['h'][t].T
            db += dz

            # ä¼ é€’åˆ°å‰ä¸€æ—¶åˆ»
            dh_next = self.W.T @ dz

        # æ¢¯åº¦è£å‰ªï¼ˆé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼‰
        for dparam in [dU, dW, dV, db, dc]:
            np.clip(dparam, -5, 5, out=dparam)

        return dU, dW, dV, db, dc

    def update_parameters(self, dU, dW, dV, db, dc):
        """æ›´æ–°å‚æ•°"""
        self.U -= self.learning_rate * dU
        self.W -= self.learning_rate * dW
        self.V -= self.learning_rate * dV
        self.b -= self.learning_rate * db
        self.c -= self.learning_rate * dc

# ç¤ºä¾‹ï¼šé¢„æµ‹æ•°å­—åºåˆ—
def train_rnn():
    # è¶…å‚æ•°
    input_size = 1
    hidden_size = 10
    output_size = 1
    seq_len = 5

    rnn = VanillaRNN(input_size, hidden_size, output_size,
                     learning_rate=0.01)

    # ç”Ÿæˆç®€å•æ•°æ®ï¼ˆt æ—¶åˆ»é¢„æµ‹ t+1ï¼‰
    X_train = [np.array([[i]]) for i in range(5)]
    y_train = [np.array([[i+1]]) for i in range(5)]

    # è®­ç»ƒ
    for epoch in range(100):
        Y_pred, cache = rnn.forward(X_train)

        # è®¡ç®—æŸå¤±
        loss = np.mean((Y_pred - np.array(y_train))**2)

        # æ¢¯åº¦è®¡ç®—
        dY = 2 * (Y_pred - np.array(y_train)) / len(y_train)
        dU, dW, dV, db, dc = rnn.backward(dY, cache)

        # æ›´æ–°å‚æ•°
        rnn.update_parameters(dU, dW, dV, db, dc)

        if epoch % 20 == 0:
            print(f"Epoch {epoch}: Loss = {loss:.6f}")

train_rnn()
```

---

## 7.3 æ¢¯åº¦é—®é¢˜ï¼šæ¶ˆå¤±å’Œçˆ†ç‚¸

### ğŸš¨ æ¢¯åº¦æ¶ˆå¤± (Vanishing Gradient)

**é—®é¢˜**ï¼šé•¿æœŸä¾èµ–éš¾ä»¥å­¦ä¹ 

```
h(t) = tanh(UÂ·x(t) + WÂ·h(t-1) + b)

âˆ‚h(t)/âˆ‚h(t-1) = WÂ·diag(1 - tanhÂ²(...))

å¯¹ t æ­¥ä¹‹å‰çš„æ¢¯åº¦ï¼š
âˆ‚h(T)/âˆ‚h(t) = âˆ(Ï„=t+1 to T) [WÂ·diag(1-tanhÂ²(...))]

å¦‚æœ ||W|| < 1ï¼Œåˆ™ï¼š
||âˆ‚h(T)/âˆ‚h(t)|| â‰ˆ ||W||^(T-t)

T-t å¾ˆå¤§æ—¶ï¼Œæ¢¯åº¦æ¥è¿‘ 0 â†’ æ¢¯åº¦æ¶ˆå¤±
```

**åæœ**ï¼š
- æ—©æœŸæƒé‡å‡ ä¹ä¸æ›´æ–°
- æ— æ³•å­¦ä¹ é•¿æœŸä¾èµ–

### ğŸ’¥ æ¢¯åº¦çˆ†ç‚¸ (Exploding Gradient)

**é—®é¢˜**ï¼šå¦‚æœ ||W|| > 1

```
æ¢¯åº¦ âˆ ||W||^(T-t) â†’ âˆ

å¯¼è‡´ï¼š
- å‚æ•°æ›´æ–°ä¸ç¨³å®š
- NaN/Inf å€¼
- è®­ç»ƒå´©æºƒ
```

### âœ… è§£å†³æ–¹æ¡ˆ

#### **1. æ¢¯åº¦è£å‰ª (Gradient Clipping)**

**é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸**

```python
def clip_gradients(gradients, max_norm=5):
    """
    L2 èŒƒæ•°è£å‰ª
    """
    total_norm = 0
    for g in gradients:
        total_norm += np.sum(g**2)
    total_norm = np.sqrt(total_norm)

    clip_ratio = max_norm / (total_norm + 1e-8)
    clip_ratio = min(clip_ratio, 1.0)

    clipped_grads = []
    for g in gradients:
        clipped_grads.append(g * clip_ratio)

    return clipped_grads
```

#### **2. æƒé‡åˆå§‹åŒ–**

```python
# æ­£äº¤åˆå§‹åŒ–
def orthogonal_init(shape):
    """æ­£äº¤åˆå§‹åŒ– W çŸ©é˜µ"""
    Q, R = np.linalg.qr(np.random.randn(*shape))
    return Q

# ä½¿ç”¨
W = orthogonal_init((hidden_size, hidden_size))
```

#### **3. æ¿€æ´»å‡½æ•°é€‰æ‹©**

```
ReLU çš„æ¢¯åº¦æ’ä¸º 1ï¼ˆåœ¨æ­£åŒºåŸŸï¼‰
ä¸å®¹æ˜“æ¢¯åº¦æ¶ˆå¤±

tanh çš„æ¢¯åº¦æœ€å¤§ä¸º 0.25
å®¹æ˜“æ¢¯åº¦æ¶ˆå¤±
```

---

## 7.4 LSTM (Long Short-Term Memory) â­

### ğŸ¯ æ ¸å¿ƒæ€æƒ³

**ä½¿ç”¨"è®°å¿†å•å…ƒ"ä»£æ›¿éšè—çŠ¶æ€**

```
ä¼ ç»Ÿ RNNï¼š
  ä¿¡æ¯é€šè¿‡éšè—çŠ¶æ€ä¼ é€’
  æ¯æ­¥éƒ½è¢«ç ´åæ€§åœ°æ”¹å˜

LSTMï¼š
  æœ‰ä¸“é—¨çš„"ç»†èƒçŠ¶æ€"C(t)
  ä¿¡æ¯å¯ä»¥é•¿æœŸä¿ç•™
  é€šè¿‡é—¨æ§æœºåˆ¶æœ‰é€‰æ‹©åœ°æ›´æ–°
```

### ğŸ“ LSTM çš„å››ä¸ªé—¨

**1. é—å¿˜é—¨ (Forget Gate)**

```
f(t) = Ïƒ(W_fÂ·[h(t-1), x(t)] + b_f)

ä½œç”¨ï¼šå†³å®šå“ªäº›ä¿¡æ¯è¢«ä¸¢å¼ƒ
f(t) â‰ˆ 0: ä¸¢å¼ƒ
f(t) â‰ˆ 1: ä¿ç•™
```

**2. è¾“å…¥é—¨ (Input Gate)**

```
i(t) = Ïƒ(W_iÂ·[h(t-1), x(t)] + b_i)
CÌƒ(t) = tanh(W_cÂ·[h(t-1), x(t)] + b_c)

ä½œç”¨ï¼šå†³å®šæ–°ä¿¡æ¯
i(t): æœ‰å¤šå°‘æ–°ä¿¡æ¯è¿›å…¥
CÌƒ(t): æ–°ä¿¡æ¯çš„å†…å®¹
```

**3. ç»†èƒçŠ¶æ€æ›´æ–° (Cell State Update)**

```
C(t) = f(t) âŠ™ C(t-1) + i(t) âŠ™ CÌƒ(t)

âŠ™ è¡¨ç¤ºé€å…ƒç´ ä¹˜æ³•ï¼ˆHadamard ç§¯ï¼‰

è¿‡ç¨‹ï¼š
  å‰ä¸€ä¸ªç»†èƒçŠ¶æ€ Ã— é—å¿˜é—¨
  + æ–°ä¿¡æ¯ Ã— è¾“å…¥é—¨
```

**4. è¾“å‡ºé—¨ (Output Gate)**

```
o(t) = Ïƒ(W_oÂ·[h(t-1), x(t)] + b_o)
h(t) = o(t) âŠ™ tanh(C(t))

ä½œç”¨ï¼šå†³å®šè¾“å‡ºå¤šå°‘ä¿¡æ¯
```

### ğŸ“Š LSTM å•å…ƒå›¾

```
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  ç»†èƒçŠ¶æ€ C(t)      â”‚ â† é•¿æœŸè®°å¿†
        â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
        â”‚   âŠ™ f(t)    â”‚ â† é—å¿˜é—¨ï¼ˆä¿ç•™å¤šå°‘ï¼‰
        â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ âŠ™ + âŠ™ i(t) CÌƒ(t) â”‚ â† æ–°ä¿¡æ¯åŠ å…¥
        â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ tanh âŠ™ o(t) â”‚ â† è¾“å‡ºé—¨
        â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
            h(t) â† çŸ­æœŸè®°å¿†
```

### ğŸ’» PyTorch å®ç°

```python
import torch
import torch.nn as nn

# æ–¹å¼1ï¼šä½¿ç”¨é«˜çº§ LSTM
class LSTMModel(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size):
        super(LSTMModel, self).__init__()

        self.hidden_size = hidden_size
        self.num_layers = num_layers

        # LSTM å±‚
        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,  # è¾“å…¥æ ¼å¼ (batch, seq, feature)
            dropout=0.3 if num_layers > 1 else 0
        )

        # è¾“å‡ºå±‚
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        """
        å‚æ•°ï¼š
            x: (batch_size, seq_len, input_size)
        """
        # LSTM è¾“å‡º
        lstm_out, (h_n, c_n) = self.lstm(x)
        # lstm_out: (batch, seq_len, hidden_size)
        # h_n: (num_layers, batch, hidden_size)
        # c_n: (num_layers, batch, hidden_size)

        # ä½¿ç”¨æœ€åä¸€ä¸ªæ—¶åˆ»çš„è¾“å‡º
        last_out = lstm_out[:, -1, :]  # (batch, hidden_size)

        # å…¨è¿æ¥å±‚
        output = self.fc(last_out)  # (batch, output_size)

        return output

# æ–¹å¼2ï¼šä»é›¶å®ç° LSTM å•å…ƒ
class LSTMCell(nn.Module):
    def __init__(self, input_size, hidden_size):
        super(LSTMCell, self).__init__()
        self.input_size = input_size
        self.hidden_size = hidden_size

        # å››ä¸ªé—¨çš„æƒé‡çŸ©é˜µ
        self.W_f = nn.Linear(input_size + hidden_size, hidden_size)
        self.W_i = nn.Linear(input_size + hidden_size, hidden_size)
        self.W_c = nn.Linear(input_size + hidden_size, hidden_size)
        self.W_o = nn.Linear(input_size + hidden_size, hidden_size)

    def forward(self, x, h_prev, c_prev):
        """
        å‚æ•°ï¼š
            x: (batch, input_size)
            h_prev: (batch, hidden_size)
            c_prev: (batch, hidden_size)

        è¿”å›ï¼š
            h: (batch, hidden_size)
            c: (batch, hidden_size)
        """
        # æ‹¼æ¥è¾“å…¥å’Œå‰ä¸€éšè—çŠ¶æ€
        combined = torch.cat([x, h_prev], dim=1)

        # å››ä¸ªé—¨
        f = torch.sigmoid(self.W_f(combined))  # é—å¿˜é—¨
        i = torch.sigmoid(self.W_i(combined))  # è¾“å…¥é—¨
        c_tilde = torch.tanh(self.W_c(combined))  # å€™é€‰å€¼
        o = torch.sigmoid(self.W_o(combined))  # è¾“å‡ºé—¨

        # æ›´æ–°ç»†èƒçŠ¶æ€
        c = f * c_prev + i * c_tilde

        # è®¡ç®—éšè—çŠ¶æ€
        h = o * torch.tanh(c)

        return h, c

# ä½¿ç”¨è‡ªå®šä¹‰ LSTM Cell
class CustomLSTM(nn.Module):
    def __init__(self, input_size, hidden_size, output_size, num_layers=1):
        super(CustomLSTM, self).__init__()

        self.hidden_size = hidden_size
        self.num_layers = num_layers

        # å¤šå±‚ LSTM
        self.lstm_cells = nn.ModuleList([
            LSTMCell(
                input_size if layer == 0 else hidden_size,
                hidden_size
            )
            for layer in range(num_layers)
        ])

        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        """
        å‚æ•°ï¼š
            x: (batch, seq_len, input_size)
        """
        batch_size, seq_len, _ = x.size()

        # åˆå§‹åŒ–éšè—çŠ¶æ€å’Œç»†èƒçŠ¶æ€
        h = [torch.zeros(batch_size, self.hidden_size)
             for _ in range(self.num_layers)]
        c = [torch.zeros(batch_size, self.hidden_size)
             for _ in range(self.num_layers)]

        # å‰å‘ä¼ æ’­
        for t in range(seq_len):
            x_t = x[:, t, :]  # (batch, input_size)

            for layer in range(self.num_layers):
                h[layer], c[layer] = self.lstm_cells[layer](
                    x_t, h[layer], c[layer]
                )
                x_t = h[layer]

        # ä½¿ç”¨æœ€åä¸€ä¸ªæ—¶åˆ»çš„éšè—çŠ¶æ€
        output = self.fc(h[-1])

        return output
```

---

## 7.5 GRU (Gated Recurrent Unit)

### ğŸ¯ ç®€åŒ–çš„ LSTM

**LSTM é—®é¢˜**ï¼šå‚æ•°å¤šï¼Œè®¡ç®—å¤æ‚

**GRU è§£å†³**ï¼šåªç”¨ä¸¤ä¸ªé—¨ï¼Œç»“æ„æ›´ç®€æ´

### ğŸ“ GRU çš„ä¸¤ä¸ªé—¨

**1. é‡ç½®é—¨ (Reset Gate)**

```
r(t) = Ïƒ(W_rÂ·[h(t-1), x(t)] + b_r)

ä½œç”¨ï¼šå†³å®šæœ‰å¤šå°‘å†å²ä¿¡æ¯è¢«é—å¿˜
```

**2. æ›´æ–°é—¨ (Update Gate)**

```
z(t) = Ïƒ(W_zÂ·[h(t-1), x(t)] + b_z)

ä½œç”¨ï¼šå†³å®šæ–°æ—§ä¿¡æ¯çš„æ¯”ä¾‹
```

**3. å€™é€‰éšè—çŠ¶æ€**

```
hÌƒ(t) = tanh(WÂ·[r(t) âŠ™ h(t-1), x(t)] + b)

ä½¿ç”¨é‡ç½®é—¨æ¥é€‰æ‹©å†å²ä¿¡æ¯
```

**4. éšè—çŠ¶æ€æ›´æ–°**

```
h(t) = (1 - z(t)) âŠ™ hÌƒ(t) + z(t) âŠ™ h(t-1)

= æ–°ä¿¡æ¯æ¯”ä¾‹ Ã— å€™é€‰å€¼ + å†å²ä¿¡æ¯æ¯”ä¾‹ Ã— å‰å€¼
```

### ğŸ“Š LSTM vs GRU

```
LSTMï¼š
  - ç»†èƒçŠ¶æ€ C(t) ç”¨äºé•¿æœŸè®°å¿†
  - éšè—çŠ¶æ€ h(t) ç”¨äºçŸ­æœŸè¾“å‡º
  - 3ä¸ªé—¨ï¼ˆé—å¿˜ã€è¾“å…¥ã€è¾“å‡ºï¼‰
  - å‚æ•°å¤šï¼Œè¡¨è¾¾èƒ½åŠ›å¼º

GRUï¼š
  - ç»†èƒçŠ¶æ€å’Œéšè—çŠ¶æ€åˆå¹¶
  - 2ä¸ªé—¨ï¼ˆé‡ç½®ã€æ›´æ–°ï¼‰
  - å‚æ•°å°‘ï¼ˆçº¦ LSTM çš„ 2/3ï¼‰
  - è®¡ç®—é€Ÿåº¦å¿«
  - åœ¨å¤§å¤šæ•°ä»»åŠ¡ä¸Šæ€§èƒ½ç›¸å½“
```

### ğŸ’» å®ç°

```python
class GRUCell(nn.Module):
    def __init__(self, input_size, hidden_size):
        super(GRUCell, self).__init__()
        self.input_size = input_size
        self.hidden_size = hidden_size

        # é‡ç½®é—¨å’Œæ›´æ–°é—¨
        self.W_r = nn.Linear(input_size + hidden_size, hidden_size)
        self.W_z = nn.Linear(input_size + hidden_size, hidden_size)
        # å€™é€‰éšè—çŠ¶æ€
        self.W_h = nn.Linear(input_size + hidden_size, hidden_size)

    def forward(self, x, h_prev):
        """
        å‚æ•°ï¼š
            x: (batch, input_size)
            h_prev: (batch, hidden_size)

        è¿”å›ï¼š
            h: (batch, hidden_size)
        """
        combined = torch.cat([x, h_prev], dim=1)

        # é‡ç½®é—¨
        r = torch.sigmoid(self.W_r(combined))

        # æ›´æ–°é—¨
        z = torch.sigmoid(self.W_z(combined))

        # å€™é€‰éšè—çŠ¶æ€
        combined_reset = torch.cat([x, r * h_prev], dim=1)
        h_tilde = torch.tanh(self.W_h(combined_reset))

        # æ›´æ–°éšè—çŠ¶æ€
        h = (1 - z) * h_tilde + z * h_prev

        return h

# PyTorch é«˜çº§æ¥å£
model = nn.GRU(
    input_size=10,
    hidden_size=20,
    num_layers=2,
    batch_first=True
)
```

---

## 7.6 åŒå‘ RNN (Bidirectional RNN)

### ğŸ¯ é—®é¢˜ï¼šå‰å‘ RNN çš„å±€é™

```
å‰å‘ RNNï¼š
  xâ‚ â†’ xâ‚‚ â†’ xâ‚ƒ â†’ xâ‚„

hâ‚ƒ æ— æ³•çœ‹åˆ° xâ‚„ çš„ä¿¡æ¯
ä½†æœ‰äº›ä»»åŠ¡éœ€è¦å®Œæ•´çš„ä¸Šä¸‹æ–‡ï¼
```

### âœ… åŒå‘ RNN è§£å†³

**åŒæ—¶è¿è¡Œå‰å‘å’Œåå‘ RNN**

```
å‰å‘ï¼šxâ‚ â†’ xâ‚‚ â†’ xâ‚ƒ â†’ xâ‚„
      â†’hâ‚â†’ â†’hâ‚‚â†’ â†’hâ‚ƒâ†’ â†’hâ‚„

åå‘ï¼š        â† â† â† â†
      â†hÌ„â‚â†  â†hÌ„â‚‚â†  â†hÌ„â‚ƒâ†  â†hÌ„â‚„â†

è¾“å‡ºï¼š[hâ‚ƒ, hÌ„â‚ƒ] = ç»“åˆä¸¤ä¸ªæ–¹å‘çš„ä¿¡æ¯
```

**è®¡ç®—**ï¼š

```
hâ‚ƒ = [hâ‚ƒ_forward, hâ‚ƒ_backward]
   = [LSTM_fwd(xâ‚:xâ‚ƒ), LSTM_bwd(xâ‚ƒ:xâ‚)]
```

### ğŸ’» å®ç°

```python
class BiLSTM(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size):
        super(BiLSTM, self).__init__()

        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            bidirectional=True  # å…³é”®ï¼
        )

        # åŒå‘ LSTM è¾“å‡ºå¤§å°æ˜¯ 2Ã—hidden_size
        self.fc = nn.Linear(hidden_size * 2, output_size)

    def forward(self, x):
        """
        å‚æ•°ï¼š
            x: (batch, seq_len, input_size)
        """
        # LSTM è¾“å‡º
        lstm_out, _ = self.lstm(x)
        # lstm_out: (batch, seq_len, 2Ã—hidden_size)

        # ä½¿ç”¨æœ€åæ—¶åˆ»
        last_out = lstm_out[:, -1, :]

        output = self.fc(last_out)
        return output

# æˆ–è€…ç”¨æ‰€æœ‰æ—¶åˆ»ï¼ˆå¦‚ NER æ ‡ç­¾ï¼‰
class BiLSTMSequence(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super().__init__()
        self.lstm = nn.LSTM(input_size, hidden_size,
                           batch_first=True, bidirectional=True)
        self.fc = nn.Linear(hidden_size * 2, output_size)

    def forward(self, x):
        lstm_out, _ = self.lstm(x)  # (batch, seq, 2Ã—hidden)

        # å¯¹æ¯ä¸ªæ—¶åˆ»åšåˆ†ç±»
        output = self.fc(lstm_out)  # (batch, seq, output_size)
        return output
```

---

## 7.7 å®æˆ˜ 1ï¼šæ–‡æœ¬æƒ…æ„Ÿåˆ†æ

### ğŸ“‹ ä»»åŠ¡è®¾å®š

**æ•°æ®**ï¼šç”µå½±è¯„è®º
```
"è¿™éƒ¨ç”µå½±å¤ªæ£’äº†ï¼" â†’ æ­£é¢ (1)
"å®Œå…¨æ˜¯æµªè´¹æ—¶é—´ã€‚" â†’ è´Ÿé¢ (0)
```

### ğŸ’» å®Œæ•´å®ç°

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import numpy as np
from collections import Counter
import re

# ==================== æ•°æ®é¢„å¤„ç† ====================

class Tokenizer:
    def __init__(self, vocab_size=5000):
        self.vocab_size = vocab_size
        self.word2idx = {'<PAD>': 0, '<UNK>': 1}
        self.idx2word = {0: '<PAD>', 1: '<UNK>'}

    def build_vocab(self, texts):
        """æ„å»ºè¯æ±‡è¡¨"""
        word_freq = Counter()
        for text in texts:
            words = self.tokenize(text)
            word_freq.update(words)

        idx = 2
        for word, freq in word_freq.most_common(self.vocab_size - 2):
            self.word2idx[word] = idx
            self.idx2word[idx] = word
            idx += 1

    def tokenize(self, text):
        """åˆ†è¯"""
        text = text.lower()
        text = re.sub(r'[^a-z\s]', '', text)
        return text.split()

    def encode(self, text, max_len=100):
        """ç¼–ç æ–‡æœ¬"""
        words = self.tokenize(text)
        ids = [self.word2idx.get(w, 1) for w in words]

        # Padding æˆ–æˆªæ–­
        if len(ids) < max_len:
            ids = ids + [0] * (max_len - len(ids))
        else:
            ids = ids[:max_len]

        return ids

    def decode(self, ids):
        """è§£ç """
        return ' '.join([self.idx2word.get(i, '<UNK>') for i in ids])

# ==================== æ¨¡å‹å®šä¹‰ ====================

class SentimentLSTM(nn.Module):
    def __init__(self, vocab_size, embedding_size, hidden_size,
                 num_layers=2, dropout=0.3):
        super(SentimentLSTM, self).__init__()

        # Embedding å±‚
        self.embedding = nn.Embedding(vocab_size, embedding_size,
                                     padding_idx=0)

        # LSTM å±‚
        self.lstm = nn.LSTM(
            input_size=embedding_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout,
            bidirectional=True
        )

        # æ³¨æ„åŠ›æœºåˆ¶ï¼ˆå¯é€‰ï¼‰
        self.attention = nn.MultiheadAttention(
            embed_dim=hidden_size * 2,
            num_heads=4,
            batch_first=True,
            dropout=dropout
        )

        # å…¨è¿æ¥å±‚
        self.fc1 = nn.Linear(hidden_size * 2, 128)
        self.dropout = nn.Dropout(dropout)
        self.fc2 = nn.Linear(128, 1)

        # Batch Normalization
        self.bn = nn.BatchNorm1d(128)

    def forward(self, x):
        """
        å‚æ•°ï¼š
            x: (batch, seq_len)
        """
        # Embedding
        embedded = self.embedding(x)  # (batch, seq_len, embedding_size)

        # LSTM
        lstm_out, (h_n, c_n) = self.lstm(embedded)
        # lstm_out: (batch, seq_len, hidden_size*2)

        # ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶
        attn_out, attn_weights = self.attention(
            lstm_out, lstm_out, lstm_out
        )

        # å–æœ€åä¸€ä¸ªæ—¶åˆ»æˆ–æ± åŒ–
        # æ–¹å¼1ï¼šæœ€åæ—¶åˆ»
        # last_hidden = lstm_out[:, -1, :]

        # æ–¹å¼2ï¼šå¹³å‡æ± åŒ–
        # last_hidden = torch.mean(lstm_out, dim=1)

        # æ–¹å¼3ï¼šæœ€å¤§æ± åŒ–
        # last_hidden, _ = torch.max(lstm_out, dim=1)

        # æ–¹å¼4ï¼šä½¿ç”¨æ³¨æ„åŠ›è¾“å‡º
        last_hidden = torch.mean(attn_out, dim=1)

        # å…¨è¿æ¥å±‚
        x = torch.relu(self.fc1(last_hidden))
        x = self.bn(x)
        x = self.dropout(x)
        x = self.fc2(x)

        return torch.sigmoid(x)

# ==================== è®­ç»ƒä»£ç  ====================

def train_sentiment_model():
    # è¶…å‚æ•°
    VOCAB_SIZE = 5000
    EMBEDDING_SIZE = 128
    HIDDEN_SIZE = 64
    NUM_LAYERS = 2
    MAX_LEN = 100
    BATCH_SIZE = 64
    EPOCHS = 20
    LEARNING_RATE = 0.001

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # ç¤ºä¾‹æ•°æ®ï¼ˆå®é™…åº”ä½¿ç”¨çœŸå®æ•°æ®é›†å¦‚ IMDBï¼‰
    texts_train = [
        "This movie is great and wonderful",
        "I love this film so much",
        "Amazing performance by the actors",
        "Terrible waste of time",
        "Boring and dull movie",
        "I hate this film"
    ] * 100  # å¤åˆ¶ä»¥å¢åŠ æ•°æ®é‡

    labels_train = [1, 1, 1, 0, 0, 0] * 100

    # æ„å»ºè¯æ±‡è¡¨
    tokenizer = Tokenizer(vocab_size=VOCAB_SIZE)
    tokenizer.build_vocab(texts_train)

    # ç¼–ç æ•°æ®
    X_train = torch.tensor([tokenizer.encode(t, MAX_LEN) for t in texts_train])
    y_train = torch.tensor(labels_train, dtype=torch.float32).unsqueeze(1)

    # æ•°æ®åŠ è½½å™¨
    train_dataset = TensorDataset(X_train, y_train)
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)

    # æ¨¡å‹
    model = SentimentLSTM(
        vocab_size=VOCAB_SIZE,
        embedding_size=EMBEDDING_SIZE,
        hidden_size=HIDDEN_SIZE,
        num_layers=NUM_LAYERS
    ).to(device)

    # æŸå¤±å’Œä¼˜åŒ–å™¨
    criterion = nn.BCELoss()
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', patience=3, factor=0.5
    )

    # è®­ç»ƒ
    for epoch in range(EPOCHS):
        model.train()
        total_loss = 0
        correct = 0
        total = 0

        for X_batch, y_batch in train_loader:
            X_batch = X_batch.to(device)
            y_batch = y_batch.to(device)

            # å‰å‘ä¼ æ’­
            outputs = model(X_batch)
            loss = criterion(outputs, y_batch)

            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()

            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5)

            optimizer.step()

            # ç»Ÿè®¡
            total_loss += loss.item()
            predicted = (outputs > 0.5).float()
            correct += (predicted == y_batch).sum().item()
            total += y_batch.size(0)

        avg_loss = total_loss / len(train_loader)
        accuracy = 100 * correct / total

        print(f'Epoch [{epoch+1}/{EPOCHS}], Loss: {avg_loss:.4f}, Acc: {accuracy:.2f}%')

        scheduler.step(avg_loss)

    # ä¿å­˜æ¨¡å‹
    torch.save(model.state_dict(), 'sentiment_lstm.pth')

    return model, tokenizer

# ==================== é¢„æµ‹å‡½æ•° ====================

def predict_sentiment(model, tokenizer, text, device):
    """é¢„æµ‹å•ä¸ªæ–‡æœ¬çš„æƒ…æ„Ÿ"""
    model.eval()

    # ç¼–ç 
    encoded = tokenizer.encode(text, max_len=100)
    x = torch.tensor([encoded]).to(device)

    # é¢„æµ‹
    with torch.no_grad():
        output = model(x)
        prob = output.item()
        sentiment = "æ­£é¢" if prob > 0.5 else "è´Ÿé¢"

    return sentiment, prob

# ä½¿ç”¨
if __name__ == "__main__":
    model, tokenizer = train_sentiment_model()

    # æµ‹è¯•
    test_texts = [
        "This is an amazing movie!",
        "Terrible and boring film.",
        "Not bad, quite enjoyable."
    ]

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.to(device)

    for text in test_texts:
        sentiment, prob = predict_sentiment(model, tokenizer, text, device)
        print(f"\næ–‡æœ¬: {text}")
        print(f"æƒ…æ„Ÿ: {sentiment} (ç½®ä¿¡åº¦: {prob:.4f})")
```

---

## 7.8 å®æˆ˜ 2ï¼šæ—¶é—´åºåˆ—é¢„æµ‹

### ğŸ“‹ ä»»åŠ¡ï¼šé¢„æµ‹è‚¡ç¥¨ä»·æ ¼

```python
import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler

# ==================== æ•°æ®å‡†å¤‡ ====================

def create_sequences(data, seq_length):
    """
    åˆ›å»ºåºåˆ—æ•°æ®

    å‚æ•°ï¼š
        data: åŸå§‹æ•°æ® (n_samples,)
        seq_length: åºåˆ—é•¿åº¦

    è¿”å›ï¼š
        X: (n_samples - seq_length, seq_length, 1)
        y: (n_samples - seq_length, 1)
    """
    X, y = [], []
    for i in range(len(data) - seq_length):
        X.append(data[i:i+seq_length])
        y.append(data[i+seq_length])

    return np.array(X), np.array(y)

# ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®ï¼ˆå®é™…åº”ä½¿ç”¨çœŸå®è‚¡ç¥¨æ•°æ®ï¼‰
def generate_stock_data(n_samples=1000):
    """ç”Ÿæˆæ¨¡æ‹Ÿè‚¡ç¥¨ä»·æ ¼"""
    t = np.linspace(0, 100, n_samples)
    # è¶‹åŠ¿ + å­£èŠ‚æ€§ + å™ªå£°
    trend = 0.02 * t
    seasonal = 10 * np.sin(2 * np.pi * t / 50)
    noise = np.random.randn(n_samples) * 2

    price = 100 + trend + seasonal + noise
    return price

# ==================== æ¨¡å‹å®šä¹‰ ====================

class StockLSTM(nn.Module):
    def __init__(self, input_size=1, hidden_size=64, num_layers=2, dropout=0.2):
        super(StockLSTM, self).__init__()

        self.hidden_size = hidden_size
        self.num_layers = num_layers

        # LSTM å±‚
        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0
        )

        # å…¨è¿æ¥å±‚
        self.fc = nn.Sequential(
            nn.Linear(hidden_size, 32),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(32, 1)
        )

    def forward(self, x):
        """
        å‚æ•°ï¼š
            x: (batch, seq_len, input_size)
        """
        # LSTM
        lstm_out, _ = self.lstm(x)

        # å–æœ€åä¸€ä¸ªæ—¶åˆ»
        last_out = lstm_out[:, -1, :]

        # é¢„æµ‹
        output = self.fc(last_out)

        return output

# ==================== è®­ç»ƒä»£ç  ====================

def train_stock_predictor():
    # è¶…å‚æ•°
    SEQ_LENGTH = 30  # ä½¿ç”¨30å¤©é¢„æµ‹ä¸‹ä¸€å¤©
    BATCH_SIZE = 32
    EPOCHS = 100
    LEARNING_RATE = 0.001

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # ç”Ÿæˆæ•°æ®
    data = generate_stock_data(n_samples=1000)

    # å½’ä¸€åŒ–
    scaler = MinMaxScaler()
    data_normalized = scaler.fit_transform(data.reshape(-1, 1)).flatten()

    # åˆ›å»ºåºåˆ—
    X, y = create_sequences(data_normalized, SEQ_LENGTH)
    X = X.reshape(-1, SEQ_LENGTH, 1)

    # è®­ç»ƒé›†å’Œæµ‹è¯•é›†åˆ†å‰²
    train_size = int(0.8 * len(X))
    X_train, X_test = X[:train_size], X[train_size:]
    y_train, y_test = y[:train_size], y[train_size:]

    # è½¬æ¢ä¸º Tensor
    X_train = torch.FloatTensor(X_train)
    y_train = torch.FloatTensor(y_train).unsqueeze(1)
    X_test = torch.FloatTensor(X_test)
    y_test = torch.FloatTensor(y_test).unsqueeze(1)

    # æ•°æ®åŠ è½½å™¨
    train_dataset = torch.utils.data.TensorDataset(X_train, y_train)
    train_loader = torch.utils.data.DataLoader(
        train_dataset, batch_size=BATCH_SIZE, shuffle=True
    )

    # æ¨¡å‹
    model = StockLSTM(
        input_size=1,
        hidden_size=64,
        num_layers=2,
        dropout=0.2
    ).to(device)

    # æŸå¤±å’Œä¼˜åŒ–å™¨
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, patience=5, factor=0.5, verbose=True
    )

    # è®­ç»ƒ
    train_losses = []

    for epoch in range(EPOCHS):
        model.train()
        epoch_loss = 0

        for X_batch, y_batch in train_loader:
            X_batch = X_batch.to(device)
            y_batch = y_batch.to(device)

            # å‰å‘ä¼ æ’­
            outputs = model(X_batch)
            loss = criterion(outputs, y_batch)

            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)
            optimizer.step()

            epoch_loss += loss.item()

        avg_loss = epoch_loss / len(train_loader)
        train_losses.append(avg_loss)

        # éªŒè¯
        model.eval()
        with torch.no_grad():
            X_test_device = X_test.to(device)
            test_pred = model(X_test_device)
            test_loss = criterion(test_pred, y_test.to(device))

        scheduler.step(test_loss)

        if (epoch + 1) % 10 == 0:
            print(f'Epoch [{epoch+1}/{EPOCHS}], '
                  f'Train Loss: {avg_loss:.6f}, Test Loss: {test_loss:.6f}')

    # ==================== å¯è§†åŒ–é¢„æµ‹ç»“æœ ====================

    model.eval()
    with torch.no_grad():
        train_pred = model(X_train.to(device)).cpu().numpy()
        test_pred = model(X_test.to(device)).cpu().numpy()

    # åå½’ä¸€åŒ–
    train_pred = scaler.inverse_transform(train_pred)
    test_pred = scaler.inverse_transform(test_pred)
    y_train_actual = scaler.inverse_transform(y_train.numpy())
    y_test_actual = scaler.inverse_transform(y_test.numpy())

    # ç»˜å›¾
    plt.figure(figsize=(15, 6))

    # è®­ç»ƒé›†
    plt.subplot(1, 2, 1)
    plt.plot(y_train_actual, label='çœŸå®å€¼', alpha=0.7)
    plt.plot(train_pred, label='é¢„æµ‹å€¼', alpha=0.7)
    plt.title('è®­ç»ƒé›†é¢„æµ‹')
    plt.xlabel('æ—¶é—´æ­¥')
    plt.ylabel('ä»·æ ¼')
    plt.legend()
    plt.grid(True, alpha=0.3)

    # æµ‹è¯•é›†
    plt.subplot(1, 2, 2)
    plt.plot(y_test_actual, label='çœŸå®å€¼', alpha=0.7)
    plt.plot(test_pred, label='é¢„æµ‹å€¼', alpha=0.7)
    plt.title('æµ‹è¯•é›†é¢„æµ‹')
    plt.xlabel('æ—¶é—´æ­¥')
    plt.ylabel('ä»·æ ¼')
    plt.legend()
    plt.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

    # è®¡ç®—æŒ‡æ ‡
    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

    test_mse = mean_squared_error(y_test_actual, test_pred)
    test_mae = mean_absolute_error(y_test_actual, test_pred)
    test_r2 = r2_score(y_test_actual, test_pred)

    print(f"\næµ‹è¯•é›†æŒ‡æ ‡ï¼š")
    print(f"MSE: {test_mse:.4f}")
    print(f"MAE: {test_mae:.4f}")
    print(f"RÂ²: {test_r2:.4f}")

    return model, scaler

# è¿è¡Œ
if __name__ == "__main__":
    model, scaler = train_stock_predictor()
```

---

## 7.9 å®æˆ˜ 3ï¼šåºåˆ—åˆ°åºåˆ— (Seq2Seq)

### ğŸ“‹ ä»»åŠ¡ï¼šæœºå™¨ç¿»è¯‘

```python
class Seq2SeqLSTM(nn.Module):
    def __init__(self, input_vocab_size, output_vocab_size,
                 embedding_size, hidden_size, num_layers=2):
        super(Seq2SeqLSTM, self).__init__()

        # Encoder
        self.encoder_embedding = nn.Embedding(input_vocab_size, embedding_size)
        self.encoder = nn.LSTM(
            embedding_size, hidden_size, num_layers,
            batch_first=True
        )

        # Decoder
        self.decoder_embedding = nn.Embedding(output_vocab_size, embedding_size)
        self.decoder = nn.LSTM(
            embedding_size, hidden_size, num_layers,
            batch_first=True
        )

        # è¾“å‡ºå±‚
        self.fc = nn.Linear(hidden_size, output_vocab_size)

    def forward(self, src, tgt, teacher_forcing_ratio=0.5):
        """
        å‚æ•°ï¼š
            src: (batch, src_seq_len) æºè¯­è¨€
            tgt: (batch, tgt_seq_len) ç›®æ ‡è¯­è¨€
            teacher_forcing_ratio: ä½¿ç”¨çœŸå®ç›®æ ‡çš„æ¦‚ç‡
        """
        batch_size = src.size(0)
        tgt_len = tgt.size(1)
        tgt_vocab_size = self.fc.out_features

        # ç¼–ç å™¨
        embedded_src = self.encoder_embedding(src)
        encoder_outputs, (hidden, cell) = self.encoder(embedded_src)

        # è§£ç å™¨åˆå§‹è¾“å…¥ï¼ˆ<SOS> tokenï¼‰
        decoder_input = tgt[:, 0].unsqueeze(1)

        # å­˜å‚¨è¾“å‡º
        outputs = torch.zeros(batch_size, tgt_len, tgt_vocab_size)

        # é€æ­¥è§£ç 
        for t in range(1, tgt_len):
            # è§£ç ä¸€æ­¥
            embedded_tgt = self.decoder_embedding(decoder_input)
            decoder_output, (hidden, cell) = self.decoder(
                embedded_tgt, (hidden, cell)
            )

            # é¢„æµ‹
            output = self.fc(decoder_output.squeeze(1))
            outputs[:, t, :] = output

            # Teacher forcing
            use_teacher_forcing = np.random.random() < teacher_forcing_ratio
            top1 = output.argmax(1)

            decoder_input = tgt[:, t].unsqueeze(1) if use_teacher_forcing else top1.unsqueeze(1)

        return outputs

# ä½¿ç”¨
model = Seq2SeqLSTM(
    input_vocab_size=5000,
    output_vocab_size=5000,
    embedding_size=256,
    hidden_size=512,
    num_layers=2
)
```

---

## ğŸ“ æœ¬ç« ä½œä¸š

### ä½œä¸š 1ï¼šRNN æ¢¯åº¦åˆ†æ

```python
# TODO:
# 1. å®ç° vanilla RNN
# 2. åœ¨é•¿åºåˆ—ä¸Šè®­ç»ƒ
# 3. å¯è§†åŒ–æ¢¯åº¦æµ
# 4. è§‚å¯Ÿæ¢¯åº¦æ¶ˆå¤±ç°è±¡
# 5. å¯¹æ¯” LSTM çš„æ¢¯åº¦æµ
```

### ä½œä¸š 2ï¼šæƒ…æ„Ÿåˆ†æå®Œæ•´é¡¹ç›®

```python
# ä½¿ç”¨çœŸå®æ•°æ®é›†ï¼ˆIMDB æˆ–ä¸­æ–‡è¯„è®ºï¼‰
# è¦æ±‚ï¼š
# 1. æ•°æ®é¢„å¤„ç†å’Œ EDA
# 2. å®ç° LSTM å’Œ GRU æ¨¡å‹
# 3. å¯¹æ¯”åŒå‘å’Œå•å‘
# 4. åŠ å…¥æ³¨æ„åŠ›æœºåˆ¶
# 5. è¶…å‚æ•°è°ƒä¼˜
# 6. å¯è§†åŒ–æ³¨æ„åŠ›æƒé‡
# 7. ç¼–å†™å®Œæ•´æŠ¥å‘Š
```

### ä½œä¸š 3ï¼šæ–‡æœ¬ç”Ÿæˆ

```python
# å­—ç¬¦çº§è¯­è¨€æ¨¡å‹
# 1. ä½¿ç”¨èå£«æ¯”äºšæ–‡æœ¬è®­ç»ƒ
# 2. å®ç° LSTM ç”Ÿæˆæ¨¡å‹
# 3. å°è¯•ä¸åŒçš„é‡‡æ ·ç­–ç•¥ï¼ˆè´ªå¿ƒã€top-kã€nucleusï¼‰
# 4. ç”Ÿæˆæ–°æ–‡æœ¬å¹¶è¯„ä¼°è´¨é‡
```

---

## ğŸ”‘ æœ¬ç« å…³é”®æ¦‚å¿µ

| æ¦‚å¿µ | è¯´æ˜ |
|------|------|
| åºåˆ—æ•°æ® | å‰åæœ‰ä¾èµ–å…³ç³»çš„æ•°æ® |
| RNN | å¾ªç¯ç¥ç»ç½‘ç»œï¼Œå¤„ç†åºåˆ— |
| éšè—çŠ¶æ€ | åºåˆ—ä¿¡æ¯çš„è½½ä½“ |
| BPTT | åå‘ä¼ æ’­ç©¿è¶Šæ—¶é—´ |
| æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸ | RNN çš„æ ¸å¿ƒé—®é¢˜ |
| LSTM | é•¿çŸ­æœŸè®°å¿†ç½‘ç»œ |
| é—¨æ§æœºåˆ¶ | æ§åˆ¶ä¿¡æ¯æµ |
| ç»†èƒçŠ¶æ€ | LSTM çš„é•¿æœŸè®°å¿† |
| GRU | ç®€åŒ–çš„ LSTM |
| åŒå‘ RNN | åŒæ—¶çœ‹å‰åæ–‡ |

---
