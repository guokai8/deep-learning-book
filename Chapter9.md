# ç¬¬ä¹ç« ï¼šè¿ç§»å­¦ä¹ ä¸å¾®è°ƒ (Transfer Learning & Fine-tuning)

## ğŸ“Œ ç« èŠ‚ç›®æ ‡
- ç†è§£è¿ç§»å­¦ä¹ çš„åŠ¨æœºå’ŒåŸç†
- æŒæ¡é¢„è®­ç»ƒæ¨¡å‹çš„ä½¿ç”¨æ–¹æ³•
- å­¦ä¹ ä¸åŒçš„å¾®è°ƒç­–ç•¥
- äº†è§£é¢†åŸŸè‡ªé€‚åº”æŠ€æœ¯
- å®æˆ˜ï¼šå›¾åƒåˆ†ç±»ã€æ–‡æœ¬åˆ†ç±»çš„è¿ç§»å­¦ä¹ 

---

## 9.1 ä¸ºä»€ä¹ˆéœ€è¦è¿ç§»å­¦ä¹ ï¼Ÿ

### ğŸš¨ ä»é›¶è®­ç»ƒçš„é—®é¢˜

**ä¼ ç»Ÿæ·±åº¦å­¦ä¹ **ï¼š

```
æ”¶é›†å¤§é‡æ•°æ® â†’ è®¾è®¡ç½‘ç»œ â†’ ä»é›¶è®­ç»ƒ â†’ éƒ¨ç½²

é—®é¢˜ï¼š
  âŒ éœ€è¦æµ·é‡æ ‡æ³¨æ•°æ®
  âŒ è®­ç»ƒæ—¶é—´é•¿ï¼ˆå‡ å¤©åˆ°å‡ å‘¨ï¼‰
  âŒ è®¡ç®—èµ„æºæ˜‚è´µ
  âŒ å®¹æ˜“è¿‡æ‹Ÿåˆï¼ˆå°æ•°æ®é›†ï¼‰
```

**ä¾‹å­**ï¼šè®­ç»ƒ ResNet-50 on ImageNet

```
æ•°æ®ï¼š120ä¸‡å¼ æ ‡æ³¨å›¾ç‰‡
æ—¶é—´ï¼š8ä¸ª GPUï¼Œå‡ å¤©åˆ°ä¸€å‘¨
æˆæœ¬ï¼šæ•°åƒç¾å…ƒ
```

### âœ… è¿ç§»å­¦ä¹ çš„ä¼˜åŠ¿

**æ ¸å¿ƒæ€æƒ³**ï¼šåˆ©ç”¨å·²æœ‰çŸ¥è¯†åŠ é€Ÿæ–°ä»»åŠ¡å­¦ä¹ 

```
é¢„è®­ç»ƒï¼ˆå¤§æ•°æ®é›†ï¼‰â†’ å¾®è°ƒï¼ˆå°æ•°æ®é›†ï¼‰â†’ éƒ¨ç½²

ä¼˜åŠ¿ï¼š
  âœ“ éœ€è¦æ›´å°‘çš„æ•°æ®
  âœ“ è®­ç»ƒæ›´å¿«ï¼ˆå°æ—¶çº§åˆ«ï¼‰
  âœ“ æ€§èƒ½æ›´å¥½ï¼ˆç‰¹åˆ«æ˜¯å°æ•°æ®é›†ï¼‰
  âœ“ é™ä½æˆæœ¬
```

### ğŸ§  ç›´è§‰ç†è§£

**äººç±»å­¦ä¹ çš„ç±»æ¯”**ï¼š

```
å­¦ä¹ è¯†åˆ«çŒ«ï¼š
  ä¸éœ€è¦ä»é›¶å­¦ä¹ "ä»€ä¹ˆæ˜¯è¾¹ç¼˜"ã€"ä»€ä¹ˆæ˜¯çº¹ç†"
  å·²ç»æœ‰è§†è§‰ç³»ç»Ÿçš„åŸºç¡€çŸ¥è¯†
  åªéœ€è¦å­¦ä¹ "çŒ«çš„ç‰¹å¾"

è¿ç§»å­¦ä¹ ï¼š
  é¢„è®­ç»ƒæ¨¡å‹ = å·²æœ‰çš„è§†è§‰/è¯­è¨€çŸ¥è¯†
  å¾®è°ƒ = é’ˆå¯¹ç‰¹å®šä»»åŠ¡è°ƒæ•´
```

---

## 9.2 è¿ç§»å­¦ä¹ çš„åˆ†ç±»

### ğŸ“Š æŒ‰ä»»åŠ¡å…³ç³»åˆ†ç±»

#### **1. å½’çº³è¿ç§» (Inductive Transfer)**

```
æºä»»åŠ¡ â‰  ç›®æ ‡ä»»åŠ¡ï¼Œä½†ç›¸å…³

ä¾‹ï¼š
  æºï¼šImageNet åˆ†ç±» (1000ç±»)
  ç›®æ ‡ï¼šåŒ»å­¦å›¾åƒåˆ†ç±» (5ç±»)
```

#### **2. è½¬å¯¼è¿ç§» (Transductive Transfer)**

```
æºä»»åŠ¡ = ç›®æ ‡ä»»åŠ¡ï¼Œä½†æ•°æ®åˆ†å¸ƒä¸åŒ

ä¾‹ï¼š
  æºï¼šç”µå½±è¯„è®ºæƒ…æ„Ÿåˆ†æ
  ç›®æ ‡ï¼šäº§å“è¯„è®ºæƒ…æ„Ÿåˆ†æ
```

#### **3. æ— ç›‘ç£è¿ç§» (Unsupervised Transfer)**

```
æºä»»åŠ¡å’Œç›®æ ‡ä»»åŠ¡éƒ½æ— æ ‡ç­¾

ä¾‹ï¼š
  èšç±»ã€é™ç»´ä»»åŠ¡
```

---

### ğŸ“Š æŒ‰è¿ç§»å†…å®¹åˆ†ç±»

#### **1. ç‰¹å¾è¿ç§» (Feature Transfer)**

```
è¿ç§»å­¦åˆ°çš„ç‰¹å¾è¡¨ç¤º

æ–¹æ³•ï¼šå›ºå®šé¢„è®­ç»ƒæ¨¡å‹ï¼Œåªè®­ç»ƒåˆ†ç±»å™¨
```

#### **2. å‚æ•°è¿ç§» (Parameter Transfer)**

```
è¿ç§»æ¨¡å‹å‚æ•°ä½œä¸ºåˆå§‹åŒ–

æ–¹æ³•ï¼šç”¨é¢„è®­ç»ƒæƒé‡åˆå§‹åŒ–ï¼Œç„¶åå¾®è°ƒ
```

#### **3. å…³ç³»è¿ç§» (Relation Transfer)**

```
è¿ç§»æ ·æœ¬é—´çš„å…³ç³»

ä¾‹ï¼šçŸ¥è¯†å›¾è°±ã€ç»“æ„åŒ–é¢„æµ‹
```

---

## 9.3 è®¡ç®—æœºè§†è§‰ä¸­çš„è¿ç§»å­¦ä¹ 

### ğŸ–¼ï¸ é¢„è®­ç»ƒæ¨¡å‹

**å¸¸ç”¨é¢„è®­ç»ƒæ¨¡å‹**ï¼ˆåœ¨ ImageNet ä¸Šè®­ç»ƒï¼‰ï¼š

```
è½»é‡çº§ï¼š
  - MobileNet (4M å‚æ•°)
  - EfficientNet-B0 (5M)

ä¸­ç­‰ï¼š
  - ResNet-50 (25M)
  - VGG-16 (138M)

å¤§å‹ï¼š
  - ResNet-152 (60M)
  - EfficientNet-B7 (66M)
  - Vision Transformer (ViT) (86M)
```

### ğŸ“ ç‰¹å¾æå– vs å¾®è°ƒ

#### **ç‰¹å¾æå– (Feature Extraction)**

```
å†»ç»“é¢„è®­ç»ƒæ¨¡å‹ â†’ åªè®­ç»ƒæ–°çš„åˆ†ç±»å™¨

é€‚ç”¨åœºæ™¯ï¼š
  âœ“ æ•°æ®é›†å¾ˆå° (< 1000 æ ·æœ¬)
  âœ“ ç›®æ ‡ä»»åŠ¡ä¸æºä»»åŠ¡ç›¸ä¼¼
  âœ“ è®¡ç®—èµ„æºæœ‰é™
```

**å®ç°**ï¼š

```python
import torch
import torch.nn as nn
from torchvision import models

# åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
model = models.resnet50(pretrained=True)

# å†»ç»“æ‰€æœ‰å‚æ•°
for param in model.parameters():
    param.requires_grad = False

# æ›¿æ¢æœ€åçš„å…¨è¿æ¥å±‚
num_features = model.fc.in_features
model.fc = nn.Linear(num_features, num_classes)  # åªæœ‰è¿™å±‚ä¼šè®­ç»ƒ

# åªä¼˜åŒ–æ–°æ·»åŠ çš„å±‚
optimizer = torch.optim.Adam(model.fc.parameters(), lr=0.001)
```

---

#### **å¾®è°ƒ (Fine-tuning)**

```
è§£å†»éƒ¨åˆ†æˆ–å…¨éƒ¨å±‚ â†’ åœ¨æ–°æ•°æ®ä¸Šè®­ç»ƒ

é€‚ç”¨åœºæ™¯ï¼š
  âœ“ æ•°æ®é›†ä¸­ç­‰å¤§å° (1k - 100k)
  âœ“ ç›®æ ‡ä»»åŠ¡ä¸æºä»»åŠ¡æœ‰å·®å¼‚
  âœ“ è¿½æ±‚æ›´å¥½æ€§èƒ½
```

**ç­–ç•¥**ï¼š

```
1. å…¨å±€å¾®è°ƒï¼š
   è§£å†»æ‰€æœ‰å±‚ï¼Œç”¨å°å­¦ä¹ ç‡è®­ç»ƒ

2. é€å±‚å¾®è°ƒï¼š
   å…ˆè®­ç»ƒé¡¶å±‚ï¼Œé€æ¸è§£å†»åº•å±‚

3. åˆ¤åˆ«å¼å¾®è°ƒï¼š
   ä¸åŒå±‚ç”¨ä¸åŒå­¦ä¹ ç‡
```

**å®ç°**ï¼š

```python
# æ–¹æ³•1ï¼šå…¨å±€å¾®è°ƒï¼ˆå°å­¦ä¹ ç‡ï¼‰
model = models.resnet50(pretrained=True)

# æ›¿æ¢åˆ†ç±»å™¨
model.fc = nn.Linear(model.fc.in_features, num_classes)

# æ‰€æœ‰å‚æ•°éƒ½è®­ç»ƒï¼Œä½†ç”¨å°å­¦ä¹ ç‡
optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)
```

```python
# æ–¹æ³•2ï¼šé€å±‚å¾®è°ƒ
model = models.resnet50(pretrained=True)
model.fc = nn.Linear(model.fc.in_features, num_classes)

# ç¬¬ä¸€é˜¶æ®µï¼šåªè®­ç»ƒåˆ†ç±»å™¨
for param in model.parameters():
    param.requires_grad = False
for param in model.fc.parameters():
    param.requires_grad = True

optimizer = torch.optim.Adam(model.fc.parameters(), lr=0.001)
# è®­ç»ƒå‡ ä¸ª epoch...

# ç¬¬äºŒé˜¶æ®µï¼šè§£å†» layer4
for param in model.layer4.parameters():
    param.requires_grad = True

optimizer = torch.optim.Adam([
    {'params': model.fc.parameters(), 'lr': 0.001},
    {'params': model.layer4.parameters(), 'lr': 0.0001}
])
# ç»§ç»­è®­ç»ƒ...
```

```python
# æ–¹æ³•3ï¼šåˆ¤åˆ«å¼å­¦ä¹ ç‡
model = models.resnet50(pretrained=True)
model.fc = nn.Linear(model.fc.in_features, num_classes)

# ä¸åŒå±‚ç»„ä½¿ç”¨ä¸åŒå­¦ä¹ ç‡
optimizer = torch.optim.Adam([
    {'params': model.conv1.parameters(), 'lr': 1e-5},
    {'params': model.layer1.parameters(), 'lr': 1e-5},
    {'params': model.layer2.parameters(), 'lr': 1e-4},
    {'params': model.layer3.parameters(), 'lr': 1e-4},
    {'params': model.layer4.parameters(), 'lr': 1e-3},
    {'params': model.fc.parameters(), 'lr': 1e-2}
])
```

---

### ğŸ¯ å®æˆ˜ï¼šçŒ«ç‹—åˆ†ç±»ï¼ˆè¿ç§»å­¦ä¹ ï¼‰

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import datasets, transforms, models
import matplotlib.pyplot as plt
from tqdm import tqdm
import os

# ==================== è¶…å‚æ•° ====================
DATA_DIR = './data/cats_and_dogs'
BATCH_SIZE = 32
EPOCHS = 10
LEARNING_RATE = 0.001
NUM_CLASSES = 2
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# ==================== æ•°æ®å¢å¼º ====================
data_transforms = {
    'train': transforms.Compose([
        transforms.RandomResizedCrop(224),
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation(15),
        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]),
    'val': transforms.Compose([
        transforms.Resize(256),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ])
}

# ==================== æ•°æ®åŠ è½½ ====================
image_datasets = {
    x: datasets.ImageFolder(os.path.join(DATA_DIR, x), data_transforms[x])
    for x in ['train', 'val']
}

dataloaders = {
    x: DataLoader(image_datasets[x], batch_size=BATCH_SIZE,
                  shuffle=(x=='train'), num_workers=4)
    for x in ['train', 'val']
}

dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}
class_names = image_datasets['train'].classes

print(f"è®­ç»ƒé›†: {dataset_sizes['train']} å¼ ")
print(f"éªŒè¯é›†: {dataset_sizes['val']} å¼ ")
print(f"ç±»åˆ«: {class_names}")

# ==================== æ¨¡å‹å®šä¹‰ ====================

def create_model(model_name='resnet50', num_classes=2, feature_extract=False):
    """
    åˆ›å»ºè¿ç§»å­¦ä¹ æ¨¡å‹

    å‚æ•°:
        model_name: é¢„è®­ç»ƒæ¨¡å‹åç§°
        num_classes: è¾“å‡ºç±»åˆ«æ•°
        feature_extract: True=ç‰¹å¾æå–, False=å¾®è°ƒ
    """
    model = None

    if model_name == 'resnet50':
        model = models.resnet50(pretrained=True)

        # ç‰¹å¾æå–æ¨¡å¼ï¼šå†»ç»“å‚æ•°
        if feature_extract:
            for param in model.parameters():
                param.requires_grad = False

        # æ›¿æ¢åˆ†ç±»å™¨
        num_features = model.fc.in_features
        model.fc = nn.Sequential(
            nn.Dropout(0.5),
            nn.Linear(num_features, num_classes)
        )

    elif model_name == 'efficientnet_b0':
        model = models.efficientnet_b0(pretrained=True)

        if feature_extract:
            for param in model.parameters():
                param.requires_grad = False

        num_features = model.classifier[1].in_features
        model.classifier = nn.Sequential(
            nn.Dropout(0.3),
            nn.Linear(num_features, num_classes)
        )

    elif model_name == 'vgg16':
        model = models.vgg16(pretrained=True)

        if feature_extract:
            for param in model.features.parameters():
                param.requires_grad = False

        num_features = model.classifier[6].in_features
        model.classifier[6] = nn.Linear(num_features, num_classes)

    return model

# ==================== è®­ç»ƒå‡½æ•° ====================

def train_model(model, criterion, optimizer, scheduler, num_epochs):
    """è®­ç»ƒæ¨¡å‹"""
    best_acc = 0.0
    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}

    for epoch in range(num_epochs):
        print(f'\nEpoch {epoch+1}/{num_epochs}')
        print('-' * 60)

        # è®­ç»ƒå’ŒéªŒè¯
        for phase in ['train', 'val']:
            if phase == 'train':
                model.train()
            else:
                model.eval()

            running_loss = 0.0
            running_corrects = 0

            # è¿›åº¦æ¡
            pbar = tqdm(dataloaders[phase], desc=f'{phase.capitalize()}')

            for inputs, labels in pbar:
                inputs = inputs.to(DEVICE)
                labels = labels.to(DEVICE)

                optimizer.zero_grad()

                # å‰å‘ä¼ æ’­
                with torch.set_grad_enabled(phase == 'train'):
                    outputs = model(inputs)
                    _, preds = torch.max(outputs, 1)
                    loss = criterion(outputs, labels)

                    # åå‘ä¼ æ’­
                    if phase == 'train':
                        loss.backward()
                        optimizer.step()

                # ç»Ÿè®¡
                running_loss += loss.item() * inputs.size(0)
                running_corrects += torch.sum(preds == labels.data)

                # æ›´æ–°è¿›åº¦æ¡
                pbar.set_postfix({
                    'loss': f'{loss.item():.4f}',
                    'acc': f'{torch.sum(preds == labels.data).item() / len(labels):.4f}'
                })

            # Epoch ç»Ÿè®¡
            epoch_loss = running_loss / dataset_sizes[phase]
            epoch_acc = running_corrects.double() / dataset_sizes[phase]

            print(f'{phase.capitalize()} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')

            # è®°å½•å†å²
            history[f'{phase}_loss'].append(epoch_loss)
            history[f'{phase}_acc'].append(epoch_acc.item())

            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if phase == 'val' and epoch_acc > best_acc:
                best_acc = epoch_acc
                torch.save(model.state_dict(), 'best_model.pth')
                print(f'âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Acc: {best_acc:.4f})')

        # å­¦ä¹ ç‡è°ƒåº¦
        scheduler.step()

    print(f'\næœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_acc:.4f}')

    # åŠ è½½æœ€ä½³æ¨¡å‹
    model.load_state_dict(torch.load('best_model.pth'))

    return model, history

# ==================== å¯¹æ¯”å®éªŒ ====================

def compare_strategies():
    """å¯¹æ¯”ä¸åŒè¿ç§»å­¦ä¹ ç­–ç•¥"""
    strategies = {
        'Feature Extraction': {
            'model': create_model('resnet50', NUM_CLASSES, feature_extract=True),
            'lr': 0.001,
            'color': 'blue'
        },
        'Fine-tuning (å…¨å±€)': {
            'model': create_model('resnet50', NUM_CLASSES, feature_extract=False),
            'lr': 0.0001,
            'color': 'red'
        }
    }

    results = {}

    for strategy_name, config in strategies.items():
        print(f'\n{"="*70}')
        print(f'ç­–ç•¥: {strategy_name}')
        print(f'{"="*70}')

        model = config['model'].to(DEVICE)

        # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
        criterion = nn.CrossEntropyLoss()
        optimizer = optim.Adam(
            filter(lambda p: p.requires_grad, model.parameters()),
            lr=config['lr']
        )
        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)

        # è®­ç»ƒ
        model, history = train_model(model, criterion, optimizer, scheduler, EPOCHS)

        results[strategy_name] = {
            'model': model,
            'history': history,
            'color': config['color']
        }

    # ==================== å¯è§†åŒ–å¯¹æ¯” ====================

    fig, axes = plt.subplots(1, 2, figsize=(15, 5))

    # Loss å¯¹æ¯”
    for strategy_name, data in results.items():
        epochs_range = range(1, len(data['history']['train_loss']) + 1)
        axes[0].plot(epochs_range, data['history']['train_loss'],
                    label=f'{strategy_name} (Train)',
                    linestyle='--', color=data['color'])
        axes[0].plot(epochs_range, data['history']['val_loss'],
                    label=f'{strategy_name} (Val)',
                    linestyle='-', color=data['color'])

    axes[0].set_xlabel('Epoch')
    axes[0].set_ylabel('Loss')
    axes[0].set_title('Loss Comparison')
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)

    # Accuracy å¯¹æ¯”
    for strategy_name, data in results.items():
        epochs_range = range(1, len(data['history']['train_acc']) + 1)
        axes[1].plot(epochs_range, data['history']['train_acc'],
                    label=f'{strategy_name} (Train)',
                    linestyle='--', color=data['color'])
        axes[1].plot(epochs_range, data['history']['val_acc'],
                    label=f'{strategy_name} (Val)',
                    linestyle='-', color=data['color'])

    axes[1].set_xlabel('Epoch')
    axes[1].set_ylabel('Accuracy')
    axes[1].set_title('Accuracy Comparison')
    axes[1].legend()
    axes[1].grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig('transfer_learning_comparison.png', dpi=300)
    plt.show()

    return results

# ==================== å¯è§†åŒ–é¢„æµ‹ ====================

def visualize_predictions(model, num_images=16):
    """å¯è§†åŒ–æ¨¡å‹é¢„æµ‹"""
    model.eval()

    images_so_far = 0
    fig = plt.figure(figsize=(16, 12))

    with torch.no_grad():
        for inputs, labels in dataloaders['val']:
            inputs = inputs.to(DEVICE)
            labels = labels.to(DEVICE)

            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)

            for j in range(inputs.size()[0]):
                images_so_far += 1
                ax = plt.subplot(4, 4, images_so_far)
                ax.axis('off')

                # åå½’ä¸€åŒ–æ˜¾ç¤º
                img = inputs.cpu().data[j]
                img = img.numpy().transpose((1, 2, 0))
                mean = np.array([0.485, 0.456, 0.406])
                std = np.array([0.229, 0.224, 0.225])
                img = std * img + mean
                img = np.clip(img, 0, 1)

                ax.imshow(img)

                # æ ‡é¢˜ï¼šé¢„æµ‹ vs çœŸå®
                color = 'green' if preds[j] == labels[j] else 'red'
                ax.set_title(f'Pred: {class_names[preds[j]]}\nTrue: {class_names[labels[j]]}',
                           color=color, fontsize=10)

                if images_so_far == num_images:
                    plt.tight_layout()
                    plt.savefig('predictions.png', dpi=300)
                    plt.show()
                    return

# ==================== ä¸»ç¨‹åº ====================

if __name__ == '__main__':
    # å¯¹æ¯”ä¸åŒç­–ç•¥
    results = compare_strategies()

    # å¯è§†åŒ–æœ€ä½³æ¨¡å‹çš„é¢„æµ‹
    best_strategy = max(results.items(),
                       key=lambda x: max(x[1]['history']['val_acc']))
    print(f'\næœ€ä½³ç­–ç•¥: {best_strategy[0]}')

    visualize_predictions(best_strategy[1]['model'])
```

---

## 9.4 è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„è¿ç§»å­¦ä¹ 

### ğŸ“ é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹

**å‘å±•å†ç¨‹**ï¼š

```
2013: Word2Vec, GloVe
      â†“
2018: ELMo (åŠ¨æ€è¯å‘é‡)
      â†“
2018: BERT (åŒå‘é¢„è®­ç»ƒ)
      â†“
2019: GPT-2 (å¤§è§„æ¨¡ç”Ÿæˆ)
      â†“
2020: GPT-3 (è¶…å¤§è§„æ¨¡)
      â†“
2023: ChatGPT, GPT-4
```

### ğŸ”¹ ä½¿ç”¨ Hugging Face Transformers

```python
from transformers import (
    BertTokenizer, BertForSequenceClassification,
    GPT2Tokenizer, GPT2LMHeadModel,
    AutoTokenizer, AutoModelForSequenceClassification,
    Trainer, TrainingArguments
)
import torch
from torch.utils.data import Dataset
import numpy as np

# ==================== æ•°æ®é›†å®šä¹‰ ====================

class TextClassificationDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_length=128):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts[idx]
        label = self.labels[idx]

        encoding = self.tokenizer(
            text,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )

        return {
            'input_ids': encoding['input_ids'].squeeze(),
            'attention_mask': encoding['attention_mask'].squeeze(),
            'labels': torch.tensor(label, dtype=torch.long)
        }

# ==================== BERT å¾®è°ƒ ====================

def fine_tune_bert(train_texts, train_labels, val_texts, val_labels,
                   num_labels=2, epochs=3):
    """
    BERT å¾®è°ƒç”¨äºæ–‡æœ¬åˆ†ç±»
    """
    # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å’Œåˆ†è¯å™¨
    model_name = 'bert-base-uncased'
    tokenizer = BertTokenizer.from_pretrained(model_name)
    model = BertForSequenceClassification.from_pretrained(
        model_name,
        num_labels=num_labels
    )

    # åˆ›å»ºæ•°æ®é›†
    train_dataset = TextClassificationDataset(
        train_texts, train_labels, tokenizer
    )
    val_dataset = TextClassificationDataset(
        val_texts, val_labels, tokenizer
    )

    # è®­ç»ƒå‚æ•°
    training_args = TrainingArguments(
        output_dir='./results',
        num_train_epochs=epochs,
        per_device_train_batch_size=16,
        per_device_eval_batch_size=64,
        warmup_steps=500,
        weight_decay=0.01,
        logging_dir='./logs',
        logging_steps=100,
        evaluation_strategy='epoch',
        save_strategy='epoch',
        load_best_model_at_end=True,
        learning_rate=2e-5,
    )

    # å®šä¹‰è¯„ä¼°æŒ‡æ ‡
    def compute_metrics(eval_pred):
        predictions, labels = eval_pred
        predictions = np.argmax(predictions, axis=1)
        accuracy = (predictions == labels).mean()
        return {'accuracy': accuracy}

    # è®­ç»ƒå™¨
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        compute_metrics=compute_metrics
    )

    # è®­ç»ƒ
    trainer.train()

    # è¯„ä¼°
    eval_results = trainer.evaluate()
    print(f"\nè¯„ä¼°ç»“æœ: {eval_results}")

    return model, tokenizer

# ==================== å®æˆ˜ç¤ºä¾‹ï¼šæƒ…æ„Ÿåˆ†æ ====================

def sentiment_analysis_example():
    """æƒ…æ„Ÿåˆ†æå®Œæ•´ç¤ºä¾‹"""

    # ç¤ºä¾‹æ•°æ®ï¼ˆå®é™…åº”ä½¿ç”¨ IMDB ç­‰æ•°æ®é›†ï¼‰
    train_texts = [
        "This movie is fantastic! I loved it.",
        "Great film, highly recommended.",
        "Amazing performance by the actors.",
        "Terrible waste of time.",
        "Boring and predictable plot.",
        "I hated every minute of it."
    ] * 100

    train_labels = [1, 1, 1, 0, 0, 0] * 100  # 1=æ­£é¢, 0=è´Ÿé¢

    val_texts = [
        "Excellent movie!",
        "Not worth watching.",
        "Pretty good film.",
        "Absolutely awful."
    ]
    val_labels = [1, 0, 1, 0]

    # å¾®è°ƒ BERT
    model, tokenizer = fine_tune_bert(
        train_texts, train_labels,
        val_texts, val_labels,
        num_labels=2,
        epochs=3
    )

    # é¢„æµ‹å‡½æ•°
    def predict(text):
        encoding = tokenizer(
            text,
            max_length=128,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )

        model.eval()
        with torch.no_grad():
            outputs = model(**encoding)
            predictions = torch.softmax(outputs.logits, dim=1)
            label = torch.argmax(predictions, dim=1).item()
            confidence = predictions[0][label].item()

        sentiment = "æ­£é¢" if label == 1 else "è´Ÿé¢"
        return sentiment, confidence

    # æµ‹è¯•
    test_texts = [
        "This is an amazing movie!",
        "Worst film I've ever seen.",
        "It was okay, nothing special."
    ]

    print("\né¢„æµ‹ç»“æœ:")
    for text in test_texts:
        sentiment, confidence = predict(text)
        print(f"\næ–‡æœ¬: {text}")
        print(f"æƒ…æ„Ÿ: {sentiment} (ç½®ä¿¡åº¦: {confidence:.4f})")

    return model, tokenizer

# è¿è¡Œç¤ºä¾‹
if __name__ == '__main__':
    model, tokenizer = sentiment_analysis_example()
```

---

## 9.5 é¢†åŸŸè‡ªé€‚åº” (Domain Adaptation)

### ğŸ¯ é—®é¢˜è®¾å®š

```
æºé¢†åŸŸï¼šæœ‰å¤§é‡æ ‡æ³¨æ•°æ®
ç›®æ ‡é¢†åŸŸï¼šæ•°æ®åˆ†å¸ƒä¸åŒï¼Œæ ‡æ³¨å°‘æˆ–æ— 

ä¾‹å­ï¼š
  æºï¼šæ–°é—»æ–‡æœ¬åˆ†ç±»
  ç›®æ ‡ï¼šç¤¾äº¤åª’ä½“æ–‡æœ¬åˆ†ç±»
```

### ğŸ“ æ–¹æ³•åˆ†ç±»

#### **1. åŸºäºå®ä¾‹çš„æ–¹æ³•**

**é‡è¦æ€§åŠ æƒ**ï¼š

```python
class ImportanceWeightedLoss(nn.Module):
    """æ ¹æ®æ ·æœ¬ç›¸ä¼¼åº¦åŠ æƒæŸå¤±"""

    def __init__(self, base_criterion):
        super().__init__()
        self.base_criterion = base_criterion

    def compute_weights(self, source_features, target_features):
        """
        è®¡ç®—æºåŸŸæ ·æœ¬çš„é‡è¦æ€§æƒé‡
        ä½¿å¾—æºåŸŸåˆ†å¸ƒæ¥è¿‘ç›®æ ‡åŸŸ
        """
        # ç®€åŒ–ç‰ˆï¼šåŸºäºç‰¹å¾è·ç¦»
        distances = torch.cdist(source_features, target_features)
        min_distances = distances.min(dim=1)[0]
        weights = torch.exp(-min_distances)
        weights = weights / weights.sum()
        return weights

    def forward(self, outputs, targets, weights):
        loss = self.base_criterion(outputs, targets)
        weighted_loss = (loss * weights).mean()
        return weighted_loss
```

---

#### **2. åŸºäºç‰¹å¾çš„æ–¹æ³•**

**é¢†åŸŸå¯¹æŠ—è®­ç»ƒ (Domain Adversarial Training)**ï¼š

```python
class GradientReversalLayer(torch.autograd.Function):
    """æ¢¯åº¦åè½¬å±‚"""

    @staticmethod
    def forward(ctx, x, alpha):
        ctx.alpha = alpha
        return x.view_as(x)

    @staticmethod
    def backward(ctx, grad_output):
        output = grad_output.neg() * ctx.alpha
        return output, None

class DomainAdversarialNetwork(nn.Module):
    """é¢†åŸŸå¯¹æŠ—ç½‘ç»œ"""

    def __init__(self, input_size, hidden_size, num_classes):
        super().__init__()

        # ç‰¹å¾æå–å™¨ï¼ˆå…±äº«ï¼‰
        self.feature_extractor = nn.Sequential(
            nn.Linear(input_size, hidden_size),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU()
        )

        # æ ‡ç­¾é¢„æµ‹å™¨
        self.label_predictor = nn.Sequential(
            nn.Dropout(0.5),
            nn.Linear(hidden_size, num_classes)
        )

        # åŸŸåˆ†ç±»å™¨
        self.domain_classifier = nn.Sequential(
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(hidden_size, 2)  # 2ä¸ªåŸŸ
        )

    def forward(self, x, alpha=1.0):
        # ç‰¹å¾æå–
        features = self.feature_extractor(x)

        # æ ‡ç­¾é¢„æµ‹
        class_output = self.label_predictor(features)

        # åŸŸåˆ†ç±»ï¼ˆæ¢¯åº¦åè½¬ï¼‰
        reverse_features = GradientReversalLayer.apply(features, alpha)
        domain_output = self.domain_classifier(reverse_features)

        return class_output, domain_output

# è®­ç»ƒ
def train_domain_adaptation(model, source_loader, target_loader,
                            num_epochs=50):
    """é¢†åŸŸè‡ªé€‚åº”è®­ç»ƒ"""

    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    class_criterion = nn.CrossEntropyLoss()
    domain_criterion = nn.CrossEntropyLoss()

    for epoch in range(num_epochs):
        model.train()

        for (source_data, source_labels), (target_data, _) in zip(source_loader, target_loader):

            # Alpha éšç€è®­ç»ƒå¢åŠ ï¼ˆä» 0 åˆ° 1ï¼‰
            p = float(epoch) / num_epochs
            alpha = 2. / (1. + np.exp(-10 * p)) - 1

            # æºåŸŸæ•°æ®
            class_output_s, domain_output_s = model(source_data, alpha)

            # ç›®æ ‡åŸŸæ•°æ®
            _, domain_output_t = model(target_data, alpha)

            # åŸŸæ ‡ç­¾ï¼šæº=0ï¼Œç›®æ ‡=1
            domain_label_s = torch.zeros(len(source_data)).long()
            domain_label_t = torch.ones(len(target_data)).long()

            # è®¡ç®—æŸå¤±
            class_loss = class_criterion(class_output_s, source_labels)
            domain_loss_s = domain_criterion(domain_output_s, domain_label_s)
            domain_loss_t = domain_criterion(domain_output_t, domain_label_t)
            domain_loss = domain_loss_s + domain_loss_t

            total_loss = class_loss + domain_loss

            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            total_loss.backward()
            optimizer.step()

            if epoch % 10 == 0:
                print(f'Epoch {epoch}: Class Loss={class_loss.item():.4f}, '
                      f'Domain Loss={domain_loss.item():.4f}')

    return model
```

---

#### **3. è‡ªè®­ç»ƒ (Self-Training)**

```python
class SelfTraining:
    """è‡ªè®­ç»ƒ/ä¼ªæ ‡ç­¾æ–¹æ³•"""

    def __init__(self, model, confidence_threshold=0.9):
        self.model = model
        self.threshold = confidence_threshold

    def generate_pseudo_labels(self, unlabeled_loader):
        """ç”Ÿæˆä¼ªæ ‡ç­¾"""
        self.model.eval()

        pseudo_data = []
        pseudo_labels = []

        with torch.no_grad():
            for data in unlabeled_loader:
                outputs = self.model(data)
                probs = torch.softmax(outputs, dim=1)

                # åªé€‰æ‹©é«˜ç½®ä¿¡åº¦æ ·æœ¬
                max_probs, predictions = torch.max(probs, dim=1)

                mask = max_probs > self.threshold

                pseudo_data.append(data[mask])
                pseudo_labels.append(predictions[mask])

        return torch.cat(pseudo_data), torch.cat(pseudo_labels)

    def train(self, labeled_loader, unlabeled_loader, num_iterations=5):
        """è¿­ä»£è®­ç»ƒ"""

        optimizer = torch.optim.Adam(self.model.parameters(), lr=0.001)
        criterion = nn.CrossEntropyLoss()

        for iteration in range(num_iterations):
            print(f'\n=== Iteration {iteration+1}/{num_iterations} ===')

            # åœ¨æ ‡æ³¨æ•°æ®ä¸Šè®­ç»ƒ
            self.model.train()
            for data, labels in labeled_loader:
                outputs = self.model(data)
                loss = criterion(outputs, labels)

                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

            # ç”Ÿæˆä¼ªæ ‡ç­¾
            pseudo_data, pseudo_labels = self.generate_pseudo_labels(unlabeled_loader)

            if len(pseudo_data) > 0:
                print(f'ç”Ÿæˆäº† {len(pseudo_data)} ä¸ªä¼ªæ ‡ç­¾')

                # åœ¨ä¼ªæ ‡ç­¾æ•°æ®ä¸Šè®­ç»ƒ
                pseudo_dataset = torch.utils.data.TensorDataset(
                    pseudo_data, pseudo_labels
                )
                pseudo_loader = torch.utils.data.DataLoader(
                    pseudo_dataset, batch_size=32, shuffle=True
                )

                self.model.train()
                for data, labels in pseudo_loader:
                    outputs = self.model(data)
                    loss = criterion(outputs, labels)

                    optimizer.zero_grad()
                    loss.backward()
                    optimizer.step()

        return self.model
```

---

## 9.6 å°‘æ ·æœ¬å­¦ä¹  (Few-Shot Learning)

### ğŸ¯ é—®é¢˜å®šä¹‰

```
N-way K-shot åˆ†ç±»ï¼š
  N: ç±»åˆ«æ•°
  K: æ¯ç±»çš„æ ·æœ¬æ•°

ä¾‹ï¼š5-way 1-shot
  5ä¸ªç±»åˆ«ï¼Œæ¯ç±»åªæœ‰1ä¸ªæ ·æœ¬
```

### ğŸ“ å…ƒå­¦ä¹  (Meta-Learning)

**MAML (Model-Agnostic Meta-Learning)**ï¼š

```python
class MAML:
    """æ¨¡å‹æ— å…³çš„å…ƒå­¦ä¹ """

    def __init__(self, model, inner_lr=0.01, outer_lr=0.001):
        self.model = model
        self.inner_lr = inner_lr  # ä»»åŠ¡å†…å­¦ä¹ ç‡
        self.outer_lr = outer_lr  # è·¨ä»»åŠ¡å­¦ä¹ ç‡
        self.meta_optimizer = torch.optim.Adam(
            model.parameters(), lr=outer_lr
        )

    def inner_loop(self, support_x, support_y, num_steps=5):
        """
        ä»»åŠ¡å†…é€‚åº”ï¼ˆå¿«é€Ÿå­¦ä¹ ï¼‰

        å‚æ•°ï¼š
            support_x: æ”¯æŒé›†è¾“å…¥
            support_y: æ”¯æŒé›†æ ‡ç­¾
            num_steps: å†…å¾ªç¯æ­¥æ•°
        """
        # å¤åˆ¶æ¨¡å‹å‚æ•°
        fast_weights = [p.clone() for p in self.model.parameters()]

        for step in range(num_steps):
            # å‰å‘ä¼ æ’­
            outputs = self.model(support_x)
            loss = nn.CrossEntropyLoss()(outputs, support_y)

            # è®¡ç®—æ¢¯åº¦
            grads = torch.autograd.grad(
                loss, fast_weights, create_graph=True
            )

            # æ›´æ–°å‚æ•°ï¼ˆä¸€æ­¥æ¢¯åº¦ä¸‹é™ï¼‰
            fast_weights = [
                w - self.inner_lr * g
                for w, g in zip(fast_weights, grads)
            ]

        return fast_weights

    def outer_loop(self, tasks, num_epochs=1000):
        """
        è·¨ä»»åŠ¡å­¦ä¹ ï¼ˆå…ƒå­¦ä¹ ï¼‰

        å‚æ•°ï¼š
            tasks: ä»»åŠ¡åˆ—è¡¨ï¼Œæ¯ä¸ªä»»åŠ¡åŒ…å« (support_set, query_set)
        """
        for epoch in range(num_epochs):
            meta_loss = 0

            for task in tasks:
                support_x, support_y = task['support']
                query_x, query_y = task['query']

                # å†…å¾ªç¯ï¼šåœ¨æ”¯æŒé›†ä¸Šå¿«é€Ÿé€‚åº”
                fast_weights = self.inner_loop(support_x, support_y)

                # åœ¨æŸ¥è¯¢é›†ä¸Šè¯„ä¼°
                # ä½¿ç”¨æ›´æ–°åçš„å‚æ•°
                outputs = self.model(query_x, weights=fast_weights)
                loss = nn.CrossEntropyLoss()(outputs, query_y)

                meta_loss += loss

            # å¤–å¾ªç¯ï¼šæ›´æ–°å…ƒå‚æ•°
            meta_loss = meta_loss / len(tasks)

            self.meta_optimizer.zero_grad()
            meta_loss.backward()
            self.meta_optimizer.step()

            if epoch % 100 == 0:
                print(f'Epoch {epoch}: Meta Loss = {meta_loss.item():.4f}')
```

---

### ğŸ“ åŸå‹ç½‘ç»œ (Prototypical Networks)

```python
class PrototypicalNetwork(nn.Module):
    """åŸå‹ç½‘ç»œ"""

    def __init__(self, encoder):
        super().__init__()
        self.encoder = encoder

    def compute_prototypes(self, support_x, support_y, n_way):
        """
        è®¡ç®—æ¯ä¸ªç±»çš„åŸå‹ï¼ˆå‡å€¼ï¼‰

        å‚æ•°ï¼š
            support_x: (n_way * n_support, *)
            support_y: (n_way * n_support,)
            n_way: ç±»åˆ«æ•°
        """
        # ç¼–ç 
        embeddings = self.encoder(support_x)  # (n_way*n_support, d)

        # è®¡ç®—åŸå‹
        prototypes = []
        for c in range(n_way):
            mask = (support_y == c)
            class_embeddings = embeddings[mask]
            prototype = class_embeddings.mean(dim=0)
            prototypes.append(prototype)

        prototypes = torch.stack(prototypes)  # (n_way, d)
        return prototypes

    def forward(self, support_x, support_y, query_x, n_way):
        """
        å‚æ•°ï¼š
            support_x: æ”¯æŒé›†è¾“å…¥
            support_y: æ”¯æŒé›†æ ‡ç­¾
            query_x: æŸ¥è¯¢é›†è¾“å…¥
            n_way: ç±»åˆ«æ•°
        """
        # è®¡ç®—åŸå‹
        prototypes = self.compute_prototypes(support_x, support_y, n_way)

        # ç¼–ç æŸ¥è¯¢é›†
        query_embeddings = self.encoder(query_x)  # (n_query, d)

        # è®¡ç®—è·ç¦»ï¼ˆæ¬§æ°è·ç¦»ï¼‰
        distances = torch.cdist(query_embeddings, prototypes)  # (n_query, n_way)

        # è½¬æ¢ä¸ºæ¦‚ç‡ï¼ˆè´Ÿè·ç¦»çš„ softmaxï¼‰
        logits = -distances

        return logits

# è®­ç»ƒ
def train_prototypical_network(model, tasks, num_epochs=1000):
    """è®­ç»ƒåŸå‹ç½‘ç»œ"""

    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    criterion = nn.CrossEntropyLoss()

    for epoch in range(num_epochs):
        total_loss = 0
        total_acc = 0

        for task in tasks:
            support_x, support_y = task['support']
            query_x, query_y = task['query']
            n_way = len(torch.unique(support_y))

            # å‰å‘ä¼ æ’­
            logits = model(support_x, support_y, query_x, n_way)

            # è®¡ç®—æŸå¤±
            loss = criterion(logits, query_y)

            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            # ç»Ÿè®¡
            total_loss += loss.item()
            preds = torch.argmax(logits, dim=1)
            total_acc += (preds == query_y).float().mean().item()

        if epoch % 100 == 0:
            avg_loss = total_loss / len(tasks)
            avg_acc = total_acc / len(tasks)
            print(f'Epoch {epoch}: Loss={avg_loss:.4f}, Acc={avg_acc:.4f}')

    return model
```

---

## 9.7 çŸ¥è¯†è’¸é¦ (Knowledge Distillation)

### ğŸ¯ æ ¸å¿ƒæ€æƒ³

```
æ•™å¸ˆæ¨¡å‹ï¼ˆå¤§ï¼‰â†’ çŸ¥è¯† â†’ å­¦ç”Ÿæ¨¡å‹ï¼ˆå°ï¼‰

ç›®æ ‡ï¼š
  ç”¨å¤§æ¨¡å‹çš„"è½¯æ ‡ç­¾"è®­ç»ƒå°æ¨¡å‹
  å°æ¨¡å‹è·å¾—å¤§æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›
```

### ğŸ“ æ¸©åº¦ Softmax

```
æ ‡å‡† Softmaxï¼š
  p_i = exp(z_i) / Î£_j exp(z_j)

æ¸©åº¦ Softmaxï¼š
  p_i = exp(z_i/T) / Î£_j exp(z_j/T)

T > 1: è¾“å‡ºæ›´å¹³æ»‘ï¼ˆ"è½¯"æ ‡ç­¾ï¼‰
T = 1: æ ‡å‡† softmax
T â†’ âˆ: å‡åŒ€åˆ†å¸ƒ
```

### ğŸ’» å®ç°

```python
class DistillationLoss(nn.Module):
    """çŸ¥è¯†è’¸é¦æŸå¤±"""

    def __init__(self, temperature=3.0, alpha=0.5):
        super().__init__()
        self.temperature = temperature
        self.alpha = alpha
        self.criterion = nn.CrossEntropyLoss()
        self.kl_div = nn.KLDivLoss(reduction='batchmean')

    def forward(self, student_logits, teacher_logits, targets):
        """
        å‚æ•°ï¼š
            student_logits: å­¦ç”Ÿæ¨¡å‹è¾“å‡º
            teacher_logits: æ•™å¸ˆæ¨¡å‹è¾“å‡º
            targets: çœŸå®æ ‡ç­¾
        """
        # ç¡¬æ ‡ç­¾æŸå¤±
        hard_loss = self.criterion(student_logits, targets)

        # è½¯æ ‡ç­¾æŸå¤±ï¼ˆKL æ•£åº¦ï¼‰
        student_soft = F.log_softmax(student_logits / self.temperature, dim=1)
        teacher_soft = F.softmax(teacher_logits / self.temperature, dim=1)
        soft_loss = self.kl_div(student_soft, teacher_soft) * (self.temperature ** 2)

        # ç»„åˆæŸå¤±
        total_loss = self.alpha * hard_loss + (1 - self.alpha) * soft_loss

        return total_loss

def knowledge_distillation(teacher_model, student_model, train_loader,
                           num_epochs=50, temperature=3.0):
    """
    çŸ¥è¯†è’¸é¦è®­ç»ƒ

    å‚æ•°ï¼š
        teacher_model: é¢„è®­ç»ƒçš„å¤§æ¨¡å‹
        student_model: å¾…è®­ç»ƒçš„å°æ¨¡å‹
        train_loader: æ•°æ®åŠ è½½å™¨
        num_epochs: è®­ç»ƒè½®æ•°
        temperature: æ¸©åº¦å‚æ•°
    """
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    teacher_model.to(device)
    student_model.to(device)

    teacher_model.eval()  # æ•™å¸ˆæ¨¡å‹ä¸è®­ç»ƒ

    optimizer = torch.optim.Adam(student_model.parameters(), lr=0.001)
    distillation_loss = DistillationLoss(temperature=temperature, alpha=0.7)

    for epoch in range(num_epochs):
        student_model.train()

        total_loss = 0
        correct = 0
        total = 0

        for inputs, targets in train_loader:
            inputs, targets = inputs.to(device), targets.to(device)

            # æ•™å¸ˆæ¨¡å‹è¾“å‡ºï¼ˆä¸éœ€è¦æ¢¯åº¦ï¼‰
            with torch.no_grad():
                teacher_logits = teacher_model(inputs)

            # å­¦ç”Ÿæ¨¡å‹è¾“å‡º
            student_logits = student_model(inputs)

            # è®¡ç®—è’¸é¦æŸå¤±
            loss = distillation_loss(student_logits, teacher_logits, targets)

            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            # ç»Ÿè®¡
            total_loss += loss.item()
            _, predicted = student_logits.max(1)
            total += targets.size(0)
            correct += predicted.eq(targets).sum().item()

        accuracy = 100. * correct / total
        print(f'Epoch {epoch+1}: Loss={total_loss/len(train_loader):.4f}, '
              f'Acc={accuracy:.2f}%')

    return student_model

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == '__main__':
    # æ•™å¸ˆæ¨¡å‹ï¼ˆå¤§ï¼‰
    teacher = models.resnet50(pretrained=True)
    teacher.fc = nn.Linear(teacher.fc.in_features, 10)

    # å­¦ç”Ÿæ¨¡å‹ï¼ˆå°ï¼‰
    student = models.resnet18(pretrained=False)
    student.fc = nn.Linear(student.fc.in_features, 10)

    # çŸ¥è¯†è’¸é¦
    student = knowledge_distillation(
        teacher, student, train_loader,
        num_epochs=50, temperature=3.0
    )
```

---

## 9.8 å®æˆ˜ï¼šå®Œæ•´è¿ç§»å­¦ä¹ é¡¹ç›®

### ğŸ“‹ ä»»åŠ¡ï¼šåŒ»å­¦å›¾åƒåˆ†ç±»

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
from torchvision import transforms, models
from PIL import Image
import os
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns

# ==================== è‡ªå®šä¹‰æ•°æ®é›† ====================

class MedicalImageDataset(Dataset):
    """åŒ»å­¦å›¾åƒæ•°æ®é›†"""

    def __init__(self, root_dir, transform=None):
        self.root_dir = root_dir
        self.transform = transform
        self.classes = os.listdir(root_dir)
        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}

        # æ”¶é›†æ‰€æœ‰å›¾åƒè·¯å¾„
        self.images = []
        self.labels = []

        for class_name in self.classes:
            class_dir = os.path.join(root_dir, class_name)
            for img_name in os.listdir(class_dir):
                img_path = os.path.join(class_dir, img_name)
                self.images.append(img_path)
                self.labels.append(self.class_to_idx[class_name])

    def __len__(self):
        return len(self.images)

    def __getitem__(self, idx):
        img_path = self.images[idx]
        image = Image.open(img_path).convert('RGB')
        label = self.labels[idx]

        if self.transform:
            image = self.transform(image)

        return image, label

# ==================== æ•°æ®å¢å¼ºç­–ç•¥ ====================

def get_transforms(phase='train'):
    """è·å–æ•°æ®å˜æ¢"""

    if phase == 'train':
        return transforms.Compose([
            transforms.Resize((256, 256)),
            transforms.RandomCrop(224),
            transforms.RandomHorizontalFlip(),
            transforms.RandomVerticalFlip(),
            transforms.RandomRotation(20),
            transforms.ColorJitter(brightness=0.2, contrast=0.2,
                                 saturation=0.2, hue=0.1),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406],
                               [0.229, 0.224, 0.225])
        ])
    else:
        return transforms.Compose([
            transforms.Resize((256, 256)),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406],
                               [0.229, 0.224, 0.225])
        ])

# ==================== æ¨¡å‹æ„å»º ====================

class TransferLearningModel(nn.Module):
    """è¿ç§»å­¦ä¹ æ¨¡å‹"""

    def __init__(self, model_name='resnet50', num_classes=5,
                 pretrained=True, freeze_backbone=False):
        super().__init__()

        # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
        if model_name == 'resnet50':
            self.backbone = models.resnet50(pretrained=pretrained)
            num_features = self.backbone.fc.in_features

            # å†»ç»“éª¨å¹²ç½‘ç»œ
            if freeze_backbone:
                for param in self.backbone.parameters():
                    param.requires_grad = False

            # æ›¿æ¢åˆ†ç±»å™¨
            self.backbone.fc = nn.Sequential(
                nn.Dropout(0.5),
                nn.Linear(num_features, 512),
                nn.ReLU(),
                nn.BatchNorm1d(512),
                nn.Dropout(0.3),
                nn.Linear(512, num_classes)
            )

        elif model_name == 'efficientnet_b3':
            self.backbone = models.efficientnet_b3(pretrained=pretrained)

            if freeze_backbone:
                for param in self.backbone.parameters():
                    param.requires_grad = False

            num_features = self.backbone.classifier[1].in_features
            self.backbone.classifier = nn.Sequential(
                nn.Dropout(0.3),
                nn.Linear(num_features, num_classes)
            )

        elif model_name == 'vit_b_16':
            self.backbone = models.vit_b_16(pretrained=pretrained)

            if freeze_backbone:
                for param in self.backbone.parameters():
                    param.requires_grad = False

            num_features = self.backbone.heads.head.in_features
            self.backbone.heads.head = nn.Linear(num_features, num_classes)

    def forward(self, x):
        return self.backbone(x)

# ==================== è®­ç»ƒå™¨ç±» ====================

class Trainer:
    """è®­ç»ƒå™¨"""

    def __init__(self, model, train_loader, val_loader,
                 criterion, optimizer, scheduler, device,
                 num_epochs=50, early_stopping_patience=10):
        self.model = model
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.criterion = criterion
        self.optimizer = optimizer
        self.scheduler = scheduler
        self.device = device
        self.num_epochs = num_epochs
        self.early_stopping_patience = early_stopping_patience

        self.best_val_acc = 0.0
        self.patience_counter = 0
        self.history = {
            'train_loss': [], 'train_acc': [],
            'val_loss': [], 'val_acc': []
        }

    def train_epoch(self):
        """è®­ç»ƒä¸€ä¸ª epoch"""
        self.model.train()
        running_loss = 0.0
        correct = 0
        total = 0

        for inputs, labels in self.train_loader:
            inputs, labels = inputs.to(self.device), labels.to(self.device)

            self.optimizer.zero_grad()
            outputs = self.model(inputs)
            loss = self.criterion(outputs, labels)
            loss.backward()

            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)

            self.optimizer.step()

            running_loss += loss.item() * inputs.size(0)
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()

        epoch_loss = running_loss / total
        epoch_acc = 100. * correct / total

        return epoch_loss, epoch_acc

    def validate(self):
        """éªŒè¯"""
        self.model.eval()
        running_loss = 0.0
        correct = 0
        total = 0

        all_preds = []
        all_labels = []

        with torch.no_grad():
            for inputs, labels in self.val_loader:
                inputs, labels = inputs.to(self.device), labels.to(self.device)

                outputs = self.model(inputs)
                loss = self.criterion(outputs, labels)

                running_loss += loss.item() * inputs.size(0)
                _, predicted = outputs.max(1)
                total += labels.size(0)
                correct += predicted.eq(labels).sum().item()

                all_preds.extend(predicted.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())

        epoch_loss = running_loss / total
        epoch_acc = 100. * correct / total

        return epoch_loss, epoch_acc, all_preds, all_labels

    def train(self):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        print(f"å¼€å§‹è®­ç»ƒï¼Œè®¾å¤‡: {self.device}")
        print("="*70)

        for epoch in range(self.num_epochs):
            print(f'\nEpoch {epoch+1}/{self.num_epochs}')
            print('-' * 70)

            # è®­ç»ƒ
            train_loss, train_acc = self.train_epoch()

            # éªŒè¯
            val_loss, val_acc, _, _ = self.validate()

            # è®°å½•å†å²
            self.history['train_loss'].append(train_loss)
            self.history['train_acc'].append(train_acc)
            self.history['val_loss'].append(val_loss)
            self.history['val_acc'].append(val_acc)

            # æ‰“å°ç»“æœ
            print(f'è®­ç»ƒ - Loss: {train_loss:.4f}, Acc: {train_acc:.2f}%')
            print(f'éªŒè¯ - Loss: {val_loss:.4f}, Acc: {val_acc:.2f}%')

            # å­¦ä¹ ç‡è°ƒåº¦
            if self.scheduler:
                self.scheduler.step(val_loss)
                print(f'å­¦ä¹ ç‡: {self.optimizer.param_groups[0]["lr"]:.6f}')

            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_acc > self.best_val_acc:
                self.best_val_acc = val_acc
                torch.save(self.model.state_dict(), 'best_model.pth')
                print(f'âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Val Acc: {val_acc:.2f}%)')
                self.patience_counter = 0
            else:
                self.patience_counter += 1

            # Early Stopping
            if self.patience_counter >= self.early_stopping_patience:
                print(f'\nEarly stopping triggered at epoch {epoch+1}')
                break

        print(f'\nè®­ç»ƒå®Œæˆï¼æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {self.best_val_acc:.2f}%')

        # åŠ è½½æœ€ä½³æ¨¡å‹
        self.model.load_state_dict(torch.load('best_model.pth'))

        return self.history

# ==================== è¯„ä¼°å’Œå¯è§†åŒ– ====================

class Evaluator:
    """è¯„ä¼°å™¨"""

    def __init__(self, model, test_loader, class_names, device):
        self.model = model
        self.test_loader = test_loader
        self.class_names = class_names
        self.device = device

    def evaluate(self):
        """å®Œæ•´è¯„ä¼°"""
        self.model.eval()

        all_preds = []
        all_labels = []
        all_probs = []

        with torch.no_grad():
            for inputs, labels in self.test_loader:
                inputs = inputs.to(self.device)

                outputs = self.model(inputs)
                probs = torch.softmax(outputs, dim=1)
                _, predicted = outputs.max(1)

                all_preds.extend(predicted.cpu().numpy())
                all_labels.extend(labels.numpy())
                all_probs.extend(probs.cpu().numpy())

        all_preds = np.array(all_preds)
        all_labels = np.array(all_labels)
        all_probs = np.array(all_probs)

        # å‡†ç¡®ç‡
        accuracy = (all_preds == all_labels).mean()
        print(f'\næµ‹è¯•é›†å‡†ç¡®ç‡: {accuracy*100:.2f}%\n')

        # åˆ†ç±»æŠ¥å‘Š
        print("åˆ†ç±»æŠ¥å‘Š:")
        print(classification_report(all_labels, all_preds,
                                   target_names=self.class_names))

        # æ··æ·†çŸ©é˜µ
        self.plot_confusion_matrix(all_labels, all_preds)

        # ROC æ›²çº¿ï¼ˆå¤šåˆ†ç±»ï¼‰
        if len(self.class_names) <= 10:
            self.plot_roc_curves(all_labels, all_probs)

        return all_preds, all_labels, all_probs

    def plot_confusion_matrix(self, labels, preds):
        """ç»˜åˆ¶æ··æ·†çŸ©é˜µ"""
        cm = confusion_matrix(labels, preds)

        plt.figure(figsize=(10, 8))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                   xticklabels=self.class_names,
                   yticklabels=self.class_names)
        plt.xlabel('é¢„æµ‹æ ‡ç­¾')
        plt.ylabel('çœŸå®æ ‡ç­¾')
        plt.title('æ··æ·†çŸ©é˜µ')
        plt.tight_layout()
        plt.savefig('confusion_matrix.png', dpi=300)
        plt.show()

    def plot_roc_curves(self, labels, probs):
        """ç»˜åˆ¶ ROC æ›²çº¿"""
        from sklearn.metrics import roc_curve, auc
        from sklearn.preprocessing import label_binarize

        # äºŒå€¼åŒ–æ ‡ç­¾
        labels_bin = label_binarize(labels, classes=range(len(self.class_names)))

        plt.figure(figsize=(10, 8))

        for i, class_name in enumerate(self.class_names):
            fpr, tpr, _ = roc_curve(labels_bin[:, i], probs[:, i])
            roc_auc = auc(fpr, tpr)

            plt.plot(fpr, tpr, label=f'{class_name} (AUC = {roc_auc:.2f})')

        plt.plot([0, 1], [0, 1], 'k--', label='éšæœºçŒœæµ‹')
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.title('ROC æ›²çº¿')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.savefig('roc_curves.png', dpi=300)
        plt.show()

# ==================== ä¸»ç¨‹åº ====================

def main():
    """ä¸»å‡½æ•°"""

    # è¶…å‚æ•°
    DATA_DIR = './data/medical_images'
    BATCH_SIZE = 32
    NUM_EPOCHS = 50
    LEARNING_RATE = 0.001
    NUM_CLASSES = 5
    MODEL_NAME = 'resnet50'  # 'resnet50', 'efficientnet_b3', 'vit_b_16'

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # æ•°æ®åŠ è½½
    train_dataset = MedicalImageDataset(
        os.path.join(DATA_DIR, 'train'),
        transform=get_transforms('train')
    )
    val_dataset = MedicalImageDataset(
        os.path.join(DATA_DIR, 'val'),
        transform=get_transforms('val')
    )
    test_dataset = MedicalImageDataset(
        os.path.join(DATA_DIR, 'test'),
        transform=get_transforms('test')
    )

    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,
                             shuffle=True, num_workers=4)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE,
                           shuffle=False, num_workers=4)
    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,
                            shuffle=False, num_workers=4)

    print(f"è®­ç»ƒé›†: {len(train_dataset)} å¼ ")
    print(f"éªŒè¯é›†: {len(val_dataset)} å¼ ")
    print(f"æµ‹è¯•é›†: {len(test_dataset)} å¼ ")
    print(f"ç±»åˆ«: {train_dataset.classes}")

    # åˆ›å»ºæ¨¡å‹
    model = TransferLearningModel(
        model_name=MODEL_NAME,
        num_classes=NUM_CLASSES,
        pretrained=True,
        freeze_backbone=False  # å…ˆå…¨å±€å¾®è°ƒ
    ).to(device)

    # æŸå¤±å’Œä¼˜åŒ–å™¨
    criterion = nn.CrossEntropyLoss()

    # åˆ†å±‚å­¦ä¹ ç‡
    backbone_params = []
    classifier_params = []

    for name, param in model.named_parameters():
        if 'fc' in name or 'classifier' in name or 'head' in name:
            classifier_params.append(param)
        else:
            backbone_params.append(param)

    optimizer = optim.Adam([
        {'params': backbone_params, 'lr': LEARNING_RATE * 0.1},
        {'params': classifier_params, 'lr': LEARNING_RATE}
    ], weight_decay=1e-4)

    # å­¦ä¹ ç‡è°ƒåº¦
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', patience=5, factor=0.5, verbose=True
    )

    # è®­ç»ƒ
    trainer = Trainer(
        model=model,
        train_loader=train_loader,
        val_loader=val_loader,
        criterion=criterion,
        optimizer=optimizer,
        scheduler=scheduler,
        device=device,
        num_epochs=NUM_EPOCHS,
        early_stopping_patience=10
    )

    history = trainer.train()

    # å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹
    plot_training_history(history)

    # è¯„ä¼°
    evaluator = Evaluator(
        model=model,
        test_loader=test_loader,
        class_names=train_dataset.classes,
        device=device
    )

    preds, labels, probs = evaluator.evaluate()

    # Grad-CAM å¯è§†åŒ–
    visualize_gradcam(model, test_loader, device, num_images=8)

    return model, history

# ==================== å¯è§†åŒ–å‡½æ•° ====================

def plot_training_history(history):
    """ç»˜åˆ¶è®­ç»ƒå†å²"""
    fig, axes = plt.subplots(1, 2, figsize=(15, 5))

    epochs = range(1, len(history['train_loss']) + 1)

    # Loss
    axes[0].plot(epochs, history['train_loss'], 'b-', label='è®­ç»ƒ Loss', linewidth=2)
    axes[0].plot(epochs, history['val_loss'], 'r-', label='éªŒè¯ Loss', linewidth=2)
    axes[0].set_xlabel('Epoch', fontsize=12)
    axes[0].set_ylabel('Loss', fontsize=12)
    axes[0].set_title('è®­ç»ƒå’ŒéªŒè¯ Loss', fontsize=14)
    axes[0].legend(fontsize=11)
    axes[0].grid(True, alpha=0.3)

    # Accuracy
    axes[1].plot(epochs, history['train_acc'], 'b-', label='è®­ç»ƒå‡†ç¡®ç‡', linewidth=2)
    axes[1].plot(epochs, history['val_acc'], 'r-', label='éªŒè¯å‡†ç¡®ç‡', linewidth=2)
    axes[1].set_xlabel('Epoch', fontsize=12)
    axes[1].set_ylabel('å‡†ç¡®ç‡ (%)', fontsize=12)
    axes[1].set_title('è®­ç»ƒå’ŒéªŒè¯å‡†ç¡®ç‡', fontsize=14)
    axes[1].legend(fontsize=11)
    axes[1].grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig('training_history.png', dpi=300)
    plt.show()

# ==================== Grad-CAM å¯è§†åŒ– ====================

class GradCAM:
    """Grad-CAM ç±»æ¿€æ´»æ˜ å°„"""

    def __init__(self, model, target_layer):
        self.model = model
        self.target_layer = target_layer
        self.gradients = None
        self.activations = None

        # æ³¨å†Œé’©å­
        target_layer.register_forward_hook(self.save_activation)
        target_layer.register_backward_hook(self.save_gradient)

    def save_activation(self, module, input, output):
        self.activations = output.detach()

    def save_gradient(self, module, grad_input, grad_output):
        self.gradients = grad_output[0].detach()

    def generate_cam(self, input_image, target_class=None):
        """ç”Ÿæˆ CAM"""
        # å‰å‘ä¼ æ’­
        output = self.model(input_image)

        if target_class is None:
            target_class = output.argmax(dim=1)

        # åå‘ä¼ æ’­
        self.model.zero_grad()
        class_score = output[:, target_class]
        class_score.backward()

        # è®¡ç®—æƒé‡
        pooled_gradients = torch.mean(self.gradients, dim=[2, 3])

        # åŠ æƒæ±‚å’Œ
        for i in range(self.activations.size(1)):
            self.activations[:, i, :, :] *= pooled_gradients[:, i].view(-1, 1, 1)

        # CAM
        cam = torch.mean(self.activations, dim=1).squeeze()
        cam = F.relu(cam)
        cam = cam - cam.min()
        cam = cam / cam.max()

        return cam.cpu().numpy()

def visualize_gradcam(model, test_loader, device, num_images=8):
    """å¯è§†åŒ– Grad-CAM"""

    # è·å–ç›®æ ‡å±‚
    if hasattr(model.backbone, 'layer4'):
        target_layer = model.backbone.layer4[-1]
    elif hasattr(model.backbone, 'features'):
        target_layer = model.backbone.features[-1]
    else:
        print("æ— æ³•æ‰¾åˆ°ç›®æ ‡å±‚ï¼Œè·³è¿‡ Grad-CAM")
        return

    gradcam = GradCAM(model, target_layer)

    model.eval()

    fig, axes = plt.subplots(4, 4, figsize=(16, 16))
    axes = axes.flatten()

    images_shown = 0

    for images, labels in test_loader:
        if images_shown >= num_images:
            break

        for i in range(min(len(images), num_images - images_shown)):
            image = images[i:i+1].to(device)
            label = labels[i].item()

            # ç”Ÿæˆ CAM
            cam = gradcam.generate_cam(image)

            # åå½’ä¸€åŒ–å›¾åƒ
            img = images[i].cpu().numpy().transpose(1, 2, 0)
            mean = np.array([0.485, 0.456, 0.406])
            std = np.array([0.229, 0.224, 0.225])
            img = std * img + mean
            img = np.clip(img, 0, 1)

            # è°ƒæ•´ CAM å¤§å°
            import cv2
            cam_resized = cv2.resize(cam, (224, 224))

            # å åŠ æ˜¾ç¤º
            ax = axes[images_shown * 2]
            ax.imshow(img)
            ax.set_title(f'åŸå›¾ (æ ‡ç­¾: {label})')
            ax.axis('off')

            ax = axes[images_shown * 2 + 1]
            ax.imshow(img)
            ax.imshow(cam_resized, cmap='jet', alpha=0.5)
            ax.set_title('Grad-CAM')
            ax.axis('off')

            images_shown += 1

            if images_shown >= num_images:
                break

    plt.tight_layout()
    plt.savefig('gradcam_visualization.png', dpi=300)
    plt.show()

# ==================== è¿è¡Œ ====================

if __name__ == '__main__':
    model, history = main()
```

---

## 9.9 è¿ç§»å­¦ä¹ æœ€ä½³å®è·µ

### âœ… æ•°æ®é›†å¤§å°ç­–ç•¥

```python
def choose_strategy(dataset_size, similarity_to_pretrain):
    """
    æ ¹æ®æ•°æ®é›†å¤§å°å’Œç›¸ä¼¼åº¦é€‰æ‹©ç­–ç•¥

    å‚æ•°ï¼š
        dataset_size: æ•°æ®é›†å¤§å°
        similarity_to_pretrain: ä¸é¢„è®­ç»ƒæ•°æ®çš„ç›¸ä¼¼åº¦
    """

    if dataset_size < 1000:
        if similarity_to_pretrain == 'high':
            return "ç‰¹å¾æå–ï¼ˆå†»ç»“éª¨å¹²ç½‘ç»œï¼‰"
        else:
            return "æ•°æ®å¢å¼º + è½»å¾®å¾®è°ƒï¼ˆå°å­¦ä¹ ç‡ï¼‰"

    elif 1000 <= dataset_size < 10000:
        if similarity_to_pretrain == 'high':
            return "å¾®è°ƒé¡¶å±‚ï¼ˆè§£å†»åå‡ å±‚ï¼‰"
        else:
            return "å…¨å±€å¾®è°ƒï¼ˆå°å­¦ä¹ ç‡ + æ•°æ®å¢å¼ºï¼‰"

    else:  # > 10000
        if similarity_to_pretrain == 'high':
            return "å…¨å±€å¾®è°ƒ"
        else:
            return "å…¨å±€å¾®è°ƒ æˆ– ä»å¤´è®­ç»ƒ"
```

### ğŸ“Š å­¦ä¹ ç‡ç­–ç•¥

```python
# ç­–ç•¥1ï¼šåˆ¤åˆ«å¼å­¦ä¹ ç‡
optimizer = optim.Adam([
    {'params': model.layer1.parameters(), 'lr': 1e-5},
    {'params': model.layer2.parameters(), 'lr': 1e-4},
    {'params': model.layer3.parameters(), 'lr': 1e-3},
    {'params': model.fc.parameters(), 'lr': 1e-2}
])

# ç­–ç•¥2ï¼šæ¸è¿›å¼è§£å†»
def progressive_unfreezing(model, epoch, unfreeze_schedule):
    """
    æ¸è¿›å¼è§£å†»

    unfreeze_schedule: {epoch: [layer_names]}
    """
    if epoch in unfreeze_schedule:
        for layer_name in unfreeze_schedule[epoch]:
            layer = getattr(model, layer_name)
            for param in layer.parameters():
                param.requires_grad = True
        print(f"Epoch {epoch}: è§£å†» {unfreeze_schedule[epoch]}")

# ä½¿ç”¨
unfreeze_schedule = {
    0: ['fc'],           # epoch 0: åªè®­ç»ƒåˆ†ç±»å™¨
    5: ['layer4'],       # epoch 5: è§£å†» layer4
    10: ['layer3'],      # epoch 10: è§£å†» layer3
    15: ['layer2'],      # epoch 15: è§£å†» layer2
}
```

### ğŸ¯ æ•°æ®å¢å¼ºæŠ€å·§

```python
# é«˜çº§æ•°æ®å¢å¼º
from torchvision.transforms import autoaugment, v2

advanced_transforms = transforms.Compose([
    transforms.Resize((256, 256)),
    transforms.RandomCrop(224),

    # AutoAugment
    autoaugment.AutoAugment(
        autoaugment.AutoAugmentPolicy.IMAGENET
    ),

    # RandAugment
    # autoaugment.RandAugment(),

    # Mixup / CutMix (éœ€è¦ç‰¹æ®Šå¤„ç†)
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406],
                        [0.229, 0.224, 0.225])
])

# Mixup
class MixupDataset(Dataset):
    def __init__(self, dataset, alpha=0.2):
        self.dataset = dataset
        self.alpha = alpha

    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, idx):
        img1, label1 = self.dataset[idx]

        # éšæœºé€‰æ‹©å¦ä¸€ä¸ªæ ·æœ¬
        idx2 = np.random.randint(0, len(self.dataset))
        img2, label2 = self.dataset[idx2]

        # Mixup
        lam = np.random.beta(self.alpha, self.alpha)
        mixed_img = lam * img1 + (1 - lam) * img2

        return mixed_img, (label1, label2, lam)
```

---

## ğŸ“ æœ¬ç« ä½œä¸š

### ä½œä¸š 1ï¼šå¯¹æ¯”å®éªŒ

```python
# åœ¨åŒä¸€ä¸ªæ•°æ®é›†ä¸Šå¯¹æ¯”ï¼š
# 1. ä»é›¶è®­ç»ƒ
# 2. ç‰¹å¾æå–
# 3. å¾®è°ƒï¼ˆå†»ç»“éƒ¨åˆ†å±‚ï¼‰
# 4. å¾®è°ƒï¼ˆå…¨å±€ï¼‰
# 5. çŸ¥è¯†è’¸é¦

# è®°å½•ï¼š
#   - è®­ç»ƒæ—¶é—´
#   - æœ€ç»ˆå‡†ç¡®ç‡
#   - å‚æ•°é‡
#   - æ”¶æ•›æ›²çº¿

# åˆ†æï¼šå“ªç§ç­–ç•¥æœ€é€‚åˆä½ çš„æ•°æ®é›†ï¼Ÿ
```

### ä½œä¸š 2ï¼šåŒ»å­¦å›¾åƒåˆ†ç±»

```python
# ä½¿ç”¨çœŸå®åŒ»å­¦å›¾åƒæ•°æ®é›†ï¼ˆå¦‚ Chest X-Rayï¼‰
# è¦æ±‚ï¼š
# 1. EDA å’Œæ•°æ®é¢„å¤„ç†
# 2. å°è¯•è‡³å°‘ 3 ç§é¢„è®­ç»ƒæ¨¡å‹
# 3. å®ç°æ•°æ®å¢å¼º
# 4. ä½¿ç”¨ Grad-CAM å¯è§†åŒ–
# 5. è¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼ˆå‡†ç¡®ç‡ã€F1ã€AUCï¼‰
# 6. åˆ†æé”™è¯¯æ¡ˆä¾‹
# 7. ç¼–å†™å®Œæ•´æŠ¥å‘Š
```

### ä½œä¸š 3ï¼šå°‘æ ·æœ¬å­¦ä¹ 

```python
# å®ç°å°‘æ ·æœ¬å­¦ä¹ 
# ä»»åŠ¡ï¼š5-way 1-shot / 5-shot åˆ†ç±»
# æ–¹æ³•ï¼š
# 1. åŸå‹ç½‘ç»œ
# 2. MAML
# 3. å¯¹æ¯”å­¦ä¹ ï¼ˆSimCLRï¼‰

# åœ¨ Omniglot æˆ– Mini-ImageNet ä¸Šæµ‹è¯•
```

### ä½œä¸š 4ï¼šé¢†åŸŸè‡ªé€‚åº”

```python
# å®ç°é¢†åŸŸè‡ªé€‚åº”
# æºåŸŸï¼šMNIST
# ç›®æ ‡åŸŸï¼šSVHN æˆ– USPS

# æ–¹æ³•ï¼š
# 1. é¢†åŸŸå¯¹æŠ—è®­ç»ƒ
# 2. è‡ªè®­ç»ƒ
# 3. å¯¹æ¯”ä¸¤ç§æ–¹æ³•çš„æ•ˆæœ
```

---

## ğŸ”‘ æœ¬ç« å…³é”®æ¦‚å¿µ

| æ¦‚å¿µ | è¯´æ˜ |
|------|------|
| è¿ç§»å­¦ä¹  | åˆ©ç”¨å·²æœ‰çŸ¥è¯†åŠ é€Ÿæ–°ä»»åŠ¡å­¦ä¹  |
| é¢„è®­ç»ƒæ¨¡å‹ | åœ¨å¤§æ•°æ®é›†ä¸Šè®­ç»ƒçš„æ¨¡å‹ |
| ç‰¹å¾æå– | å†»ç»“éª¨å¹²ç½‘ç»œï¼Œåªè®­ç»ƒåˆ†ç±»å™¨ |
| å¾®è°ƒ | è§£å†»éƒ¨åˆ†æˆ–å…¨éƒ¨å±‚è¿›è¡Œè®­ç»ƒ |
| åˆ¤åˆ«å¼å­¦ä¹ ç‡ | ä¸åŒå±‚ä½¿ç”¨ä¸åŒå­¦ä¹ ç‡ |
| é¢†åŸŸè‡ªé€‚åº” | å¤„ç†æºåŸŸå’Œç›®æ ‡åŸŸåˆ†å¸ƒä¸åŒ |
| å°‘æ ·æœ¬å­¦ä¹  |ç”¨æå°‘æ ·æœ¬å­¦ä¹ æ–°ä»»åŠ¡ |
| å…ƒå­¦ä¹  | å­¦ä¹ å¦‚ä½•å­¦ä¹  |
| çŸ¥è¯†è’¸é¦ | ç”¨å¤§æ¨¡å‹è®­ç»ƒå°æ¨¡å‹ |
| MAML | æ¨¡å‹æ— å…³çš„å…ƒå­¦ä¹  |
| åŸå‹ç½‘ç»œ | åŸºäºè·ç¦»çš„å°‘æ ·æœ¬å­¦ä¹  |
| Grad-CAM | ç±»æ¿€æ´»æ˜ å°„å¯è§†åŒ– |

---

## 9.10 è¿›é˜¶è¯é¢˜

### ğŸ”¹ å¤šä»»åŠ¡å­¦ä¹  (Multi-Task Learning)

```python
class MultiTaskModel(nn.Module):
    """å¤šä»»åŠ¡å­¦ä¹ æ¨¡å‹"""

    def __init__(self, backbone, num_classes_list):
        """
        å‚æ•°ï¼š
            backbone: å…±äº«çš„ç‰¹å¾æå–å™¨
            num_classes_list: æ¯ä¸ªä»»åŠ¡çš„ç±»åˆ«æ•°åˆ—è¡¨
        """
        super().__init__()

        self.backbone = backbone

        # ä¸ºæ¯ä¸ªä»»åŠ¡åˆ›å»ºç‹¬ç«‹çš„åˆ†ç±»å™¨
        self.task_heads = nn.ModuleList([
            nn.Linear(backbone.output_dim, num_classes)
            for num_classes in num_classes_list
        ])

    def forward(self, x):
        # å…±äº«ç‰¹å¾æå–
        features = self.backbone(x)

        # å¤šä¸ªä»»åŠ¡çš„è¾“å‡º
        outputs = [head(features) for head in self.task_heads]

        return outputs

# å¤šä»»åŠ¡æŸå¤±
class MultiTaskLoss(nn.Module):
    """å¤šä»»åŠ¡æŸå¤±ï¼ˆä¸ç¡®å®šæ€§åŠ æƒï¼‰"""

    def __init__(self, num_tasks):
        super().__init__()
        # å¯å­¦ä¹ çš„ä»»åŠ¡æƒé‡ï¼ˆlog æ–¹å·®ï¼‰
        self.log_vars = nn.Parameter(torch.zeros(num_tasks))

    def forward(self, losses):
        """
        å‚æ•°ï¼š
            losses: æ¯ä¸ªä»»åŠ¡çš„æŸå¤±åˆ—è¡¨
        """
        weighted_losses = []

        for i, loss in enumerate(losses):
            precision = torch.exp(-self.log_vars[i])
            weighted_loss = precision * loss + self.log_vars[i]
            weighted_losses.append(weighted_loss)

        return sum(weighted_losses)

# ä½¿ç”¨ç¤ºä¾‹
def train_multitask():
    # åˆ›å»ºæ¨¡å‹
    backbone = models.resnet50(pretrained=True)
    backbone.fc = nn.Identity()  # ç§»é™¤æœ€åçš„ FC å±‚
    backbone.output_dim = 2048

    model = MultiTaskModel(
        backbone=backbone,
        num_classes_list=[10, 5, 2]  # 3ä¸ªä»»åŠ¡
    )

    # å¤šä»»åŠ¡æŸå¤±
    criterion = MultiTaskLoss(num_tasks=3)

    # è®­ç»ƒå¾ªç¯
    for images, (labels1, labels2, labels3) in dataloader:
        outputs = model(images)

        # è®¡ç®—æ¯ä¸ªä»»åŠ¡çš„æŸå¤±
        losses = [
            nn.CrossEntropyLoss()(outputs[0], labels1),
            nn.CrossEntropyLoss()(outputs[1], labels2),
            nn.CrossEntropyLoss()(outputs[2], labels3)
        ]

        # ç»„åˆæŸå¤±
        total_loss = criterion(losses)

        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        total_loss.backward()
        optimizer.step()
```

---

### ğŸ”¹ æŒç»­å­¦ä¹  (Continual Learning)

```python
class ElasticWeightConsolidation:
    """å¼¹æ€§æƒé‡å·©å›º (EWC)"""

    def __init__(self, model, dataloader, lambda_ewc=1000):
        self.model = model
        self.lambda_ewc = lambda_ewc

        # è®¡ç®— Fisher ä¿¡æ¯çŸ©é˜µ
        self.fisher_matrix = self._compute_fisher(dataloader)

        # ä¿å­˜å½“å‰å‚æ•°
        self.optimal_params = {
            name: param.clone().detach()
            for name, param in model.named_parameters()
        }

    def _compute_fisher(self, dataloader):
        """è®¡ç®— Fisher ä¿¡æ¯çŸ©é˜µ"""
        fisher = {}

        self.model.eval()

        for name, param in self.model.named_parameters():
            fisher[name] = torch.zeros_like(param)

        for inputs, labels in dataloader:
            self.model.zero_grad()

            outputs = self.model(inputs)
            loss = nn.CrossEntropyLoss()(outputs, labels)
            loss.backward()

            for name, param in self.model.named_parameters():
                if param.grad is not None:
                    fisher[name] += param.grad.pow(2)

        # å½’ä¸€åŒ–
        num_samples = len(dataloader.dataset)
        for name in fisher:
            fisher[name] /= num_samples

        return fisher

    def penalty(self):
        """EWC æƒ©ç½šé¡¹"""
        loss = 0

        for name, param in self.model.named_parameters():
            if name in self.fisher_matrix:
                loss += (self.fisher_matrix[name] *
                        (param - self.optimal_params[name]).pow(2)).sum()

        return self.lambda_ewc * loss

# ä½¿ç”¨
def train_with_ewc(model, old_task_loader, new_task_loader):
    """ä½¿ç”¨ EWC è®­ç»ƒæ–°ä»»åŠ¡"""

    # åœ¨æ—§ä»»åŠ¡ä¸Šè®¡ç®— Fisher çŸ©é˜µ
    ewc = ElasticWeightConsolidation(model, old_task_loader)

    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    criterion = nn.CrossEntropyLoss()

    # åœ¨æ–°ä»»åŠ¡ä¸Šè®­ç»ƒ
    for epoch in range(num_epochs):
        for inputs, labels in new_task_loader:
            outputs = model(inputs)

            # æ–°ä»»åŠ¡æŸå¤± + EWC æƒ©ç½š
            loss = criterion(outputs, labels) + ewc.penalty()

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
```

---

### ğŸ”¹ å¯¹æ¯”å­¦ä¹  (Contrastive Learning)

```python
class SimCLR(nn.Module):
    """SimCLR å¯¹æ¯”å­¦ä¹ """

    def __init__(self, base_encoder, projection_dim=128):
        super().__init__()

        # ç¼–ç å™¨
        self.encoder = base_encoder

        # æŠ•å½±å¤´
        self.projector = nn.Sequential(
            nn.Linear(base_encoder.output_dim, 2048),
            nn.ReLU(),
            nn.Linear(2048, projection_dim)
        )

    def forward(self, x):
        # ç¼–ç 
        features = self.encoder(x)

        # æŠ•å½±
        z = self.projector(features)

        # L2 å½’ä¸€åŒ–
        z = F.normalize(z, dim=1)

        return z

class NTXentLoss(nn.Module):
    """å½’ä¸€åŒ–æ¸©åº¦äº¤å‰ç†µæŸå¤± (NT-Xent)"""

    def __init__(self, temperature=0.5):
        super().__init__()
        self.temperature = temperature

    def forward(self, z_i, z_j):
        """
        å‚æ•°ï¼š
            z_i, z_j: ä¸¤ä¸ªå¢å¼ºè§†å›¾çš„è¡¨ç¤º (batch_size, projection_dim)
        """
        batch_size = z_i.size(0)

        # æ‹¼æ¥
        z = torch.cat([z_i, z_j], dim=0)  # (2*batch_size, dim)

        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        sim_matrix = torch.mm(z, z.T) / self.temperature

        # åˆ›å»ºæ ‡ç­¾ï¼šå¯¹è§’çº¿å¤–çš„å¯¹åº”ä½ç½®ä¸ºæ­£æ ·æœ¬
        labels = torch.arange(batch_size).to(z.device)
        labels = torch.cat([labels + batch_size, labels])

        # æ©ç ï¼šå»æ‰è‡ªå·±å’Œè‡ªå·±çš„ç›¸ä¼¼åº¦
        mask = torch.eye(2 * batch_size, dtype=torch.bool).to(z.device)
        sim_matrix = sim_matrix.masked_fill(mask, -1e9)

        # è®¡ç®—æŸå¤±
        loss = nn.CrossEntropyLoss()(sim_matrix, labels)

        return loss

# è®­ç»ƒ SimCLR
def train_simclr(model, dataloader, num_epochs=100):
    """è®­ç»ƒ SimCLR"""

    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    criterion = NTXentLoss(temperature=0.5)

    for epoch in range(num_epochs):
        for (x_i, x_j), _ in dataloader:  # x_i, x_j æ˜¯ä¸¤ä¸ªå¢å¼ºè§†å›¾
            # å‰å‘ä¼ æ’­
            z_i = model(x_i)
            z_j = model(x_j)

            # è®¡ç®—æŸå¤±
            loss = criterion(z_i, z_j)

            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

        if epoch % 10 == 0:
            print(f'Epoch {epoch}: Loss = {loss.item():.4f}')

    return model

# ä½¿ç”¨é¢„è®­ç»ƒçš„ SimCLR è¿›è¡Œä¸‹æ¸¸ä»»åŠ¡
def finetune_simclr(pretrained_encoder, train_loader, num_classes):
    """å¾®è°ƒ SimCLR ç¼–ç å™¨"""

    # å†»ç»“ç¼–ç å™¨
    for param in pretrained_encoder.parameters():
        param.requires_grad = False

    # æ·»åŠ çº¿æ€§åˆ†ç±»å™¨
    classifier = nn.Linear(pretrained_encoder.output_dim, num_classes)

    # è®­ç»ƒåˆ†ç±»å™¨
    optimizer = torch.optim.Adam(classifier.parameters(), lr=0.001)
    criterion = nn.CrossEntropyLoss()

    for epoch in range(num_epochs):
        for inputs, labels in train_loader:
            # æå–ç‰¹å¾ï¼ˆå†»ç»“ï¼‰
            with torch.no_grad():
                features = pretrained_encoder(inputs)

            # åˆ†ç±»
            outputs = classifier(features)
            loss = criterion(outputs, labels)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
```

---

## 9.11 å®ç”¨å·¥å…·å’ŒæŠ€å·§

### ğŸ› ï¸ æ¨¡å‹è½¬æ¢å’Œéƒ¨ç½²

```python
# 1. ONNX å¯¼å‡º
def export_to_onnx(model, dummy_input, output_path):
    """å¯¼å‡ºä¸º ONNX æ ¼å¼"""
    model.eval()

    torch.onnx.export(
        model,
        dummy_input,
        output_path,
        export_params=True,
        opset_version=11,
        do_constant_folding=True,
        input_names=['input'],
        output_names=['output'],
        dynamic_axes={
            'input': {0: 'batch_size'},
            'output': {0: 'batch_size'}
        }
    )

    print(f"æ¨¡å‹å·²å¯¼å‡ºåˆ° {output_path}")

# ä½¿ç”¨
dummy_input = torch.randn(1, 3, 224, 224)
export_to_onnx(model, dummy_input, 'model.onnx')

# 2. TorchScript è½¬æ¢
scripted_model = torch.jit.script(model)
scripted_model.save('model_scripted.pt')

# 3. é‡åŒ–ï¼ˆå‡å°æ¨¡å‹å¤§å°ï¼‰
def quantize_model(model):
    """åŠ¨æ€é‡åŒ–"""
    quantized_model = torch.quantization.quantize_dynamic(
        model,
        {nn.Linear, nn.Conv2d},
        dtype=torch.qint8
    )
    return quantized_model

quantized = quantize_model(model)
```

---

### ğŸ“Š æ¨¡å‹åˆ†æå·¥å…·

```python
from torchinfo import summary
from fvcore.nn import FlopCountAnalysis, parameter_count

def analyze_model(model, input_size=(1, 3, 224, 224)):
    """åˆ†ææ¨¡å‹"""

    # æ¨¡å‹æ‘˜è¦
    print("="*70)
    print("æ¨¡å‹ç»“æ„:")
    print("="*70)
    summary(model, input_size=input_size)

    # å‚æ•°é‡
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

    print(f"\næ€»å‚æ•°: {total_params:,}")
    print(f"å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    print(f"ä¸å¯è®­ç»ƒå‚æ•°: {total_params - trainable_params:,}")

    # FLOPs
    dummy_input = torch.randn(input_size)
    flops = FlopCountAnalysis(model, dummy_input)
    print(f"\nFLOPs: {flops.total():,}")

    # å†…å­˜å ç”¨
    print(f"æ¨¡å‹å¤§å°: {total_params * 4 / 1024 / 1024:.2f} MB (FP32)")

    # æ¨ç†é€Ÿåº¦æµ‹è¯•
    model.eval()
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.to(device)
    dummy_input = dummy_input.to(device)

    import time

    # é¢„çƒ­
    for _ in range(10):
        _ = model(dummy_input)

    # æµ‹è¯•
    num_iterations = 100
    torch.cuda.synchronize() if torch.cuda.is_available() else None
    start = time.time()

    with torch.no_grad():
        for _ in range(num_iterations):
            _ = model(dummy_input)

    torch.cuda.synchronize() if torch.cuda.is_available() else None
    end = time.time()

    avg_time = (end - start) / num_iterations
    fps = 1 / avg_time

    print(f"\næ¨ç†æ—¶é—´: {avg_time*1000:.2f} ms")
    print(f"FPS: {fps:.2f}")

# ä½¿ç”¨
analyze_model(model)
```

---

### ğŸ” é”™è¯¯åˆ†æå·¥å…·

```python
class ErrorAnalyzer:
    """é”™è¯¯åˆ†æå·¥å…·"""

    def __init__(self, model, dataloader, class_names, device):
        self.model = model
        self.dataloader = dataloader
        self.class_names = class_names
        self.device = device

    def analyze(self):
        """å®Œæ•´é”™è¯¯åˆ†æ"""
        self.model.eval()

        errors = []

        with torch.no_grad():
            for inputs, labels in self.dataloader:
                inputs, labels = inputs.to(self.device), labels.to(self.device)

                outputs = self.model(inputs)
                probs = torch.softmax(outputs, dim=1)
                preds = outputs.argmax(dim=1)

                # æ‰¾å‡ºé”™è¯¯æ ·æœ¬
                wrong_mask = preds != labels

                if wrong_mask.any():
                    for i in torch.where(wrong_mask)[0]:
                        errors.append({
                            'image': inputs[i].cpu(),
                            'true_label': labels[i].item(),
                            'pred_label': preds[i].item(),
                            'confidence': probs[i, preds[i]].item(),
                            'true_prob': probs[i, labels[i]].item()
                        })

        print(f"æ€»é”™è¯¯æ•°: {len(errors)}")

        # æŒ‰ç±»åˆ«ç»Ÿè®¡é”™è¯¯
        self._error_by_class(errors)

        # æŒ‰ç½®ä¿¡åº¦åˆ†æ
        self._error_by_confidence(errors)

        # å¯è§†åŒ–æœ€éš¾æ ·æœ¬
        self._visualize_hard_examples(errors)

        return errors

    def _error_by_class(self, errors):
        """æŒ‰ç±»åˆ«ç»Ÿè®¡é”™è¯¯"""
        from collections import defaultdict

        error_count = defaultdict(int)
        confusion = defaultdict(lambda: defaultdict(int))

        for error in errors:
            true_label = error['true_label']
            pred_label = error['pred_label']

            error_count[true_label] += 1
            confusion[true_label][pred_label] += 1

        print("\næ¯ç±»é”™è¯¯æ•°:")
        for class_id, count in sorted(error_count.items()):
            print(f"  {self.class_names[class_id]}: {count}")

        print("\næœ€å¸¸è§çš„æ··æ·†:")
        for true_id, pred_dict in confusion.items():
            for pred_id, count in sorted(pred_dict.items(),
                                        key=lambda x: x[1],
                                        reverse=True)[:3]:
                print(f"  {self.class_names[true_id]} â†’ "
                      f"{self.class_names[pred_id]}: {count}")

    def _error_by_confidence(self, errors):
        """æŒ‰ç½®ä¿¡åº¦åˆ†æ"""
        confidences = [e['confidence'] for e in errors]

        print(f"\né”™è¯¯é¢„æµ‹çš„ç½®ä¿¡åº¦:")
        print(f"  å¹³å‡: {np.mean(confidences):.4f}")
        print(f"  ä¸­ä½æ•°: {np.median(confidences):.4f}")
        print(f"  æœ€å¤§: {np.max(confidences):.4f}")
        print(f"  æœ€å°: {np.min(confidences):.4f}")

    def _visualize_hard_examples(self, errors, num_examples=16):
        """å¯è§†åŒ–æœ€éš¾çš„æ ·æœ¬"""
        # æŒ‰ç½®ä¿¡åº¦æ’åºï¼ˆé«˜ç½®ä¿¡åº¦ä½†é”™è¯¯ï¼‰
        sorted_errors = sorted(errors,
                              key=lambda x: x['confidence'],
                              reverse=True)[:num_examples]

        fig, axes = plt.subplots(4, 4, figsize=(16, 16))
        axes = axes.flatten()

        for i, error in enumerate(sorted_errors):
            if i >= num_examples:
                break

            # åå½’ä¸€åŒ–
            img = error['image'].numpy().transpose(1, 2, 0)
            mean = np.array([0.485, 0.456, 0.406])
            std = np.array([0.229, 0.224, 0.225])
            img = std * img + mean
            img = np.clip(img, 0, 1)

            ax = axes[i]
            ax.imshow(img)
            ax.set_title(
                f"çœŸå®: {self.class_names[error['true_label']]}\n"
                f"é¢„æµ‹: {self.class_names[error['pred_label']]}\n"
                f"ç½®ä¿¡åº¦: {error['confidence']:.3f}",
                fontsize=10,
                color='red'
            )
            ax.axis('off')

        plt.tight_layout()
        plt.savefig('hard_examples.png', dpi=300)
        plt.show()

# ä½¿ç”¨
analyzer = ErrorAnalyzer(model, test_loader, class_names, device)
errors = analyzer.analyze()
```

---

## ğŸ“š æ¨èèµ„æº

### ğŸ“– è®ºæ–‡

**è¿ç§»å­¦ä¹ åŸºç¡€**ï¼š
- "A Survey on Transfer Learning" (Pan & Yang, 2010)
- "How transferable are features in deep neural networks?" (Yosinski et al., 2014)

**é¢†åŸŸè‡ªé€‚åº”**ï¼š
- "Domain-Adversarial Training of Neural Networks" (Ganin et al., 2016)
- "Unsupervised Domain Adaptation by Backpropagation" (Ganin & Lempitsky, 2015)

**å°‘æ ·æœ¬å­¦ä¹ **ï¼š
- "Model-Agnostic Meta-Learning (MAML)" (Finn et al., 2017)
- "Prototypical Networks for Few-shot Learning" (Snell et al., 2017)
- "Matching Networks for One Shot Learning" (Vinyals et al., 2016)

**çŸ¥è¯†è’¸é¦**ï¼š
- "Distilling the Knowledge in a Neural Network" (Hinton et al., 2015)

**å¯¹æ¯”å­¦ä¹ **ï¼š
- "A Simple Framework for Contrastive Learning (SimCLR)" (Chen et al., 2020)
- "Momentum Contrast (MoCo)" (He et al., 2020)

### ğŸ”§ å·¥å…·å’Œåº“

```python
# Hugging Face Transformers
from transformers import AutoModel, AutoTokenizer

# Timm (PyTorch Image Models)
import timm
model = timm.create_model('resnet50', pretrained=True)

# PyTorch Lightning (ç®€åŒ–è®­ç»ƒ)
import pytorch_lightning as pl

# Weights & Biases (å®éªŒè·Ÿè¸ª)
import wandb

# TensorBoard
from torch.utils.tensorboard import SummaryWriter
```

---

## ğŸ“ æ€»ç»“

### âœ… è¿ç§»å­¦ä¹ ä½•æ—¶æœ‰æ•ˆï¼Ÿ

```
âœ“ æºä»»åŠ¡å’Œç›®æ ‡ä»»åŠ¡ç›¸å…³
âœ“ ç›®æ ‡ä»»åŠ¡æ•°æ®è¾ƒå°‘
âœ“ æºä»»åŠ¡æ•°æ®é‡å¤§ä¸”è´¨é‡é«˜
âœ“ æœ‰åˆé€‚çš„é¢„è®­ç»ƒæ¨¡å‹å¯ç”¨
```

### âŒ è¿ç§»å­¦ä¹ ä½•æ—¶æ— æ•ˆï¼Ÿ

```
âœ— æºä»»åŠ¡å’Œç›®æ ‡ä»»åŠ¡å®Œå…¨ä¸ç›¸å…³
âœ— ç›®æ ‡ä»»åŠ¡æ•°æ®å……è¶³
âœ— é¢„è®­ç»ƒæ¨¡å‹ä¸åŒ¹é…ç›®æ ‡ä»»åŠ¡
âœ— è®¡ç®—èµ„æºå……è¶³ï¼Œå¯ä»å¤´è®­ç»ƒ
```

### ğŸ¯ å®è·µå»ºè®®

1. **ä¼˜å…ˆå°è¯•é¢„è®­ç»ƒæ¨¡å‹**
2. **æ ¹æ®æ•°æ®é‡é€‰æ‹©ç­–ç•¥**ï¼ˆç‰¹å¾æå– vs å¾®è°ƒï¼‰
3. **ä½¿ç”¨åˆ¤åˆ«å¼å­¦ä¹ ç‡**
4. **å……åˆ†åˆ©ç”¨æ•°æ®å¢å¼º**
5. **ç›‘æ§éªŒè¯é›†é¿å…è¿‡æ‹Ÿåˆ**
6. **å¯è§†åŒ–ç†è§£æ¨¡å‹è¡Œä¸º**ï¼ˆGrad-CAMç­‰ï¼‰
7. **è®°å½•å®éªŒç»“æœ**ï¼ˆWeights & Biasesï¼‰
8. **é”™è¯¯åˆ†ææŒ‡å¯¼æ”¹è¿›**

---
