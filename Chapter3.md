# ç¬¬ä¸‰ç« ï¼šåˆ†ç±»ä¸é€»è¾‘å›å½’ (Classification & Logistic Regression)

## ğŸ“Œ ç« èŠ‚ç›®æ ‡
- ç†è§£åˆ†ç±»é—®é¢˜ä¸å›å½’é—®é¢˜çš„åŒºåˆ«
- æŒæ¡é€»è¾‘å›å½’çš„åŸç†å’Œå®ç°
- å­¦ä¹  Sigmoid å’Œ Softmax å‡½æ•°
- ç†è§£äº¤å‰ç†µæŸå¤±å‡½æ•°
- å®æˆ˜ï¼šæ‰‹å†™æ•°å­—è¯†åˆ«ã€ä¿¡ç”¨å¡æ¬ºè¯ˆæ£€æµ‹

---

## 3.1 ä»€ä¹ˆæ˜¯åˆ†ç±»ï¼Ÿ

### ğŸ¯ åˆ†ç±» vs å›å½’

```
å›å½’ (Regression):
  é¢„æµ‹è¿ç»­æ•°å€¼
  ä¾‹ï¼šæˆ¿ä»· $350,000, æ¸©åº¦ 25.3Â°C
  è¾“å‡ºï¼šå®æ•° â„

åˆ†ç±» (Classification):
  é¢„æµ‹ç¦»æ•£ç±»åˆ«
  ä¾‹ï¼šçŒ«/ç‹—, åƒåœ¾é‚®ä»¶/æ­£å¸¸é‚®ä»¶
  è¾“å‡ºï¼šç±»åˆ«æ ‡ç­¾
```

### ğŸ“Š åˆ†ç±»é—®é¢˜çš„ç±»å‹

#### **1. äºŒå…ƒåˆ†ç±» (Binary Classification)**
åªæœ‰ä¸¤ä¸ªç±»åˆ«

**ä¾‹å­**ï¼š
- é‚®ä»¶ï¼šåƒåœ¾é‚®ä»¶ (1) / æ­£å¸¸é‚®ä»¶ (0)
- åŒ»å­¦ï¼šæœ‰ç—… (1) / æ²¡ç—… (0)
- ä¿¡ç”¨å¡ï¼šæ¬ºè¯ˆ (1) / æ­£å¸¸ (0)
- å®¢æˆ·ï¼šä¼šè´­ä¹° (1) / ä¸ä¼šè´­ä¹° (0)

#### **2. å¤šå…ƒåˆ†ç±» (Multi-class Classification)**
å¤šä¸ªç±»åˆ«ï¼ˆä½†åªèƒ½å±äºä¸€ä¸ªï¼‰

**ä¾‹å­**ï¼š
- æ‰‹å†™æ•°å­—è¯†åˆ«ï¼š0, 1, 2, ..., 9
- æ–°é—»åˆ†ç±»ï¼šä½“è‚²ã€æ”¿æ²»ã€å¨±ä¹ã€ç§‘æŠ€
- åŠ¨ç‰©åˆ†ç±»ï¼šçŒ«ã€ç‹—ã€é¸Ÿã€é±¼

#### **3. å¤šæ ‡ç­¾åˆ†ç±» (Multi-label Classification)**
å¯ä»¥åŒæ—¶å±äºå¤šä¸ªç±»åˆ«

**ä¾‹å­**ï¼š
- ç”µå½±æ ‡ç­¾ï¼š[åŠ¨ä½œ, å–œå‰§, çˆ±æƒ…]
- æ–‡ç« æ ‡ç­¾ï¼š[æœºå™¨å­¦ä¹ , Python, æ·±åº¦å­¦ä¹ ]

---

## 3.2 ä¸ºä»€ä¹ˆä¸èƒ½ç”¨çº¿æ€§å›å½’åšåˆ†ç±»ï¼Ÿ

### ğŸ¤” å°è¯•ç”¨å›å½’åšåˆ†ç±»

å‡è®¾æˆ‘ä»¬è¦åˆ†ç±»ï¼šçŒ« (0) / ç‹— (1)

```
è®­ç»ƒæ•°æ®ï¼š
x (ç‰¹å¾) | y (æ ‡ç­¾)
  1.0    |   0  (çŒ«)
  2.0    |   0  (çŒ«)
  3.0    |   1  (ç‹—)
  4.0    |   1  (ç‹—)
```

**çº¿æ€§å›å½’**ï¼š`y = b + wÂ·x`

```
y
â†‘
1 |         â—  â—   (ç‹—)
  |       /
  |     /
0 |   â—  â—       (çŒ«)
  |_____________â†’ x
```

çœ‹èµ·æ¥è¿˜ä¸é”™ï¼Ÿ

### âš ï¸ é—®é¢˜æ¥äº†ï¼

**æ–°æ•°æ®ç‚¹**ï¼šx = 10

```
y
â†‘
3 |               â—  (é¢„æµ‹å€¼ = 3ï¼Ÿï¼Ÿ)
2 |             /
1 |         â—  â—
  |       /
  |     /
0 |   â—  â—
  |_____________â†’ x
            10
```

**é—®é¢˜**ï¼š
1. è¾“å‡ºä¸æ˜¯ 0 æˆ– 1ï¼ˆå¯èƒ½æ˜¯ 3, -1, 0.7...ï¼‰
2. è¿œç¦»è®­ç»ƒæ•°æ®çš„ç‚¹ä¼šå½±å“å†³ç­–è¾¹ç•Œ
3. æ— æ³•è¡¨ç¤ºæ¦‚ç‡

### ğŸ’¡ æˆ‘ä»¬éœ€è¦ä»€ä¹ˆï¼Ÿ

```
ç†æƒ³çš„åˆ†ç±»å™¨ï¼š
  è¾“å‡ºèŒƒå›´åœ¨ [0, 1]
  å¯ä»¥è§£é‡Šä¸ºæ¦‚ç‡
  0.9 â†’ 90% ç¡®å®šæ˜¯ç‹—
  0.1 â†’ 10% ç¡®å®šæ˜¯ç‹—ï¼ˆ90% æ˜¯çŒ«ï¼‰
```

---

## 3.3 Logistic Regression

### ğŸ”¹ æ ¸å¿ƒæ€æƒ³

**æ”¹é€ çº¿æ€§å›å½’**ï¼š

```
Step 1: è®¡ç®—çº¿æ€§ç»„åˆ
  z = b + wÂ·x

Step 2: é€šè¿‡ Sigmoid å‡½æ•°å‹ç¼©åˆ° (0, 1)
  y = Ïƒ(z) = 1 / (1 + e^(-z))
```

### ğŸ“ Sigmoid å‡½æ•°

```python
import numpy as np
import matplotlib.pyplot as plt

def sigmoid(z):
    return 1 / (1 + np.exp(-z))

z = np.linspace(-10, 10, 100)
y = sigmoid(z)

plt.figure(figsize=(10, 6))
plt.plot(z, y, 'b-', linewidth=2)
plt.axhline(y=0.5, color='r', linestyle='--', label='é˜ˆå€¼ = 0.5')
plt.axvline(x=0, color='g', linestyle='--', alpha=0.5)
plt.xlabel('z = b + wx')
plt.ylabel('Ïƒ(z)')
plt.title('Sigmoid Function')
plt.grid(True, alpha=0.3)
plt.legend()
plt.show()
```

**å›¾å½¢**ï¼š
```
Ïƒ(z)
 1 |         ______
   |       /
0.5|      * (z=0, Ïƒ=0.5)
   |     /
 0 |____/
   |_____________â†’ z
  -10      0     10
```

### ğŸ” Sigmoid çš„æ€§è´¨

```
Ïƒ(z) = 1 / (1 + e^(-z))

æ€§è´¨ï¼š
1. è¾“å‡ºèŒƒå›´ï¼š(0, 1)
2. Ïƒ(0) = 0.5 (ä¸­ç‚¹)
3. z â†’ +âˆ, Ïƒ(z) â†’ 1
4. z â†’ -âˆ, Ïƒ(z) â†’ 0
5. å…³äº (0, 0.5) ä¸­å¿ƒå¯¹ç§°
6. å¯¼æ•°ï¼šÏƒ'(z) = Ïƒ(z)Â·(1 - Ïƒ(z))
```

### ğŸ¯ å†³ç­–è§„åˆ™

```
ç»™å®šè¾“å…¥ xï¼Œè®¡ç®—ï¼š
  z = b + wÂ·x
  P(y=1|x) = Ïƒ(z)

å†³ç­–ï¼š
  å¦‚æœ P(y=1|x) â‰¥ 0.5  â†’ é¢„æµ‹ä¸ºç±»åˆ« 1
  å¦‚æœ P(y=1|x) < 0.5  â†’ é¢„æµ‹ä¸ºç±»åˆ« 0

ç­‰ä»·äºï¼š
  å¦‚æœ z â‰¥ 0  â†’ é¢„æµ‹ä¸ºç±»åˆ« 1
  å¦‚æœ z < 0  â†’ é¢„æµ‹ä¸ºç±»åˆ« 0
```

### ğŸ“Š å†³ç­–è¾¹ç•Œ (Decision Boundary)

**ä¸€ç»´æƒ…å†µ**ï¼š

```
z = b + wÂ·x = 0
â†’ x = -b/w  (å†³ç­–è¾¹ç•Œ)

ä¾‹ï¼šb = -3, w = 1
  x < 3 â†’ é¢„æµ‹ç±»åˆ« 0
  x > 3 â†’ é¢„æµ‹ç±»åˆ« 1
```

**äºŒç»´æƒ…å†µ**ï¼š

```
z = b + wâ‚xâ‚ + wâ‚‚xâ‚‚ = 0
â†’ xâ‚‚ = -(b + wâ‚xâ‚)/wâ‚‚  (ä¸€æ¡ç›´çº¿)

xâ‚‚
â†‘
|     /
|    /  ç±»åˆ« 1
|   /
|  /_________ å†³ç­–è¾¹ç•Œ
| /
|/ ç±»åˆ« 0
|________â†’ xâ‚
```

---

## 3.4 Loss Function for Classification

### ğŸš« ä¸ºä»€ä¹ˆä¸ç”¨ MSEï¼Ÿ

**å°è¯•**ï¼š`L = (y - Ïƒ(z))Â²`

**é—®é¢˜**ï¼š
1. **éå‡¸å‡½æ•°**ï¼šæœ‰å¾ˆå¤šå±€éƒ¨æœ€ä¼˜è§£
2. **æ¢¯åº¦æ¶ˆå¤±**ï¼šå½“é¢„æµ‹å¾ˆé”™æ—¶ï¼Œæ¢¯åº¦åè€Œå¾ˆå°

```
Loss
 â†‘
 |  *     *
 | / \   / \   å¤šä¸ªå±€éƒ¨æœ€ä¼˜ï¼
 |/   \_/   \
 |___________â†’ w
```

### âœ… Cross Entropy Loss

**å…¬å¼**ï¼š

```
å¯¹äºå•ä¸ªæ ·æœ¬ï¼š
L(y, Å·) = -[yÂ·log(Å·) + (1-y)Â·log(1-Å·)]

å…¶ä¸­ï¼š
  y âˆˆ {0, 1}     çœŸå®æ ‡ç­¾
  Å· = Ïƒ(z)       é¢„æµ‹æ¦‚ç‡
```

**ç†è§£**ï¼š

```
æƒ…å†µ1ï¼šçœŸå®æ ‡ç­¾ y = 1
  L = -log(Å·)
  å¦‚æœ Å· â†’ 1 (é¢„æµ‹æ­£ç¡®)  â†’ L â†’ 0   (æŸå¤±å°)
  å¦‚æœ Å· â†’ 0 (é¢„æµ‹é”™è¯¯)  â†’ L â†’ âˆ   (æŸå¤±å¤§)

æƒ…å†µ2ï¼šçœŸå®æ ‡ç­¾ y = 0
  L = -log(1-Å·)
  å¦‚æœ Å· â†’ 0 (é¢„æµ‹æ­£ç¡®)  â†’ L â†’ 0   (æŸå¤±å°)
  å¦‚æœ Å· â†’ 1 (é¢„æµ‹é”™è¯¯)  â†’ L â†’ âˆ   (æŸå¤±å¤§)
```

### ğŸ“Š å¯è§†åŒ– Cross Entropy

```python
import numpy as np
import matplotlib.pyplot as plt

y_pred = np.linspace(0.01, 0.99, 100)

# y = 1 æ—¶çš„æŸå¤±
loss_y1 = -np.log(y_pred)

# y = 0 æ—¶çš„æŸå¤±
loss_y0 = -np.log(1 - y_pred)

plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(y_pred, loss_y1, 'b-', linewidth=2)
plt.title('çœŸå®æ ‡ç­¾ y = 1')
plt.xlabel('é¢„æµ‹æ¦‚ç‡ Å·')
plt.ylabel('Loss = -log(Å·)')
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
plt.plot(y_pred, loss_y0, 'r-', linewidth=2)
plt.title('çœŸå®æ ‡ç­¾ y = 0')
plt.xlabel('é¢„æµ‹æ¦‚ç‡ Å·')
plt.ylabel('Loss = -log(1-Å·)')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

### ğŸ§® å®Œæ•´çš„ Loss Function

å¯¹äº N ä¸ªè®­ç»ƒæ ·æœ¬ï¼š

```
L(w, b) = -(1/N) Î£[yâ¿Â·log(Å·â¿) + (1-yâ¿)Â·log(1-Å·â¿)]

å…¶ä¸­ï¼š
  Å·â¿ = Ïƒ(b + wÂ·xâ¿)
```

---

## 3.5 æ¢¯åº¦ä¸‹é™æ±‚è§£

### ğŸ“ è®¡ç®—æ¢¯åº¦

```
Å· = Ïƒ(z) = Ïƒ(b + wÂ·x)

L = -[yÂ·log(Å·) + (1-y)Â·log(1-Å·)]

æ±‚å¯¼ï¼ˆé“¾å¼æ³•åˆ™ï¼‰ï¼š

âˆ‚L/âˆ‚w = (Å· - y)Â·x
âˆ‚L/âˆ‚b = (Å· - y)

æƒŠå–œï¼å½¢å¼å’Œçº¿æ€§å›å½’ä¸€æ ·ç®€å•ï¼
```

### ğŸ’» ä»é›¶å®ç° Logistic Regression

```python
import numpy as np

class LogisticRegression:
    def __init__(self, learning_rate=0.01, epochs=1000):
        self.lr = learning_rate
        self.epochs = epochs
        self.w = None
        self.b = None
        self.losses = []

    def sigmoid(self, z):
        """Sigmoid å‡½æ•°"""
        return 1 / (1 + np.exp(-z))

    def fit(self, X, y):
        """è®­ç»ƒæ¨¡å‹"""
        n_samples, n_features = X.shape

        # åˆå§‹åŒ–å‚æ•°
        self.w = np.zeros(n_features)
        self.b = 0

        # æ¢¯åº¦ä¸‹é™
        for epoch in range(self.epochs):
            # å‰å‘ä¼ æ’­
            z = np.dot(X, self.w) + self.b
            y_pred = self.sigmoid(z)

            # è®¡ç®—æŸå¤±
            loss = -np.mean(y * np.log(y_pred + 1e-9) +
                           (1 - y) * np.log(1 - y_pred + 1e-9))
            self.losses.append(loss)

            # è®¡ç®—æ¢¯åº¦
            dw = np.dot(X.T, (y_pred - y)) / n_samples
            db = np.mean(y_pred - y)

            # æ›´æ–°å‚æ•°
            self.w -= self.lr * dw
            self.b -= self.lr * db

            # æ‰“å°è¿›åº¦
            if epoch % 100 == 0:
                print(f"Epoch {epoch}: Loss = {loss:.4f}")

    def predict_proba(self, X):
        """é¢„æµ‹æ¦‚ç‡"""
        z = np.dot(X, self.w) + self.b
        return self.sigmoid(z)

    def predict(self, X, threshold=0.5):
        """é¢„æµ‹ç±»åˆ«"""
        proba = self.predict_proba(X)
        return (proba >= threshold).astype(int)

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
    np.random.seed(42)

    # ç±»åˆ« 0
    X0 = np.random.randn(100, 2) + np.array([2, 2])
    y0 = np.zeros(100)

    # ç±»åˆ« 1
    X1 = np.random.randn(100, 2) + np.array([5, 5])
    y1 = np.ones(100)

    # åˆå¹¶
    X = np.vstack([X0, X1])
    y = np.hstack([y0, y1])

    # è®­ç»ƒ
    model = LogisticRegression(learning_rate=0.1, epochs=1000)
    model.fit(X, y)

    # é¢„æµ‹
    predictions = model.predict(X)
    accuracy = np.mean(predictions == y)
    print(f"\nå‡†ç¡®ç‡: {accuracy:.4f}")
```

**è¾“å‡º**ï¼š
```
Epoch 0: Loss = 0.6931
Epoch 100: Loss = 0.2156
Epoch 200: Loss = 0.1398
Epoch 300: Loss = 0.1045
Epoch 400: Loss = 0.0850
Epoch 500: Loss = 0.0722
Epoch 600: Loss = 0.0632
Epoch 700: Loss = 0.0565
Epoch 800: Loss = 0.0513
Epoch 900: Loss = 0.0472

å‡†ç¡®ç‡: 1.0000
```

### ğŸ“Š å¯è§†åŒ–å†³ç­–è¾¹ç•Œ

```python
import matplotlib.pyplot as plt

def plot_decision_boundary(model, X, y):
    # åˆ›å»ºç½‘æ ¼
    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1

    xx1, xx2 = np.meshgrid(
        np.linspace(x1_min, x1_max, 100),
        np.linspace(x2_min, x2_max, 100)
    )

    # é¢„æµ‹æ¯ä¸ªç½‘æ ¼ç‚¹
    Z = model.predict(np.c_[xx1.ravel(), xx2.ravel()])
    Z = Z.reshape(xx1.shape)

    # ç»˜å›¾
    plt.figure(figsize=(10, 8))
    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap='RdYlBu')
    plt.contour(xx1, xx2, Z, levels=[0.5], colors='black', linewidths=2)

    # ç»˜åˆ¶æ•°æ®ç‚¹
    plt.scatter(X[y==0, 0], X[y==0, 1], c='blue', label='ç±»åˆ« 0',
                edgecolors='k', s=50)
    plt.scatter(X[y==1, 0], X[y==1, 1], c='red', label='ç±»åˆ« 1',
                edgecolors='k', s=50)

    plt.xlabel('ç‰¹å¾ 1')
    plt.ylabel('ç‰¹å¾ 2')
    plt.title('é€»è¾‘å›å½’å†³ç­–è¾¹ç•Œ')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.show()

# ç»˜åˆ¶
plot_decision_boundary(model, X, y)
```

---

## 3.6 è¯„ä¼°åˆ†ç±»æ¨¡å‹

### ğŸ“Š Confusion Matrix (æ··æ·†çŸ©é˜µ)

```
                é¢„æµ‹
              0      1
çœŸ   0      TN     FP
å®   1      FN     TP

TN (True Negative):  æ­£ç¡®é¢„æµ‹ä¸ºè´Ÿç±»
TP (True Positive):  æ­£ç¡®é¢„æµ‹ä¸ºæ­£ç±»
FN (False Negative): é”™è¯¯é¢„æµ‹ä¸ºè´Ÿç±»ï¼ˆæ¼æŠ¥ï¼‰
FP (False Positive): é”™è¯¯é¢„æµ‹ä¸ºæ­£ç±»ï¼ˆè¯¯æŠ¥ï¼‰
```

**ä¾‹å­**ï¼šç™Œç—‡æ£€æµ‹

```
                é¢„æµ‹
           æ²¡ç—…    æœ‰ç—…
çœŸ æ²¡ç—…    90      10    (10ä¸ªå‡é˜³æ€§)
å® æœ‰ç—…     5      95    (5ä¸ªå‡é˜´æ€§)
```

### ğŸ”¢ è¯„ä¼°æŒ‡æ ‡

#### **1. Accuracy (å‡†ç¡®ç‡)**

```
Accuracy = (TP + TN) / (TP + TN + FP + FN)

= æ­£ç¡®é¢„æµ‹çš„æ•°é‡ / æ€»æ•°é‡

ä¾‹ï¼š(90 + 95) / 200 = 0.925 = 92.5%
```

**é—®é¢˜**ï¼šç±»åˆ«ä¸å¹³è¡¡æ—¶ä¼šè¯¯å¯¼

```
ä¾‹ï¼š100ä¸ªæ ·æœ¬ï¼Œ95ä¸ªè´Ÿç±»ï¼Œ5ä¸ªæ­£ç±»
å¦‚æœå…¨éƒ¨é¢„æµ‹ä¸ºè´Ÿç±»ï¼š
  Accuracy = 95/100 = 95%  (çœ‹èµ·æ¥å¾ˆå¥½ï¼)
  ä½†å®Œå…¨æ²¡æœ‰æ£€æµ‹åˆ°æ­£ç±»ï¼
```

#### **2. Precision (ç²¾ç¡®ç‡)**

```
Precision = TP / (TP + FP)

= é¢„æµ‹ä¸ºæ­£ç±»ä¸­ï¼ŒçœŸæ­£æ˜¯æ­£ç±»çš„æ¯”ä¾‹

ä¾‹ï¼š95 / (95 + 10) = 0.905 = 90.5%

ç†è§£ï¼šåœ¨æˆ‘è¯´"æœ‰ç—…"çš„äººä¸­ï¼ŒçœŸçš„æœ‰ç—…çš„æ¯”ä¾‹
```

**ä½¿ç”¨åœºæ™¯**ï¼šå½“ **è¯¯æŠ¥ä»£ä»·é«˜** æ—¶
- åƒåœ¾é‚®ä»¶è¿‡æ»¤ï¼šä¸è¦æŠŠæ­£å¸¸é‚®ä»¶æ ‡è®°ä¸ºåƒåœ¾é‚®ä»¶
- ä¿¡ç”¨å¡æ¬ºè¯ˆï¼šä¸è¦è¯¯æŠ¥æ­£å¸¸äº¤æ˜“

#### **3. Recall (å¬å›ç‡ / çµæ•åº¦)**

```
Recall = TP / (TP + FN)

= çœŸæ­£çš„æ­£ç±»ä¸­ï¼Œè¢«æ­£ç¡®é¢„æµ‹çš„æ¯”ä¾‹

ä¾‹ï¼š95 / (95 + 5) = 0.95 = 95%

ç†è§£ï¼šæ‰€æœ‰çœŸæ­£æœ‰ç—…çš„äººä¸­ï¼Œè¢«æ£€æµ‹å‡ºæ¥çš„æ¯”ä¾‹
```

**ä½¿ç”¨åœºæ™¯**ï¼šå½“ **æ¼æŠ¥ä»£ä»·é«˜** æ—¶
- ç–¾ç—…æ£€æµ‹ï¼šä¸èƒ½æ¼æ‰çœŸæ­£çš„ç—…äºº
- æ¬ºè¯ˆæ£€æµ‹ï¼šä¸èƒ½æ¼æ‰çœŸæ­£çš„æ¬ºè¯ˆ

#### **4. F1 Score**

```
F1 = 2 Â· (Precision Â· Recall) / (Precision + Recall)

= Precision å’Œ Recall çš„è°ƒå’Œå¹³å‡æ•°

ä¾‹ï¼š2 Â· (0.905 Â· 0.95) / (0.905 + 0.95) = 0.927
```

**ç‰¹ç‚¹**ï¼šå¹³è¡¡ Precision å’Œ Recall

#### **5. ROC æ›²çº¿å’Œ AUC**

**ROC (Receiver Operating Characteristic) æ›²çº¿**ï¼š

```
TPR (True Positive Rate) = Recall = TP/(TP+FN)
FPR (False Positive Rate) = FP/(FP+TN)

æ¨ªè½´ï¼šFPR (å‡é˜³æ€§ç‡)
çºµè½´ï¼šTPR (çœŸé˜³æ€§ç‡)
```

**AUC (Area Under Curve)**ï¼šROC æ›²çº¿ä¸‹é¢ç§¯
- AUC = 1: å®Œç¾åˆ†ç±»å™¨
- AUC = 0.5: éšæœºçŒœæµ‹
- AUC > 0.8: é€šå¸¸è®¤ä¸ºä¸é”™

### ğŸ’» è®¡ç®—è¯„ä¼°æŒ‡æ ‡

```python
from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    confusion_matrix,
    classification_report,
    roc_curve,
    roc_auc_score
)
import matplotlib.pyplot as plt

# é¢„æµ‹
y_pred = model.predict(X)
y_proba = model.predict_proba(X)

# 1. æ··æ·†çŸ©é˜µ
cm = confusion_matrix(y, y_pred)
print("æ··æ·†çŸ©é˜µï¼š")
print(cm)

# 2. åŸºæœ¬æŒ‡æ ‡
accuracy = accuracy_score(y, y_pred)
precision = precision_score(y, y_pred)
recall = recall_score(y, y_pred)
f1 = f1_score(y, y_pred)

print(f"\nAccuracy:  {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall:    {recall:.4f}")
print(f"F1 Score:  {f1:.4f}")

# 3. å®Œæ•´æŠ¥å‘Š
print("\nåˆ†ç±»æŠ¥å‘Šï¼š")
print(classification_report(y, y_pred,
                           target_names=['ç±»åˆ«0', 'ç±»åˆ«1']))

# 4. ROC æ›²çº¿
fpr, tpr, thresholds = roc_curve(y, y_proba)
auc = roc_auc_score(y, y_proba)

plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, 'b-', linewidth=2, label=f'ROC (AUC = {auc:.3f})')
plt.plot([0, 1], [0, 1], 'r--', label='éšæœºçŒœæµ‹')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()

# 5. Precision-Recall æ›²çº¿
from sklearn.metrics import precision_recall_curve

precision_vals, recall_vals, _ = precision_recall_curve(y, y_proba)

plt.figure(figsize=(8, 6))
plt.plot(recall_vals, precision_vals, 'b-', linewidth=2)
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.grid(True, alpha=0.3)
plt.show()
```

---

## 3.7 å¤šå…ƒåˆ†ç±» (Multi-class Classification)

### ğŸ¯ ä»äºŒå…ƒåˆ°å¤šå…ƒ

**é—®é¢˜**ï¼šæ‰‹å†™æ•°å­—è¯†åˆ« (0-9)ï¼Œå…± 10 ä¸ªç±»åˆ«

### ğŸ”¹ æ–¹æ³• 1ï¼šOne-vs-Rest (OvR)

**ç­–ç•¥**ï¼šè®­ç»ƒ K ä¸ªäºŒå…ƒåˆ†ç±»å™¨

```
åˆ†ç±»å™¨ 1: ç±»åˆ« 0 vs å…¶ä»–
åˆ†ç±»å™¨ 2: ç±»åˆ« 1 vs å…¶ä»–
...
åˆ†ç±»å™¨ K: ç±»åˆ« K-1 vs å…¶ä»–

é¢„æµ‹æ—¶ï¼šé€‰æ‹©è¾“å‡ºæ¦‚ç‡æœ€å¤§çš„åˆ†ç±»å™¨
```

**ä¾‹å­**ï¼šè¯†åˆ«æ•°å­— 3

```
åˆ†ç±»å™¨ 1 (0 vs å…¶ä»–): P = 0.05
åˆ†ç±»å™¨ 2 (1 vs å…¶ä»–): P = 0.10
åˆ†ç±»å™¨ 3 (2 vs å…¶ä»–): P = 0.08
åˆ†ç±»å™¨ 4 (3 vs å…¶ä»–): P = 0.95  âœ“ æœ€å¤§
...
åˆ†ç±»å™¨ 10 (9 vs å…¶ä»–): P = 0.03

â†’ é¢„æµ‹ä¸ºç±»åˆ« 3
```

### ğŸ”¹ æ–¹æ³• 2ï¼šSoftmax Regression

**æ ¸å¿ƒæ€æƒ³**ï¼šæ‰©å±• Sigmoid åˆ°å¤šä¸ªç±»åˆ«

#### Softmax å‡½æ•°

```
ç»™å®š K ä¸ªç±»åˆ«ï¼Œè®¡ç®— K ä¸ªåˆ†æ•°ï¼š

zâ‚ = bâ‚ + wâ‚áµ€x
zâ‚‚ = bâ‚‚ + wâ‚‚áµ€x
...
zâ‚– = bâ‚– + wâ‚–áµ€x

Softmax:
P(y=i|x) = e^(záµ¢) / Î£â±¼ e^(zâ±¼)

æ€§è´¨ï¼š
1. æ‰€æœ‰æ¦‚ç‡å’Œä¸º 1: Î£áµ¢ P(y=i|x) = 1
2. æ¯ä¸ªæ¦‚ç‡éƒ½åœ¨ (0, 1)
3. å¦‚æœ K=2ï¼Œé€€åŒ–ä¸º Sigmoid
```

#### å¯è§†åŒ–ç†è§£

```
ä¾‹ï¼š3ä¸ªç±»åˆ«

zâ‚ = 2.0    â†’  e^2.0 = 7.39
zâ‚‚ = 1.0    â†’  e^1.0 = 2.72
zâ‚ƒ = 0.1    â†’  e^0.1 = 1.11
                ________
                Sum = 11.22

P(y=1) = 7.39/11.22 = 0.659  (65.9%)
P(y=2) = 2.72/11.22 = 0.242  (24.2%)
P(y=3) = 1.11/11.22 = 0.099  (9.9%)

é¢„æµ‹ï¼šç±»åˆ« 1 (æ¦‚ç‡æœ€é«˜)
```

### ğŸ“ Cross Entropy for Multi-class

```
å¯¹äºå•ä¸ªæ ·æœ¬ï¼š
L = -Î£áµ¢ yáµ¢Â·log(Å·áµ¢)

å…¶ä¸­ï¼š
  yáµ¢: one-hot ç¼–ç çš„çœŸå®æ ‡ç­¾
  Å·áµ¢: softmax è¾“å‡ºçš„é¢„æµ‹æ¦‚ç‡

ä¾‹ï¼šçœŸå®ç±»åˆ«æ˜¯ 2 (å…±3ä¸ªç±»åˆ«)
y = [0, 1, 0]       (one-hot)
Å· = [0.1, 0.7, 0.2]  (é¢„æµ‹)

L = -(0Â·log(0.1) + 1Â·log(0.7) + 0Â·log(0.2))
  = -log(0.7)
  = 0.357
```

### ğŸ’» å®ç° Softmax Regression

```python
import numpy as np

class SoftmaxRegression:
    def __init__(self, learning_rate=0.01, epochs=1000):
        self.lr = learning_rate
        self.epochs = epochs
        self.W = None
        self.b = None

    def softmax(self, z):
        """Softmax å‡½æ•°"""
        # å‡å»æœ€å¤§å€¼é˜²æ­¢æ•°å€¼æº¢å‡º
        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))
        return exp_z / np.sum(exp_z, axis=1, keepdims=True)

    def fit(self, X, y):
        """è®­ç»ƒæ¨¡å‹"""
        n_samples, n_features = X.shape
        n_classes = len(np.unique(y))

        # One-hot ç¼–ç æ ‡ç­¾
        y_onehot = np.eye(n_classes)[y]

        # åˆå§‹åŒ–å‚æ•°
        self.W = np.random.randn(n_features, n_classes) * 0.01
        self.b = np.zeros(n_classes)

        # æ¢¯åº¦ä¸‹é™
        for epoch in range(self.epochs):
            # å‰å‘ä¼ æ’­
            z = np.dot(X, self.W) + self.b
            y_pred = self.softmax(z)

            # è®¡ç®—æŸå¤±
            loss = -np.mean(np.sum(y_onehot * np.log(y_pred + 1e-9), axis=1))

            # è®¡ç®—æ¢¯åº¦
            dz = y_pred - y_onehot
            dW = np.dot(X.T, dz) / n_samples
            db = np.mean(dz, axis=0)

            # æ›´æ–°å‚æ•°
            self.W -= self.lr * dW
            self.b -= self.lr * db

            if epoch % 100 == 0:
                accuracy = np.mean(np.argmax(y_pred, axis=1) == y)
                print(f"Epoch {epoch}: Loss = {loss:.4f}, Acc = {accuracy:.4f}")

    def predict_proba(self, X):
        """é¢„æµ‹æ¦‚ç‡"""
        z = np.dot(X, self.W) + self.b
        return self.softmax(z)

    def predict(self, X):
        """é¢„æµ‹ç±»åˆ«"""
        proba = self.predict_proba(X)
        return np.argmax(proba, axis=1)

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    from sklearn.datasets import load_iris
    from sklearn.model_selection import train_test_split

    # åŠ è½½é¸¢å°¾èŠ±æ•°æ®é›†ï¼ˆ3ä¸ªç±»åˆ«ï¼‰
    iris = load_iris()
    X, y = iris.data, iris.target

    # åˆ†å‰²æ•°æ®
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )

    # è®­ç»ƒ
    model = SoftmaxRegression(learning_rate=0.1, epochs=1000)
    model.fit(X_train, y_train)

    # æµ‹è¯•
    y_pred = model.predict(X_test)
    accuracy = np.mean(y_pred == y_test)
    print(f"\næµ‹è¯•é›†å‡†ç¡®ç‡: {accuracy:.4f}")
```

---

## 3.8 å®æˆ˜ 1ï¼šä¿¡ç”¨å¡æ¬ºè¯ˆæ£€æµ‹

### ğŸ“‹ é—®é¢˜æè¿°

**æ•°æ®é›†**ï¼šKaggle Credit Card Fraud Detection
- 284,807 ç¬”äº¤æ˜“è®°å½•
- 492 ç¬”æ¬ºè¯ˆï¼ˆ0.172%ï¼‰â† æåº¦ä¸å¹³è¡¡ï¼
- 30 ä¸ªç‰¹å¾ï¼ˆPCA å¤„ç†è¿‡ï¼Œå·²è„±æ•ï¼‰

### âš ï¸ ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜

```
æ­£å¸¸äº¤æ˜“: 284,315  (99.83%)
æ¬ºè¯ˆäº¤æ˜“:     492  (0.17%)

å¦‚æœå…¨éƒ¨é¢„æµ‹ä¸º"æ­£å¸¸"ï¼š
  Accuracy = 99.83%  (çœ‹èµ·æ¥å¾ˆé«˜ï¼)
  ä½†å®Œå…¨æ²¡æœ‰æ£€æµ‹åˆ°æ¬ºè¯ˆï¼
```

### ğŸ’¡ å¤„ç†æ–¹æ³•

#### **æ–¹æ³• 1ï¼šé‡é‡‡æ ·**

**æ¬ é‡‡æ · (Under-sampling)**ï¼š
```
å‡å°‘å¤šæ•°ç±»æ ·æœ¬
æ­£å¸¸: 284,315 â†’ 492
æ¬ºè¯ˆ:     492 â†’ 492

ä¼˜ç‚¹ï¼šå¹³è¡¡æ•°æ®é›†
ç¼ºç‚¹ï¼šä¸¢å¤±å¤§é‡ä¿¡æ¯
```

**è¿‡é‡‡æ · (Over-sampling)**ï¼š
```
å¢åŠ å°‘æ•°ç±»æ ·æœ¬ï¼ˆå¤åˆ¶æˆ–ç”Ÿæˆï¼‰
æ­£å¸¸: 284,315 â†’ 284,315
æ¬ºè¯ˆ:     492 â†’ 284,315

ä¼˜ç‚¹ï¼šä¿ç•™æ‰€æœ‰ä¿¡æ¯
ç¼ºç‚¹ï¼šå¯èƒ½è¿‡æ‹Ÿåˆ
```

**SMOTE (Synthetic Minority Over-sampling)**ï¼š
```
åˆæˆæ–°çš„å°‘æ•°ç±»æ ·æœ¬
ä¸æ˜¯ç®€å•å¤åˆ¶ï¼Œè€Œæ˜¯åœ¨ç‰¹å¾ç©ºé—´ä¸­æ’å€¼ç”Ÿæˆ
```

#### **æ–¹æ³• 2ï¼šè°ƒæ•´ç±»åˆ«æƒé‡**

```python
from sklearn.linear_model import LogisticRegression

# è‡ªåŠ¨è®¡ç®—æƒé‡
model = LogisticRegression(class_weight='balanced')

# æ‰‹åŠ¨è®¾ç½®
model = LogisticRegression(class_weight={0: 1, 1: 100})
```

#### **æ–¹æ³• 3ï¼šè°ƒæ•´å†³ç­–é˜ˆå€¼**

```python
# é»˜è®¤é˜ˆå€¼ 0.5
y_pred = (y_proba >= 0.5).astype(int)

# é™ä½é˜ˆå€¼ï¼Œæé«˜å¬å›ç‡
y_pred = (y_proba >= 0.3).astype(int)

# æé«˜é˜ˆå€¼ï¼Œæé«˜ç²¾ç¡®ç‡
y_pred = (y_proba >= 0.7).astype(int)
```

#### **æ–¹æ³• 4ï¼šä½¿ç”¨é€‚å½“çš„è¯„ä¼°æŒ‡æ ‡**

```
ä¸è¦ç”¨ Accuracyï¼
åº”è¯¥ç”¨ï¼š
  - F1 Score
  - Precision-Recall AUC
  - ROC AUC
```

### ğŸ’» å®Œæ•´ä»£ç 

```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (
    classification_report,
    confusion_matrix,
    roc_auc_score,
    precision_recall_curve
)
from imblearn.over_sampling import SMOTE
import matplotlib.pyplot as plt
import seaborn as sns

# 1. åŠ è½½æ•°æ®
df = pd.read_csv('creditcard.csv')

print("æ•°æ®å½¢çŠ¶:", df.shape)
print("\nç±»åˆ«åˆ†å¸ƒ:")
print(df['Class'].value_counts())
print(f"\næ¬ºè¯ˆæ¯”ä¾‹: {df['Class'].mean():.4%}")

# 2. å‡†å¤‡æ•°æ®
X = df.drop('Class', axis=1)
y = df['Class']

# 3. åˆ†å‰²æ•°æ®
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# 4. ç‰¹å¾ç¼©æ”¾
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# 5. å¤„ç†ä¸å¹³è¡¡ï¼šSMOTE
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(
    X_train_scaled, y_train
)

print(f"\nSMOTE å‰: {len(y_train)} æ ·æœ¬")
print(f"SMOTE å: {len(y_train_resampled)} æ ·æœ¬")
print(f"æ–°çš„ç±»åˆ«åˆ†å¸ƒ:\n{pd.Series(y_train_resampled).value_counts()}")

# 6. è®­ç»ƒæ¨¡å‹
model = LogisticRegression(max_iter=1000, random_state=42)
model.fit(X_train_resampled, y_train_resampled)

# 7. é¢„æµ‹
y_pred = model.predict(X_test_scaled)
y_proba = model.predict_proba(X_test_scaled)[:, 1]

# 8. è¯„ä¼°
print("\næ··æ·†çŸ©é˜µ:")
cm = confusion_matrix(y_test, y_pred)
print(cm)

print("\nåˆ†ç±»æŠ¥å‘Š:")
print(classification_report(y_test, y_pred,
                           target_names=['æ­£å¸¸', 'æ¬ºè¯ˆ']))

print(f"\nROC AUC: {roc_auc_score(y_test, y_proba):.4f}")

# 9. å¯è§†åŒ–æ··æ·†çŸ©é˜µ
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel('é¢„æµ‹æ ‡ç­¾')
plt.ylabel('çœŸå®æ ‡ç­¾')
plt.title('æ··æ·†çŸ©é˜µ')
plt.show()

# 10. Precision-Recall æ›²çº¿
precision, recall, thresholds = precision_recall_curve(y_test, y_proba)

plt.figure(figsize=(10, 6))
plt.plot(thresholds, precision[:-1], 'b-', label='Precision')
plt.plot(thresholds, recall[:-1], 'r-', label='Recall')
plt.xlabel('é˜ˆå€¼')
plt.ylabel('åˆ†æ•°')
plt.title('Precision vs Recall vs é˜ˆå€¼')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()

# 11. æ‰¾æœ€ä¼˜é˜ˆå€¼
f1_scores = 2 * (precision * recall) / (precision + recall + 1e-9)
optimal_idx = np.argmax(f1_scores[:-1])
optimal_threshold = thresholds[optimal_idx]

print(f"\næœ€ä¼˜é˜ˆå€¼: {optimal_threshold:.4f}")
print(f"å¯¹åº” F1 Score: {f1_scores[optimal_idx]:.4f}")

# 12. ä½¿ç”¨æœ€ä¼˜é˜ˆå€¼é‡æ–°é¢„æµ‹
y_pred_optimal = (y_proba >= optimal_threshold).astype(int)

print("\nä½¿ç”¨æœ€ä¼˜é˜ˆå€¼çš„ç»“æœ:")
print(classification_report(y_test, y_pred_optimal,
                           target_names=['æ­£å¸¸', 'æ¬ºè¯ˆ']))
```

---

## 3.9 å®æˆ˜ 2ï¼šæ‰‹å†™æ•°å­—è¯†åˆ« (MNIST)

### ğŸ“‹ MNIST æ•°æ®é›†

```
70,000 å¼ æ‰‹å†™æ•°å­—å›¾ç‰‡
  - 60,000 è®­ç»ƒé›†
  - 10,000 æµ‹è¯•é›†

æ¯å¼ å›¾ç‰‡ï¼š
  - 28Ã—28 åƒç´ 
  - ç°åº¦å€¼ 0-255
  - æ ‡ç­¾ï¼š0-9
```

### ğŸ’» å®Œæ•´ä»£ç 

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_openml
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import seaborn as sns

# 1. åŠ è½½æ•°æ®
print("åŠ è½½ MNIST æ•°æ®...")
mnist = fetch_openml('mnist_784', version=1, parser='auto')
X, y = mnist['data'], mnist['target'].astype(int)

print(f"æ•°æ®å½¢çŠ¶: {X.shape}")
print(f"æ ‡ç­¾å½¢çŠ¶: {y.shape}")

# 2. å¯è§†åŒ–ä¸€äº›æ ·æœ¬
fig, axes = plt.subplots(2, 5, figsize=(12, 5))
for i, ax in enumerate(axes.flat):
    ax.imshow(X.iloc[i].values.reshape(28, 28), cmap='gray')
    ax.set_title(f'æ ‡ç­¾: {y.iloc[i]}')
    ax.axis('off')
plt.tight_layout()
plt.show()

# 3. æ•°æ®é¢„å¤„ç†
# å½’ä¸€åŒ–åˆ° [0, 1]
X = X / 255.0

# ä½¿ç”¨éƒ¨åˆ†æ•°æ®ï¼ˆåŠ å¿«è®­ç»ƒï¼‰
X_subset = X[:10000]
y_subset = y[:10000]

# åˆ†å‰²æ•°æ®
X_train, X_test, y_train, y_test = train_test_split(
    X_subset, y_subset, test_size=0.2, random_state=42
)

# 4. è®­ç»ƒæ¨¡å‹
print("\nè®­ç»ƒé€»è¾‘å›å½’æ¨¡å‹...")
model = LogisticRegression(
    max_iter=100,
    multi_class='multinomial',  # Softmax
    solver='lbfgs',             # ä¼˜åŒ–ç®—æ³•
    random_state=42
)
model.fit(X_train, y_train)

# 5. é¢„æµ‹
y_pred = model.predict(X_test)

# 6. è¯„ä¼°
accuracy = accuracy_score(y_test, y_pred)
print(f"\nå‡†ç¡®ç‡: {accuracy:.4f}")

print("\nåˆ†ç±»æŠ¥å‘Š:")
print(classification_report(y_test, y_pred))

# 7. æ··æ·†çŸ©é˜µ
cm = confusion_matrix(y_test, y_pred)

plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel('é¢„æµ‹æ ‡ç­¾')
plt.ylabel('çœŸå®æ ‡ç­¾')
plt.title('æ··æ·†çŸ©é˜µ')
plt.show()

# 8. å¯è§†åŒ–ä¸€äº›é¢„æµ‹ç»“æœ
fig, axes = plt.subplots(3, 5, figsize=(12, 7))
indices = np.random.choice(len(X_test), 15, replace=False)

for i, (ax, idx) in enumerate(zip(axes.flat, indices)):
    image = X_test.iloc[idx].values.reshape(28, 28)
    true_label = y_test.iloc[idx]
    pred_label = y_pred[idx]

    ax.imshow(image, cmap='gray')
    color = 'green' if true_label == pred_label else 'red'
    ax.set_title(f'çœŸå®: {true_label}, é¢„æµ‹: {pred_label}', color=color)
    ax.axis('off')

plt.tight_layout()
plt.show()

# 9. æŸ¥çœ‹æ¨¡å‹å­¦åˆ°çš„æƒé‡
# æ¯ä¸ªæ•°å­—çš„æƒé‡å¯ä»¥çœ‹ä½œä¸€ä¸ª"æ¨¡æ¿"
fig, axes = plt.subplots(2, 5, figsize=(12, 5))
for digit, ax in enumerate(axes.flat):
    weight = model.coef_[digit].reshape(28, 28)
    ax.imshow(weight, cmap='RdBu', vmin=-weight.max(), vmax=weight.max())
    ax.set_title(f'æ•°å­— {digit}çš„æƒé‡')
    ax.axis('off')
plt.tight_layout()
plt.show()
```

---

## 3.10 é€»è¾‘å›å½’çš„ä¼˜ç¼ºç‚¹

### âœ… ä¼˜ç‚¹

1. **ç®€å•é«˜æ•ˆ**
   - æ˜“äºå®ç°å’Œç†è§£
   - è®­ç»ƒé€Ÿåº¦å¿«
   - é¢„æµ‹é€Ÿåº¦å¿«

2. **å¯è§£é‡Šæ€§å¼º**
   - å¯ä»¥çœ‹æƒé‡äº†è§£ç‰¹å¾é‡è¦æ€§
   - è¾“å‡ºæ¦‚ç‡ï¼Œæ–¹ä¾¿å†³ç­–

3. **ä¸éœ€è¦å¤ªå¤šæ•°æ®**
   - ç›¸æ¯”æ·±åº¦å­¦ä¹ ï¼Œæ•°æ®éœ€æ±‚å°‘

4. **ä¸å®¹æ˜“è¿‡æ‹Ÿåˆ**
   - æ¨¡å‹ç®€å•ï¼Œæ³›åŒ–èƒ½åŠ›å¥½
   - å¯ä»¥ç”¨æ­£åˆ™åŒ–è¿›ä¸€æ­¥æ§åˆ¶

### âŒ ç¼ºç‚¹

1. **çº¿æ€§æ¨¡å‹**
   - åªèƒ½å­¦ä¹ çº¿æ€§å†³ç­–è¾¹ç•Œ
   - æ— æ³•å¤„ç†å¤æ‚çš„éçº¿æ€§å…³ç³»

2. **ç‰¹å¾å·¥ç¨‹ä¾èµ–**
   - éœ€è¦æ‰‹åŠ¨è®¾è®¡å¥½çš„ç‰¹å¾
   - ç‰¹å¾è´¨é‡å†³å®šæ¨¡å‹ä¸Šé™

3. **å¤šé‡å…±çº¿æ€§æ•æ„Ÿ**
   - ç‰¹å¾é«˜åº¦ç›¸å…³æ—¶ï¼Œæ¨¡å‹ä¸ç¨³å®š

4. **ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜**
   - éœ€è¦ç‰¹æ®Šå¤„ç†

---

## ğŸ“ æœ¬ç« ä½œä¸š

### ä½œä¸š 1ï¼šæ¦‚å¿µé¢˜

1. **ä¸ºä»€ä¹ˆä¸èƒ½ç”¨çº¿æ€§å›å½’åšåˆ†ç±»ï¼Ÿ**
   - ä¸¾ä¾‹è¯´æ˜é—®é¢˜
   - ç”»å›¾è§£é‡Š

2. **Sigmoid vs Softmax**
   - ä»€ä¹ˆæ—¶å€™ç”¨ Sigmoidï¼Ÿ
   - ä»€ä¹ˆæ—¶å€™ç”¨ Softmaxï¼Ÿ
   - å®ƒä»¬çš„å…³ç³»æ˜¯ä»€ä¹ˆï¼Ÿ

3. **è¯„ä¼°æŒ‡æ ‡é€‰æ‹©**
   - ç™Œç—‡æ£€æµ‹åº”è¯¥å…³æ³¨ Precision è¿˜æ˜¯ Recallï¼Ÿä¸ºä»€ä¹ˆï¼Ÿ
   - åƒåœ¾é‚®ä»¶è¿‡æ»¤åº”è¯¥å…³æ³¨å“ªä¸ªï¼Ÿ
   - ç»™å‡º 3 ä¸ªåœºæ™¯å’Œå¯¹åº”çš„æœ€é‡è¦æŒ‡æ ‡

### ä½œä¸š 2ï¼šç¼–ç¨‹å®è·µ

#### ä»»åŠ¡ 1ï¼šä¹³è…ºç™Œæ£€æµ‹

```python
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report

# åŠ è½½æ•°æ®
data = load_breast_cancer()
X, y = data.data, data.target

# TODO:
# 1. åˆ†å‰²æ•°æ®ï¼ˆ80/20ï¼‰
# 2. ç‰¹å¾ç¼©æ”¾
# 3. è®­ç»ƒé€»è¾‘å›å½’æ¨¡å‹
# 4. è¯„ä¼°æ¨¡å‹ï¼ˆæ··æ·†çŸ©é˜µã€ROCæ›²çº¿ï¼‰
# 5. è°ƒæ•´å†³ç­–é˜ˆå€¼ï¼Œä¼˜åŒ– Recall
# 6. æ¯”è¾ƒä¸åŒæ­£åˆ™åŒ–å¼ºåº¦ï¼ˆCå‚æ•°ï¼‰çš„æ•ˆæœ
```

#### ä»»åŠ¡ 2ï¼šå¤šåˆ†ç±»å®æˆ˜

ä½¿ç”¨ Scikit-learn çš„ `load_digits` æ•°æ®é›†ï¼ˆ8Ã—8 æ‰‹å†™æ•°å­—ï¼‰

```python
from sklearn.datasets import load_digits

# TODO:
# 1. åŠ è½½æ•°æ®å¹¶å¯è§†åŒ–
# 2. è®­ç»ƒ Softmax Regression
# 3. åˆ†æå“ªäº›æ•°å­—å®¹æ˜“æ··æ·†ï¼ˆçœ‹æ··æ·†çŸ©é˜µï¼‰
# 4. å¯è§†åŒ–æ¨¡å‹å­¦åˆ°çš„æƒé‡
# 5. å®ç° One-vs-Rest æ–¹æ³•å¹¶å¯¹æ¯”æ€§èƒ½
```

### ä½œä¸š 3ï¼šKaggle ç«èµ›

å‚åŠ  "Titanic - Machine Learning from Disaster"

**è¦æ±‚**ï¼š
1. æ•°æ®æ¢ç´¢å’Œå¯è§†åŒ–
2. ç‰¹å¾å·¥ç¨‹ï¼ˆå¤„ç†ç¼ºå¤±å€¼ã€åˆ›å»ºæ–°ç‰¹å¾ï¼‰
3. è®­ç»ƒé€»è¾‘å›å½’æ¨¡å‹
4. è°ƒæ•´è¶…å‚æ•°
5. æäº¤é¢„æµ‹ç»“æœ
6. å†™ä¸€ä»½å®Œæ•´æŠ¥å‘Š

---

## ğŸ”‘ æœ¬ç« å…³é”®æ¦‚å¿µæ€»ç»“

| æ¦‚å¿µ | è¯´æ˜ |
|------|------|
| åˆ†ç±» | é¢„æµ‹ç¦»æ•£ç±»åˆ« |
| Sigmoid | å°†å®æ•°æ˜ å°„åˆ° (0,1) |
| é€»è¾‘å›å½’ | ç”¨äºäºŒå…ƒåˆ†ç±» |
| Cross Entropy | åˆ†ç±»é—®é¢˜çš„æŸå¤±å‡½æ•° |
| Softmax | å¤šå…ƒåˆ†ç±»çš„æ¿€æ´»å‡½æ•° |
| Confusion Matrix | è¯„ä¼°åˆ†ç±»æ€§èƒ½ |
| Precision | é¢„æµ‹ä¸ºæ­£çš„å‡†ç¡®ç‡ |
| Recall | æ‰¾å‡ºæ‰€æœ‰æ­£ä¾‹çš„èƒ½åŠ› |
| F1 Score | Precision å’Œ Recall çš„è°ƒå’Œå¹³å‡ |
| ROC/AUC | è¯„ä¼°åˆ†ç±»å™¨æ€§èƒ½ |
| ç±»åˆ«ä¸å¹³è¡¡ | ç±»åˆ«æ ·æœ¬æ•°å·®å¼‚å¤§ |
| SMOTE | åˆæˆå°‘æ•°ç±»æ ·æœ¬ |

---

## ğŸ¯ ä¸‹ä¸€ç« é¢„å‘Š

**ç¬¬å››ç« ï¼šæ·±åº¦ç¥ç»ç½‘ç»œåŸºç¡€ (Deep Neural Networks)**
- ä»é€»è¾‘å›å½’åˆ°ç¥ç»ç½‘ç»œ
- æ¿€æ´»å‡½æ•° (ReLU, Tanh, etc.)
- åå‘ä¼ æ’­ç®—æ³•
- æ·±åº¦ç½‘ç»œçš„è®­ç»ƒæŠ€å·§
- å®æˆ˜ï¼šç”¨ç¥ç»ç½‘ç»œæ”¹è¿› MNIST

---

-----
