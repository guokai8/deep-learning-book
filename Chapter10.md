# ç¬¬åç« ï¼šå¼ºåŒ–å­¦ä¹  (Reinforcement Learning)

## ğŸ“Œ ç« èŠ‚ç›®æ ‡
- ç†è§£å¼ºåŒ–å­¦ä¹ çš„åŸºæœ¬æ¦‚å¿µå’Œæ¡†æ¶
- æŒæ¡ Q-Learning å’Œ DQN ç®—æ³•
- å­¦ä¹ ç­–ç•¥æ¢¯åº¦æ–¹æ³•
- äº†è§£ Actor-Critic æ¶æ„
- å®æˆ˜ï¼šè®­ç»ƒ Agent ç©æ¸¸æˆ

---

## 10.1 å¼ºåŒ–å­¦ä¹ åŸºç¡€

### ğŸ¯ ä»€ä¹ˆæ˜¯å¼ºåŒ–å­¦ä¹ ï¼Ÿ

**å®šä¹‰**ï¼šæ™ºèƒ½ä½“ï¼ˆAgentï¼‰é€šè¿‡ä¸ç¯å¢ƒï¼ˆEnvironmentï¼‰äº¤äº’ï¼Œå­¦ä¹ æœ€ä¼˜ç­–ç•¥ä»¥æœ€å¤§åŒ–ç´¯ç§¯å¥–åŠ±ã€‚

**ä¸ç›‘ç£å­¦ä¹ çš„åŒºåˆ«**ï¼š

```
ç›‘ç£å­¦ä¹ ï¼š
  è¾“å…¥ â†’ æ¨¡å‹ â†’ è¾“å‡º
  æœ‰æ˜ç¡®çš„æ ‡ç­¾æŒ‡å¯¼

å¼ºåŒ–å­¦ä¹ ï¼š
  çŠ¶æ€ â†’ åŠ¨ä½œ â†’ å¥–åŠ± + æ–°çŠ¶æ€
  æ²¡æœ‰æ˜ç¡®æ ‡ç­¾ï¼Œåªæœ‰å¥–åŠ±ä¿¡å·
  éœ€è¦æ¢ç´¢å’Œè¯•é”™
```

### ğŸ“ æ ¸å¿ƒæ¦‚å¿µ

#### **é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ (MDP)**

```
MDP = (S, A, P, R, Î³)

S: çŠ¶æ€ç©ºé—´ (States)
A: åŠ¨ä½œç©ºé—´ (Actions)
P: çŠ¶æ€è½¬ç§»æ¦‚ç‡ P(s'|s,a)
R: å¥–åŠ±å‡½æ•° R(s,a,s')
Î³: æŠ˜æ‰£å› å­ (0 â‰¤ Î³ < 1)
```

**ç¤ºä¾‹ï¼šè¿·å®«æ¸¸æˆ**

```
â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”
â”‚ S â”‚   â”‚   â”‚   â”‚  S = èµ·ç‚¹
â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤  G = ç»ˆç‚¹
â”‚   â”‚ â–“ â”‚   â”‚   â”‚  â–“ = éšœç¢
â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤
â”‚   â”‚   â”‚ â–“ â”‚   â”‚  åŠ¨ä½œ: â†‘â†“â†â†’
â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤
â”‚   â”‚   â”‚   â”‚ G â”‚  å¥–åŠ±: åˆ°è¾¾G=+10, ç¢°å£=-1
â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜
```

---

#### **å…³é”®æœ¯è¯­**

**1. ç­–ç•¥ (Policy) Ï€**

```
Ï€(a|s): åœ¨çŠ¶æ€ s ä¸‹é€‰æ‹©åŠ¨ä½œ a çš„æ¦‚ç‡

ç¡®å®šæ€§ç­–ç•¥: a = Ï€(s)
éšæœºç­–ç•¥: a ~ Ï€(Â·|s)
```

**2. ä»·å€¼å‡½æ•° (Value Function)**

```
çŠ¶æ€ä»·å€¼å‡½æ•° V^Ï€(s):
  V^Ï€(s) = E[âˆ‘(t=0 to âˆ) Î³^tÂ·r_t | s_0=s, Ï€]

  è¡¨ç¤ºï¼šä»çŠ¶æ€ s å¼€å§‹ï¼Œéµå¾ªç­–ç•¥ Ï€ çš„æœŸæœ›ç´¯ç§¯å¥–åŠ±

åŠ¨ä½œä»·å€¼å‡½æ•° Q^Ï€(s,a):
  Q^Ï€(s,a) = E[âˆ‘(t=0 to âˆ) Î³^tÂ·r_t | s_0=s, a_0=a, Ï€]

  è¡¨ç¤ºï¼šåœ¨çŠ¶æ€ s æ‰§è¡ŒåŠ¨ä½œ a åï¼Œéµå¾ªç­–ç•¥ Ï€ çš„æœŸæœ›ç´¯ç§¯å¥–åŠ±
```

**3. æœ€ä¼˜ç­–ç•¥**

```
Ï€* = argmax_Ï€ V^Ï€(s)  å¯¹æ‰€æœ‰ s

æœ€ä¼˜ä»·å€¼å‡½æ•°:
  V*(s) = max_Ï€ V^Ï€(s)
  Q*(s,a) = max_Ï€ Q^Ï€(s,a)

è´å°”æ›¼æœ€ä¼˜æ–¹ç¨‹:
  V*(s) = max_a [R(s,a) + Î³Â·âˆ‘_s' P(s'|s,a)Â·V*(s')]
  Q*(s,a) = R(s,a) + Î³Â·âˆ‘_s' P(s'|s,a)Â·max_a' Q*(s',a')
```

---

#### **æ¢ç´¢ vs åˆ©ç”¨ (Exploration vs Exploitation)**

```
åˆ©ç”¨ (Exploitation):
  é€‰æ‹©å½“å‰å·²çŸ¥æœ€å¥½çš„åŠ¨ä½œ

æ¢ç´¢ (Exploration):
  å°è¯•æ–°åŠ¨ä½œï¼Œå‘ç°æ›´å¥½çš„ç­–ç•¥

å¹³è¡¡ç­–ç•¥:
  Îµ-greedy: ä»¥æ¦‚ç‡ Îµ éšæœºæ¢ç´¢ï¼Œå¦åˆ™åˆ©ç”¨
  Softmax: æ ¹æ® Q å€¼åˆ†å¸ƒé‡‡æ ·
  UCB (Upper Confidence Bound)
```

---

## 10.2 Q-Learning

### ğŸ“ ç®—æ³•åŸç†

**æ ¸å¿ƒæ€æƒ³**ï¼šå­¦ä¹ åŠ¨ä½œä»·å€¼å‡½æ•° Q(s,a)

**æ›´æ–°è§„åˆ™**ï¼š

```
Q(s,a) â† Q(s,a) + Î±Â·[r + Î³Â·max_a' Q(s',a') - Q(s,a)]

å…¶ä¸­ï¼š
  Î±: å­¦ä¹ ç‡
  r: å³æ—¶å¥–åŠ±
  Î³: æŠ˜æ‰£å› å­
  s': ä¸‹ä¸€çŠ¶æ€

æ—¶åºå·®åˆ†è¯¯å·® (TD Error):
  Î´ = r + Î³Â·max_a' Q(s',a') - Q(s,a)
```

### ğŸ’» ä»é›¶å®ç° Q-Learning

```python
import numpy as np
import matplotlib.pyplot as plt
from collections import defaultdict

class QLearningAgent:
    """Q-Learning æ™ºèƒ½ä½“"""

    def __init__(self, n_states, n_actions,
                 learning_rate=0.1, discount_factor=0.95,
                 epsilon=0.1, epsilon_decay=0.995, epsilon_min=0.01):
        self.n_states = n_states
        self.n_actions = n_actions
        self.lr = learning_rate
        self.gamma = discount_factor
        self.epsilon = epsilon
        self.epsilon_decay = epsilon_decay
        self.epsilon_min = epsilon_min

        # Q è¡¨ï¼šå­—å…¸å½¢å¼
        self.q_table = defaultdict(lambda: np.zeros(n_actions))

    def get_action(self, state, training=True):
        """
        é€‰æ‹©åŠ¨ä½œï¼ˆÎµ-greedy ç­–ç•¥ï¼‰

        å‚æ•°:
            state: å½“å‰çŠ¶æ€
            training: æ˜¯å¦è®­ç»ƒæ¨¡å¼
        """
        if training and np.random.random() < self.epsilon:
            # æ¢ç´¢ï¼šéšæœºåŠ¨ä½œ
            return np.random.randint(self.n_actions)
        else:
            # åˆ©ç”¨ï¼šé€‰æ‹©æœ€ä¼˜åŠ¨ä½œ
            return np.argmax(self.q_table[state])

    def update(self, state, action, reward, next_state, done):
        """
        æ›´æ–° Q å€¼

        å‚æ•°:
            state: å½“å‰çŠ¶æ€
            action: æ‰§è¡Œçš„åŠ¨ä½œ
            reward: è·å¾—çš„å¥–åŠ±
            next_state: ä¸‹ä¸€çŠ¶æ€
            done: æ˜¯å¦ç»ˆæ­¢
        """
        # å½“å‰ Q å€¼
        current_q = self.q_table[state][action]

        # ç›®æ ‡ Q å€¼
        if done:
            target_q = reward
        else:
            target_q = reward + self.gamma * np.max(self.q_table[next_state])

        # æ›´æ–° Q å€¼
        self.q_table[state][action] += self.lr * (target_q - current_q)

        # è¡°å‡ epsilon
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay

    def get_q_values(self, state):
        """è·å–çŠ¶æ€çš„æ‰€æœ‰ Q å€¼"""
        return self.q_table[state]

# ==================== ç¯å¢ƒï¼šç½‘æ ¼ä¸–ç•Œ ====================

class GridWorld:
    """ç®€å•çš„ç½‘æ ¼ä¸–ç•Œç¯å¢ƒ"""

    def __init__(self, size=5):
        self.size = size
        self.n_states = size * size
        self.n_actions = 4  # ä¸Šä¸‹å·¦å³

        # èµ·ç‚¹å’Œç»ˆç‚¹
        self.start_pos = (0, 0)
        self.goal_pos = (size-1, size-1)

        # éšœç¢ç‰©
        self.obstacles = [(1, 1), (2, 2), (3, 1)]

        # å½“å‰ä½ç½®
        self.current_pos = self.start_pos

    def reset(self):
        """é‡ç½®ç¯å¢ƒ"""
        self.current_pos = self.start_pos
        return self._pos_to_state(self.current_pos)

    def _pos_to_state(self, pos):
        """ä½ç½®è½¬çŠ¶æ€ç¼–å·"""
        return pos[0] * self.size + pos[1]

    def _state_to_pos(self, state):
        """çŠ¶æ€ç¼–å·è½¬ä½ç½®"""
        return (state // self.size, state % self.size)

    def step(self, action):
        """
        æ‰§è¡ŒåŠ¨ä½œ

        åŠ¨ä½œç¼–ç : 0=ä¸Š, 1=ä¸‹, 2=å·¦, 3=å³

        è¿”å›: (next_state, reward, done, info)
        """
        row, col = self.current_pos

        # ç§»åŠ¨
        if action == 0:  # ä¸Š
            row = max(0, row - 1)
        elif action == 1:  # ä¸‹
            row = min(self.size - 1, row + 1)
        elif action == 2:  # å·¦
            col = max(0, col - 1)
        elif action == 3:  # å³
            col = min(self.size - 1, col + 1)

        next_pos = (row, col)

        # æ£€æŸ¥éšœç¢ç‰©
        if next_pos in self.obstacles:
            next_pos = self.current_pos  # ç¢°å£ä¸ç§»åŠ¨
            reward = -1
        elif next_pos == self.goal_pos:
            reward = 10  # åˆ°è¾¾ç»ˆç‚¹
        else:
            reward = -0.1  # æ¯æ­¥å°æƒ©ç½š

        self.current_pos = next_pos
        next_state = self._pos_to_state(next_pos)
        done = (next_pos == self.goal_pos)

        return next_state, reward, done, {}

    def render(self):
        """å¯è§†åŒ–ç¯å¢ƒ"""
        grid = np.zeros((self.size, self.size))

        # æ ‡è®°éšœç¢ç‰©
        for obs in self.obstacles:
            grid[obs] = -1

        # æ ‡è®°ç»ˆç‚¹
        grid[self.goal_pos] = 2

        # æ ‡è®°å½“å‰ä½ç½®
        grid[self.current_pos] = 1

        return grid

# ==================== è®­ç»ƒ Q-Learning ====================

def train_qlearning(env, agent, num_episodes=1000):
    """è®­ç»ƒ Q-Learning Agent"""

    rewards_history = []
    epsilon_history = []

    for episode in range(num_episodes):
        state = env.reset()
        total_reward = 0
        done = False
        steps = 0

        while not done and steps < 100:
            # é€‰æ‹©åŠ¨ä½œ
            action = agent.get_action(state, training=True)

            # æ‰§è¡ŒåŠ¨ä½œ
            next_state, reward, done, _ = env.step(action)

            # æ›´æ–° Q å€¼
            agent.update(state, action, reward, next_state, done)

            state = next_state
            total_reward += reward
            steps += 1

        rewards_history.append(total_reward)
        epsilon_history.append(agent.epsilon)

        if (episode + 1) % 100 == 0:
            avg_reward = np.mean(rewards_history[-100:])
            print(f'Episode {episode+1}: Avg Reward = {avg_reward:.2f}, '
                  f'Epsilon = {agent.epsilon:.3f}')

    return rewards_history, epsilon_history

# ==================== å¯è§†åŒ– ====================

def visualize_policy(agent, env):
    """å¯è§†åŒ–å­¦åˆ°çš„ç­–ç•¥"""

    action_symbols = ['â†‘', 'â†“', 'â†', 'â†’']

    fig, ax = plt.subplots(figsize=(8, 8))

    # ç»˜åˆ¶ç½‘æ ¼
    for i in range(env.size):
        for j in range(env.size):
            pos = (i, j)
            state = env._pos_to_state(pos)

            # èƒŒæ™¯è‰²
            if pos == env.goal_pos:
                color = 'green'
            elif pos in env.obstacles:
                color = 'gray'
            else:
                color = 'white'

            rect = plt.Rectangle((j, env.size-1-i), 1, 1,
                                 facecolor=color, edgecolor='black')
            ax.add_patch(rect)

            # æœ€ä¼˜åŠ¨ä½œ
            if pos not in env.obstacles and pos != env.goal_pos:
                action = agent.get_action(state, training=False)
                ax.text(j+0.5, env.size-1-i+0.5, action_symbols[action],
                       ha='center', va='center', fontsize=20)

    ax.set_xlim(0, env.size)
    ax.set_ylim(0, env.size)
    ax.set_aspect('equal')
    ax.set_title('å­¦åˆ°çš„ç­–ç•¥', fontsize=16)
    ax.axis('off')

    plt.tight_layout()
    plt.savefig('policy.png', dpi=300)
    plt.show()

def plot_training_results(rewards_history, epsilon_history):
    """ç»˜åˆ¶è®­ç»ƒç»“æœ"""

    fig, axes = plt.subplots(1, 2, figsize=(15, 5))

    # å¥–åŠ±æ›²çº¿
    episodes = np.arange(len(rewards_history))
    axes[0].plot(episodes, rewards_history, alpha=0.3, label='åŸå§‹')

    # ç§»åŠ¨å¹³å‡
    window = 100
    moving_avg = np.convolve(rewards_history,
                            np.ones(window)/window,
                            mode='valid')
    axes[0].plot(episodes[window-1:], moving_avg,
                label=f'{window} Episode ç§»åŠ¨å¹³å‡', linewidth=2)

    axes[0].set_xlabel('Episode')
    axes[0].set_ylabel('Total Reward')
    axes[0].set_title('è®­ç»ƒå¥–åŠ±')
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)

    # Epsilon è¡°å‡
    axes[1].plot(epsilon_history)
    axes[1].set_xlabel('Episode')
    axes[1].set_ylabel('Epsilon')
    axes[1].set_title('æ¢ç´¢ç‡è¡°å‡')
    axes[1].grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig('training_results.png', dpi=300)
    plt.show()

# ==================== ä¸»ç¨‹åº ====================

if __name__ == '__main__':
    # åˆ›å»ºç¯å¢ƒ
    env = GridWorld(size=5)

    # åˆ›å»º Agent
    agent = QLearningAgent(
        n_states=env.n_states,
        n_actions=env.n_actions,
        learning_rate=0.1,
        discount_factor=0.95,
        epsilon=1.0,
        epsilon_decay=0.995,
        epsilon_min=0.01
    )

    # è®­ç»ƒ
    print("å¼€å§‹è®­ç»ƒ...")
    rewards_history, epsilon_history = train_qlearning(env, agent, num_episodes=1000)

    # å¯è§†åŒ–ç»“æœ
    plot_training_results(rewards_history, epsilon_history)
    visualize_policy(agent, env)

    # æµ‹è¯•å­¦åˆ°çš„ç­–ç•¥
    print("\næµ‹è¯•ç­–ç•¥:")
    state = env.reset()
    done = False
    steps = 0

    print("åˆå§‹çŠ¶æ€:")
    print(env.render())

    while not done and steps < 20:
        action = agent.get_action(state, training=False)
        state, reward, done, _ = env.step(action)
        steps += 1

        print(f"\næ­¥éª¤ {steps}:")
        print(env.render())

        if done:
            print(f"\næˆåŠŸåˆ°è¾¾ç»ˆç‚¹ï¼æ€»æ­¥æ•°: {steps}")
```

---

## 10.3 Deep Q-Network (DQN)

### ğŸ¯ ä» Q-Learning åˆ° DQN

**Q-Learning çš„å±€é™**ï¼š

```
é—®é¢˜ï¼š
  1. çŠ¶æ€ç©ºé—´å¤§æ—¶ï¼ŒQ è¡¨æ— æ³•å­˜å‚¨
     (å¦‚ Atari æ¸¸æˆï¼šåƒç´ çŠ¶æ€ç©ºé—´å·¨å¤§)

  2. æ— æ³•æ³›åŒ–åˆ°æœªè§è¿‡çš„çŠ¶æ€

è§£å†³ï¼š
  ç”¨ç¥ç»ç½‘ç»œé€¼è¿‘ Q å‡½æ•°
  Q(s,a) â‰ˆ Q(s,a;Î¸)
```

### ğŸ“ DQN æ ¸å¿ƒåˆ›æ–°

**1. ç»éªŒå›æ”¾ (Experience Replay)**

```
é—®é¢˜ï¼š
  - è¿ç»­æ ·æœ¬é«˜åº¦ç›¸å…³
  - ç ´å IID å‡è®¾

è§£å†³ï¼š
  - å­˜å‚¨ç»éªŒåˆ° Replay Buffer
  - éšæœºé‡‡æ · mini-batch è®­ç»ƒ

Buffer: (s, a, r, s', done)
```

**2. ç›®æ ‡ç½‘ç»œ (Target Network)**

```
é—®é¢˜ï¼š
  - ç›®æ ‡ Q å€¼ä¹Ÿåœ¨å˜åŒ–
  - è®­ç»ƒä¸ç¨³å®š

è§£å†³ï¼š
  - ä½¿ç”¨å•ç‹¬çš„ç›®æ ‡ç½‘ç»œ Q'(s,a;Î¸â»)
  - å®šæœŸä»ä¸»ç½‘ç»œå¤åˆ¶å‚æ•°

æŸå¤±å‡½æ•°:
  L(Î¸) = E[(r + Î³Â·max_a' Q'(s',a';Î¸â») - Q(s,a;Î¸))Â²]
```

### ğŸ’» å®ç° DQN

```python
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from collections import deque
import random
import gym
import numpy as np

# ==================== ç¥ç»ç½‘ç»œ ====================

class DQN(nn.Module):
    """Deep Q-Network"""

    def __init__(self, state_dim, action_dim, hidden_dim=128):
        super(DQN, self).__init__()

        self.fc1 = nn.Linear(state_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, action_dim)

    def forward(self, state):
        """
        å‰å‘ä¼ æ’­

        å‚æ•°:
            state: (batch_size, state_dim)

        è¿”å›:
            Qå€¼: (batch_size, action_dim)
        """
        x = F.relu(self.fc1(state))
        x = F.relu(self.fc2(x))
        q_values = self.fc3(x)
        return q_values

# ==================== ç»éªŒå›æ”¾ ====================

class ReplayBuffer:
    """ç»éªŒå›æ”¾ç¼“å†²åŒº"""

    def __init__(self, capacity=10000):
        self.buffer = deque(maxlen=capacity)

    def push(self, state, action, reward, next_state, done):
        """æ·»åŠ ç»éªŒ"""
        self.buffer.append((state, action, reward, next_state, done))

    def sample(self, batch_size):
        """éšæœºé‡‡æ ·"""
        batch = random.sample(self.buffer, batch_size)

        states, actions, rewards, next_states, dones = zip(*batch)

        return (
            np.array(states),
            np.array(actions),
            np.array(rewards),
            np.array(next_states),
            np.array(dones)
        )

    def __len__(self):
        return len(self.buffer)

# ==================== DQN Agent ====================

class DQNAgent:
    """DQN æ™ºèƒ½ä½“"""

    def __init__(self, state_dim, action_dim,
                 learning_rate=1e-3, gamma=0.99,
                 epsilon_start=1.0, epsilon_end=0.01, epsilon_decay=0.995,
                 buffer_size=10000, batch_size=64,
                 target_update_freq=10):

        self.state_dim = state_dim
        self.action_dim = action_dim
        self.gamma = gamma
        self.epsilon = epsilon_start
        self.epsilon_end = epsilon_end
        self.epsilon_decay = epsilon_decay
        self.batch_size = batch_size
        self.target_update_freq = target_update_freq

        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

        # ä¸»ç½‘ç»œå’Œç›®æ ‡ç½‘ç»œ
        self.policy_net = DQN(state_dim, action_dim).to(self.device)
        self.target_net = DQN(state_dim, action_dim).to(self.device)
        self.target_net.load_state_dict(self.policy_net.state_dict())
        self.target_net.eval()

        # ä¼˜åŒ–å™¨
        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=learning_rate)

        # ç»éªŒå›æ”¾
        self.memory = ReplayBuffer(buffer_size)

        # è®­ç»ƒæ­¥æ•°
        self.steps = 0

    def select_action(self, state, training=True):
        """é€‰æ‹©åŠ¨ä½œï¼ˆÎµ-greedyï¼‰"""
        if training and random.random() < self.epsilon:
            return random.randrange(self.action_dim)
        else:
            with torch.no_grad():
                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
                q_values = self.policy_net(state_tensor)
                return q_values.argmax(dim=1).item()

    def train_step(self):
        """è®­ç»ƒä¸€æ­¥"""
        if len(self.memory) < self.batch_size:
            return None

        # é‡‡æ ·ç»éªŒ
        states, actions, rewards, next_states, dones = self.memory.sample(self.batch_size)

        # è½¬ä¸º tensor
        states = torch.FloatTensor(states).to(self.device)
        actions = torch.LongTensor(actions).unsqueeze(1).to(self.device)
        rewards = torch.FloatTensor(rewards).unsqueeze(1).to(self.device)
        next_states = torch.FloatTensor(next_states).to(self.device)
        dones = torch.FloatTensor(dones).unsqueeze(1).to(self.device)

        # å½“å‰ Q å€¼
        current_q_values = self.policy_net(states).gather(1, actions)

        # ç›®æ ‡ Q å€¼
        with torch.no_grad():
            next_q_values = self.target_net(next_states).max(1)[0].unsqueeze(1)
            target_q_values = rewards + self.gamma * next_q_values * (1 - dones)

        # è®¡ç®—æŸå¤±
        loss = F.mse_loss(current_q_values, target_q_values)

        # åå‘ä¼ æ’­
        self.optimizer.zero_grad()
        loss.backward()

        # æ¢¯åº¦è£å‰ª
        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), max_norm=10)

        self.optimizer.step()

        # æ›´æ–°ç›®æ ‡ç½‘ç»œ
        self.steps += 1
        if self.steps % self.target_update_freq == 0:
            self.target_net.load_state_dict(self.policy_net.state_dict())

        # è¡°å‡ epsilon
        if self.epsilon > self.epsilon_end:
            self.epsilon *= self.epsilon_decay

        return loss.item()

    def save(self, path):
        """ä¿å­˜æ¨¡å‹"""
        torch.save({
            'policy_net': self.policy_net.state_dict(),
            'target_net': self.target_net.state_dict(),
            'optimizer': self.optimizer.state_dict(),
            'steps': self.steps,
            'epsilon': self.epsilon
        }, path)

    def load(self, path):
        """åŠ è½½æ¨¡å‹"""
        checkpoint = torch.load(path)
        self.policy_net.load_state_dict(checkpoint['policy_net'])
        self.target_net.load_state_dict(checkpoint['target_net'])
        self.optimizer.load_state_dict(checkpoint['optimizer'])
        self.steps = checkpoint['steps']
        self.epsilon = checkpoint['epsilon']

# ==================== è®­ç»ƒ DQN ====================

def train_dqn(env_name='CartPole-v1', num_episodes=500):
    """è®­ç»ƒ DQN"""

    # åˆ›å»ºç¯å¢ƒ
    env = gym.make(env_name)
    state_dim = env.observation_space.shape[0]
    action_dim = env.action_space.n

    # åˆ›å»º Agent
    agent = DQNAgent(
        state_dim=state_dim,
        action_dim=action_dim,
        learning_rate=1e-3,
        gamma=0.99,
        epsilon_start=1.0,
        epsilon_end=0.01,
        epsilon_decay=0.995,
        buffer_size=10000,
        batch_size=64,
        target_update_freq=10
    )

    # è®­ç»ƒå†å²
    rewards_history = []
    losses_history = []

    for episode in range(num_episodes):
        state = env.reset()
        if isinstance(state, tuple):
            state = state[0]  # gym æ–°ç‰ˆæœ¬è¿”å› (state, info)

        total_reward = 0
        episode_losses = []
        done = False

        while not done:
            # é€‰æ‹©åŠ¨ä½œ
            action = agent.select_action(state, training=True)

            # æ‰§è¡ŒåŠ¨ä½œ
            result = env.step(action)
            if len(result) == 5:  # æ–°ç‰ˆæœ¬ gym
                next_state, reward, terminated, truncated, _ = result
                done = terminated or truncated
            else:  # æ—§ç‰ˆæœ¬
                next_state, reward, done, _ = result

            # å­˜å‚¨ç»éªŒ
            agent.memory.push(state, action, reward, next_state, done)

            # è®­ç»ƒ
            loss = agent.train_step()
            if loss is not None:
                episode_losses.append(loss)

            state = next_state
            total_reward += reward

        rewards_history.append(total_reward)
        if episode_losses:
            losses_history.append(np.mean(episode_losses))

        if (episode + 1) % 10 == 0:
            avg_reward = np.mean(rewards_history[-10:])
            print(f'Episode {episode+1}: Avg Reward = {avg_reward:.2f}, '
                  f'Epsilon = {agent.epsilon:.3f}, '
                  f'Buffer Size = {len(agent.memory)}')

    env.close()

    return agent, rewards_history, losses_history

# ==================== å¯è§†åŒ– ====================

def plot_dqn_results(rewards_history, losses_history):
    """ç»˜åˆ¶ DQN è®­ç»ƒç»“æœ"""

    fig, axes = plt.subplots(1, 2, figsize=(15, 5))

    # å¥–åŠ±æ›²çº¿
    episodes = np.arange(len(rewards_history))
    axes[0].plot(episodes, rewards_history, alpha=0.3)

    window = 10
    moving_avg = np.convolve(rewards_history,
                            np.ones(window)/window,
                            mode='valid')
    axes[0].plot(episodes[window-1:], moving_avg,
                linewidth=2, label=f'{window} Episode MA')

    axes[0].set_xlabel('Episode')
    axes[0].set_ylabel('Total Reward')
    axes[0].set_title('DQN è®­ç»ƒå¥–åŠ±')
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)

    # æŸå¤±æ›²çº¿
    if losses_history:
        axes[1].plot(losses_history)
        axes[1].set_xlabel('Episode')
        axes[1].set_ylabel('Loss')
        axes[1].set_title('DQN è®­ç»ƒæŸå¤±')
        axes[1].grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig('dqn_results.png', dpi=300)
    plt.show()

# ==================== ä¸»ç¨‹åº ====================

if __name__ == '__main__':
    print("å¼€å§‹è®­ç»ƒ DQN...")
    agent, rewards, losses = train_dqn('CartPole-v1', num_episodes=500)

    # å¯è§†åŒ–
    plot_dqn_results(rewards, losses)

    # ä¿å­˜æ¨¡å‹
    agent.save('dqn_model.pth')

    # æµ‹è¯•
    print("\næµ‹è¯•è®­ç»ƒå¥½çš„ Agent:")
    env = gym.make('CartPole-v1', render_mode='human')
    state = env.reset()[0]

    total_reward = 0
    done = False

    while not done:
        action = agent.select_action(state, training=False)
        result = env.step(action)
        state, reward, terminated, truncated, _ = result
        done = terminated or truncated
        total_reward += reward

    print(f"æµ‹è¯•æ€»å¥–åŠ±: {total_reward}")
    env.close()
```

---

## 10.4 Policy Gradient ç­–ç•¥æ¢¯åº¦

### ğŸ¯ æ ¸å¿ƒæ€æƒ³

**ä¸ Q-Learning çš„åŒºåˆ«**ï¼š

```
Q-Learning (Value-based):
  å­¦ä¹ ä»·å€¼å‡½æ•° Q(s,a)
  é—´æ¥å¾—åˆ°ç­–ç•¥ï¼šÏ€(s) = argmax_a Q(s,a)

Policy Gradient (Policy-based):
  ç›´æ¥å­¦ä¹ ç­–ç•¥ Ï€(a|s;Î¸)
  ä¼˜åŒ–ç›®æ ‡ï¼šæœ€å¤§åŒ–æœŸæœ›å›æŠ¥
```

### ğŸ“ REINFORCE ç®—æ³•

**ç›®æ ‡å‡½æ•°**ï¼š

```
J(Î¸) = E_Ï„~Ï€_Î¸ [âˆ‘_t r_t]

æ¢¯åº¦ï¼ˆç­–ç•¥æ¢¯åº¦å®šç†ï¼‰ï¼š
âˆ‡_Î¸ J(Î¸) = E_Ï„~Ï€_Î¸ [âˆ‘_t âˆ‡_Î¸ log Ï€_Î¸(a_t|s_t) Â· G_t]

å…¶ä¸­ï¼š
  G_t = âˆ‘_{t'=t}^T Î³^{t'-t} Â· r_{t'}  (ç´¯ç§¯å›æŠ¥)
```

**ç›´è§‰ç†è§£**ï¼š

```
å¦‚æœåŠ¨ä½œ a å¸¦æ¥äº†æ­£å›æŠ¥ï¼š
  â†’ å¢åŠ  log Ï€(a|s)
  â†’ æé«˜è¯¥åŠ¨ä½œçš„æ¦‚ç‡

å¦‚æœåŠ¨ä½œ a å¸¦æ¥äº†è´Ÿå›æŠ¥ï¼š
  â†’ å‡å°‘ log Ï€(a|s)
  â†’ é™ä½è¯¥åŠ¨ä½œçš„æ¦‚ç‡
```

### ğŸ’» å®ç° REINFORCE

```python
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.distributions import Categorical
import gym
import numpy as np
import matplotlib.pyplot as plt

# ==================== ç­–ç•¥ç½‘ç»œ ====================

class PolicyNetwork(nn.Module):
    """ç­–ç•¥ç½‘ç»œ"""

    def __init__(self, state_dim, action_dim, hidden_dim=128):
        super(PolicyNetwork, self).__init__()

        self.fc1 = nn.Linear(state_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, action_dim)

    def forward(self, state):
        """
        å‰å‘ä¼ æ’­

        è¿”å›åŠ¨ä½œæ¦‚ç‡åˆ†å¸ƒ
        """
        x = F.relu(self.fc1(state))
        x = F.relu(self.fc2(x))
        action_probs = F.softmax(self.fc3(x), dim=-1)
        return action_probs

# ==================== REINFORCE Agent ====================

class REINFORCEAgent:
    """REINFORCE æ™ºèƒ½ä½“"""

    def __init__(self, state_dim, action_dim,
                 learning_rate=1e-3, gamma=0.99):

        self.gamma = gamma
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

        # ç­–ç•¥ç½‘ç»œ
        self.policy = PolicyNetwork(state_dim, action_dim).to(self.device)
        self.optimizer = optim.Adam(self.policy.parameters(), lr=learning_rate)

        # å­˜å‚¨è½¨è¿¹
        self.saved_log_probs = []
        self.rewards = []

    def select_action(self, state):
        """
        æ ¹æ®ç­–ç•¥é‡‡æ ·åŠ¨ä½œ

        è¿”å›: åŠ¨ä½œ + log æ¦‚ç‡
        """
        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)

        # è·å–åŠ¨ä½œæ¦‚ç‡åˆ†å¸ƒ
        action_probs = self.policy(state_tensor)

        # é‡‡æ ·åŠ¨ä½œ
        dist = Categorical(action_probs)
        action = dist.sample()

        # ä¿å­˜ log æ¦‚ç‡
        self.saved_log_probs.append(dist.log_prob(action))

        return action.item()

    def compute_returns(self, rewards):
        """
        è®¡ç®—ç´¯ç§¯å›æŠ¥ (discounted returns)

        G_t = r_t + Î³Â·r_{t+1} + Î³Â²Â·r_{t+2} + ...
        """
        returns = []
        G = 0

        for r in reversed(rewards):
            G = r + self.gamma * G
            returns.insert(0, G)

        # æ ‡å‡†åŒ–ï¼ˆå‡å°æ–¹å·®ï¼‰
        returns = torch.tensor(returns).to(self.device)
        returns = (returns - returns.mean()) / (returns.std() + 1e-8)

        return returns

    def update(self):
        """æ›´æ–°ç­–ç•¥"""
        # è®¡ç®—ç´¯ç§¯å›æŠ¥
        returns = self.compute_returns(self.rewards)

        # è®¡ç®—ç­–ç•¥æ¢¯åº¦
        policy_loss = []
        for log_prob, G in zip(self.saved_log_probs, returns):
            policy_loss.append(-log_prob * G)

        policy_loss = torch.cat(policy_loss).sum()

        # åå‘ä¼ æ’­
        self.optimizer.zero_grad()
        policy_loss.backward()
        self.optimizer.step()

        # æ¸…ç©ºè½¨è¿¹
        self.saved_log_probs = []
        self.rewards = []

        return policy_loss.item()

# ==================== è®­ç»ƒ REINFORCE ====================

def train_reinforce(env_name='CartPole-v1', num_episodes=1000):
    """è®­ç»ƒ REINFORCE"""

    # åˆ›å»ºç¯å¢ƒ
    env = gym.make(env_name)
    state_dim = env.observation_space.shape[0]
    action_dim = env.action_space.n

    # åˆ›å»º Agent
    agent = REINFORCEAgent(
        state_dim=state_dim,
        action_dim=action_dim,
        learning_rate=1e-3,
        gamma=0.99
    )

    rewards_history = []

    for episode in range(num_episodes):
        state = env.reset()
        if isinstance(state, tuple):
            state = state[0]

        episode_reward = 0
        done = False

        # æ”¶é›†ä¸€æ¡è½¨è¿¹
        while not done:
            action = agent.select_action(state)

            result = env.step(action)
            if len(result) == 5:
                next_state, reward, terminated, truncated, _ = result
                done = terminated or truncated
            else:
                next_state, reward, done, _ = result

            agent.rewards.append(reward)
            episode_reward += reward
            state = next_state

        # æ›´æ–°ç­–ç•¥
        loss = agent.update()

        rewards_history.append(episode_reward)

        if (episode + 1) % 10 == 0:
            avg_reward = np.mean(rewards_history[-10:])
            print(f'Episode {episode+1}: Avg Reward = {avg_reward:.2f}')

    env.close()

    return agent, rewards_history

# ==================== ä¸»ç¨‹åº ====================

if __name__ == '__main__':
    print("å¼€å§‹è®­ç»ƒ REINFORCE...")
    agent, rewards = train_reinforce('CartPole-v1', num_episodes=1000)

    # å¯è§†åŒ–
    plt.figure(figsize=(12, 5))

    episodes = np.arange(len(rewards))
    plt.plot(episodes, rewards, alpha=0.3)

    window = 10
    moving_avg = np.convolve(rewards, np.ones(window)/window, mode='valid')
    plt.plot(episodes[window-1:], moving_avg, linewidth=2,
            label=f'{window} Episode MA')

    plt.xlabel('Episode')
    plt.ylabel('Total Reward')
    plt.title('REINFORCE è®­ç»ƒå¥–åŠ±')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig('reinforce_results.png', dpi=300)
    plt.show()
```

---

## 10.5 Actor-Critic ç®—æ³•

### ğŸ¯ æ ¸å¿ƒæ€æƒ³

**ç»“åˆ Value-based å’Œ Policy-based**ï¼š

```
Actor (ç­–ç•¥ç½‘ç»œ):
  Ï€(a|s;Î¸)
  è´Ÿè´£é€‰æ‹©åŠ¨ä½œ

Critic (ä»·å€¼ç½‘ç»œ):
  V(s;w) æˆ– Q(s,a;w)
  è´Ÿè´£è¯„ä¼°åŠ¨ä½œ

ä¼˜åŠ¿ï¼š
  - Actor æä¾›ç­–ç•¥
  - Critic å‡å°æ–¹å·®ï¼ˆä¸éœ€è¦ç­‰åˆ° episode ç»“æŸï¼‰
```

### ğŸ“ ä¼˜åŠ¿å‡½æ•° (Advantage Function)

```
A(s,a) = Q(s,a) - V(s)

å«ä¹‰ï¼š
  åŠ¨ä½œ a æ¯”å¹³å‡å¥½å¤šå°‘

ç­–ç•¥æ¢¯åº¦ï¼š
  âˆ‡_Î¸ J(Î¸) = E[âˆ‡_Î¸ log Ï€_Î¸(a|s) Â· A(s,a)]
```

### ğŸ’» å®ç° A2C (Advantage Actor-Critic)

```python
class ActorCritic(nn.Module):
    """Actor-Critic ç½‘ç»œ"""

    def __init__(self, state_dim, action_dim, hidden_dim=128):
        super(ActorCritic, self).__init__()

        # å…±äº«ç‰¹å¾æå–å±‚
        self.fc1 = nn.Linear(state_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)

        # Actor å¤´ï¼ˆç­–ç•¥ï¼‰
        self.actor = nn.Linear(hidden_dim, action_dim)

        # Critic å¤´ï¼ˆä»·å€¼ï¼‰
        self.critic = nn.Linear(hidden_dim, 1)

    def forward(self, state):
        """
        å‰å‘ä¼ æ’­

        è¿”å›: action_probs, state_value
        """
        x = F.relu(self.fc1(state))
        x = F.relu(self.fc2(x))

        # Actor è¾“å‡ºåŠ¨ä½œæ¦‚ç‡
        action_probs = F.softmax(self.actor(x), dim=-1)

        # Critic è¾“å‡ºçŠ¶æ€ä»·å€¼
        state_value = self.critic(x)

        return action_probs, state_value

class A2CAgent:
    """A2C æ™ºèƒ½ä½“"""

    def __init__(self, state_dim, action_dim,
                 learning_rate=1e-3, gamma=0.99, entropy_coef=0.01):

        self.gamma = gamma
        self.entropy_coef = entropy_coef
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

        # Actor-Critic ç½‘ç»œ
        self.ac_net = ActorCritic(state_dim, action_dim).to(self.device)
        self.optimizer = optim.Adam(self.ac_net.parameters(), lr=learning_rate)

    def select_action(self, state):
        """é€‰æ‹©åŠ¨ä½œ"""
        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)

        action_probs, state_value = self.ac_net(state_tensor)

        # é‡‡æ ·åŠ¨ä½œ
        dist = Categorical(action_probs)
        action = dist.sample()

        return action.item(), dist.log_prob(action), dist.entropy(), state_value

    def update(self, states, actions, rewards, next_states, dones):
        """
        æ›´æ–°ç½‘ç»œ

        å‚æ•°ä¸ºä¸€ä¸ª batch çš„ç»éªŒ
        """
        states = torch.FloatTensor(states).to(self.device)
        actions = torch.LongTensor(actions).to(self.device)
        rewards = torch.FloatTensor(rewards).to(self.device)
        next_states = torch.FloatTensor(next_states).to(self.device)
        dones = torch.FloatTensor(dones).to(self.device)

        # è®¡ç®—å½“å‰çŠ¶æ€çš„ä»·å€¼å’ŒåŠ¨ä½œæ¦‚ç‡
        action_probs, state_values = self.ac_net(states)

        # è®¡ç®—ä¸‹ä¸€çŠ¶æ€çš„ä»·å€¼
        with torch.no_grad():
            _, next_state_values = self.ac_net(next_states)
            # TD ç›®æ ‡
            td_targets = rewards + self.gamma * next_state_values.squeeze() * (1 - dones)

        # ä¼˜åŠ¿å‡½æ•°
        advantages = td_targets - state_values.squeeze()

        # Actor æŸå¤±ï¼ˆç­–ç•¥æ¢¯åº¦ï¼‰
        dist = Categorical(action_probs)
        log_probs = dist.log_prob(actions)
        actor_loss = -(log_probs * advantages.detach()).mean()

        # Critic æŸå¤±ï¼ˆTD errorï¼‰
        critic_loss = F.mse_loss(state_values.squeeze(), td_targets)

        # ç†µæ­£åˆ™åŒ–ï¼ˆé¼“åŠ±æ¢ç´¢ï¼‰
        entropy = dist.entropy().mean()

        # æ€»æŸå¤±
        total_loss = actor_loss + 0.5 * critic_loss - self.entropy_coef * entropy

        # åå‘ä¼ æ’­
        self.optimizer.zero_grad()
        total_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.ac_net.parameters(), max_norm=0.5)
        self.optimizer.step()

        return total_loss.item(), actor_loss.item(), critic_loss.item()

# ==================== è®­ç»ƒ A2C ====================

def train_a2c(env_name='CartPole-v1', num_episodes=500, batch_size=5):
    """è®­ç»ƒ A2C"""

    env = gym.make(env_name)
    state_dim = env.observation_space.shape[0]
    action_dim = env.action_space.n

    agent = A2CAgent(
        state_dim=state_dim,
        action_dim=action_dim,
        learning_rate=1e-3,
        gamma=0.99,
        entropy_coef=0.01
    )

    rewards_history = []

    for episode in range(num_episodes):
        states, actions, rewards, next_states, dones = [], [], [], [], []

        state = env.reset()
        if isinstance(state, tuple):
            state = state[0]

        episode_reward = 0
        done = False

        # æ”¶é›† batch ä¸ªæ ·æœ¬
        while not done:
            action, log_prob, entropy, state_value = agent.select_action(state)

            result = env.step(action)
            if len(result) == 5:
                next_state, reward, terminated, truncated, _ = result
                done = terminated or truncated
            else:
                next_state, reward, done, _ = result

            states.append(state)
            actions.append(action)
            rewards.append(reward)
            next_states.append(next_state)
            dones.append(done)

            state = next_state
            episode_reward += reward

            # è¾¾åˆ° batch_size æˆ– episode ç»“æŸï¼Œæ›´æ–°
            if len(states) >= batch_size or done:
                loss, actor_loss, critic_loss = agent.update(
                    states, actions, rewards, next_states, dones
                )
                states, actions, rewards, next_states, dones = [], [], [], [], []

        rewards_history.append(episode_reward)

        if (episode + 1) % 10 == 0:
            avg_reward = np.mean(rewards_history[-10:])
            print(f'Episode {episode+1}: Avg Reward = {avg_reward:.2f}')

    env.close()

    return agent, rewards_history

# è¿è¡Œ
if __name__ == '__main__':
    print("å¼€å§‹è®­ç»ƒ A2C...")
    agent, rewards = train_a2c('CartPole-v1', num_episodes=500)
```

---

## 10.6 å®æˆ˜ï¼šAtari æ¸¸æˆ

### ğŸ® ç¯å¢ƒè®¾ç½®

```python
import gym
from gym.wrappers import AtariPreprocessing, FrameStack
import ale_py

def make_atari_env(env_name='BreakoutNoFrameskip-v4', frame_stack=4):
    """
    åˆ›å»º Atari ç¯å¢ƒ

    é¢„å¤„ç†ï¼š
      - ç°åº¦åŒ–
      - é™é‡‡æ ·åˆ° 84x84
      - Frame stacking (å †å å¤šå¸§)
    """
    env = gym.make(env_name)

    # Atari é¢„å¤„ç†
    env = AtariPreprocessing(
        env,
        noop_max=30,
        frame_skip=4,
        screen_size=84,
        terminal_on_life_loss=False,
        grayscale_obs=True,
        grayscale_newaxis=False,
        scale_obs=True
    )

    # å †å å¸§
    env = FrameStack(env, num_stack=frame_stack)

    return env
```

### ğŸ§  CNN-based DQN

```python
class AtariDQN(nn.Module):
    """ç”¨äº Atari æ¸¸æˆçš„ DQN"""

    def __init__(self, num_actions, frame_stack=4):
        super(AtariDQN, self).__init__()

        # å·ç§¯å±‚
        self.conv1 = nn.Conv2d(frame_stack, 32, kernel_size=8, stride=4)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)
        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)

        # å…¨è¿æ¥å±‚
        self.fc1 = nn.Linear(64 * 7 * 7, 512)
        self.fc2 = nn.Linear(512, num_actions)

    def forward(self, x):
        """
        å‚æ•°:
            x: (batch, frame_stack, 84, 84)
        """
        x = F.relu(self.conv1(x))
        x = F.relu(self.conv2(x))
        x = F.relu(self.conv3(x))

        x = x.view(x.size(0), -1)  # Flatten

        x = F.relu(self.fc1(x))
        q_values = self.fc2(x)

        return q_values

# è®­ç»ƒï¼ˆä¸ä¹‹å‰ç±»ä¼¼ï¼Œä½†è¾“å…¥æ˜¯å›¾åƒï¼‰
# æ³¨æ„ï¼šAtari æ¸¸æˆè®­ç»ƒéœ€è¦å¤§é‡è®¡ç®—èµ„æºå’Œæ—¶é—´
```

---

## 10.7 é«˜çº§è¯é¢˜

### ğŸ”¹ ä¼˜å…ˆç»éªŒå›æ”¾ (Prioritized Experience Replay)

**æ ¸å¿ƒæ€æƒ³**ï¼šé‡è¦çš„ç»éªŒæ›´é¢‘ç¹åœ°è¢«é‡‡æ ·

```python
class PrioritizedReplayBuffer:
    """ä¼˜å…ˆç»éªŒå›æ”¾"""

    def __init__(self, capacity=10000, alpha=0.6, beta=0.4):
        self.capacity = capacity
        self.alpha = alpha  # ä¼˜å…ˆçº§æŒ‡æ•°
        self.beta = beta    # é‡è¦æ€§é‡‡æ ·æŒ‡æ•°
        self.buffer = []
        self.priorities = np.zeros(capacity)
        self.pos = 0

    def push(self, transition):
        """æ·»åŠ ç»éªŒ"""
        max_priority = self.priorities.max() if self.buffer else 1.0

        if len(self.buffer) < self.capacity:
            self.buffer.append(transition)
        else:
            self.buffer[self.pos] = transition

        self.priorities[self.pos] = max_priority
        self.pos = (self.pos + 1) % self.capacity

    def sample(self, batch_size):
        """æŒ‰ä¼˜å…ˆçº§é‡‡æ ·"""
        if len(self.buffer) == self.capacity:
            priorities = self.priorities
        else:
            priorities = self.priorities[:self.pos]

        # è®¡ç®—é‡‡æ ·æ¦‚ç‡
        probs = priorities ** self.alpha
        probs /= probs.sum()

        # é‡‡æ ·ç´¢å¼•
        indices = np.random.choice(len(self.buffer), batch_size, p=probs)

        # è®¡ç®—é‡è¦æ€§é‡‡æ ·æƒé‡
        total = len(self.buffer)
        weights = (total * probs[indices]) ** (-self.beta)
        weights /= weights.max()

        samples = [self.buffer[idx] for idx in indices]

        return samples, indices, weights

    def update_priorities(self, indices, priorities):
        """æ›´æ–°ä¼˜å…ˆçº§"""
        for idx, priority in zip(indices, priorities):
            self.priorities[idx] = priority
```

---

### ğŸ”¹ Double DQN

**é—®é¢˜**ï¼šDQN å€¾å‘äºé«˜ä¼° Q å€¼

**è§£å†³**ï¼šç”¨ä¸»ç½‘ç»œé€‰æ‹©åŠ¨ä½œï¼Œç›®æ ‡ç½‘ç»œè¯„ä¼° Q å€¼

```python
# æ ‡å‡† DQN
target_q = rewards + gamma * target_net(next_states).max(1)[0]

# Double DQN
best_actions = policy_net(next_states).argmax(1)
target_q = rewards + gamma * target_net(next_states).gather(1, best_actions)
```

---

### ğŸ”¹ Dueling DQN

**æ¶æ„æ”¹è¿›**ï¼šåˆ†ç¦»ä»·å€¼å’Œä¼˜åŠ¿

```python
class DuelingDQN(nn.Module):
    def __init__(self, state_dim, action_dim, hidden_dim=128):
        super().__init__()

        self.feature = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU()
        )

        # çŠ¶æ€ä»·å€¼æµ
        self.value_stream = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1)
        )

        # ä¼˜åŠ¿æµ
        self.advantage_stream = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, action_dim)
        )

    def forward(self, state):
        features = self.feature(state)

        value = self.value_stream(features)
        advantages = self.advantage_stream(features)

        # Q(s,a) = V(s) + (A(s,a) - mean(A(s,Â·)))
        q_values = value + (advantages - advantages.mean(dim=1, keepdim=True))

        return q_values
```

---

### ğŸ”¹ PPO (Proximal Policy Optimization)

**ç›®å‰æœ€æµè¡Œçš„ç­–ç•¥æ¢¯åº¦ç®—æ³•**

```python
class PPOAgent:
    """PPO æ™ºèƒ½ä½“"""

    def __init__(self, state_dim, action_dim, clip_epsilon=0.2):
        self.clip_epsilon = clip_epsilon

        self.actor_critic = ActorCritic(state_dim, action_dim)
        self.optimizer = optim.Adam(self.actor_critic.parameters(), lr=3e-4)

    def compute_ppo_loss(self, states, actions, old_log_probs,
                         advantages, returns):
        """
        è®¡ç®— PPO æŸå¤±

        PPO-Clip ç›®æ ‡:
          L = min(r_t(Î¸)Â·A_t, clip(r_t(Î¸), 1-Îµ, 1+Îµ)Â·A_t)

        å…¶ä¸­ r_t(Î¸) = Ï€_Î¸(a|s) / Ï€_Î¸_old(a|s)
        """
        # æ–°çš„åŠ¨ä½œæ¦‚ç‡
        action_probs, state_values = self.actor_critic(states)
        dist = Categorical(action_probs)
        new_log_probs = dist.log_prob(actions)

        # æ¦‚ç‡æ¯”
        ratio = torch.exp(new_log_probs - old_log_probs)

        # Clipped objective
        surr1 = ratio * advantages
        surr2 = torch.clamp(ratio, 1 - self.clip_epsilon,
                          1 + self.clip_epsilon) * advantages
        actor_loss = -torch.min(surr1, surr2).mean()

        # Critic æŸå¤±
        critic_loss = F.mse_loss(state_values.squeeze(), returns)

        # ç†µ bonus
        entropy = dist.entropy().mean()

        total_loss = actor_loss + 0.5 * critic_loss - 0.01 * entropy

        return total_loss
```

---

## ğŸ“ æœ¬ç« ä½œä¸š

### ä½œä¸š 1ï¼šå®ç° Q-Learning

```python
# åœ¨ç½‘æ ¼ä¸–ç•Œæˆ– FrozenLake ç¯å¢ƒä¸­ï¼š
# 1. å®ç° Q-Learning
# 2. å¯è§†åŒ– Q è¡¨çš„æ¼”åŒ–è¿‡ç¨‹
# 3. å¯¹æ¯”ä¸åŒè¶…å‚æ•°ï¼ˆlr, Î³, Îµï¼‰çš„å½±å“
# 4. åˆ†ææ”¶æ•›é€Ÿåº¦
```

### ä½œä¸š 2ï¼šDQN ç© CartPole

```python
# 1. å®ç°å®Œæ•´çš„ DQN
# 2. æ·»åŠ ç»éªŒå›æ”¾å’Œç›®æ ‡ç½‘ç»œ
# 3. è®°å½•è®­ç»ƒæ›²çº¿
# 4. å¯¹æ¯”ï¼š
#    - DQN vs Q-Learning
#    - ä¸åŒç½‘ç»œæ¶æ„
#    - ä¸åŒè¶…å‚æ•°
```

### ä½œä¸š 3ï¼šPolicy Gradient

```python
# 1. å®ç° REINFORCE å’Œ A2C
# 2. åœ¨ CartPole æˆ– LunarLander ä¸Šè®­ç»ƒ
# 3. å¯¹æ¯”ä¸¤ç§ç®—æ³•çš„ï¼š
#    - æ”¶æ•›é€Ÿåº¦
#    - æ ·æœ¬æ•ˆç‡
#    - æœ€ç»ˆæ€§èƒ½
# 4. å¯è§†åŒ–ç­–ç•¥çš„æ¼”åŒ–
```

### ä½œä¸š 4ï¼šæŒ‘æˆ˜é¡¹ç›®

```python
# é€‰æ‹©ä»¥ä¸‹ä¹‹ä¸€ï¼š
#
# 1. å®ç° Double DQN æˆ– Dueling DQN
#    åœ¨ Atari æ¸¸æˆä¸Šæµ‹è¯•
#
# 2. å®ç° PPO
#    è®­ç»ƒè¿ç»­æ§åˆ¶ä»»åŠ¡ï¼ˆå¦‚ MuJoCoï¼‰
#
# 3. å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ 
#    å®ç°ç®€å•çš„åˆä½œ/ç«äº‰ç¯å¢ƒ
```

---

## ğŸ”‘ æœ¬ç« å…³é”®æ¦‚å¿µ

| æ¦‚å¿µ | è¯´æ˜ |
|------|------|
| MDP | é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ |
| ç­–ç•¥ | çŠ¶æ€åˆ°åŠ¨ä½œçš„æ˜ å°„ |
| ä»·å€¼å‡½æ•° | æœŸæœ›ç´¯ç§¯å¥–åŠ± |
| Q-Learning | åŸºäºå€¼çš„ RL |
| DQN | æ·±åº¦ Q ç½‘ç»œ |
| ç»éªŒå›æ”¾ | æ‰“ç ´æ ·æœ¬ç›¸å…³æ€§ |
| ç›®æ ‡ç½‘ç»œ | ç¨³å®šè®­ç»ƒ |
| Policy Gradient | åŸºäºç­–ç•¥çš„ RL |
| REINFORCE | è’™ç‰¹å¡æ´›ç­–ç•¥æ¢¯åº¦ |
| Actor-Critic | ç»“åˆä»·å€¼å’Œç­–ç•¥ |
| PPO | è¿‘ç«¯ç­–ç•¥ä¼˜åŒ– |

éœ€è¦æˆ‘ç»§ç»­å†™**ç¬¬åä¸€ç« ï¼šæ— ç›‘ç£å­¦ä¹ **å—ï¼Ÿ

-----

