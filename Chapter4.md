# ç¬¬å››ç« ï¼šæ·±åº¦ç¥ç»ç½‘ç»œåŸºç¡€ (Deep Neural Networks)

## ğŸ“Œ ç« èŠ‚ç›®æ ‡
- ç†è§£ä»é€»è¾‘å›å½’åˆ°ç¥ç»ç½‘ç»œçš„æ¼”è¿›
- æŒæ¡å‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­ç®—æ³•
- äº†è§£å„ç§æ¿€æ´»å‡½æ•°åŠå…¶ä½œç”¨
- å­¦ä¹ æ·±åº¦ç½‘ç»œçš„åˆå§‹åŒ–å’Œè®­ç»ƒæŠ€å·§
- å®æˆ˜ï¼šç”¨ PyTorch/TensorFlow æ„å»ºç¥ç»ç½‘ç»œ

---

## 4.1 ä»é€»è¾‘å›å½’åˆ°ç¥ç»ç½‘ç»œ

### ğŸ”„ å›é¡¾ï¼šé€»è¾‘å›å½’

```
è¾“å…¥ x â†’ çº¿æ€§ç»„åˆ z = wÂ·x + b â†’ Sigmoid Ïƒ(z) â†’ è¾“å‡º Å·
```

**å±€é™æ€§**ï¼šåªèƒ½å­¦ä¹ çº¿æ€§å†³ç­–è¾¹ç•Œ

```
ç¤ºä¾‹ï¼šXOR é—®é¢˜

è¾“å…¥          è¾“å‡º
xâ‚  xâ‚‚       y
0   0    â†’   0
0   1    â†’   1
1   0    â†’   1
1   1    â†’   0

xâ‚‚
â†‘
1 | 0   1    æ— æ³•ç”¨ä¸€æ¡ç›´çº¿åˆ†å¼€ï¼
0 | 1   0
  |_____â†’ xâ‚
  0     1
```

### ğŸ’¡ è§£å†³æ–¹æ¡ˆï¼šå †å å¤šå±‚

**ç¥ç»ç½‘ç»œçš„æ ¸å¿ƒæ€æƒ³**ï¼š

> ä¸€å±‚é€»è¾‘å›å½’å¤ªç®€å•ï¼Ÿé‚£å°±å å¾ˆå¤šå±‚ï¼

```
è¾“å…¥å±‚ â†’ éšè—å±‚1 â†’ éšè—å±‚2 â†’ ... â†’ è¾“å‡ºå±‚
```

### ğŸ§  ç¥ç»å…ƒ (Neuron)

**å•ä¸ªç¥ç»å…ƒ = é€»è¾‘å›å½’å•å…ƒ**

```
       xâ‚ â”€â”
       xâ‚‚ â”€â”¤
       xâ‚ƒ â”€â”¼â†’ z = Î£wáµ¢xáµ¢ + b â†’ a = Ïƒ(z) â†’ è¾“å‡º
      ...  â”¤
       xâ‚™ â”€â”˜

è¾“å…¥ â†’ åŠ æƒæ±‚å’Œ â†’ æ¿€æ´»å‡½æ•° â†’ è¾“å‡º
```

**æ•°å­¦è¡¨ç¤º**ï¼š

```
z = wâ‚xâ‚ + wâ‚‚xâ‚‚ + ... + wâ‚™xâ‚™ + b
a = Ïƒ(z)

å‘é‡å½¢å¼ï¼š
z = wáµ€x + b
a = Ïƒ(z)
```

### ğŸ•¸ï¸ ç¥ç»ç½‘ç»œ = å¾ˆå¤šç¥ç»å…ƒçš„ç»„åˆ

```
è¾“å…¥å±‚        éšè—å±‚1         éšè—å±‚2        è¾“å‡ºå±‚
  xâ‚ â”€â”€â”€â”¬â”€â”€â†’ hâ‚â½Â¹â¾ â”€â”€â”€â”¬â”€â”€â†’ hâ‚â½Â²â¾ â”€â”€â”€â”¬â”€â”€â†’ Å·â‚
        â”‚              â”‚              â”‚
  xâ‚‚ â”€â”€â”€â”¼â”€â”€â†’ hâ‚‚â½Â¹â¾ â”€â”€â”€â”¼â”€â”€â†’ hâ‚‚â½Â²â¾ â”€â”€â”€â”¼â”€â”€â†’ Å·â‚‚
        â”‚              â”‚              â”‚
  xâ‚ƒ â”€â”€â”€â”¼â”€â”€â†’ hâ‚ƒâ½Â¹â¾ â”€â”€â”€â”¼â”€â”€â†’ hâ‚ƒâ½Â²â¾ â”€â”€â”€â”¼â”€â”€â†’ Å·â‚ƒ
        â”‚              â”‚              â”‚
  ...   â””â”€â”€â†’ ...   â”€â”€â”€â””â”€â”€â†’ ...   â”€â”€â”€â”˜

ç¬¬0å±‚         ç¬¬1å±‚           ç¬¬2å±‚         ç¬¬3å±‚
(è¾“å…¥)       (éšè—)         (éšè—)       (è¾“å‡º)
```

---

## 4.2 å‰å‘ä¼ æ’­ (Forward Propagation)

### ğŸ“ æ•°å­¦æ¨å¯¼

**ç¬¦å·å®šä¹‰**ï¼š

```
L: ç½‘ç»œå±‚æ•°
nâ½Ë¡â¾: ç¬¬ l å±‚çš„ç¥ç»å…ƒæ•°é‡
wâ½Ë¡â¾: ç¬¬ l å±‚çš„æƒé‡çŸ©é˜µ
bâ½Ë¡â¾: ç¬¬ l å±‚çš„åç½®å‘é‡
aâ½Ë¡â¾: ç¬¬ l å±‚çš„æ¿€æ´»å€¼ï¼ˆè¾“å‡ºï¼‰
zâ½Ë¡â¾: ç¬¬ l å±‚çš„åŠ æƒè¾“å…¥
```

**å•å±‚è®¡ç®—**ï¼š

```
zâ½Ë¡â¾ = Wâ½Ë¡â¾Â·aâ½Ë¡â»Â¹â¾ + bâ½Ë¡â¾
aâ½Ë¡â¾ = Ïƒ(zâ½Ë¡â¾)
```

### ğŸ”¢ å…·ä½“ä¾‹å­

**ç½‘ç»œç»“æ„**ï¼š2 â†’ 3 â†’ 1

```
è¾“å…¥å±‚ï¼š2ä¸ªç¥ç»å…ƒ [xâ‚, xâ‚‚]
éšè—å±‚ï¼š3ä¸ªç¥ç»å…ƒ [hâ‚, hâ‚‚, hâ‚ƒ]
è¾“å‡ºå±‚ï¼š1ä¸ªç¥ç»å…ƒ [Å·]
```

**ç¬¬1å±‚ï¼ˆè¾“å…¥â†’éšè—ï¼‰**ï¼š

```
zâ‚â½Â¹â¾ = wâ‚â‚â½Â¹â¾xâ‚ + wâ‚â‚‚â½Â¹â¾xâ‚‚ + bâ‚â½Â¹â¾
zâ‚‚â½Â¹â¾ = wâ‚‚â‚â½Â¹â¾xâ‚ + wâ‚‚â‚‚â½Â¹â¾xâ‚‚ + bâ‚‚â½Â¹â¾
zâ‚ƒâ½Â¹â¾ = wâ‚ƒâ‚â½Â¹â¾xâ‚ + wâ‚ƒâ‚‚â½Â¹â¾xâ‚‚ + bâ‚ƒâ½Â¹â¾

aâ‚â½Â¹â¾ = Ïƒ(zâ‚â½Â¹â¾)
aâ‚‚â½Â¹â¾ = Ïƒ(zâ‚‚â½Â¹â¾)
aâ‚ƒâ½Â¹â¾ = Ïƒ(zâ‚ƒâ½Â¹â¾)

çŸ©é˜µå½¢å¼ï¼š
zâ½Â¹â¾ = Wâ½Â¹â¾Â·x + bâ½Â¹â¾

å…¶ä¸­ï¼š
     [wâ‚â‚ wâ‚â‚‚]       [bâ‚]
Wâ½Â¹â¾=[wâ‚‚â‚ wâ‚‚â‚‚]  bâ½Â¹â¾=[bâ‚‚]
     [wâ‚ƒâ‚ wâ‚ƒâ‚‚]       [bâ‚ƒ]
```

**ç¬¬2å±‚ï¼ˆéšè—â†’è¾“å‡ºï¼‰**ï¼š

```
zâ½Â²â¾ = Wâ½Â²â¾Â·aâ½Â¹â¾ + bâ½Â²â¾
Å· = aâ½Â²â¾ = Ïƒ(zâ½Â²â¾)

å…¶ä¸­ï¼š
Wâ½Â²â¾ = [wâ‚ wâ‚‚ wâ‚ƒ]  (1Ã—3 çŸ©é˜µ)
bâ½Â²â¾ = [b]          (æ ‡é‡)
```

### ğŸ’» ä»£ç å®ç°

```python
import numpy as np

def sigmoid(z):
    """Sigmoid æ¿€æ´»å‡½æ•°"""
    return 1 / (1 + np.exp(-z))

def forward_propagation(X, parameters):
    """
    å‰å‘ä¼ æ’­

    å‚æ•°ï¼š
        X: è¾“å…¥æ•°æ® (n_features, m_samples)
        parameters: å­—å…¸ï¼ŒåŒ…å« W1, b1, W2, b2

    è¿”å›ï¼š
        A2: è¾“å‡ºå±‚æ¿€æ´»å€¼
        cache: ä¸­é—´å€¼ï¼Œç”¨äºåå‘ä¼ æ’­
    """
    # è·å–å‚æ•°
    W1 = parameters['W1']
    b1 = parameters['b1']
    W2 = parameters['W2']
    b2 = parameters['b2']

    # ç¬¬1å±‚
    Z1 = np.dot(W1, X) + b1
    A1 = sigmoid(Z1)

    # ç¬¬2å±‚
    Z2 = np.dot(W2, A1) + b2
    A2 = sigmoid(Z2)

    # ä¿å­˜ä¸­é—´å€¼
    cache = {
        'Z1': Z1,
        'A1': A1,
        'Z2': Z2,
        'A2': A2
    }

    return A2, cache

# ç¤ºä¾‹
np.random.seed(42)

# åˆå§‹åŒ–å‚æ•°
parameters = {
    'W1': np.random.randn(3, 2) * 0.01,  # 3Ã—2
    'b1': np.zeros((3, 1)),              # 3Ã—1
    'W2': np.random.randn(1, 3) * 0.01,  # 1Ã—3
    'b2': np.zeros((1, 1))               # 1Ã—1
}

# è¾“å…¥æ•°æ®
X = np.array([[1.0, 2.0],
              [0.5, 1.5]]).T  # 2Ã—2 (2ä¸ªæ ·æœ¬)

# å‰å‘ä¼ æ’­
A2, cache = forward_propagation(X, parameters)

print("è¾“å…¥ X:")
print(X)
print("\nè¾“å‡º A2:")
print(A2)
print("\néšè—å±‚æ¿€æ´» A1:")
print(cache['A1'])
```

---

## 4.3 æ¿€æ´»å‡½æ•° (Activation Functions)

### ğŸ¤” ä¸ºä»€ä¹ˆéœ€è¦æ¿€æ´»å‡½æ•°ï¼Ÿ

**å¦‚æœæ²¡æœ‰æ¿€æ´»å‡½æ•°**ï¼ˆæˆ–ä½¿ç”¨çº¿æ€§æ¿€æ´»ï¼‰ï¼š

```
zâ½Â¹â¾ = Wâ½Â¹â¾x + bâ½Â¹â¾
aâ½Â¹â¾ = zâ½Â¹â¾              â† çº¿æ€§

zâ½Â²â¾ = Wâ½Â²â¾aâ½Â¹â¾ + bâ½Â²â¾
    = Wâ½Â²â¾(Wâ½Â¹â¾x + bâ½Â¹â¾) + bâ½Â²â¾
    = (Wâ½Â²â¾Wâ½Â¹â¾)x + (Wâ½Â²â¾bâ½Â¹â¾ + bâ½Â²â¾)
    = W'x + b'           â† è¿˜æ˜¯çº¿æ€§ï¼

å¤šå±‚çº¿æ€§å˜æ¢ = å•å±‚çº¿æ€§å˜æ¢
æ·±åº¦ç½‘ç»œé€€åŒ–æˆæµ…å±‚ç½‘ç»œï¼
```

**ç»“è®º**ï¼šæ¿€æ´»å‡½æ•°å¼•å…¥éçº¿æ€§ï¼Œè®©ç¥ç»ç½‘ç»œèƒ½å­¦ä¹ å¤æ‚å‡½æ•°

---

### ğŸ”¹ å¸¸è§æ¿€æ´»å‡½æ•°

#### **1. Sigmoid**

```
Ïƒ(z) = 1 / (1 + eâ»á¶»)

èŒƒå›´ï¼š(0, 1)
å¯¼æ•°ï¼šÏƒ'(z) = Ïƒ(z)Â·(1 - Ïƒ(z))
```

**å›¾å½¢**ï¼š
```
  1 |         ____
    |       /
0.5 |      /
    |     /
  0 |____/
    |___________ z
   -5  0   5
```

**ä¼˜ç‚¹**ï¼š
- è¾“å‡ºèŒƒå›´ (0,1)ï¼Œé€‚åˆè¡¨ç¤ºæ¦‚ç‡
- å¹³æ»‘å¯å¯¼

**ç¼ºç‚¹**ï¼š
- **æ¢¯åº¦æ¶ˆå¤±**ï¼šz å¾ˆå¤§æˆ–å¾ˆå°æ—¶ï¼Œæ¢¯åº¦æ¥è¿‘ 0
- **è¾“å‡ºä¸æ˜¯é›¶ä¸­å¿ƒ**ï¼šéƒ½æ˜¯æ­£æ•°ï¼Œå¯¼è‡´æƒé‡æ›´æ–°æ•ˆç‡ä½
- **è®¡ç®—é‡å¤§**ï¼šæœ‰æŒ‡æ•°è¿ç®—

**ä½¿ç”¨åœºæ™¯**ï¼š
- è¾“å‡ºå±‚ï¼ˆäºŒå…ƒåˆ†ç±»ï¼‰
- ä¸æ¨èåœ¨éšè—å±‚ä½¿ç”¨

#### **2. Tanh (åŒæ›²æ­£åˆ‡)**

```
tanh(z) = (eá¶» - eâ»á¶») / (eá¶» + eâ»á¶»)
        = 2Â·Ïƒ(2z) - 1

èŒƒå›´ï¼š(-1, 1)
å¯¼æ•°ï¼štanh'(z) = 1 - tanhÂ²(z)
```

**å›¾å½¢**ï¼š
```
  1 |         ____
    |       /
  0 |      /
    |     /
 -1 |____/
    |___________ z
   -5  0   5
```

**ä¼˜ç‚¹**ï¼š
- é›¶ä¸­å¿ƒè¾“å‡º
- æ¯” Sigmoid å¥½

**ç¼ºç‚¹**ï¼š
- ä»æœ‰æ¢¯åº¦æ¶ˆå¤±é—®é¢˜
- è®¡ç®—é‡å¤§

**ä½¿ç”¨åœºæ™¯**ï¼š
- RNN/LSTM ä¸­å¸¸ç”¨
- éšè—å±‚ï¼ˆä½†ç°åœ¨æ›´æ¨è ReLUï¼‰

#### **3. ReLU (Rectified Linear Unit)** â­

```
ReLU(z) = max(0, z)

       â§ z,  å¦‚æœ z > 0
     = â¨
       â© 0,  å¦‚æœ z â‰¤ 0

å¯¼æ•°ï¼šReLU'(z) = â§ 1, å¦‚æœ z > 0
                 â© 0, å¦‚æœ z â‰¤ 0
```

**å›¾å½¢**ï¼š
```
    |    /
    |   /
    |  /
    | /
____|/_______ z
    0
```

**ä¼˜ç‚¹**ï¼š
- âœ… **è®¡ç®—ç®€å•**ï¼šä¸æ¶‰åŠæŒ‡æ•°è¿ç®—
- âœ… **ç¼“è§£æ¢¯åº¦æ¶ˆå¤±**ï¼šæ­£åŒºåŸŸæ¢¯åº¦æ’ä¸º1
- âœ… **æ”¶æ•›å¿«**ï¼šæ¯” Sigmoid/Tanh å¿«å¾ˆå¤š
- âœ… **ç¨€ç–æ¿€æ´»**ï¼šçº¦50%ç¥ç»å…ƒè¢«æ¿€æ´»

**ç¼ºç‚¹**ï¼š
- âŒ **Dead ReLU**ï¼šè´ŸåŒºåŸŸæ¢¯åº¦ä¸º0ï¼Œç¥ç»å…ƒå¯èƒ½"æ­»äº¡"
- âŒ è¾“å‡ºä¸æ˜¯é›¶ä¸­å¿ƒ

**ä½¿ç”¨åœºæ™¯**ï¼š
- ğŸŒŸ **é»˜è®¤é¦–é€‰**ï¼éšè—å±‚çš„æ ‡å‡†é€‰æ‹©
- CNN ä¸­å¹¿æ³›ä½¿ç”¨

#### **4. Leaky ReLU**

```
Leaky ReLU(z) = max(Î±z, z)

              â§ z,   å¦‚æœ z > 0
            = â¨
              â© Î±z,  å¦‚æœ z â‰¤ 0

é€šå¸¸ Î± = 0.01
```

**å›¾å½¢**ï¼š
```
    |    /
    |   /
    |  /
    | /
___/|_______ z
  / 0
```

**ä¼˜ç‚¹**ï¼š
- è§£å†³ Dead ReLU é—®é¢˜
- è´ŸåŒºåŸŸä»æœ‰å°æ¢¯åº¦

**å˜ç§**ï¼š
- **PReLU** (Parametric ReLU)ï¼šÎ± æ˜¯å¯å­¦ä¹ çš„å‚æ•°
- **ELU** (Exponential Linear Unit)

#### **5. Softmax** (è¾“å‡ºå±‚)

```
å¯¹äº K ä¸ªç±»åˆ«ï¼š

Softmax(záµ¢) = e^záµ¢ / Î£â±¼ e^zâ±¼

æ€§è´¨ï¼š
- Î£áµ¢ Softmax(záµ¢) = 1
- è¾“å‡ºå¯è§£é‡Šä¸ºæ¦‚ç‡
```

**ä½¿ç”¨åœºæ™¯**ï¼š
- å¤šåˆ†ç±»é—®é¢˜çš„è¾“å‡ºå±‚

#### **6. å…¶ä»–æ¿€æ´»å‡½æ•°**

**Swish** (Google 2017):
```
Swish(z) = zÂ·Ïƒ(z)

ä¼˜ç‚¹ï¼šæ— ç•Œã€å¹³æ»‘ã€éå•è°ƒ
```

**GELU** (Gaussian Error Linear Unit):
```
GELU(z) â‰ˆ 0.5z(1 + tanh(âˆš(2/Ï€)(z + 0.044715zÂ³)))

ç”¨äº BERTã€GPT
```

---

### ğŸ“Š æ¿€æ´»å‡½æ•°å¯¹æ¯”

```python
import numpy as np
import matplotlib.pyplot as plt

def sigmoid(z):
    return 1 / (1 + np.exp(-z))

def tanh(z):
    return np.tanh(z)

def relu(z):
    return np.maximum(0, z)

def leaky_relu(z, alpha=0.01):
    return np.maximum(alpha * z, z)

def elu(z, alpha=1.0):
    return np.where(z > 0, z, alpha * (np.exp(z) - 1))

# ç”Ÿæˆæ•°æ®
z = np.linspace(-5, 5, 1000)

# ç»˜å›¾
plt.figure(figsize=(15, 10))

# æ¿€æ´»å‡½æ•°
plt.subplot(2, 2, 1)
plt.plot(z, sigmoid(z), label='Sigmoid', linewidth=2)
plt.plot(z, tanh(z), label='Tanh', linewidth=2)
plt.plot(z, relu(z), label='ReLU', linewidth=2)
plt.plot(z, leaky_relu(z), label='Leaky ReLU', linewidth=2)
plt.axhline(y=0, color='k', linestyle='--', alpha=0.3)
plt.axvline(x=0, color='k', linestyle='--', alpha=0.3)
plt.xlabel('z')
plt.ylabel('Activation')
plt.title('æ¿€æ´»å‡½æ•°')
plt.legend()
plt.grid(True, alpha=0.3)

# å¯¼æ•°
plt.subplot(2, 2, 2)
sigmoid_derivative = sigmoid(z) * (1 - sigmoid(z))
tanh_derivative = 1 - tanh(z)**2
relu_derivative = (z > 0).astype(float)
leaky_relu_derivative = np.where(z > 0, 1, 0.01)

plt.plot(z, sigmoid_derivative, label='Sigmoid\'', linewidth=2)
plt.plot(z, tanh_derivative, label='Tanh\'', linewidth=2)
plt.plot(z, relu_derivative, label='ReLU\'', linewidth=2)
plt.plot(z, leaky_relu_derivative, label='Leaky ReLU\'', linewidth=2)
plt.axhline(y=0, color='k', linestyle='--', alpha=0.3)
plt.axvline(x=0, color='k', linestyle='--', alpha=0.3)
plt.xlabel('z')
plt.ylabel('Derivative')
plt.title('æ¿€æ´»å‡½æ•°çš„å¯¼æ•°')
plt.legend()
plt.grid(True, alpha=0.3)

# ReLU å˜ç§å¯¹æ¯”
plt.subplot(2, 2, 3)
plt.plot(z, relu(z), label='ReLU', linewidth=2)
plt.plot(z, leaky_relu(z), label='Leaky ReLU', linewidth=2)
plt.plot(z, elu(z), label='ELU', linewidth=2)
plt.axhline(y=0, color='k', linestyle='--', alpha=0.3)
plt.axvline(x=0, color='k', linestyle='--', alpha=0.3)
plt.xlabel('z')
plt.ylabel('Activation')
plt.title('ReLU å˜ç§å¯¹æ¯”')
plt.legend()
plt.grid(True, alpha=0.3)

# æ¢¯åº¦æ¶ˆå¤±é—®é¢˜æ¼”ç¤º
plt.subplot(2, 2, 4)
z_grad = np.linspace(-10, 10, 1000)
sigmoid_grad = sigmoid(z_grad) * (1 - sigmoid(z_grad))
tanh_grad = 1 - tanh(z_grad)**2
relu_grad = (z_grad > 0).astype(float)

plt.plot(z_grad, sigmoid_grad, label='Sigmoid', linewidth=2)
plt.plot(z_grad, tanh_grad, label='Tanh', linewidth=2)
plt.plot(z_grad, relu_grad, label='ReLU', linewidth=2)
plt.axhline(y=0, color='k', linestyle='--', alpha=0.3)
plt.xlabel('z')
plt.ylabel('Gradient')
plt.title('æ¢¯åº¦æ¶ˆå¤±é—®é¢˜')
plt.legend()
plt.grid(True, alpha=0.3)
plt.ylim(-0.1, 1.1)

plt.tight_layout()
plt.show()
```

### ğŸ’¡ æ¿€æ´»å‡½æ•°é€‰æ‹©æŒ‡å—

```
éšè—å±‚ï¼š
  â”œâ”€ é»˜è®¤ï¼šReLU â­
  â”œâ”€ å°è¯•ï¼šLeaky ReLU / PReLU / ELU
  â””â”€ é¿å…ï¼šSigmoid / Tanh (é™¤éç‰¹æ®Šéœ€æ±‚)

è¾“å‡ºå±‚ï¼š
  â”œâ”€ äºŒå…ƒåˆ†ç±»ï¼šSigmoid
  â”œâ”€ å¤šå…ƒåˆ†ç±»ï¼šSoftmax
  â”œâ”€ å›å½’ï¼šLinear (æ— æ¿€æ´»å‡½æ•°)
  â””â”€ ç‰¹æ®ŠèŒƒå›´ï¼šTanh (è¾“å‡º[-1,1])
```

---

## 4.4 æŸå¤±å‡½æ•° (Loss Functions)

### ğŸ¯ å¸¸è§æŸå¤±å‡½æ•°

#### **1. å›å½’é—®é¢˜**

**Mean Squared Error (MSE)**:
```
L = (1/N) Î£(Å·â¿ - yâ¿)Â²
```

**Mean Absolute Error (MAE)**:
```
L = (1/N) Î£|Å·â¿ - yâ¿|
```

**Huber Loss** (ç»“åˆ MSE å’Œ MAE):
```
        â§ 0.5Â·(y - Å·)Â²,           if |y - Å·| â‰¤ Î´
L_Î´(y,Å·)=â¨
        â© Î´Â·(|y - Å·| - 0.5Â·Î´),   otherwise
```

#### **2. äºŒå…ƒåˆ†ç±»**

**Binary Cross Entropy**:
```
L = -(1/N) Î£[yâ¿Â·log(Å·â¿) + (1-yâ¿)Â·log(1-Å·â¿)]
```

#### **3. å¤šå…ƒåˆ†ç±»**

**Categorical Cross Entropy**:
```
L = -(1/N) Î£Î£ yáµ¢â¿Â·log(Å·áµ¢â¿)
           n i

å…¶ä¸­ yáµ¢â¿ æ˜¯ one-hot ç¼–ç 
```

---

## 4.5 åå‘ä¼ æ’­ (Backpropagation)

### ğŸ¯ æ ¸å¿ƒæ€æƒ³

> åå‘ä¼ æ’­ = é“¾å¼æ³•åˆ™ + ä»åå¾€å‰è®¡ç®—æ¢¯åº¦

**ç›®æ ‡**ï¼šè®¡ç®— âˆ‚L/âˆ‚Wâ½Ë¡â¾ å’Œ âˆ‚L/âˆ‚bâ½Ë¡â¾

### ğŸ“ é“¾å¼æ³•åˆ™å›é¡¾

```
å¦‚æœ y = f(u) ä¸” u = g(x)
åˆ™ dy/dx = (dy/du)Â·(du/dx)

ä¾‹å­ï¼š
y = (xÂ² + 1)Â³
ä»¤ u = xÂ² + 1, åˆ™ y = uÂ³

dy/dx = (dy/du)Â·(du/dx)
      = 3uÂ²Â·2x
      = 3(xÂ²+1)Â²Â·2x
```

### ğŸ”„ åå‘ä¼ æ’­æ¨å¯¼

**ç½‘ç»œç»“æ„**ï¼šè¾“å…¥ â†’ éšè—å±‚ â†’ è¾“å‡ºå±‚

```
å‰å‘ä¼ æ’­ï¼š
  zâ½Â¹â¾ = Wâ½Â¹â¾x + bâ½Â¹â¾
  aâ½Â¹â¾ = Ïƒ(zâ½Â¹â¾)
  zâ½Â²â¾ = Wâ½Â²â¾aâ½Â¹â¾ + bâ½Â²â¾
  aâ½Â²â¾ = Ïƒ(zâ½Â²â¾) = Å·

æŸå¤±ï¼š
  L = (y - Å·)Â²
```

**åå‘ä¼ æ’­**ï¼š

**è¾“å‡ºå±‚**ï¼š
```
âˆ‚L/âˆ‚Å· = -2(y - Å·)

âˆ‚L/âˆ‚zâ½Â²â¾ = âˆ‚L/âˆ‚Å· Â· âˆ‚Å·/âˆ‚zâ½Â²â¾
         = âˆ‚L/âˆ‚Å· Â· Ïƒ'(zâ½Â²â¾)

âˆ‚L/âˆ‚Wâ½Â²â¾ = âˆ‚L/âˆ‚zâ½Â²â¾ Â· âˆ‚zâ½Â²â¾/âˆ‚Wâ½Â²â¾
         = âˆ‚L/âˆ‚zâ½Â²â¾ Â· aâ½Â¹â¾áµ€

âˆ‚L/âˆ‚bâ½Â²â¾ = âˆ‚L/âˆ‚zâ½Â²â¾
```

**éšè—å±‚**ï¼š
```
âˆ‚L/âˆ‚aâ½Â¹â¾ = (Wâ½Â²â¾)áµ€ Â· âˆ‚L/âˆ‚zâ½Â²â¾

âˆ‚L/âˆ‚zâ½Â¹â¾ = âˆ‚L/âˆ‚aâ½Â¹â¾ âŠ™ Ïƒ'(zâ½Â¹â¾)
         (âŠ™ è¡¨ç¤ºé€å…ƒç´ ä¹˜æ³•)

âˆ‚L/âˆ‚Wâ½Â¹â¾ = âˆ‚L/âˆ‚zâ½Â¹â¾ Â· xáµ€

âˆ‚L/âˆ‚bâ½Â¹â¾ = âˆ‚L/âˆ‚zâ½Â¹â¾
```

### ğŸ’» ä»£ç å®ç°

```python
def backward_propagation(X, Y, parameters, cache):
    """
    åå‘ä¼ æ’­

    å‚æ•°ï¼š
        X: è¾“å…¥ (n_features, m_samples)
        Y: çœŸå®æ ‡ç­¾ (1, m_samples)
        parameters: æƒé‡å’Œåç½®
        cache: å‰å‘ä¼ æ’­çš„ä¸­é—´å€¼

    è¿”å›ï¼š
        gradients: æ¢¯åº¦å­—å…¸
    """
    m = X.shape[1]  # æ ·æœ¬æ•°

    # è·å–å‚æ•°
    W1 = parameters['W1']
    W2 = parameters['W2']

    # è·å–å‰å‘ä¼ æ’­çš„å€¼
    A1 = cache['A1']
    A2 = cache['A2']

    # è¾“å‡ºå±‚æ¢¯åº¦
    dZ2 = A2 - Y  # å¯¹äº sigmoid + MSE
    dW2 = (1/m) * np.dot(dZ2, A1.T)
    db2 = (1/m) * np.sum(dZ2, axis=1, keepdims=True)

    # éšè—å±‚æ¢¯åº¦
    dA1 = np.dot(W2.T, dZ2)
    dZ1 = dA1 * A1 * (1 - A1)  # sigmoid çš„å¯¼æ•°
    dW1 = (1/m) * np.dot(dZ1, X.T)
    db1 = (1/m) * np.sum(dZ1, axis=1, keepdims=True)

    gradients = {
        'dW1': dW1,
        'db1': db1,
        'dW2': dW2,
        'db2': db2
    }

    return gradients

def update_parameters(parameters, gradients, learning_rate):
    """
    æ›´æ–°å‚æ•°
    """
    parameters['W1'] -= learning_rate * gradients['dW1']
    parameters['b1'] -= learning_rate * gradients['db1']
    parameters['W2'] -= learning_rate * gradients['dW2']
    parameters['b2'] -= learning_rate * gradients['db2']

    return parameters
```

### ğŸ” å®Œæ•´è®­ç»ƒå¾ªç¯

```python
def train_neural_network(X, Y, hidden_size=4, learning_rate=0.01, epochs=10000):
    """
    è®­ç»ƒç¥ç»ç½‘ç»œ
    """
    n_x = X.shape[0]  # è¾“å…¥ç‰¹å¾æ•°
    n_y = Y.shape[0]  # è¾“å‡ºæ•°

    # åˆå§‹åŒ–å‚æ•°
    np.random.seed(42)
    parameters = {
        'W1': np.random.randn(hidden_size, n_x) * 0.01,
        'b1': np.zeros((hidden_size, 1)),
        'W2': np.random.randn(n_y, hidden_size) * 0.01,
        'b2': np.zeros((n_y, 1))
    }

    losses = []

    for epoch in range(epochs):
        # å‰å‘ä¼ æ’­
        A2, cache = forward_propagation(X, parameters)

        # è®¡ç®—æŸå¤±
        loss = np.mean((A2 - Y) ** 2)
        losses.append(loss)

        # åå‘ä¼ æ’­
        gradients = backward_propagation(X, Y, parameters, cache)

        # æ›´æ–°å‚æ•°
        parameters = update_parameters(parameters, gradients, learning_rate)

        # æ‰“å°è¿›åº¦
        if epoch % 1000 == 0:
            print(f"Epoch {epoch}: Loss = {loss:.6f}")

    return parameters, losses

# ç¤ºä¾‹ï¼šè§£å†³ XOR é—®é¢˜
X = np.array([[0, 0, 1, 1],
              [0, 1, 0, 1]])
Y = np.array([[0, 1, 1, 0]])

parameters, losses = train_neural_network(
    X, Y,
    hidden_size=4,
    learning_rate=0.5,
    epochs=10000
)

# æµ‹è¯•
A2, _ = forward_propagation(X, parameters)
print("\nè¾“å…¥:")
print(X.T)
print("\né¢„æµ‹:")
print(A2.T)
print("\nçœŸå®æ ‡ç­¾:")
print(Y.T)
```

**è¾“å‡º**ï¼š
```
Epoch 0: Loss = 0.250615
Epoch 1000: Loss = 0.062439
Epoch 2000: Loss = 0.013152
Epoch 3000: Loss = 0.005862
Epoch 4000: Loss = 0.003494
Epoch 5000: Loss = 0.002388
Epoch 6000: Loss = 0.001756
Epoch 7000: Loss = 0.001353
Epoch 8000: Loss = 0.001081
Epoch 9000: Loss = 0.000887

è¾“å…¥:
[[0 0]
 [0 1]
 [1 0]
 [1 1]]

é¢„æµ‹:
[[0.02458917]
 [0.97201347]
 [0.97412658]
 [0.02907213]]

çœŸå®æ ‡ç­¾:
[[0]
 [1]
 [1]
 [0]]
```

æˆåŠŸè§£å†³äº† XOR é—®é¢˜ï¼ğŸ‰

---

## 4.6 åˆå§‹åŒ–ç­–ç•¥

### ğŸ¤” ä¸ºä»€ä¹ˆåˆå§‹åŒ–é‡è¦ï¼Ÿ

**å…¨é›¶åˆå§‹åŒ–**ï¼š
```python
W = np.zeros((n_out, n_in))
```

**é—®é¢˜**ï¼šæ‰€æœ‰ç¥ç»å…ƒå­¦åˆ°ç›¸åŒçš„ç‰¹å¾ï¼ˆå¯¹ç§°æ€§é—®é¢˜ï¼‰

**éšæœºåˆå§‹åŒ–**ï¼š
```python
W = np.random.randn(n_out, n_in)
```

**é—®é¢˜**ï¼šæ–¹å·®å¯èƒ½å¤ªå¤§æˆ–å¤ªå°

### âœ… å¥½çš„åˆå§‹åŒ–æ–¹æ³•

#### **1. Xavier åˆå§‹åŒ–** (Glorot)

**é€‚ç”¨äº**ï¼šSigmoid / Tanh

```python
W = np.random.randn(n_out, n_in) * np.sqrt(1 / n_in)

# æˆ–

W = np.random.randn(n_out, n_in) * np.sqrt(2 / (n_in + n_out))
```

**åŸç†**ï¼šä¿æŒæ–¹å·®åœ¨å„å±‚ä¹‹é—´å¹³è¡¡

#### **2. He åˆå§‹åŒ–**

**é€‚ç”¨äº**ï¼šReLU åŠå…¶å˜ç§

```python
W = np.random.randn(n_out, n_in) * np.sqrt(2 / n_in)
```

**åŸç†**ï¼šè€ƒè™‘ ReLU ä¼š"æ€æ­»"ä¸€åŠç¥ç»å…ƒ

### ğŸ’» å®ç°

```python
def initialize_parameters_xavier(layer_dims):
    """
    Xavier åˆå§‹åŒ–

    å‚æ•°ï¼š
        layer_dims: åˆ—è¡¨ï¼Œæ¯å±‚çš„ç¥ç»å…ƒæ•°é‡
                   ä¾‹å¦‚ [784, 128, 64, 10]
    """
    parameters = {}
    L = len(layer_dims)

    for l in range(1, L):
        parameters[f'W{l}'] = np.random.randn(
            layer_dims[l],
            layer_dims[l-1]
        ) * np.sqrt(1 / layer_dims[l-1])

        parameters[f'b{l}'] = np.zeros((layer_dims[l], 1))

    return parameters

def initialize_parameters_he(layer_dims):
    """
    He åˆå§‹åŒ–
    """
    parameters = {}
    L = len(layer_dims)

    for l in range(1, L):
        parameters[f'W{l}'] = np.random.randn(
            layer_dims[l],
            layer_dims[l-1]
        ) * np.sqrt(2 / layer_dims[l-1])

        parameters[f'b{l}'] = np.zeros((layer_dims[l], 1))

    return parameters
```

---

## 4.7 æ¢¯åº¦æ£€éªŒ (Gradient Checking)

### ğŸ¯ ç›®çš„

éªŒè¯åå‘ä¼ æ’­çš„å®ç°æ˜¯å¦æ­£ç¡®

### ğŸ“ æ•°å€¼æ¢¯åº¦

```
f'(Î¸) â‰ˆ [f(Î¸ + Îµ) - f(Î¸ - Îµ)] / (2Îµ)

é€šå¸¸ Îµ = 10â»â·
```

### ğŸ’» å®ç°

```python
def gradient_check(parameters, gradients, X, Y, epsilon=1e-7):
    """
    æ¢¯åº¦æ£€éªŒ

    è¿”å›ï¼š
        difference: æ•°å€¼æ¢¯åº¦å’Œè§£ææ¢¯åº¦çš„ç›¸å¯¹å·®å¼‚
    """
    # å°†å‚æ•°å±•å¹³ä¸ºå‘é‡
    parameters_values, _ = dictionary_to_vector(parameters)
    grad = gradients_to_vector(gradients)
    num_parameters = parameters_values.shape[0]

    # è®¡ç®—æ•°å€¼æ¢¯åº¦
    J_plus = np.zeros((num_parameters, 1))
    J_minus = np.zeros((num_parameters, 1))
    gradapprox = np.zeros((num_parameters, 1))

    for i in range(num_parameters):
        # è®¡ç®— J_plus[i]
        thetaplus = np.copy(parameters_values)
        thetaplus[i][0] += epsilon
        J_plus[i] = forward_propagation_cost(X, Y, vector_to_dictionary(thetaplus))

        # è®¡ç®— J_minus[i]
        thetaminus = np.copy(parameters_values)
        thetaminus[i][0] -= epsilon
        J_minus[i] = forward_propagation_cost(X, Y, vector_to_dictionary(thetaminus))

        # è®¡ç®—æ•°å€¼æ¢¯åº¦
        gradapprox[i] = (J_plus[i] - J_minus[i]) / (2 * epsilon)

    # è®¡ç®—ç›¸å¯¹å·®å¼‚
    numerator = np.linalg.norm(grad - gradapprox)
    denominator = np.linalg.norm(grad) + np.linalg.norm(gradapprox)
    difference = numerator / denominator

    if difference > 2e-7:
        print(f"âš ï¸  æ¢¯åº¦æ£€éªŒå¤±è´¥ï¼å·®å¼‚ = {difference}")
    else:
        print(f"âœ… æ¢¯åº¦æ£€éªŒé€šè¿‡ï¼å·®å¼‚ = {difference}")

    return difference
```

---

## 4.8 ä½¿ç”¨æ·±åº¦å­¦ä¹ æ¡†æ¶

### ğŸ”¥ PyTorch å®ç°

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset

# 1. å®šä¹‰ç½‘ç»œ
class NeuralNetwork(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(NeuralNetwork, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_size, output_size)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        x = self.sigmoid(x)
        return x

# 2. å‡†å¤‡æ•°æ®
X_train = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32)
y_train = torch.tensor([[0], [1], [1], [0]], dtype=torch.float32)

# 3. åˆ›å»ºæ¨¡å‹
model = NeuralNetwork(input_size=2, hidden_size=4, output_size=1)

# 4. å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.1)

# 5. è®­ç»ƒ
epochs = 5000
for epoch in range(epochs):
    # å‰å‘ä¼ æ’­
    outputs = model(X_train)
    loss = criterion(outputs, y_train)

    # åå‘ä¼ æ’­
    optimizer.zero_grad()  # æ¸…é›¶æ¢¯åº¦
    loss.backward()        # è®¡ç®—æ¢¯åº¦
    optimizer.step()       # æ›´æ–°å‚æ•°

    if epoch % 500 == 0:
        print(f'Epoch [{epoch}/{epochs}], Loss: {loss.item():.6f}')

# 6. æµ‹è¯•
with torch.no_grad():
    predictions = model(X_train)
    print("\né¢„æµ‹ç»“æœ:")
    print(predictions.numpy())
```

### ğŸŒ TensorFlow/Keras å®ç°

```python
import tensorflow as tf
from tensorflow import keras
import numpy as np

# 1. å‡†å¤‡æ•°æ®
X_train = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
y_train = np.array([[0], [1], [1], [0]])

# 2. æ„å»ºæ¨¡å‹
model = keras.Sequential([
    keras.layers.Dense(4, activation='relu', input_shape=(2,)),
    keras.layers.Dense(1, activation='sigmoid')
])

# 3. ç¼–è¯‘æ¨¡å‹
model.compile(
    optimizer='adam',
    loss='mse',
    metrics=['accuracy']
)

# 4. è®­ç»ƒ
history = model.fit(
    X_train, y_train,
    epochs=5000,
    verbose=0  # ä¸æ‰“å°è®­ç»ƒè¿‡ç¨‹
)

# 5. è¯„ä¼°
loss, accuracy = model.evaluate(X_train, y_train, verbose=0)
print(f'Loss: {loss:.6f}')

# 6. é¢„æµ‹
predictions = model.predict(X_train)
print("\né¢„æµ‹ç»“æœ:")
print(predictions)

# 7. æŸ¥çœ‹æ¨¡å‹ç»“æ„
model.summary()
```

---

## 4.9 å®æˆ˜ï¼šMNIST æ‰‹å†™æ•°å­—è¯†åˆ«ï¼ˆæ·±åº¦ç½‘ç»œï¼‰

### ğŸ’» å®Œæ•´å®ç°

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
import matplotlib.pyplot as plt

# 1. æ•°æ®åŠ è½½å’Œé¢„å¤„ç†
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.1307,), (0.3081,))  # MNIST çš„å‡å€¼å’Œæ ‡å‡†å·®
])

train_dataset = datasets.MNIST(
    root='./data',
    train=True,
    download=True,
    transform=transform
)

test_dataset = datasets.MNIST(
    root='./data',
    train=False,
    download=True,
    transform=transform
)

train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)

# 2. å®šä¹‰æ·±åº¦ç¥ç»ç½‘ç»œ
class DeepNN(nn.Module):
    def __init__(self):
        super(DeepNN, self).__init__()
        self.fc1 = nn.Linear(28*28, 512)
        self.fc2 = nn.Linear(512, 256)
        self.fc3 = nn.Linear(256, 128)
        self.fc4 = nn.Linear(128, 10)

        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(0.2)

    def forward(self, x):
        x = x.view(-1, 28*28)  # å±•å¹³

        x = self.fc1(x)
        x = self.relu(x)
        x = self.dropout(x)

        x = self.fc2(x)
        x = self.relu(x)
        x = self.dropout(x)

        x = self.fc3(x)
        x = self.relu(x)
        x = self.dropout(x)

        x = self.fc4(x)
        return x

# 3. åˆå§‹åŒ–æ¨¡å‹
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = DeepNN().to(device)

# 4. å®šä¹‰æŸå¤±å’Œä¼˜åŒ–å™¨
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# 5. è®­ç»ƒå‡½æ•°
def train(model, device, train_loader, optimizer, epoch):
    model.train()
    train_loss = 0
    correct = 0

    for batch_idx, (data, target) in enumerate(train_loader):
        data, target = data.to(device), target.to(device)

        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()

        train_loss += loss.item()
        pred = output.argmax(dim=1, keepdim=True)
        correct += pred.eq(target.view_as(pred)).sum().item()

    train_loss /= len(train_loader)
    accuracy = 100. * correct / len(train_loader.dataset)

    print(f'Epoch: {epoch}, Loss: {train_loss:.4f}, Accuracy: {accuracy:.2f}%')
    return train_loss, accuracy

# 6. æµ‹è¯•å‡½æ•°
def test(model, device, test_loader):
    model.eval()
    test_loss = 0
    correct = 0

    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            test_loss += criterion(output, target).item()
            pred = output.argmax(dim=1, keepdim=True)
            correct += pred.eq(target.view_as(pred)).sum().item()

    test_loss /= len(test_loader)
    accuracy = 100. * correct / len(test_loader.dataset)

    print(f'Test Loss: {test_loss:.4f}, Accuracy: {accuracy:.2f}%\n')
    return test_loss, accuracy

# 7. è®­ç»ƒæ¨¡å‹
epochs = 10
train_losses, train_accs = [], []
test_losses, test_accs = [], []

for epoch in range(1, epochs + 1):
    train_loss, train_acc = train(model, device, train_loader, optimizer, epoch)
    test_loss, test_acc = test(model, device, test_loader)

    train_losses.append(train_loss)
    train_accs.append(train_acc)
    test_losses.append(test_loss)
    test_accs.append(test_acc)

# 8. å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))

ax1.plot(train_losses, label='Train Loss')
ax1.plot(test_losses, label='Test Loss')
ax1.set_xlabel('Epoch')
ax1.set_ylabel('Loss')
ax1.set_title('Training and Test Loss')
ax1.legend()
ax1.grid(True)

ax2.plot(train_accs, label='Train Accuracy')
ax2.plot(test_accs, label='Test Accuracy')
ax2.set_xlabel('Epoch')
ax2.set_ylabel('Accuracy (%)')
ax2.set_title('Training and Test Accuracy')
ax2.legend()
ax2.grid(True)

plt.tight_layout()
plt.show()

# 9. å¯è§†åŒ–ä¸€äº›é¢„æµ‹ç»“æœ
model.eval()
with torch.no_grad():
    data, target = next(iter(test_loader))
    data, target = data.to(device), target.to(device)
    output = model(data)
    pred = output.argmax(dim=1)

    # æ˜¾ç¤ºå‰16ä¸ªæ ·æœ¬
    fig, axes = plt.subplots(4, 4, figsize=(10, 10))
    for i, ax in enumerate(axes.flat):
        image = data[i].cpu().squeeze()
        true_label = target[i].item()
        pred_label = pred[i].item()

        ax.imshow(image, cmap='gray')
        color = 'green' if true_label == pred_label else 'red'
        ax.set_title(f'True: {true_label}, Pred: {pred_label}', color=color)
        ax.axis('off')

    plt.tight_layout()
    plt.show()
```

---

## ğŸ“ æœ¬ç« ä½œä¸š

### ä½œä¸š 1ï¼šç†è®ºé¢˜

1. **æ¿€æ´»å‡½æ•°é€‰æ‹©**
   - ä¸ºä»€ä¹ˆ ReLU æ¯” Sigmoid æ›´å¸¸ç”¨ï¼Ÿ
   - ä»€ä¹ˆæ˜¯ "Dead ReLU" é—®é¢˜ï¼Ÿå¦‚ä½•è§£å†³ï¼Ÿ
   - åœ¨ä»€ä¹ˆæƒ…å†µä¸‹ä»ç„¶ä½¿ç”¨ Sigmoidï¼Ÿ

2. **åå‘ä¼ æ’­ç†è§£**
   - ç”¨è‡ªå·±çš„è¯è§£é‡Šåå‘ä¼ æ’­
   - ä¸ºä»€ä¹ˆå«"åå‘"ä¼ æ’­ï¼Ÿ
   - ç”»å‡ºä¸€ä¸ª3å±‚ç½‘ç»œçš„è®¡ç®—å›¾

3. **åˆå§‹åŒ–ç­–ç•¥**
   - ä¸ºä»€ä¹ˆä¸èƒ½å…¨é›¶åˆå§‹åŒ–ï¼Ÿ
   - Xavier å’Œ He åˆå§‹åŒ–çš„åŒºåˆ«ï¼Ÿ
   - åç½®éœ€è¦ç‰¹æ®Šåˆå§‹åŒ–å—ï¼Ÿ

### ä½œä¸š 2ï¼šç¼–ç¨‹å®è·µ

#### ä»»åŠ¡ 1ï¼šä»é›¶å®ç°å¤šå±‚ç½‘ç»œ

```python
# å®ç°ä¸€ä¸ªLå±‚å…¨è¿æ¥ç½‘ç»œ
class DeepNeuralNetwork:
    def __init__(self, layer_dims):
        """
        layer_dims: æ¯å±‚çš„ç¥ç»å…ƒæ•°é‡
                   ä¾‹å¦‚ [784, 128, 64, 10]
        """
        pass

    def forward(self, X):
        """å‰å‘ä¼ æ’­"""
        pass

    def backward(self, X, Y):
        """åå‘ä¼ æ’­"""
        pass

    def train(self, X, Y, epochs, learning_rate):
        """è®­ç»ƒ"""
        pass

# TODO:
# 1. æ”¯æŒä»»æ„å±‚æ•°
# 2. æ”¯æŒä¸åŒæ¿€æ´»å‡½æ•°ï¼ˆReLU, Sigmoid, Tanhï¼‰
# 3. å®ç°æ¢¯åº¦æ£€éªŒ
# 4. åœ¨ MNIST ä¸Šæµ‹è¯•
```

#### ä»»åŠ¡ 2ï¼šæ¿€æ´»å‡½æ•°å¯¹æ¯”å®éªŒ

```python
# åœ¨ç›¸åŒæ•°æ®é›†ä¸Šå¯¹æ¯”ä¸åŒæ¿€æ´»å‡½æ•°çš„æ•ˆæœ
# 1. Sigmoid
# 2. Tanh
# 3. ReLU
# 4. Leaky ReLU

# è®°å½•ï¼š
# - è®­ç»ƒé€Ÿåº¦ï¼ˆè¾¾åˆ°90%å‡†ç¡®ç‡éœ€è¦çš„epochæ•°ï¼‰
# - æœ€ç»ˆå‡†ç¡®ç‡
# - è®­ç»ƒç¨³å®šæ€§

# ç”»å‡ºè®­ç»ƒæ›²çº¿å¯¹æ¯”å›¾
```

#### ä»»åŠ¡ 3ï¼šæ·±åº¦ç½‘ç»œå®éªŒ

```python
# åœ¨ Fashion-MNIST ä¸Šå¯¹æ¯”ä¸åŒæ·±åº¦çš„ç½‘ç»œ
# 1. 2å±‚ï¼š784 â†’ 128 â†’ 10
# 2. 3å±‚ï¼š784 â†’ 256 â†’ 128 â†’ 10
# 3. 4å±‚ï¼š784 â†’ 512 â†’ 256 â†’ 128 â†’ 10
# 4. 5å±‚ï¼š784 â†’ 512 â†’ 256 â†’ 128 â†’ 64 â†’ 10

# åˆ†æï¼š
# - æ˜¯å¦è¶Šæ·±è¶Šå¥½ï¼Ÿ
# - è§‚å¯Ÿæ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸ç°è±¡
# - å°è¯•ä¸åŒçš„åˆå§‹åŒ–æ–¹æ³•
```

---

## ğŸ”‘ æœ¬ç« å…³é”®æ¦‚å¿µ

| æ¦‚å¿µ | è¯´æ˜ |
|------|------|
| ç¥ç»ç½‘ç»œ | å¤šå±‚æ„ŸçŸ¥æœºçš„å †å  |
| å‰å‘ä¼ æ’­ | ä»è¾“å…¥è®¡ç®—åˆ°è¾“å‡º |
| åå‘ä¼ æ’­ | ç”¨é“¾å¼æ³•åˆ™è®¡ç®—æ¢¯åº¦ |
| æ¿€æ´»å‡½æ•° | å¼•å…¥éçº¿æ€§ |
| ReLU | æœ€å¸¸ç”¨çš„æ¿€æ´»å‡½æ•° |
| Sigmoid | è¾“å‡ºå±‚ï¼ˆäºŒå…ƒåˆ†ç±»ï¼‰ |
| Softmax | è¾“å‡ºå±‚ï¼ˆå¤šå…ƒåˆ†ç±»ï¼‰ |
| Xavieråˆå§‹åŒ– | é€‚ç”¨äº Sigmoid/Tanh |
| Heåˆå§‹åŒ– | é€‚ç”¨äº ReLU |
| æ¢¯åº¦æ£€éªŒ | éªŒè¯åå‘ä¼ æ’­æ­£ç¡®æ€§ |

---

## ğŸ¯ ä¸‹ä¸€ç« é¢„å‘Š

**ç¬¬äº”ç« ï¼šä¼˜åŒ–ç®—æ³•ä¸è®­ç»ƒæŠ€å·§**
- Mini-batch æ¢¯åº¦ä¸‹é™
- åŠ¨é‡ (Momentum)
- RMSprop, Adam
- Learning Rate Scheduling
- Batch Normalization
- Dropout
- æ­£åˆ™åŒ–æŠ€æœ¯
- è°ƒå‚æŠ€å·§

---

-----

