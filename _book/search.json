[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "æ·±åº¦å­¦ä¹ å®Œæ•´æ•™ç¨‹",
    "section": "",
    "text": "0.1 ğŸ“‹ è¯¾ç¨‹æ•´ä½“æ¡†æ¶",
    "crumbs": [
      "<span class='chapter-number'>1</span>Â  <span class='chapter-title'>ğŸ“‹ è¯¾ç¨‹æ•´ä½“æ¡†æ¶</span>"
    ]
  },
  {
    "objectID": "index.html#è¯¾ç¨‹æ•´ä½“æ¡†æ¶",
    "href": "index.html#è¯¾ç¨‹æ•´ä½“æ¡†æ¶",
    "title": "æ·±åº¦å­¦ä¹ å®Œæ•´æ•™ç¨‹",
    "section": "",
    "text": "0.1.1 ç¬¬ä¸€éƒ¨åˆ†ï¼šæœºå™¨å­¦ä¹ åŸºç¡€ (ML Basics)\n\n0.1.1.1 1. å¼•å…¥ï¼šæœºå™¨å­¦ä¹ æ˜¯ä»€ä¹ˆï¼Ÿ\n\nğŸ¯ ä»ç”Ÿæ´»ä¾‹å­å‡ºå‘\n\nâ€œæœºå™¨å­¦ä¹ å°±åƒæ˜¯æ•™ç”µè„‘â€™ä¸¾ä¸€åä¸‰â€™â€\nå®ä¾‹ï¼šè¯­éŸ³è¯†åˆ«ã€å›¾åƒè¯†åˆ«ã€AlphaGo\n\nä¸‰ä¸ªæ­¥éª¤æ¡†æ¶\n\nDefine a function set (Model)\nGoodness of function (Loss)\nPick the best function (Optimization)\n\n\n\n\n0.1.1.2 2. å›å½’ (Regression)\n\nLinear Regression å®ä¾‹\nç”¨å®å¯æ¢¦CPå€¼é¢„æµ‹æ¼”ç¤º\nGradient Descent ç›´è§‚è§£é‡Š\nå¯è§†åŒ–ï¼šLoss Function åœ°å½¢å›¾\n\n\n\n0.1.1.3 3. åˆ†ç±» (Classification)\n\nLogistic Regression\nä¸ºä»€ä¹ˆä¸èƒ½ç”¨ Regression åš Classificationï¼Ÿ\nSoftmax & Cross-Entropy\nå®ä¾‹ï¼šæ‰‹å†™æ•°å­—è¯†åˆ« (MNIST)\n\n\n\n\n\n0.1.2 ç¬¬äºŒéƒ¨åˆ†ï¼šæ·±åº¦å­¦ä¹  (Deep Learning)\n\n0.1.2.1 4. ç¥ç»ç½‘ç»œåŸºç¡€\n\nğŸ§  ç›´è§‚ç†è§£ï¼šâ€œç¥ç»ç½‘ç»œå°±æ˜¯å¾ˆå¤š Logistic Regression å èµ·æ¥â€\nActivation Functions (Sigmoid, ReLU)\nBackpropagationï¼ˆç”¨è®¡ç®—å›¾è§£é‡Šï¼‰\nå®æˆ˜ï¼šå»ºç«‹ä½ çš„ç¬¬ä¸€ä¸ªç¥ç»ç½‘ç»œ\n\n\n\n0.1.2.2 5. è®­ç»ƒæŠ€å·§ (Tips for Training)\næ¨¡å‹è¡¨ç°ä¸å¥½ï¼Ÿ\nâ”œâ”€ è®­ç»ƒæ•°æ®è¡¨ç°å·®ï¼Ÿ\nâ”‚  â”œâ”€ Optimization é—®é¢˜ â†’ Adaptive Learning Rate, Batch Normalization\nâ”‚  â””â”€ Overfitting â†’ Regularization, Dropout, Early Stopping\nâ””â”€ æµ‹è¯•æ•°æ®è¡¨ç°å·®ï¼Ÿ\n   â””â”€ Overfitting â†’ æ›´å¤šæ•°æ®, Data Augmentation\n\n\n0.1.2.3 6. CNN (å·ç§¯ç¥ç»ç½‘ç»œ)\n\nä¸ºä»€ä¹ˆéœ€è¦ CNNï¼Ÿï¼ˆå‚æ•°å¤ªå¤šçš„é—®é¢˜ï¼‰\nConvolution & Pooling ç›´è§‚è§£é‡Š\nç»å…¸æ¶æ„ï¼šLeNet, AlexNet, VGG, ResNet\nåº”ç”¨ï¼šå›¾åƒåˆ†ç±»ã€ç‰©ä½“æ£€æµ‹\n\n\n\n0.1.2.4 7. RNN & Sequence Models\n\nåºåˆ—æ•°æ®çš„ç‰¹æ€§\nRNN, LSTM, GRU\nSeq2Seq æ¶æ„\nåº”ç”¨ï¼šè¯­éŸ³è¯†åˆ«ã€æœºå™¨ç¿»è¯‘\n\n\n\n\n\n0.1.3 ç¬¬ä¸‰éƒ¨åˆ†ï¼šè¿›é˜¶ä¸»é¢˜\n\n0.1.3.1 8. Self-Attention & Transformer\n\nğŸ”¥ ä» RNN çš„é™åˆ¶è°ˆèµ·\nAttention Mechanism å›¾è§£\nMulti-Head Attention\nTransformer æ¶æ„å®Œæ•´è§£æ\nBERT, GPT ç®€ä»‹\n\n\n\n0.1.3.2 9. ç”Ÿæˆæ¨¡å‹ (Generative Models)\n\nAuto-encoder\nVAE (Variational Auto-encoder)\nGAN (å¯¹æŠ—ç”Ÿæˆç½‘ç»œ)\n\nGenerator vs.Â Discriminator çš„å¯¹æŠ—æ¸¸æˆ\nè®­ç»ƒéš¾ç‚¹ä¸æŠ€å·§\n\nDiffusion Models ç®€ä»‹\n\n\n\n0.1.3.3 10. å¼ºåŒ–å­¦ä¹  (Reinforcement Learning)\n\nAgent, Environment, Reward\nQ-Learning\nPolicy Gradient\nå®ä¾‹ï¼šç© Atari æ¸¸æˆã€AlphaGo åŸç†\n\n\n\n0.1.3.4 11. æ— ç›‘ç£å­¦ä¹  (Unsupervised Learning)\n\nClustering (K-means, HAC)\nDimension Reduction (PCA, t-SNE)\nSelf-Supervised Learning\n\n\n\n\n\n0.1.4 ç¬¬å››éƒ¨åˆ†ï¼šå®è·µä¸åº”ç”¨\n\n0.1.4.1 12. è¿ç§»å­¦ä¹  (Transfer Learning)\n\nPre-training & Fine-tuning\nDomain Adaptation\n\n\n\n0.1.4.2 13. å¯è§£é‡Šæ€§ä¸å¯¹æŠ—æ”»å‡»\n\nExplainable AI\nAdversarial Attack & Defense\n\n\n\n0.1.4.3 14. å¤§è¯­è¨€æ¨¡å‹æ—¶ä»£\n\nLLM åŸç†\nPrompt Engineering\nIn-Context Learning\næœªæ¥å±•æœ›",
    "crumbs": [
      "<span class='chapter-number'>1</span>Â  <span class='chapter-title'>ğŸ“‹ è¯¾ç¨‹æ•´ä½“æ¡†æ¶</span>"
    ]
  },
  {
    "objectID": "index.html#æ¨èä½œä¸šè®¾ç½®",
    "href": "index.html#æ¨èä½œä¸šè®¾ç½®",
    "title": "æ·±åº¦å­¦ä¹ å®Œæ•´æ•™ç¨‹",
    "section": "0.2 ğŸ“š æ¨èä½œä¸šè®¾ç½®",
    "text": "0.2 ğŸ“š æ¨èä½œä¸šè®¾ç½®\n\nHW1: Linear Regression (PM2.5 é¢„æµ‹)\nHW2: Classification (æ”¶å…¥é¢„æµ‹)\nHW3: CNN (å›¾åƒåˆ†ç±»)\nHW4: RNN (æ–‡æœ¬æƒ…æ„Ÿåˆ†æ)\nHW5: Transformer (æœºå™¨ç¿»è¯‘)\nHW6: GAN (åŠ¨æ¼«äººç‰©ç”Ÿæˆ)\nFinal Project: å¼€æ”¾å¼ç«èµ›",
    "crumbs": [
      "<span class='chapter-number'>1</span>Â  <span class='chapter-title'>ğŸ“‹ è¯¾ç¨‹æ•´ä½“æ¡†æ¶</span>"
    ]
  },
  {
    "objectID": "index.html#å·¥å…·ä¸èµ„æº",
    "href": "index.html#å·¥å…·ä¸èµ„æº",
    "title": "æ·±åº¦å­¦ä¹ å®Œæ•´æ•™ç¨‹",
    "section": "0.3 ğŸ› ï¸ å·¥å…·ä¸èµ„æº",
    "text": "0.3 ğŸ› ï¸ å·¥å…·ä¸èµ„æº\n\nç¼–ç¨‹ç¯å¢ƒ: Python + PyTorch/TensorFlow\nå¹³å°: Google Colab (å…è´¹ GPU)\næ•°æ®é›†: MNIST, CIFAR-10, ImageNet, Common Voice",
    "crumbs": [
      "<span class='chapter-number'>1</span>Â  <span class='chapter-title'>ğŸ“‹ è¯¾ç¨‹æ•´ä½“æ¡†æ¶</span>"
    ]
  },
  {
    "objectID": "Chapter1.html",
    "href": "Chapter1.html",
    "title": "2Â  æœºå™¨å­¦ä¹ å®Œæ•´è¯¾ç¨‹",
    "section": "",
    "text": "3 ç¬¬ä¸€ç« ï¼šæœºå™¨å­¦ä¹ ç®€ä»‹ (Introduction to Machine Learning)",
    "crumbs": [
      "ç¬¬ä¸€éƒ¨åˆ†ï¼šåŸºç¡€ç¯‡",
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>æœºå™¨å­¦ä¹ å®Œæ•´è¯¾ç¨‹</span>"
    ]
  },
  {
    "objectID": "Chapter1.html#ç« èŠ‚ç›®æ ‡",
    "href": "Chapter1.html#ç« èŠ‚ç›®æ ‡",
    "title": "2Â  æœºå™¨å­¦ä¹ å®Œæ•´è¯¾ç¨‹",
    "section": "3.1 ğŸ“Œ ç« èŠ‚ç›®æ ‡",
    "text": "3.1 ğŸ“Œ ç« èŠ‚ç›®æ ‡\n\nç†è§£ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ \næŒæ¡æœºå™¨å­¦ä¹ çš„ä¸‰ä¸ªæ ¸å¿ƒæ­¥éª¤\näº†è§£æœºå™¨å­¦ä¹ çš„åº”ç”¨åœºæ™¯\nå»ºç«‹å­¦ä¹ è·¯çº¿å›¾",
    "crumbs": [
      "ç¬¬ä¸€éƒ¨åˆ†ï¼šåŸºç¡€ç¯‡",
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>æœºå™¨å­¦ä¹ å®Œæ•´è¯¾ç¨‹</span>"
    ]
  },
  {
    "objectID": "Chapter1.html#ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ",
    "href": "Chapter1.html#ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ",
    "title": "2Â  æœºå™¨å­¦ä¹ å®Œæ•´è¯¾ç¨‹",
    "section": "3.2 1.1 ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ",
    "text": "3.2 1.1 ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ\n\n3.2.1 ğŸ¯ ä»ç”Ÿæ´»ä¾‹å­å¼€å§‹\nåœºæ™¯ä¸€ï¼šè¯­éŸ³åŠ©æ‰‹ - ä½ å¯¹ Siri è¯´ï¼šâ€œä»Šå¤©å¤©æ°”å¦‚ä½•ï¼Ÿâ€ - Siri ç†è§£ä½ çš„è¯­éŸ³ â†’ è¯†åˆ«æ„å›¾ â†’ æŸ¥è¯¢å¤©æ°” â†’ å›ç­”ä½  - é—®é¢˜ï¼šæˆ‘ä»¬æœ‰åŠæ³•å†™ç¨‹åºæ¥â€ç†è§£â€æ‰€æœ‰å¯èƒ½çš„è¯­éŸ³å—ï¼Ÿ\nåœºæ™¯äºŒï¼šåƒåœ¾é‚®ä»¶è¿‡æ»¤ - ä¼ ç»Ÿæ–¹æ³•ï¼šå†™ä¸€å †è§„åˆ™ï¼ˆåŒ…å«â€ä¸­å¥–â€ã€â€œå…è´¹â€ç­‰å…³é”®è¯ï¼‰ - é—®é¢˜ï¼šè§„åˆ™å¤ªå¤šã€ä¾‹å¤–å¤ªå¤šã€ä¸æ–­å˜åŒ– - ML æ–¹æ³•ï¼šè®©æœºå™¨è‡ªå·±ä»æ•°æ®ä¸­å­¦ä¹ è§„åˆ™\n\n\n3.2.2 ğŸ’¡ å®šä¹‰\n\næœºå™¨å­¦ä¹  = Looking for a Function\n\næœºå™¨å­¦ä¹ å°±æ˜¯è®©æœºå™¨è‡ªåŠ¨ä»æ•°æ®ä¸­æ‰¾åˆ°ä¸€ä¸ªå‡½æ•°ï¼ˆfunctionï¼‰ï¼Œè¿™ä¸ªå‡½æ•°å¯ä»¥ï¼š - è¾“å…¥ï¼šä¸€äº›ä¿¡æ¯ï¼ˆå›¾ç‰‡ã€å£°éŸ³ã€æ–‡å­—â€¦ï¼‰ - è¾“å‡ºï¼šæˆ‘ä»¬æƒ³è¦çš„ç»“æœï¼ˆåˆ†ç±»ã€æ•°å€¼ã€å¦ä¸€å¼ å›¾ç‰‡â€¦ï¼‰\n\n\n3.2.3 ğŸ“Š æœºå™¨å­¦ä¹  vs ä¼ ç»Ÿç¼–ç¨‹\nä¼ ç»Ÿç¼–ç¨‹ï¼š\n  è§„åˆ™ï¼ˆäººå†™çš„ä»£ç ï¼‰+ æ•°æ® â†’ ç»“æœ\n\næœºå™¨å­¦ä¹ ï¼š\n  æ•°æ® + ç»“æœ â†’ è§„åˆ™ï¼ˆæœºå™¨è‡ªå·±å­¦çš„ï¼‰",
    "crumbs": [
      "ç¬¬ä¸€éƒ¨åˆ†ï¼šåŸºç¡€ç¯‡",
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>æœºå™¨å­¦ä¹ å®Œæ•´è¯¾ç¨‹</span>"
    ]
  },
  {
    "objectID": "Chapter1.html#æœºå™¨å­¦ä¹ çš„ç±»å‹",
    "href": "Chapter1.html#æœºå™¨å­¦ä¹ çš„ç±»å‹",
    "title": "2Â  æœºå™¨å­¦ä¹ å®Œæ•´è¯¾ç¨‹",
    "section": "3.3 1.2 æœºå™¨å­¦ä¹ çš„ç±»å‹",
    "text": "3.3 1.2 æœºå™¨å­¦ä¹ çš„ç±»å‹\n\n3.3.1 ğŸ”¹ æŒ‰å­¦ä¹ æ–¹å¼åˆ†ç±»\n\n3.3.1.1 1. Supervised Learningï¼ˆç›‘ç£å­¦ä¹ ï¼‰\n\nå®šä¹‰ï¼šæœ‰â€æ ‡å‡†ç­”æ¡ˆâ€çš„å­¦ä¹ \nä¾‹å­ï¼š\n\nç»™æœºå™¨çœ‹ 1000 å¼ çŒ«çš„ç…§ç‰‡ï¼ˆæ ‡è®°ä¸ºâ€çŒ«â€ï¼‰\nç»™æœºå™¨çœ‹ 1000 å¼ ç‹—çš„ç…§ç‰‡ï¼ˆæ ‡è®°ä¸ºâ€ç‹—â€ï¼‰\næœºå™¨å­¦ä¼šåŒºåˆ†çŒ«å’Œç‹—\n\n\nå¸¸è§ä»»åŠ¡ï¼š - Regressionï¼ˆå›å½’ï¼‰ï¼šé¢„æµ‹æ•°å€¼ - ä¾‹ï¼šæˆ¿ä»·é¢„æµ‹ã€è‚¡ç¥¨é¢„æµ‹ã€æ¸©åº¦é¢„æµ‹ - Classificationï¼ˆåˆ†ç±»ï¼‰ï¼šé¢„æµ‹ç±»åˆ« - ä¾‹ï¼šåƒåœ¾é‚®ä»¶è¯†åˆ«ã€å›¾åƒè¯†åˆ«ã€ç–¾ç—…è¯Šæ–­\n\n\n3.3.1.2 2. Unsupervised Learningï¼ˆæ— ç›‘ç£å­¦ä¹ ï¼‰\n\nå®šä¹‰ï¼šæ²¡æœ‰â€æ ‡å‡†ç­”æ¡ˆâ€ï¼Œè®©æœºå™¨è‡ªå·±æ‰¾è§„å¾‹\nä¾‹å­ï¼š\n\nç»™æœºå™¨ä¸€å †æ–‡ç« ï¼Œè®©å®ƒè‡ªå·±åˆ†æˆå‡ ç±»ï¼ˆæ–°é—»ã€ä½“è‚²ã€ç§‘æŠ€â€¦ï¼‰\nå®¢æˆ·åˆ†ç¾¤ã€å¼‚å¸¸æ£€æµ‹\n\n\nå¸¸è§ä»»åŠ¡ï¼š - Clusteringï¼ˆèšç±»ï¼‰ - Dimension Reductionï¼ˆé™ç»´ï¼‰ - Anomaly Detectionï¼ˆå¼‚å¸¸æ£€æµ‹ï¼‰\n\n\n3.3.1.3 3. Reinforcement Learningï¼ˆå¼ºåŒ–å­¦ä¹ ï¼‰\n\nå®šä¹‰ï¼šä»â€å¥–åŠ±â€ä¸­å­¦ä¹ \nä¾‹å­ï¼š\n\nAlphaGo ä¸‹å›´æ£‹ï¼šèµ¢äº†æœ‰å¥–åŠ±ï¼Œè¾“äº†æœ‰æƒ©ç½š\næœºå™¨äººå­¦èµ°è·¯ï¼šä¸æ‘”å€’æœ‰å¥–åŠ±\n\nç‰¹ç‚¹ï¼šåƒè®­ç»ƒå® ç‰©ï¼Œåšå¯¹äº†ç»™å¥–åŠ±ï¼Œåšé”™äº†ç»™æƒ©ç½š\n\n\n\n3.3.1.4 4. Semi-Supervised Learningï¼ˆåŠç›‘ç£å­¦ä¹ ï¼‰\n\nå°‘é‡æœ‰æ ‡ç­¾æ•°æ® + å¤§é‡æ— æ ‡ç­¾æ•°æ®\nç°å®ä¸­æœ€å¸¸è§ï¼ˆæ ‡æ³¨æ•°æ®å¾ˆè´µï¼ï¼‰",
    "crumbs": [
      "ç¬¬ä¸€éƒ¨åˆ†ï¼šåŸºç¡€ç¯‡",
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>æœºå™¨å­¦ä¹ å®Œæ•´è¯¾ç¨‹</span>"
    ]
  },
  {
    "objectID": "Chapter1.html#æœºå™¨å­¦ä¹ ä¸‰æ­¥éª¤-framework",
    "href": "Chapter1.html#æœºå™¨å­¦ä¹ ä¸‰æ­¥éª¤-framework",
    "title": "2Â  æœºå™¨å­¦ä¹ å®Œæ•´è¯¾ç¨‹",
    "section": "3.4 1.3 æœºå™¨å­¦ä¹ ä¸‰æ­¥éª¤ Framework",
    "text": "3.4 1.3 æœºå™¨å­¦ä¹ ä¸‰æ­¥éª¤ Framework\n\nè¿™æ˜¯æå®æ¯…è€å¸ˆæœ€æ ¸å¿ƒçš„æ¡†æ¶ï¼Œè´¯ç©¿æ•´ä¸ªè¯¾ç¨‹ï¼\n\nStep 1: Define a Function Set (Model)\n          â†“\nStep 2: Goodness of Function (Loss Function)\n          â†“\nStep 3: Pick the Best Function (Optimization)\n\n3.4.1 ğŸ”¸ Step 1: Define a Function Set (å®šä¹‰æ¨¡å‹)\né—®é¢˜ï¼šæˆ‘ä»¬è¦æ‰¾ä»€ä¹ˆæ ·çš„ functionï¼Ÿ\nä¾‹å­ï¼šé¢„æµ‹å®å¯æ¢¦è¿›åŒ–åçš„ CP å€¼ - Input: è¿›åŒ–å‰çš„ CP å€¼ (x) - Output: è¿›åŒ–åçš„ CP å€¼ (y)\nå¯èƒ½çš„ Function Setï¼š\n\nLinear Modelï¼ˆçº¿æ€§æ¨¡å‹ï¼‰\ny = b + wÂ·x\n\nw: weightï¼ˆæƒé‡ï¼‰\nb: biasï¼ˆåå·®ï¼‰\nä¸åŒçš„ w å’Œ b â†’ ä¸åŒçš„ function\n\nNon-linear Modelï¼ˆéçº¿æ€§æ¨¡å‹ï¼‰\ny = b + wâ‚Â·x + wâ‚‚Â·xÂ²\n\nå…³é”®æ¦‚å¿µï¼š - Model: ä¸€ç»„å¯èƒ½çš„ functions - Feature: è¾“å…¥çš„ç‰¹å¾ï¼ˆxï¼‰ - Parameter: æ§åˆ¶ function çš„å‚æ•°ï¼ˆw, bï¼‰\n\n\n\n3.4.2 ğŸ”¸ Step 2: Goodness of Function (è¯„ä¼°å‡½æ•°å¥½å)\né—®é¢˜ï¼šæ€ä¹ˆçŸ¥é“ä¸€ä¸ª function å¥½ä¸å¥½ï¼Ÿ\nç­”æ¡ˆï¼šçœ‹å®ƒåœ¨è®­ç»ƒæ•°æ®ä¸Šçš„è¡¨ç°ï¼\n\n3.4.2.1 Loss Function (æŸå¤±å‡½æ•°)\nLoss = é¢„æµ‹å€¼å’ŒçœŸå®å€¼çš„å·®è·\n\nL(w, b) = Î£(Å·â¿ - yâ¿)Â²\n\nyâ¿: ç¬¬ n ç¬”æ•°æ®çš„çœŸå®å€¼\nÅ·â¿ = b + wÂ·xâ¿: ç¬¬ n ç¬”æ•°æ®çš„é¢„æµ‹å€¼\næ€»å…±æœ‰ N ç¬”è®­ç»ƒæ•°æ®\n\nå¯è§†åŒ–ï¼š\nçœŸå®å€¼ y | é¢„æµ‹å€¼ Å· | è¯¯å·®\n   10   |    9     |  1\n   15   |   17     |  2\n   20   |   19     |  1\n  ...   |   ...    | ...\nä¸åŒçš„ Loss Functionï¼š - Mean Squared Error (MSE)ï¼š(1/N)Î£(Å·â¿ - yâ¿)Â² - Mean Absolute Error (MAE)ï¼š(1/N)Î£|Å·â¿ - yâ¿| - Cross Entropyï¼ˆç”¨äºåˆ†ç±»é—®é¢˜ï¼‰\n\n\n\n\n3.4.3 ğŸ”¸ Step 3: Pick the Best Function (æ‰¾åˆ°æœ€ä½³å‡½æ•°)\né—®é¢˜ï¼šæ€ä¹ˆæ‰¾åˆ°è®© Loss æœ€å°çš„å‚æ•°ï¼Ÿ\nç­”æ¡ˆï¼šGradient Descentï¼ˆæ¢¯åº¦ä¸‹é™ï¼‰\n\n3.4.3.1 Gradient Descent ç›´è§‚ç†è§£\næ¯”å–»ï¼šç›²äººä¸‹å±±æ‰¾æœ€ä½ç‚¹\n\nç«™åœ¨å±±ä¸ŠæŸä¸ªä½ç½®ï¼ˆéšæœºåˆå§‹åŒ–å‚æ•°ï¼‰\næ„Ÿå—è„šä¸‹çš„å¡åº¦ï¼ˆè®¡ç®—æ¢¯åº¦ï¼‰\nå¾€ä¸‹å¡çš„æ–¹å‘èµ°ä¸€æ­¥ï¼ˆæ›´æ–°å‚æ•°ï¼‰\né‡å¤ 2-3ï¼Œç›´åˆ°åˆ°è¾¾è°·åº•ï¼ˆLoss ä¸å†ä¸‹é™ï¼‰\n\n\n\n3.4.3.2 æ•°å­¦è¡¨è¾¾\nwÂ¹ = wâ° - Î· Â· âˆ‚L/âˆ‚w\nbÂ¹ = bâ° - Î· Â· âˆ‚L/âˆ‚b\n\nÎ· (eta): Learning Rateï¼ˆå­¦ä¹ ç‡ï¼‰\n\nå¤ªå¤§ï¼šå¯èƒ½è·³è¿‡æœ€ä½ç‚¹\nå¤ªå°ï¼šä¸‹å±±å¤ªæ…¢\n\nâˆ‚L/âˆ‚w: Loss å¯¹ w çš„åå¯¼æ•°ï¼ˆæ¢¯åº¦ï¼‰\n\n\n\n3.4.3.3 å¯è§†åŒ–\nLoss\n â†‘\n |     *\n |    /  \\\n |   /    \\\n |  /  â—   \\    â— å½“å‰ä½ç½®\n | /        \\   â†“ æ¢¯åº¦æ–¹å‘\n |/__________\\â†’___________\n              w",
    "crumbs": [
      "ç¬¬ä¸€éƒ¨åˆ†ï¼šåŸºç¡€ç¯‡",
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>æœºå™¨å­¦ä¹ å®Œæ•´è¯¾ç¨‹</span>"
    ]
  },
  {
    "objectID": "Chapter1.html#æœºå™¨å­¦ä¹ çš„åº”ç”¨åœºæ™¯",
    "href": "Chapter1.html#æœºå™¨å­¦ä¹ çš„åº”ç”¨åœºæ™¯",
    "title": "2Â  æœºå™¨å­¦ä¹ å®Œæ•´è¯¾ç¨‹",
    "section": "3.5 1.4 æœºå™¨å­¦ä¹ çš„åº”ç”¨åœºæ™¯",
    "text": "3.5 1.4 æœºå™¨å­¦ä¹ çš„åº”ç”¨åœºæ™¯\n\n3.5.1 ğŸ¨ è®¡ç®—æœºè§†è§‰ (Computer Vision)\n\n\n\nåº”ç”¨\nè¯´æ˜\nä¾‹å­\n\n\n\n\nå›¾åƒåˆ†ç±»\nåˆ¤æ–­å›¾ç‰‡å†…å®¹\nçŒ«ç‹—è¯†åˆ«ã€åŒ»å­¦å½±åƒè¯Šæ–­\n\n\nç‰©ä½“æ£€æµ‹\næ‰¾å‡ºå›¾ç‰‡ä¸­çš„ç‰©ä½“ä½ç½®\nè‡ªåŠ¨é©¾é©¶ã€äººè„¸è¯†åˆ«\n\n\nå›¾åƒåˆ†å‰²\næŠŠå›¾ç‰‡æ¯ä¸ªåƒç´ åˆ†ç±»\nå»èƒŒæ™¯ã€åŒ»å­¦å½±åƒåˆ†å‰²\n\n\nå›¾åƒç”Ÿæˆ\nç”Ÿæˆæ–°å›¾ç‰‡\nAI ç”»ç”»ã€é£æ ¼è¿ç§»\n\n\n\n\n\n3.5.2 ğŸ—£ï¸ è‡ªç„¶è¯­è¨€å¤„ç† (NLP)\n\n\n\nåº”ç”¨\nè¯´æ˜\nä¾‹å­\n\n\n\n\næ–‡æœ¬åˆ†ç±»\nåˆ¤æ–­æ–‡æœ¬ç±»åˆ«\nåƒåœ¾é‚®ä»¶è¿‡æ»¤ã€æƒ…æ„Ÿåˆ†æ\n\n\næœºå™¨ç¿»è¯‘\nç¿»è¯‘è¯­è¨€\nGoogle Translate\n\n\né—®ç­”ç³»ç»Ÿ\nå›ç­”é—®é¢˜\nChatGPTã€Siri\n\n\næ–‡æœ¬ç”Ÿæˆ\nç”Ÿæˆæ–‡ç« \nAI å†™ä½œåŠ©æ‰‹\n\n\n\n\n\n3.5.3 ğŸ”Š è¯­éŸ³å¤„ç† (Speech)\n\nè¯­éŸ³è¯†åˆ« (Speech Recognition)\nè¯­éŸ³åˆæˆ (Text-to-Speech)\nå£°çº¹è¯†åˆ«\nè¯­éŸ³è½¬æ¢\n\n\n\n3.5.4 ğŸ® å…¶ä»–åº”ç”¨\n\næ¨èç³»ç»Ÿï¼šNetflixã€YouTubeã€æ·˜å®æ¨è\næ¸¸æˆ AIï¼šAlphaGoã€Dota 2ã€æ˜Ÿé™…äº‰éœ¸\né‡‘èé¢„æµ‹ï¼šè‚¡ç¥¨ã€ä¿¡ç”¨è¯„åˆ†\nåŒ»ç–—è¯Šæ–­ï¼šç–¾ç—…é¢„æµ‹ã€è¯ç‰©å‘ç°\nè‡ªåŠ¨é©¾é©¶ï¼šç‰¹æ–¯æ‹‰ã€Waymo",
    "crumbs": [
      "ç¬¬ä¸€éƒ¨åˆ†ï¼šåŸºç¡€ç¯‡",
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>æœºå™¨å­¦ä¹ å®Œæ•´è¯¾ç¨‹</span>"
    ]
  },
  {
    "objectID": "Chapter1.html#æœºå™¨å­¦ä¹ çš„æŒ‘æˆ˜",
    "href": "Chapter1.html#æœºå™¨å­¦ä¹ çš„æŒ‘æˆ˜",
    "title": "2Â  æœºå™¨å­¦ä¹ å®Œæ•´è¯¾ç¨‹",
    "section": "3.6 1.5 æœºå™¨å­¦ä¹ çš„æŒ‘æˆ˜",
    "text": "3.6 1.5 æœºå™¨å­¦ä¹ çš„æŒ‘æˆ˜\n\n3.6.1 âš ï¸ å¸¸è§é—®é¢˜\n\n3.6.1.1 1. Overfittingï¼ˆè¿‡æ‹Ÿåˆï¼‰\nè®­ç»ƒæ•°æ®ï¼š100åˆ† âœ“\næµ‹è¯•æ•°æ®ï¼š60åˆ† âœ—\n\nå°±åƒæ­»è®°ç¡¬èƒŒï¼Œçœ‹è¿‡çš„é¢˜éƒ½ä¼šï¼Œæ–°é¢˜ç›®ä¸ä¼šåš\nè§£å†³æ–¹æ³•ï¼š - æ›´å¤šè®­ç»ƒæ•°æ® - ç®€åŒ–æ¨¡å‹ - Regularizationï¼ˆæ­£åˆ™åŒ–ï¼‰ - Dropout\n\n\n3.6.1.2 2. Underfittingï¼ˆæ¬ æ‹Ÿåˆï¼‰\nè®­ç»ƒæ•°æ®ï¼š60åˆ† âœ—\næµ‹è¯•æ•°æ®ï¼š55åˆ† âœ—\n\næ¨¡å‹å¤ªç®€å•ï¼Œè¿è®­ç»ƒæ•°æ®éƒ½å­¦ä¸å¥½\nè§£å†³æ–¹æ³•ï¼š - ä½¿ç”¨æ›´å¤æ‚çš„æ¨¡å‹ - å¢åŠ ç‰¹å¾ - è°ƒæ•´è¶…å‚æ•°\n\n\n3.6.1.3 3. æ•°æ®é—®é¢˜\n\næ•°æ®ä¸è¶³ï¼šæ·±åº¦å­¦ä¹ é€šå¸¸éœ€è¦å¤§é‡æ•°æ®\næ•°æ®åå·®ï¼šè®­ç»ƒæ•°æ®ä¸ä»£è¡¨çœŸå®ä¸–ç•Œ\næ ‡æ³¨é”™è¯¯ï¼šäººå·¥æ ‡æ³¨å¯èƒ½æœ‰é”™\n\n\n\n3.6.1.4 4. è®¡ç®—èµ„æº\n\nè®­ç»ƒå¤§æ¨¡å‹éœ€è¦å¼ºå¤§çš„ GPU\nè®­ç»ƒæ—¶é—´å¯èƒ½å¾ˆé•¿ï¼ˆå‡ å¤©åˆ°å‡ å‘¨ï¼‰",
    "crumbs": [
      "ç¬¬ä¸€éƒ¨åˆ†ï¼šåŸºç¡€ç¯‡",
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>æœºå™¨å­¦ä¹ å®Œæ•´è¯¾ç¨‹</span>"
    ]
  },
  {
    "objectID": "Chapter1.html#å­¦ä¹ è·¯çº¿å›¾",
    "href": "Chapter1.html#å­¦ä¹ è·¯çº¿å›¾",
    "title": "2Â  æœºå™¨å­¦ä¹ å®Œæ•´è¯¾ç¨‹",
    "section": "3.7 1.6 å­¦ä¹ è·¯çº¿å›¾",
    "text": "3.7 1.6 å­¦ä¹ è·¯çº¿å›¾\nç¬¬ä¸€é˜¶æ®µï¼šåŸºç¡€\nâ”œâ”€ çº¿æ€§å›å½’ (Linear Regression)\nâ”œâ”€ é€»è¾‘å›å½’ (Logistic Regression)\nâ””â”€ æ¢¯åº¦ä¸‹é™ (Gradient Descent)\n\nç¬¬äºŒé˜¶æ®µï¼šæ·±åº¦å­¦ä¹ \nâ”œâ”€ ç¥ç»ç½‘ç»œ (Neural Network)\nâ”œâ”€ CNN (å·ç§¯ç¥ç»ç½‘ç»œ)\nâ”œâ”€ RNN (å¾ªç¯ç¥ç»ç½‘ç»œ)\nâ””â”€ Transformer\n\nç¬¬ä¸‰é˜¶æ®µï¼šè¿›é˜¶\nâ”œâ”€ GAN (ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ)\nâ”œâ”€ å¼ºåŒ–å­¦ä¹  (RL)\nâ””â”€ å¤§è¯­è¨€æ¨¡å‹ (LLM)",
    "crumbs": [
      "ç¬¬ä¸€éƒ¨åˆ†ï¼šåŸºç¡€ç¯‡",
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>æœºå™¨å­¦ä¹ å®Œæ•´è¯¾ç¨‹</span>"
    ]
  },
  {
    "objectID": "Chapter1.html#å®è·µç¬¬ä¸€ä¸ªæœºå™¨å­¦ä¹ ç¨‹åº",
    "href": "Chapter1.html#å®è·µç¬¬ä¸€ä¸ªæœºå™¨å­¦ä¹ ç¨‹åº",
    "title": "2Â  æœºå™¨å­¦ä¹ å®Œæ•´è¯¾ç¨‹",
    "section": "3.8 1.7 å®è·µï¼šç¬¬ä¸€ä¸ªæœºå™¨å­¦ä¹ ç¨‹åº",
    "text": "3.8 1.7 å®è·µï¼šç¬¬ä¸€ä¸ªæœºå™¨å­¦ä¹ ç¨‹åº\n\n3.8.1 ğŸ ä½¿ç”¨ Python + Scikit-learn\n# å¯¼å…¥åº“\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\n\n# å‡†å¤‡æ•°æ®ï¼ˆå®å¯æ¢¦è¿›åŒ–å‰åCPå€¼ï¼‰\nX_train = np.array([[10], [20], [30], [40], [50]])  # è¿›åŒ–å‰\ny_train = np.array([20, 40, 60, 80, 100])           # è¿›åŒ–å\n\n# Step 1: å®šä¹‰æ¨¡å‹\nmodel = LinearRegression()\n\n# Step 2 + 3: è®­ç»ƒæ¨¡å‹ï¼ˆè‡ªåŠ¨è®¡ç®— Loss å’Œä¼˜åŒ–ï¼‰\nmodel.fit(X_train, y_train)\n\n# é¢„æµ‹\nX_test = np.array([[25], [35]])\npredictions = model.predict(X_test)\n\nprint(f\"è¿›åŒ–å‰ CP=25 â†’ é¢„æµ‹è¿›åŒ–å CP={predictions[0]:.2f}\")\nprint(f\"è¿›åŒ–å‰ CP=35 â†’ é¢„æµ‹è¿›åŒ–å CP={predictions[1]:.2f}\")\n\n# æŸ¥çœ‹å­¦åˆ°çš„å‚æ•°\nprint(f\"Weight (w) = {model.coef_[0]:.2f}\")\nprint(f\"Bias (b) = {model.intercept_:.2f}\")\nè¾“å‡ºï¼š\nè¿›åŒ–å‰ CP=25 â†’ é¢„æµ‹è¿›åŒ–å CP=50.00\nè¿›åŒ–å‰ CP=35 â†’ é¢„æµ‹è¿›åŒ–å CP=70.00\nWeight (w) = 2.00\nBias (b) = 0.00",
    "crumbs": [
      "ç¬¬ä¸€éƒ¨åˆ†ï¼šåŸºç¡€ç¯‡",
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>æœºå™¨å­¦ä¹ å®Œæ•´è¯¾ç¨‹</span>"
    ]
  },
  {
    "objectID": "Chapter1.html#æœ¬ç« ä½œä¸š",
    "href": "Chapter1.html#æœ¬ç« ä½œä¸š",
    "title": "2Â  æœºå™¨å­¦ä¹ å®Œæ•´è¯¾ç¨‹",
    "section": "3.9 ğŸ“ æœ¬ç« ä½œä¸š",
    "text": "3.9 ğŸ“ æœ¬ç« ä½œä¸š\n\n3.9.1 ä½œä¸š 1ï¼šæ¦‚å¿µç†è§£\n\nç”¨è‡ªå·±çš„è¯è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ \nä¸¾å‡º 3 ä¸ªæ—¥å¸¸ç”Ÿæ´»ä¸­çš„æœºå™¨å­¦ä¹ åº”ç”¨\nè¯´æ˜ç›‘ç£å­¦ä¹ å’Œæ— ç›‘ç£å­¦ä¹ çš„åŒºåˆ«\n\n\n\n3.9.2 ä½œä¸š 2ï¼šå®è·µç»ƒä¹ \n\nè¿è¡Œä¸Šé¢çš„ä»£ç ç¤ºä¾‹\nå°è¯•ä¿®æ”¹è®­ç»ƒæ•°æ®ï¼Œè§‚å¯Ÿç»“æœå˜åŒ–\nä½¿ç”¨ matplotlib ç”»å‡ºï¼š\n\nè®­ç»ƒæ•°æ®ç‚¹\nå­¦åˆ°çš„ functionï¼ˆç›´çº¿ï¼‰\n\n\n\n\n3.9.3 ä½œä¸š 3ï¼šæ€è€ƒé¢˜\n\nå¦‚æœæ•°æ®ä¸æ˜¯çº¿æ€§çš„ï¼ˆä¾‹å¦‚äºŒæ¬¡æ›²çº¿ï¼‰ï¼Œçº¿æ€§æ¨¡å‹ä¼šæ€æ ·ï¼Ÿ\nå¦‚æœè®­ç»ƒæ•°æ®æœ‰å™ªå£°ï¼ˆæ ‡æ³¨é”™è¯¯ï¼‰ï¼Œä¼šå½±å“æ¨¡å‹å—ï¼Ÿ\nä»€ä¹ˆæƒ…å†µä¸‹åº”è¯¥ç”¨æœºå™¨å­¦ä¹ ï¼Œä»€ä¹ˆæƒ…å†µä¸‹ä¸åº”è¯¥ç”¨ï¼Ÿ",
    "crumbs": [
      "ç¬¬ä¸€éƒ¨åˆ†ï¼šåŸºç¡€ç¯‡",
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>æœºå™¨å­¦ä¹ å®Œæ•´è¯¾ç¨‹</span>"
    ]
  },
  {
    "objectID": "Chapter1.html#æœ¬ç« å…³é”®æ¦‚å¿µ",
    "href": "Chapter1.html#æœ¬ç« å…³é”®æ¦‚å¿µ",
    "title": "2Â  æœºå™¨å­¦ä¹ å®Œæ•´è¯¾ç¨‹",
    "section": "3.10 ğŸ”‘ æœ¬ç« å…³é”®æ¦‚å¿µ",
    "text": "3.10 ğŸ”‘ æœ¬ç« å…³é”®æ¦‚å¿µ\n\n\n\næ¦‚å¿µ\nè‹±æ–‡\nè¯´æ˜\n\n\n\n\næœºå™¨å­¦ä¹ \nMachine Learning\nè®©æœºå™¨ä»æ•°æ®ä¸­å­¦ä¹ \n\n\næ¨¡å‹\nModel\nä¸€ç»„å¯èƒ½çš„ functions\n\n\nç‰¹å¾\nFeature\nè¾“å…¥æ•°æ®çš„å±æ€§\n\n\næ ‡ç­¾\nLabel\nç›‘ç£å­¦ä¹ ä¸­çš„â€ç­”æ¡ˆâ€\n\n\næŸå¤±å‡½æ•°\nLoss Function\nè¡¡é‡é¢„æµ‹å’ŒçœŸå®çš„å·®è·\n\n\næ¢¯åº¦ä¸‹é™\nGradient Descent\næ‰¾æœ€ä½³å‚æ•°çš„æ–¹æ³•\n\n\nè¿‡æ‹Ÿåˆ\nOverfitting\nå¤ªæ‹Ÿåˆè®­ç»ƒæ•°æ®\n\n\næ¬ æ‹Ÿåˆ\nUnderfitting\næ¨¡å‹å¤ªç®€å•ï¼Œå­¦ä¸å¥½\n\n\næ³›åŒ–èƒ½åŠ›\nGeneralization\nåœ¨æ–°æ•°æ®ä¸Šçš„è¡¨ç°",
    "crumbs": [
      "ç¬¬ä¸€éƒ¨åˆ†ï¼šåŸºç¡€ç¯‡",
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>æœºå™¨å­¦ä¹ å®Œæ•´è¯¾ç¨‹</span>"
    ]
  },
  {
    "objectID": "Chapter1.html#å»¶ä¼¸é˜…è¯»",
    "href": "Chapter1.html#å»¶ä¼¸é˜…è¯»",
    "title": "2Â  æœºå™¨å­¦ä¹ å®Œæ•´è¯¾ç¨‹",
    "section": "3.11 ğŸ“š å»¶ä¼¸é˜…è¯»",
    "text": "3.11 ğŸ“š å»¶ä¼¸é˜…è¯»\n\nä¹¦ç±\n\nã€ŠPattern Recognition and Machine Learningã€‹- Bishop\nã€ŠDeep Learningã€‹- Goodfellow, Bengio, Courville\nã€Šæœºå™¨å­¦ä¹ ã€‹- å‘¨å¿—åï¼ˆè¥¿ç“œä¹¦ï¼‰\n\nåœ¨çº¿èµ„æº\n\nAndrew Ng çš„ Machine Learning è¯¾ç¨‹ï¼ˆCourseraï¼‰\næå®æ¯…è€å¸ˆçš„ YouTube é¢‘é“\nFast.ai è¯¾ç¨‹\n\nå®è·µå¹³å°\n\nKaggleï¼ˆæ•°æ®ç§‘å­¦ç«èµ›ï¼‰\nGoogle Colabï¼ˆå…è´¹ GPUï¼‰\nPapers with Codeï¼ˆæœ€æ–°è®ºæ–‡+ä»£ç ï¼‰",
    "crumbs": [
      "ç¬¬ä¸€éƒ¨åˆ†ï¼šåŸºç¡€ç¯‡",
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>æœºå™¨å­¦ä¹ å®Œæ•´è¯¾ç¨‹</span>"
    ]
  },
  {
    "objectID": "Chapter1.html#ä¸‹ä¸€ç« é¢„å‘Š",
    "href": "Chapter1.html#ä¸‹ä¸€ç« é¢„å‘Š",
    "title": "2Â  æœºå™¨å­¦ä¹ å®Œæ•´è¯¾ç¨‹",
    "section": "3.12 ğŸ¯ ä¸‹ä¸€ç« é¢„å‘Š",
    "text": "3.12 ğŸ¯ ä¸‹ä¸€ç« é¢„å‘Š\nç¬¬äºŒç« ï¼šå›å½’ (Regression) - ä»ç®€å•çš„çº¿æ€§å›å½’å¼€å§‹ - æ·±å…¥ç†è§£ Gradient Descent - å­¦ä¹ å¦‚ä½•è¯„ä¼°æ¨¡å‹ - å®æˆ˜ï¼šPM2.5 é¢„æµ‹",
    "crumbs": [
      "ç¬¬ä¸€éƒ¨åˆ†ï¼šåŸºç¡€ç¯‡",
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>æœºå™¨å­¦ä¹ å®Œæ•´è¯¾ç¨‹</span>"
    ]
  },
  {
    "objectID": "Chapter2.html",
    "href": "Chapter2.html",
    "title": "3Â  ç¬¬äºŒç« ï¼šå›å½’ (Regression)",
    "section": "",
    "text": "3.1 ğŸ“Œ ç« èŠ‚ç›®æ ‡",
    "crumbs": [
      "ç¬¬ä¸€éƒ¨åˆ†ï¼šåŸºç¡€ç¯‡",
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>ç¬¬äºŒç« ï¼šå›å½’ (Regression)</span>"
    ]
  },
  {
    "objectID": "Chapter2.html#ç« èŠ‚ç›®æ ‡",
    "href": "Chapter2.html#ç« èŠ‚ç›®æ ‡",
    "title": "3Â  ç¬¬äºŒç« ï¼šå›å½’ (Regression)",
    "section": "",
    "text": "æŒæ¡çº¿æ€§å›å½’çš„åŸç†å’Œå®ç°\næ·±å…¥ç†è§£æ¢¯åº¦ä¸‹é™ç®—æ³•\nå­¦ä¼šè¯„ä¼°å›å½’æ¨¡å‹çš„æ€§èƒ½\nå¤„ç†å®é™…æ•°æ®é›†çš„æŠ€å·§\näº†è§£å›å½’çš„å˜ç§å’Œæ”¹è¿›æ–¹æ³•",
    "crumbs": [
      "ç¬¬ä¸€éƒ¨åˆ†ï¼šåŸºç¡€ç¯‡",
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>ç¬¬äºŒç« ï¼šå›å½’ (Regression)</span>"
    ]
  },
  {
    "objectID": "Chapter2.html#ä»€ä¹ˆæ˜¯å›å½’",
    "href": "Chapter2.html#ä»€ä¹ˆæ˜¯å›å½’",
    "title": "3Â  ç¬¬äºŒç« ï¼šå›å½’ (Regression)",
    "section": "3.2 2.1 ä»€ä¹ˆæ˜¯å›å½’ï¼Ÿ",
    "text": "3.2 2.1 ä»€ä¹ˆæ˜¯å›å½’ï¼Ÿ\n\n3.2.1 ğŸ¯ å®šä¹‰\n\nRegressionï¼ˆå›å½’ï¼‰ï¼šé¢„æµ‹ä¸€ä¸ªè¿ç»­çš„æ•°å€¼\n\nä¾‹å­ï¼š - âœ… é¢„æµ‹æˆ¿ä»·ï¼š$350,000 - âœ… é¢„æµ‹æ¸©åº¦ï¼š25.3Â°C - âœ… é¢„æµ‹è‚¡ç¥¨ä»·æ ¼ï¼š$152.50 - âŒ é¢„æµ‹ç±»åˆ«ï¼šçŒ«/ç‹—ï¼ˆè¿™æ˜¯åˆ†ç±»é—®é¢˜ï¼‰\n\n\n3.2.2 ğŸ“Š å›å½’ vs åˆ†ç±»\nå›å½’ (Regression):\n  è¾“å‡º = è¿ç»­æ•°å€¼\n  ä¾‹ï¼š0, 0.5, 1.23, 100, -5.7, ...\n\nåˆ†ç±» (Classification):\n  è¾“å‡º = ç¦»æ•£ç±»åˆ«\n  ä¾‹ï¼šçŒ«, ç‹—, çŒª / æ˜¯, å¦ / Class 1, 2, 3",
    "crumbs": [
      "ç¬¬ä¸€éƒ¨åˆ†ï¼šåŸºç¡€ç¯‡",
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>ç¬¬äºŒç« ï¼šå›å½’ (Regression)</span>"
    ]
  },
  {
    "objectID": "Chapter2.html#æ¡ˆä¾‹å®å¯æ¢¦-cp-å€¼é¢„æµ‹",
    "href": "Chapter2.html#æ¡ˆä¾‹å®å¯æ¢¦-cp-å€¼é¢„æµ‹",
    "title": "3Â  ç¬¬äºŒç« ï¼šå›å½’ (Regression)",
    "section": "3.3 2.2 æ¡ˆä¾‹ï¼šå®å¯æ¢¦ CP å€¼é¢„æµ‹",
    "text": "3.3 2.2 æ¡ˆä¾‹ï¼šå®å¯æ¢¦ CP å€¼é¢„æµ‹\n\n3.3.1 ğŸ® é—®é¢˜è®¾å®š\nèƒŒæ™¯ï¼šPokemon GO æ¸¸æˆä¸­ï¼Œå®å¯æ¢¦è¿›åŒ–åçš„ CP å€¼ï¼ˆæˆ˜æ–—åŠ›ï¼‰æ˜¯å¤šå°‘ï¼Ÿ\næ•°æ®ï¼š | è¿›åŒ–å‰ CP (x) | è¿›åŒ–å CP (y) | |â€”â€”â€”â€”â€“|â€”â€”â€”â€”â€“| | 10 | 28 | | 20 | 55 | | 30 | 82 | | 40 | 109 | | 50 | 136 | | â€¦ | â€¦ |\nç›®æ ‡ï¼šç»™å®šè¿›åŒ–å‰çš„ CP å€¼ï¼Œé¢„æµ‹è¿›åŒ–åçš„ CP å€¼",
    "crumbs": [
      "ç¬¬ä¸€éƒ¨åˆ†ï¼šåŸºç¡€ç¯‡",
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>ç¬¬äºŒç« ï¼šå›å½’ (Regression)</span>"
    ]
  },
  {
    "objectID": "Chapter2.html#step-1-å®šä¹‰æ¨¡å‹-model",
    "href": "Chapter2.html#step-1-å®šä¹‰æ¨¡å‹-model",
    "title": "3Â  ç¬¬äºŒç« ï¼šå›å½’ (Regression)",
    "section": "3.4 2.3 Step 1: å®šä¹‰æ¨¡å‹ (Model)",
    "text": "3.4 2.3 Step 1: å®šä¹‰æ¨¡å‹ (Model)\n\n3.4.1 ğŸ”¹ Linear Modelï¼ˆçº¿æ€§æ¨¡å‹ï¼‰\næœ€ç®€å•çš„æ¨¡å‹ï¼šä¸€æ¡ç›´çº¿\ny = b + wÂ·x\n\nx: è¾“å…¥ç‰¹å¾ï¼ˆè¿›åŒ–å‰ CPï¼‰\ny: è¾“å‡ºé¢„æµ‹ï¼ˆè¿›åŒ–å CPï¼‰\nw: æƒé‡ (weight)\nb: åå·® (bias)\n\n\n\n3.4.2 ğŸ“ˆ å¯è§†åŒ–\ny (è¿›åŒ–åCP)\nâ†‘\n|         â—\n|       â—   y = b + wÂ·x\n|     â—\n|   â—\n| â—\n|____________â†’ x (è¿›åŒ–å‰CP)\nä¸åŒçš„ w å’Œ b ä¼šäº§ç”Ÿä¸åŒçš„ç›´çº¿ï¼š\nw = 2, b = 10:  y = 10 + 2x  (é™¡å³­)\nw = 3, b = 0:   y = 3x       (æ›´é™¡)\nw = 1, b = 20:  y = 20 + x   (å¹³ç¼“ï¼Œèµ·ç‚¹é«˜)\n\n\n3.4.3 ğŸ”¸ æ›´å¤æ‚çš„æ¨¡å‹\næœ‰æ—¶å€™æ•°æ®ä¸æ˜¯ç›´çº¿ï¼Œå¯ä»¥ç”¨å¤šé¡¹å¼ï¼š\ny = b + wâ‚Â·x + wâ‚‚Â·xÂ²\ny = b + wâ‚Â·x + wâ‚‚Â·xÂ² + wâ‚ƒÂ·xÂ³\næ€è€ƒï¼šæ¨¡å‹è¶Šå¤æ‚è¶Šå¥½å—ï¼Ÿ",
    "crumbs": [
      "ç¬¬ä¸€éƒ¨åˆ†ï¼šåŸºç¡€ç¯‡",
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>ç¬¬äºŒç« ï¼šå›å½’ (Regression)</span>"
    ]
  },
  {
    "objectID": "Chapter2.html#step-2-æŸå¤±å‡½æ•°-loss-function",
    "href": "Chapter2.html#step-2-æŸå¤±å‡½æ•°-loss-function",
    "title": "3Â  ç¬¬äºŒç« ï¼šå›å½’ (Regression)",
    "section": "3.5 2.4 Step 2: æŸå¤±å‡½æ•° (Loss Function)",
    "text": "3.5 2.4 Step 2: æŸå¤±å‡½æ•° (Loss Function)\n\n3.5.1 ğŸ¯ å¦‚ä½•è¯„ä¼°ä¸€ä¸ª function çš„å¥½åï¼Ÿ\nç­”æ¡ˆï¼šçœ‹é¢„æµ‹å€¼å’ŒçœŸå®å€¼çš„å·®è·ï¼\n\n\n3.5.2 ğŸ“ Mean Squared Error (MSE)\næœ€å¸¸ç”¨çš„æŸå¤±å‡½æ•°ï¼š\nL(w, b) = (1/N) Î£(Å·â¿ - yâ¿)Â²\n\nå…¶ä¸­ï¼š\n- N: è®­ç»ƒæ•°æ®æ•°é‡\n- yâ¿: ç¬¬ n ç¬”æ•°æ®çš„çœŸå®å€¼\n- Å·â¿ = b + wÂ·xâ¿: ç¬¬ n ç¬”æ•°æ®çš„é¢„æµ‹å€¼\n\n\n3.5.3 ğŸ“Š ä¾‹å­è®¡ç®—\nå‡è®¾ w=2, b=10ï¼Œæœ‰ 3 ç¬”æ•°æ®ï¼š\n\n\n\nx\ny (çœŸå®)\nÅ· = 10+2x (é¢„æµ‹)\nè¯¯å·® (Å·-y)\nå¹³æ–¹è¯¯å·®\n\n\n\n\n10\n28\n30\n2\n4\n\n\n20\n55\n50\n-5\n25\n\n\n30\n82\n70\n-12\n144\n\n\n\nLoss = (4 + 25 + 144) / 3 = 57.67\n\n\n3.5.4 ğŸ—ºï¸ Loss Function æ˜¯ä¸€ä¸ªåœ°å½¢å›¾\næŠŠ Loss çœ‹æˆ w å’Œ b çš„å‡½æ•°ï¼šL(w, b)\nLoss\n â†‘\n |      å±±å³°\n |    /    \\\n |   /  â—   \\    â— æŸç»„å‚æ•°çš„ Loss\n |  /        \\\n | /    â˜…     \\  â˜… æœ€ä½³å‚æ•°ï¼ˆLoss æœ€å°ï¼‰\n |/_____è°·åº•___\\\n   w, b çš„ç»„åˆ\nç›®æ ‡ï¼šæ‰¾åˆ°è®© Loss æœ€å°çš„ w å’Œ b",
    "crumbs": [
      "ç¬¬ä¸€éƒ¨åˆ†ï¼šåŸºç¡€ç¯‡",
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>ç¬¬äºŒç« ï¼šå›å½’ (Regression)</span>"
    ]
  },
  {
    "objectID": "Chapter2.html#step-3-æ¢¯åº¦ä¸‹é™-gradient-descent",
    "href": "Chapter2.html#step-3-æ¢¯åº¦ä¸‹é™-gradient-descent",
    "title": "3Â  ç¬¬äºŒç« ï¼šå›å½’ (Regression)",
    "section": "3.6 2.5 Step 3: æ¢¯åº¦ä¸‹é™ (Gradient Descent)",
    "text": "3.6 2.5 Step 3: æ¢¯åº¦ä¸‹é™ (Gradient Descent)\n\n3.6.1 ğŸ”ï¸ ç›´è§‚ç†è§£ï¼šç›²äººä¸‹å±±\næƒ³è±¡ä½ æ˜¯ç›²äººï¼Œç«™åœ¨å±±ä¸Šï¼Œæƒ³æ‰¾åˆ°æœ€ä½ç‚¹ï¼š\n\nç«™åœ¨æŸä¸ªä½ç½®ï¼ˆåˆå§‹å‚æ•° wâ°, bâ°ï¼‰\næ„Ÿå—å‘¨å›´çš„å¡åº¦ï¼ˆè®¡ç®—æ¢¯åº¦ âˆ‚L/âˆ‚w, âˆ‚L/âˆ‚bï¼‰\nå¾€ä¸‹å¡æ–¹å‘èµ°ä¸€å°æ­¥ï¼ˆæ›´æ–°å‚æ•°ï¼‰\né‡å¤ 2-3 æ­¥ï¼Œç›´åˆ°åˆ°è¾¾è°·åº•\n\n\n\n3.6.2 ğŸ“ æ•°å­¦å…¬å¼\né‡å¤ä»¥ä¸‹æ­¥éª¤ï¼Œç›´åˆ°æ”¶æ•›ï¼š\n\nw â† w - Î· Â· âˆ‚L/âˆ‚w\nb â† b - Î· Â· âˆ‚L/âˆ‚b\nç¬¦å·è¯´æ˜ï¼š - Î· (eta): Learning Rateï¼ˆå­¦ä¹ ç‡ï¼‰ - æ§åˆ¶æ¯æ¬¡æ›´æ–°çš„æ­¥é•¿ - å¤ªå¤§ï¼šå¯èƒ½è·³è¿‡æœ€ä½ç‚¹ï¼Œç”šè‡³å‘æ•£ - å¤ªå°ï¼šæ”¶æ•›å¤ªæ…¢ - å…¸å‹å€¼ï¼š0.001, 0.01, 0.1\n\nâˆ‚L/âˆ‚w: Loss å¯¹ w çš„åå¯¼æ•°ï¼ˆæ¢¯åº¦ï¼‰\n\nè¡¨ç¤º w å¢åŠ ä¸€ç‚¹ï¼ŒLoss ä¼šå¢åŠ /å‡å°‘å¤šå°‘\næ­£å€¼ï¼šw å¢åŠ  â†’ Loss å¢åŠ  â†’ åº”è¯¥å‡å° w\nè´Ÿå€¼ï¼šw å¢åŠ  â†’ Loss å‡å°‘ â†’ åº”è¯¥å¢åŠ  w\n\n\n\n\n3.6.3 ğŸ§® æ¨å¯¼æ¢¯åº¦å…¬å¼\nå¯¹äºçº¿æ€§æ¨¡å‹ y = b + wÂ·xï¼ŒLoss ä¸ºï¼š\nL(w, b) = (1/N) Î£(b + wÂ·xâ¿ - yâ¿)Â²\nè®¡ç®—åå¯¼æ•°ï¼š\nâˆ‚L/âˆ‚w = (2/N) Î£(b + wÂ·xâ¿ - yâ¿)Â·xâ¿\nâˆ‚L/âˆ‚b = (2/N) Î£(b + wÂ·xâ¿ - yâ¿)\n\n\n3.6.4 ğŸ’» Python å®ç°\nimport numpy as np\n\n# è®­ç»ƒæ•°æ®\nX = np.array([10, 20, 30, 40, 50])\ny = np.array([28, 55, 82, 109, 136])\n\n# åˆå§‹åŒ–å‚æ•°\nw = 0.0\nb = 0.0\nlearning_rate = 0.0001\nepochs = 1000\n\n# æ¢¯åº¦ä¸‹é™\nfor epoch in range(epochs):\n    # å‰å‘ä¼ æ’­ï¼šè®¡ç®—é¢„æµ‹å€¼\n    y_pred = b + w * X\n\n    # è®¡ç®— Loss\n    loss = np.mean((y_pred - y) ** 2)\n\n    # è®¡ç®—æ¢¯åº¦\n    grad_w = 2 * np.mean((y_pred - y) * X)\n    grad_b = 2 * np.mean(y_pred - y)\n\n    # æ›´æ–°å‚æ•°\n    w = w - learning_rate * grad_w\n    b = b - learning_rate * grad_b\n\n    # æ¯ 100 ä¸ª epoch æ‰“å°ä¸€æ¬¡\n    if epoch % 100 == 0:\n        print(f\"Epoch {epoch}: Loss={loss:.2f}, w={w:.2f}, b={b:.2f}\")\n\nprint(f\"\\næœ€ç»ˆå‚æ•°: w={w:.2f}, b={b:.2f}\")\nprint(f\"é¢„æµ‹ x=25: y={b + w*25:.2f}\")\nè¾“å‡ºç¤ºä¾‹ï¼š\nEpoch 0: Loss=7289.00, w=3.51, b=1.47\nEpoch 100: Loss=138.24, w=2.54, b=6.89\nEpoch 200: Loss=11.17, w=2.63, b=4.12\n...\nEpoch 900: Loss=0.24, w=2.70, b=1.20\n\næœ€ç»ˆå‚æ•°: w=2.70, b=1.00\né¢„æµ‹ x=25: y=68.50",
    "crumbs": [
      "ç¬¬ä¸€éƒ¨åˆ†ï¼šåŸºç¡€ç¯‡",
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>ç¬¬äºŒç« ï¼šå›å½’ (Regression)</span>"
    ]
  },
  {
    "objectID": "Chapter2.html#å¯è§†åŒ–æ¢¯åº¦ä¸‹é™è¿‡ç¨‹",
    "href": "Chapter2.html#å¯è§†åŒ–æ¢¯åº¦ä¸‹é™è¿‡ç¨‹",
    "title": "3Â  ç¬¬äºŒç« ï¼šå›å½’ (Regression)",
    "section": "3.7 2.6 å¯è§†åŒ–æ¢¯åº¦ä¸‹é™è¿‡ç¨‹",
    "text": "3.7 2.6 å¯è§†åŒ–æ¢¯åº¦ä¸‹é™è¿‡ç¨‹\n\n3.7.1 ğŸ“‰ Loss éšè®­ç»ƒå˜åŒ–\nLoss\n â†‘\n |  â—\n |   â—\n |     â—\n |       â—\n |         â—____\n |              â—â—â—â—\n |__________________â†’ Epoch\nè§‚å¯Ÿï¼š - å¼€å§‹æ—¶ Loss å¿«é€Ÿä¸‹é™ - åæœŸä¸‹é™å˜æ…¢ - æœ€ç»ˆè¶‹äºå¹³ç¨³ï¼ˆæ”¶æ•›ï¼‰\n\n\n3.7.2 ğŸ¯ å‚æ•°åœ¨å‚æ•°ç©ºé—´ä¸­çš„ç§»åŠ¨\nb\nâ†‘\n|     èµ·ç‚¹â—\n|        â†˜\n|         â†˜\n|          â†˜\n|           â†˜\n|            â—ç»ˆç‚¹\n|_____________â†’ w\næ¯ä¸€æ­¥éƒ½æœç€ Loss å‡å°çš„æ–¹å‘ç§»åŠ¨",
    "crumbs": [
      "ç¬¬ä¸€éƒ¨åˆ†ï¼šåŸºç¡€ç¯‡",
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>ç¬¬äºŒç« ï¼šå›å½’ (Regression)</span>"
    ]
  },
  {
    "objectID": "Chapter2.html#learning-rate-çš„å½±å“",
    "href": "Chapter2.html#learning-rate-çš„å½±å“",
    "title": "3Â  ç¬¬äºŒç« ï¼šå›å½’ (Regression)",
    "section": "3.8 2.7 Learning Rate çš„å½±å“",
    "text": "3.8 2.7 Learning Rate çš„å½±å“\n\n3.8.1 âš™ï¸ ä¸åŒå­¦ä¹ ç‡çš„è¡¨ç°\n\n3.8.1.1 Learning Rate å¤ªå°ï¼ˆÎ· = 0.00001ï¼‰\nlearning_rate = 0.00001\nepochs = 10000  # éœ€è¦æ›´å¤šè½®æ¬¡\nç°è±¡ï¼š - âœ— æ”¶æ•›éå¸¸æ…¢ - âœ— å¯èƒ½éœ€è¦è®­ç»ƒå¾ˆä¹… - âœ“ ä½†æ¯”è¾ƒç¨³å®š\nLoss\n â†‘\n |  â—\n |   â—\n |    â—\n |     â—    ä¸‹é™å¤ªæ…¢ï¼\n |      â—\n |       â—\n |________â—________â†’ Epoch\n\n\n3.8.1.2 Learning Rate é€‚ä¸­ï¼ˆÎ· = 0.0001ï¼‰\nlearning_rate = 0.0001\nepochs = 1000\nç°è±¡ï¼š - âœ“ æ”¶æ•›é€Ÿåº¦åˆé€‚ - âœ“ ç¨³å®šä¸‹é™ - âœ“ æ•ˆæœæœ€å¥½\nLoss\n â†‘\n |  â—\n |    â—\n |      â—\n |        â—___     æ°åˆ°å¥½å¤„\n |            â—â—\n |______________â†’ Epoch\n\n\n3.8.1.3 Learning Rate å¤ªå¤§ï¼ˆÎ· = 0.01ï¼‰\nlearning_rate = 0.01\nepochs = 1000\nç°è±¡ï¼š - âœ— å‚æ•°éœ‡è¡ - âœ— å¯èƒ½è·³è¿‡æœ€ä¼˜ç‚¹ - âœ— ç”šè‡³å‘æ•£ï¼ˆLoss è¶Šæ¥è¶Šå¤§ï¼‰\nLoss\n â†‘\n |    â—\n |  â—   â—\n | â—     â—    éœ‡è¡ï¼\n |â—       â—\n |_________â†’ Epoch\n\næˆ–è€…ï¼š\n\nLoss\n â†‘          â—\n |        â—\n |      â—      å‘æ•£ï¼\n |    â—\n |  â—\n |_________â†’ Epoch\n\n\n\n3.8.2 ğŸ’¡ å¦‚ä½•é€‰æ‹© Learning Rateï¼Ÿ\næ–¹æ³• 1ï¼šè¯•éªŒæ³•\n# å°è¯•ä¸åŒçš„å­¦ä¹ ç‡\nfor lr in [0.1, 0.01, 0.001, 0.0001, 0.00001]:\n    # è®­ç»ƒæ¨¡å‹ï¼Œè§‚å¯Ÿ Loss æ›²çº¿\næ–¹æ³• 2ï¼šLearning Rate Schedule\n# å¼€å§‹ç”¨å¤§çš„å­¦ä¹ ç‡ï¼Œé€æ¸å‡å°\ninitial_lr = 0.1\nfor epoch in range(epochs):\n    lr = initial_lr / (1 + epoch * 0.001)\n    # ä½¿ç”¨å½“å‰çš„ lr æ›´æ–°å‚æ•°\næ–¹æ³• 3ï¼šAdaptive Learning Rateï¼ˆåç»­ç« èŠ‚ä¼šè®²ï¼‰ - Adam - RMSprop - AdaGrad",
    "crumbs": [
      "ç¬¬ä¸€éƒ¨åˆ†ï¼šåŸºç¡€ç¯‡",
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>ç¬¬äºŒç« ï¼šå›å½’ (Regression)</span>"
    ]
  },
  {
    "objectID": "Chapter2.html#å¤šä¸ªç‰¹å¾çš„çº¿æ€§å›å½’",
    "href": "Chapter2.html#å¤šä¸ªç‰¹å¾çš„çº¿æ€§å›å½’",
    "title": "3Â  ç¬¬äºŒç« ï¼šå›å½’ (Regression)",
    "section": "3.9 2.8 å¤šä¸ªç‰¹å¾çš„çº¿æ€§å›å½’",
    "text": "3.9 2.8 å¤šä¸ªç‰¹å¾çš„çº¿æ€§å›å½’\n\n3.9.1 ğŸ”¢ ä»ä¸€ä¸ªç‰¹å¾åˆ°å¤šä¸ªç‰¹å¾\nä¹‹å‰ï¼šåªç”¨è¿›åŒ–å‰çš„ CP å€¼\ny = b + wÂ·x\nç°åœ¨ï¼šè€ƒè™‘å¤šä¸ªå› ç´  - xâ‚: è¿›åŒ–å‰ CP å€¼ - xâ‚‚: ç‰©ç§ï¼ˆç¼–å·ï¼‰ - xâ‚ƒ: è¡€é‡ (HP) - xâ‚„: é‡é‡ - â€¦\ny = b + wâ‚Â·xâ‚ + wâ‚‚Â·xâ‚‚ + wâ‚ƒÂ·xâ‚ƒ + wâ‚„Â·xâ‚„\n\n\n3.9.2 ğŸ“ å‘é‡åŒ–è¡¨ç¤º\ny = b + wâ‚Â·xâ‚ + wâ‚‚Â·xâ‚‚ + ... + wâ‚™Â·xâ‚™\n\nå†™æˆå‘é‡å½¢å¼ï¼š\ny = b + wáµ€x\n\nå…¶ä¸­ï¼š\nw = [wâ‚, wâ‚‚, ..., wâ‚™]áµ€  (æƒé‡å‘é‡)\nx = [xâ‚, xâ‚‚, ..., xâ‚™]áµ€  (ç‰¹å¾å‘é‡)\n\n\n3.9.3 ğŸ’» å®ç°\nimport numpy as np\n\n# æ•°æ®ï¼ˆæ¯è¡Œä¸€ä¸ªæ ·æœ¬ï¼Œæ¯åˆ—ä¸€ä¸ªç‰¹å¾ï¼‰\nX = np.array([\n    [10, 1, 50, 5.0],  # CP=10, ç§ç±»=1, HP=50, é‡é‡=5.0\n    [20, 1, 60, 5.5],\n    [30, 2, 70, 6.0],\n    [40, 2, 80, 6.5],\n    [50, 3, 90, 7.0],\n])\ny = np.array([28, 55, 82, 109, 136])\n\n# åˆå§‹åŒ–å‚æ•°\nn_features = X.shape[1]  # ç‰¹å¾æ•°é‡\nw = np.zeros(n_features)  # [0, 0, 0, 0]\nb = 0.0\nlearning_rate = 0.00001\nepochs = 1000\n\n# æ¢¯åº¦ä¸‹é™\nfor epoch in range(epochs):\n    # é¢„æµ‹ï¼šy_pred = b + X @ w\n    y_pred = b + np.dot(X, w)\n\n    # Loss\n    loss = np.mean((y_pred - y) ** 2)\n\n    # æ¢¯åº¦\n    grad_w = 2 * np.dot(X.T, (y_pred - y)) / len(y)\n    grad_b = 2 * np.mean(y_pred - y)\n\n    # æ›´æ–°\n    w = w - learning_rate * grad_w\n    b = b - learning_rate * grad_b\n\n    if epoch % 200 == 0:\n        print(f\"Epoch {epoch}: Loss={loss:.2f}\")\n\nprint(f\"\\næƒé‡: {w}\")\nprint(f\"åå·®: {b:.2f}\")",
    "crumbs": [
      "ç¬¬ä¸€éƒ¨åˆ†ï¼šåŸºç¡€ç¯‡",
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>ç¬¬äºŒç« ï¼šå›å½’ (Regression)</span>"
    ]
  },
  {
    "objectID": "Chapter2.html#è¯„ä¼°å›å½’æ¨¡å‹",
    "href": "Chapter2.html#è¯„ä¼°å›å½’æ¨¡å‹",
    "title": "3Â  ç¬¬äºŒç« ï¼šå›å½’ (Regression)",
    "section": "3.10 2.9 è¯„ä¼°å›å½’æ¨¡å‹",
    "text": "3.10 2.9 è¯„ä¼°å›å½’æ¨¡å‹\n\n3.10.1 ğŸ“Š è¯„ä¼°æŒ‡æ ‡\n\n3.10.1.1 1. Mean Squared Error (MSE)\nMSE = (1/N) Î£(Å·â¿ - yâ¿)Â²\n\nå¹³å‡å¹³æ–¹è¯¯å·®\nå•ä½æ˜¯åŸå§‹å•ä½çš„å¹³æ–¹\nå¯¹å¼‚å¸¸å€¼æ•æ„Ÿï¼ˆå› ä¸ºå¹³æ–¹ï¼‰\n\n\n\n3.10.1.2 2. Root Mean Squared Error (RMSE)\nRMSE = âˆšMSE = âˆš[(1/N) Î£(Å·â¿ - yâ¿)Â²]\n\nå¯¹ MSE å¼€æ ¹å·\nå•ä½å’ŒåŸå§‹æ•°æ®ç›¸åŒ\næ›´ç›´è§‚\n\n\n\n3.10.1.3 3. Mean Absolute Error (MAE)\nMAE = (1/N) Î£|Å·â¿ - yâ¿|\n\nå¹³å‡ç»å¯¹è¯¯å·®\nå¯¹å¼‚å¸¸å€¼ä¸æ•æ„Ÿ\nä¹Ÿå¾ˆç›´è§‚\n\n\n\n3.10.1.4 4. RÂ² Score (å†³å®šç³»æ•°)\nRÂ² = 1 - (SS_res / SS_tot)\n\nSS_res = Î£(yâ¿ - Å·â¿)Â²  (æ®‹å·®å¹³æ–¹å’Œ)\nSS_tot = Î£(yâ¿ - È³)Â²   (æ€»å¹³æ–¹å’Œï¼ŒÈ³æ˜¯å¹³å‡å€¼)\n\nèŒƒå›´ï¼š-âˆ åˆ° 1\nRÂ² = 1: å®Œç¾é¢„æµ‹\nRÂ² = 0: å’Œé¢„æµ‹å¹³å‡å€¼ä¸€æ ·å¥½\nRÂ² &lt; 0: æ¯”é¢„æµ‹å¹³å‡å€¼è¿˜å·®\n\n\n\n\n3.10.2 ğŸ’» è®¡ç®—è¯„ä¼°æŒ‡æ ‡\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nimport numpy as np\n\n# çœŸå®å€¼å’Œé¢„æµ‹å€¼\ny_true = np.array([28, 55, 82, 109, 136])\ny_pred = np.array([30, 52, 80, 110, 135])\n\n# è®¡ç®—æŒ‡æ ‡\nmse = mean_squared_error(y_true, y_pred)\nrmse = np.sqrt(mse)\nmae = mean_absolute_error(y_true, y_pred)\nr2 = r2_score(y_true, y_pred)\n\nprint(f\"MSE:  {mse:.2f}\")\nprint(f\"RMSE: {rmse:.2f}\")\nprint(f\"MAE:  {mae:.2f}\")\nprint(f\"RÂ²:   {r2:.4f}\")\nè¾“å‡ºï¼š\nMSE:  5.40\nRMSE: 2.32\nMAE:  1.80\nRÂ²:   0.9996",
    "crumbs": [
      "ç¬¬ä¸€éƒ¨åˆ†ï¼šåŸºç¡€ç¯‡",
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>ç¬¬äºŒç« ï¼šå›å½’ (Regression)</span>"
    ]
  },
  {
    "objectID": "Chapter2.html#è®­ç»ƒé›†-vs-æµ‹è¯•é›†",
    "href": "Chapter2.html#è®­ç»ƒé›†-vs-æµ‹è¯•é›†",
    "title": "3Â  ç¬¬äºŒç« ï¼šå›å½’ (Regression)",
    "section": "3.11 2.10 è®­ç»ƒé›† vs æµ‹è¯•é›†",
    "text": "3.11 2.10 è®­ç»ƒé›† vs æµ‹è¯•é›†\n\n3.11.1 ğŸ¯ ä¸ºä»€ä¹ˆè¦åˆ†å¼€ï¼Ÿ\né—®é¢˜ï¼šå¦‚æœåªåœ¨è®­ç»ƒæ•°æ®ä¸Šè¯„ä¼°ï¼Œå¯èƒ½äº§ç”Ÿè¿‡æ‹Ÿåˆï¼\nå­¦ç”ŸAï¼šè€ƒè¯•å‰çœ‹è¿‡æ‰€æœ‰é¢˜ç›®å’Œç­”æ¡ˆ\n      â†’ è€ƒè¯•100åˆ†\n      â†’ ä½†çœŸçš„ç†è§£äº†å—ï¼Ÿ\n\nå­¦ç”ŸBï¼šè®¤çœŸå­¦ä¹ æ¦‚å¿µå’Œæ–¹æ³•\n      â†’ è€ƒè¯•95åˆ†\n      â†’ ä½†èƒ½è§£å†³æ–°é—®é¢˜\n\n\n3.11.2 ğŸ“‚ æ•°æ®åˆ†å‰²\nåŸå§‹æ•°æ® (100%)\n    â†“\n    â”œâ”€ è®­ç»ƒé›† (Training Set) - 80%\n    â”‚    ç”¨æ¥è®­ç»ƒæ¨¡å‹ï¼ˆå­¦ä¹ å‚æ•°ï¼‰\n    â”‚\n    â””â”€ æµ‹è¯•é›† (Test Set) - 20%\n         ç”¨æ¥è¯„ä¼°æ¨¡å‹ï¼ˆæ¨¡æ‹ŸçœŸå®åœºæ™¯ï¼‰\né‡è¦ï¼šæµ‹è¯•é›†åœ¨è®­ç»ƒæ—¶å®Œå…¨ä¸èƒ½çœ‹ï¼\n\n\n3.11.3 ğŸ’» å®ç°\nfrom sklearn.model_selection import train_test_split\n\n# åˆ†å‰²æ•°æ®\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y,\n    test_size=0.2,      # 20% ä½œä¸ºæµ‹è¯•é›†\n    random_state=42     # å›ºå®šéšæœºç§å­ï¼Œä¿è¯å¯é‡å¤\n)\n\nprint(f\"è®­ç»ƒé›†å¤§å°: {len(X_train)}\")\nprint(f\"æµ‹è¯•é›†å¤§å°: {len(X_test)}\")\n\n# åªåœ¨è®­ç»ƒé›†ä¸Šè®­ç»ƒ\nmodel.fit(X_train, y_train)\n\n# åœ¨ä¸¤ä¸ªé›†åˆä¸Šåˆ†åˆ«è¯„ä¼°\ntrain_score = model.score(X_train, y_train)\ntest_score = model.score(X_test, y_test)\n\nprint(f\"è®­ç»ƒé›† RÂ²: {train_score:.4f}\")\nprint(f\"æµ‹è¯•é›† RÂ²: {test_score:.4f}\")\n\n\n3.11.4 ğŸš¨ å¸¸è§æƒ…å†µåˆ†æ\n\n\n\nè®­ç»ƒé›†è¡¨ç°\næµ‹è¯•é›†è¡¨ç°\nè¯Šæ–­\nè§£å†³æ–¹æ¡ˆ\n\n\n\n\nå¥½\nå¥½\nâœ“ æ­£å¸¸\nç»§ç»­ä¼˜åŒ–\n\n\nå·®\nå·®\nUnderfitting\næ›´å¤æ‚æ¨¡å‹\n\n\nå¥½\nå·®\nOverfitting\næ­£åˆ™åŒ–/æ›´å¤šæ•°æ®\n\n\nå·®\nå¥½\nç½•è§ï¼Œæ•°æ®é—®é¢˜\næ£€æŸ¥æ•°æ®åˆ†å‰²",
    "crumbs": [
      "ç¬¬ä¸€éƒ¨åˆ†ï¼šåŸºç¡€ç¯‡",
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>ç¬¬äºŒç« ï¼šå›å½’ (Regression)</span>"
    ]
  },
  {
    "objectID": "Chapter2.html#æ­£åˆ™åŒ–-regularization",
    "href": "Chapter2.html#æ­£åˆ™åŒ–-regularization",
    "title": "3Â  ç¬¬äºŒç« ï¼šå›å½’ (Regression)",
    "section": "3.12 2.11 æ­£åˆ™åŒ– (Regularization)",
    "text": "3.12 2.11 æ­£åˆ™åŒ– (Regularization)\n\n3.12.1 ğŸ¯ ä»€ä¹ˆæ˜¯æ­£åˆ™åŒ–ï¼Ÿ\né—®é¢˜ï¼šæ¨¡å‹å¤ªå¤æ‚ï¼Œè¿‡æ‹Ÿåˆè®­ç»ƒæ•°æ®\ny = b + wâ‚x + wâ‚‚xÂ² + wâ‚ƒxÂ³ + ... + wâ‚â‚€â‚€xÂ¹â°â°\nè¿™ä¸ªæ¨¡å‹æœ‰ 100 ä¸ªå‚æ•°ï¼Œå¯ä»¥å®Œç¾æ‹Ÿåˆè®­ç»ƒæ•°æ®ï¼Œä½†åœ¨æ–°æ•°æ®ä¸Šè¡¨ç°å¾ˆå·®ï¼\nè§£å†³ï¼šæƒ©ç½šè¿‡å¤§çš„å‚æ•°ï¼Œè®©æ¨¡å‹æ›´â€å¹³æ»‘â€\n\n\n3.12.2 ğŸ“ L2 Regularization (Ridge)\nä¿®æ”¹ Loss Functionï¼š\nL(w, b) = (1/N) Î£(Å·â¿ - yâ¿)Â² + Î»Â·Î£wáµ¢Â²\n          \\_____________/   \\______/\n           åŸå§‹ Loss      æ­£åˆ™åŒ–é¡¹\n\nÎ» (lambda): æ­£åˆ™åŒ–å¼ºåº¦\n\nÎ» = 0: æ²¡æœ‰æ­£åˆ™åŒ–\nÎ» å¾ˆå¤§: å¼ºçƒˆæƒ©ç½šå¤§å‚æ•°\nå…¸å‹å€¼: 0.01, 0.1, 1, 10\n\n\næ•ˆæœï¼šå‚æ•°å€¾å‘äºå˜å°ï¼Œæ¨¡å‹æ›´ç®€å•\n\n\n3.12.3 ğŸ“ L1 Regularization (Lasso)\nL(w, b) = (1/N) Î£(Å·â¿ - yâ¿)Â² + Î»Â·Î£|wáµ¢|\næ•ˆæœï¼šä¸€äº›å‚æ•°ä¼šå˜æˆ 0ï¼ˆç‰¹å¾é€‰æ‹©ï¼‰\n\n\n3.12.4 ğŸ’» ä½¿ç”¨ Scikit-learn\nfrom sklearn.linear_model import Ridge, Lasso\n\n# Ridge Regression (L2)\nridge_model = Ridge(alpha=1.0)  # alpha å°±æ˜¯ Î»\nridge_model.fit(X_train, y_train)\n\n# Lasso Regression (L1)\nlasso_model = Lasso(alpha=1.0)\nlasso_model.fit(X_train, y_train)\n\n# æ¯”è¾ƒå‚æ•°\nprint(\"Ridge æƒé‡:\", ridge_model.coef_)\nprint(\"Lasso æƒé‡:\", lasso_model.coef_)\n# Lasso çš„ä¸€äº›æƒé‡å¯èƒ½æ˜¯ 0",
    "crumbs": [
      "ç¬¬ä¸€éƒ¨åˆ†ï¼šåŸºç¡€ç¯‡",
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>ç¬¬äºŒç« ï¼šå›å½’ (Regression)</span>"
    ]
  },
  {
    "objectID": "Chapter2.html#ç‰¹å¾å·¥ç¨‹-feature-engineering",
    "href": "Chapter2.html#ç‰¹å¾å·¥ç¨‹-feature-engineering",
    "title": "3Â  ç¬¬äºŒç« ï¼šå›å½’ (Regression)",
    "section": "3.13 2.12 ç‰¹å¾å·¥ç¨‹ (Feature Engineering)",
    "text": "3.13 2.12 ç‰¹å¾å·¥ç¨‹ (Feature Engineering)\n\n3.13.1 ğŸ› ï¸ ä»€ä¹ˆæ˜¯ç‰¹å¾å·¥ç¨‹ï¼Ÿ\n\nè®¾è®¡å’Œé€‰æ‹©å¥½çš„ç‰¹å¾ï¼Œæ¯”é€‰æ‹©ç®—æ³•æ›´é‡è¦ï¼\n\nåƒåœ¾è¿›ï¼Œåƒåœ¾å‡º (Garbage In, Garbage Out)\n\n\n3.13.2 ğŸ”¹ å¸¸è§æŠ€å·§\n\n3.13.2.1 1. ç‰¹å¾ç¼©æ”¾ (Feature Scaling)\né—®é¢˜ï¼šä¸åŒç‰¹å¾çš„å°ºåº¦å·®å¼‚å¾ˆå¤§\nç‰¹å¾1ï¼šæˆ¿å±‹é¢ç§¯ (50-200 mÂ²)\nç‰¹å¾2ï¼šæˆ¿é—´æ•°é‡ (1-5 ä¸ª)\nç‰¹å¾3ï¼šè·ç¦»å¸‚ä¸­å¿ƒ (0-50 km)\nç‰¹å¾4ï¼šå»ºé€ å¹´ä»½ (1950-2023)\næ–¹æ³•ï¼š\næ ‡å‡†åŒ– (Standardization)\nx' = (x - Î¼) / Ïƒ\n\nÎ¼: å¹³å‡å€¼\nÏƒ: æ ‡å‡†å·®\nç»“æœï¼šå¹³å‡å€¼=0ï¼Œæ ‡å‡†å·®=1\nå½’ä¸€åŒ– (Normalization)\nx' = (x - min) / (max - min)\n\nç»“æœï¼šèŒƒå›´ [0, 1]\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\n# æ ‡å‡†åŒ–\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X_train)\n\n# å½’ä¸€åŒ–\nnormalizer = MinMaxScaler()\nX_normalized = normalizer.fit_transform(X_train)\n\n\n3.13.2.2 2. å¤šé¡¹å¼ç‰¹å¾ (Polynomial Features)\næŠŠçº¿æ€§æ¨¡å‹å˜æˆéçº¿æ€§ï¼š\nåŸå§‹ç‰¹å¾: [xâ‚, xâ‚‚]\nâ†“\nå¤šé¡¹å¼ç‰¹å¾: [1, xâ‚, xâ‚‚, xâ‚Â², xâ‚xâ‚‚, xâ‚‚Â²]\nfrom sklearn.preprocessing import PolynomialFeatures\n\npoly = PolynomialFeatures(degree=2)\nX_poly = poly.fit_transform(X)\n\n# ä¾‹å­\n# è¾“å…¥: [[2, 3]]\n# è¾“å‡º: [[1, 2, 3, 4, 6, 9]]\n#        1  xâ‚ xâ‚‚ xâ‚Â² xâ‚xâ‚‚ xâ‚‚Â²\n\n\n3.13.2.3 3. ç±»åˆ«ç‰¹å¾ç¼–ç \né—®é¢˜ï¼šç‰¹å¾æ˜¯ç±»åˆ«ï¼Œä¸æ˜¯æ•°å­—\nåŸå¸‚: ['åŒ—äº¬', 'ä¸Šæµ·', 'æ·±åœ³']\næ–¹æ³• 1ï¼šLabel Encoding\nåŒ—äº¬ â†’ 0\nä¸Šæµ· â†’ 1\næ·±åœ³ â†’ 2\né—®é¢˜ï¼šæš—ç¤ºäº†é¡ºåºå…³ç³»ï¼ˆæ·±åœ³ &gt; ä¸Šæµ· &gt; åŒ—äº¬ï¼Ÿï¼‰\næ–¹æ³• 2ï¼šOne-Hot Encoding\nåŒ—äº¬ â†’ [1, 0, 0]\nä¸Šæµ· â†’ [0, 1, 0]\næ·±åœ³ â†’ [0, 0, 1]\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nimport pandas as pd\n\n# One-Hot Encoding\ndf = pd.DataFrame({'city': ['åŒ—äº¬', 'ä¸Šæµ·', 'æ·±åœ³', 'åŒ—äº¬']})\ndf_encoded = pd.get_dummies(df, columns=['city'])\nprint(df_encoded)\nè¾“å‡ºï¼š\n   city_åŒ—äº¬  city_ä¸Šæµ·  city_æ·±åœ³\n0         1         0         0\n1         0         1         0\n2         0         0         1\n3         1         0         0\n\n\n3.13.2.4 4. ç‰¹å¾äº¤å‰ (Feature Crossing)\nåˆ›å»ºç‰¹å¾ä¹‹é—´çš„äº¤äº’é¡¹ï¼š\nç‰¹å¾ï¼šé¢ç§¯(xâ‚), æˆ¿é—´æ•°(xâ‚‚)\n\næ–°ç‰¹å¾ï¼šæ¯ä¸ªæˆ¿é—´çš„å¹³å‡é¢ç§¯ = xâ‚ / xâ‚‚\n\n\n3.13.2.5 5. å¤„ç†ç¼ºå¤±å€¼\nfrom sklearn.impute import SimpleImputer\n\n# ç”¨å¹³å‡å€¼å¡«å……\nimputer = SimpleImputer(strategy='mean')\nX_filled = imputer.fit_transform(X)\n\n# å…¶ä»–ç­–ç•¥: 'median', 'most_frequent', 'constant'",
    "crumbs": [
      "ç¬¬ä¸€éƒ¨åˆ†ï¼šåŸºç¡€ç¯‡",
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>ç¬¬äºŒç« ï¼šå›å½’ (Regression)</span>"
    ]
  },
  {
    "objectID": "Chapter2.html#å®æˆ˜pm2.5-é¢„æµ‹",
    "href": "Chapter2.html#å®æˆ˜pm2.5-é¢„æµ‹",
    "title": "3Â  ç¬¬äºŒç« ï¼šå›å½’ (Regression)",
    "section": "3.14 2.13 å®æˆ˜ï¼šPM2.5 é¢„æµ‹",
    "text": "3.14 2.13 å®æˆ˜ï¼šPM2.5 é¢„æµ‹\n\n3.14.1 ğŸ“‹ é—®é¢˜æè¿°\næ•°æ®ï¼šä¸°åŸæ°”è±¡ç«™çš„è§‚æµ‹æ•°æ® - 18 ä¸ªç‰¹å¾ï¼šPM2.5, PM10, æ¸©åº¦, æ¹¿åº¦, é£é€Ÿâ€¦ - æ¯å°æ—¶è®°å½•ä¸€æ¬¡ - ç›®æ ‡ï¼šæ ¹æ®å‰ 9 å°æ—¶çš„æ•°æ®ï¼Œé¢„æµ‹ç¬¬ 10 å°æ—¶çš„ PM2.5\n\n\n3.14.2 ğŸ“‚ æ•°æ®æ ¼å¼\næ—¥æœŸ        AMB_TEMP  CH4    CO    ...  PM2.5\n2017-01  0     14      1.8   0.37       35\n2017-01  1     14      1.8   0.37       26\n2017-01  2     14      1.8   0.36       18\n...\n\n\n3.14.3 ğŸ’» å®Œæ•´ä»£ç \nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nimport matplotlib.pyplot as plt\n\n# 1. è¯»å–æ•°æ®\ndata = pd.read_csv('train.csv', encoding='big5')\n\n# 2. æ•°æ®é¢„å¤„ç†\n# é€‰æ‹©ç‰¹å¾\nfeatures = ['PM2.5', 'PM10', 'SO2', 'CO', 'O3', 'TEMP', 'WIND_SPEED']\ndata = data[data['è§‚æµ‹é¡¹ç›®'].isin(features)]\n\n# è½¬æ¢ä¸ºæ•°å€¼\ndata = data.iloc[:, 3:]  # å»æ‰å‰3åˆ—ï¼ˆæ—¥æœŸã€æµ‹ç«™ã€è§‚æµ‹é¡¹ç›®ï¼‰\ndata[data == 'NR'] = 0    # NR (No Rain) æ”¹ä¸º 0\ndata = data.astype(float)\n\n# 3. æ„é€ è®­ç»ƒæ ·æœ¬\n# æ¯9å°æ—¶çš„æ•°æ®é¢„æµ‹ç¬¬10å°æ—¶çš„ PM2.5\ndef create_dataset(data, look_back=9):\n    X, y = [], []\n    for i in range(0, len(data) - look_back - 1, 18):  # æ¯18è¡Œæ˜¯ä¸€å°æ—¶çš„æ‰€æœ‰ç‰¹å¾\n        # å–9å°æ—¶çš„æ•°æ®\n        sample = []\n        for j in range(look_back):\n            sample.extend(data.iloc[i+j*18:(i+1)*18, :].values.flatten())\n        X.append(sample)\n\n        # ç¬¬10å°æ—¶çš„ PM2.5ï¼ˆå‡è®¾ PM2.5 æ˜¯ç¬¬ä¸€ä¸ªç‰¹å¾ï¼‰\n        y.append(data.iloc[i + look_back*18, 0])\n\n    return np.array(X), np.array(y)\n\nX, y = create_dataset(data)\n\n# 4. åˆ†å‰²æ•°æ®é›†\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# 5. ç‰¹å¾ç¼©æ”¾\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# 6. è®­ç»ƒæ¨¡å‹\nmodel = LinearRegression()\nmodel.fit(X_train_scaled, y_train)\n\n# 7. è¯„ä¼°\ny_train_pred = model.predict(X_train_scaled)\ny_test_pred = model.predict(X_test_scaled)\n\ntrain_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\ntest_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n\nprint(f\"è®­ç»ƒé›† RMSE: {train_rmse:.2f}\")\nprint(f\"æµ‹è¯•é›† RMSE: {test_rmse:.2f}\")\n\n# 8. å¯è§†åŒ–\nplt.figure(figsize=(10, 5))\nplt.scatter(y_test, y_test_pred, alpha=0.5)\nplt.plot([0, max(y_test)], [0, max(y_test)], 'r--', lw=2)\nplt.xlabel('çœŸå® PM2.5')\nplt.ylabel('é¢„æµ‹ PM2.5')\nplt.title('PM2.5 é¢„æµ‹ç»“æœ')\nplt.show()",
    "crumbs": [
      "ç¬¬ä¸€éƒ¨åˆ†ï¼šåŸºç¡€ç¯‡",
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>ç¬¬äºŒç« ï¼šå›å½’ (Regression)</span>"
    ]
  },
  {
    "objectID": "Chapter2.html#å›å½’çš„å±€é™æ€§",
    "href": "Chapter2.html#å›å½’çš„å±€é™æ€§",
    "title": "3Â  ç¬¬äºŒç« ï¼šå›å½’ (Regression)",
    "section": "3.15 2.14 å›å½’çš„å±€é™æ€§",
    "text": "3.15 2.14 å›å½’çš„å±€é™æ€§\n\n3.15.1 âš ï¸ ä»€ä¹ˆæ—¶å€™å›å½’ä¸é€‚ç”¨ï¼Ÿ\n\n3.15.1.1 1. è¾“å‡ºæ˜¯ç±»åˆ«\nâœ— å›å½’: é¢„æµ‹ \"çŒ«\" = 1.7ï¼Ÿ\nâœ“ åˆ†ç±»: é¢„æµ‹ \"çŒ«\" çš„æ¦‚ç‡ = 0.9\n\n\n3.15.1.2 2. æ•°æ®éçº¿æ€§ä¸”å¤æ‚\nç®€å•éçº¿æ€§ï¼šå¯ä»¥ç”¨å¤šé¡¹å¼å›å½’\nå¤æ‚éçº¿æ€§ï¼šéœ€è¦ç¥ç»ç½‘ç»œ\n\n\n3.15.1.3 3. æœ‰æ˜æ˜¾çš„å¼‚å¸¸å€¼\nå›å½’å¯¹å¼‚å¸¸å€¼å¾ˆæ•æ„Ÿï¼ˆå› ä¸ºå¹³æ–¹è¯¯å·®ï¼‰\n\n\n3.15.1.4 4. ç‰¹å¾ä¹‹é—´é«˜åº¦ç›¸å…³ï¼ˆå¤šé‡å…±çº¿æ€§ï¼‰\nç‰¹å¾1ï¼šæˆ¿å±‹é¢ç§¯ (mÂ²)\nç‰¹å¾2ï¼šæˆ¿å±‹é¢ç§¯ (ftÂ²)\nâ†’ å®Œå…¨çº¿æ€§ç›¸å…³ï¼Œæ¨¡å‹ä¸ç¨³å®š\n\n\n\n3.15.2 ğŸ’¡ è§£å†³æ–¹æ¡ˆ\n\nåˆ†ç±»é—®é¢˜ â†’ ç”¨ Logistic Regression\nå¤æ‚éçº¿æ€§ â†’ ç”¨ç¥ç»ç½‘ç»œ\nå¼‚å¸¸å€¼é—®é¢˜ â†’ ç”¨ Robust Regression æˆ–å»é™¤å¼‚å¸¸å€¼\nå¤šé‡å…±çº¿æ€§ â†’ ç”¨ Ridge/Lasso æˆ–ç§»é™¤ç›¸å…³ç‰¹å¾",
    "crumbs": [
      "ç¬¬ä¸€éƒ¨åˆ†ï¼šåŸºç¡€ç¯‡",
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>ç¬¬äºŒç« ï¼šå›å½’ (Regression)</span>"
    ]
  },
  {
    "objectID": "Chapter2.html#æœ¬ç« ä½œä¸š",
    "href": "Chapter2.html#æœ¬ç« ä½œä¸š",
    "title": "3Â  ç¬¬äºŒç« ï¼šå›å½’ (Regression)",
    "section": "3.16 ğŸ“ æœ¬ç« ä½œä¸š",
    "text": "3.16 ğŸ“ æœ¬ç« ä½œä¸š\n\n3.16.1 ä½œä¸š 1ï¼šç†è®ºé¢˜\n\nè§£é‡Šæ¢¯åº¦ä¸‹é™\n\nç”¨è‡ªå·±çš„è¯è§£é‡Šæ¢¯åº¦ä¸‹é™çš„åŸç†\nç”»å‡º Loss ä¸‹é™çš„æ›²çº¿\nè¯´æ˜å­¦ä¹ ç‡çš„ä½œç”¨\n\nè¿‡æ‹Ÿåˆ vs æ¬ æ‹Ÿåˆ\n\nä»€ä¹ˆæ˜¯è¿‡æ‹Ÿåˆï¼Ÿç»™å‡ºä¾‹å­\nå¦‚ä½•æ£€æµ‹è¿‡æ‹Ÿåˆï¼Ÿ\nåˆ—å‡º 3 ç§é˜²æ­¢è¿‡æ‹Ÿåˆçš„æ–¹æ³•\n\nç‰¹å¾å·¥ç¨‹\n\nä¸ºä»€ä¹ˆéœ€è¦ç‰¹å¾ç¼©æ”¾ï¼Ÿ\nOne-Hot Encoding å’Œ Label Encoding çš„åŒºåˆ«ï¼Ÿ\nç»™æˆ¿ä»·é¢„æµ‹é—®é¢˜è®¾è®¡ 3 ä¸ªæœ‰ç”¨çš„ç‰¹å¾\n\n\n\n\n3.16.2 ä½œä¸š 2ï¼šç¼–ç¨‹å®è·µ\nä»»åŠ¡ï¼šæ³¢å£«é¡¿æˆ¿ä»·é¢„æµ‹\nfrom sklearn.datasets import load_boston\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression, Ridge\nfrom sklearn.metrics import mean_squared_error\nimport numpy as np\n\n# 1. åŠ è½½æ•°æ®\nboston = load_boston()\nX, y = boston.data, boston.target\n\n# 2. æ•°æ®åˆ†å‰²\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# 3. è®­ç»ƒçº¿æ€§å›å½’æ¨¡å‹\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\n# 4. è¯„ä¼°\ny_pred = model.predict(X_test)\nrmse = np.sqrt(mean_squared_error(y_test, y_pred))\nprint(f\"RMSE: {rmse:.2f}\")\n\n# TODO: å®Œæˆä»¥ä¸‹ä»»åŠ¡\n# a) ç‰¹å¾ç¼©æ”¾ï¼Œè§‚å¯Ÿæ€§èƒ½å˜åŒ–\n# b) å°è¯•å¤šé¡¹å¼ç‰¹å¾ (degree=2)\n# c) ä½¿ç”¨ Ridge Regressionï¼Œè°ƒæ•´ alpha å‚æ•°\n# d) ç”»å‡ºçœŸå®å€¼ vs é¢„æµ‹å€¼çš„æ•£ç‚¹å›¾\n# e) åˆ†æå“ªäº›ç‰¹å¾æœ€é‡è¦ï¼ˆæŸ¥çœ‹ model.coef_ï¼‰\næç¤ºï¼š - ä½¿ç”¨ StandardScaler è¿›è¡Œç‰¹å¾ç¼©æ”¾ - ä½¿ç”¨ PolynomialFeatures åˆ›å»ºå¤šé¡¹å¼ç‰¹å¾ - å°è¯• alpha = [0.01, 0.1, 1, 10, 100]\n\n\n3.16.3 ä½œä¸š 3ï¼šKaggle å®æˆ˜\nå‰å¾€ Kaggleï¼Œå‚åŠ  â€œHouse Prices - Advanced Regression Techniquesâ€ ç«èµ›\nè¦æ±‚ï¼š 1. ä¸‹è½½æ•°æ®é›† 2. è¿›è¡Œ EDA (Exploratory Data Analysis) 3. ç‰¹å¾å·¥ç¨‹ 4. è®­ç»ƒæ¨¡å‹ 5. æäº¤é¢„æµ‹ç»“æœ 6. å†™ä¸€ä»½æŠ¥å‘Šè¯´æ˜ä½ çš„æ–¹æ³•",
    "crumbs": [
      "ç¬¬ä¸€éƒ¨åˆ†ï¼šåŸºç¡€ç¯‡",
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>ç¬¬äºŒç« ï¼šå›å½’ (Regression)</span>"
    ]
  },
  {
    "objectID": "Chapter2.html#æœ¬ç« å…³é”®æ¦‚å¿µæ€»ç»“",
    "href": "Chapter2.html#æœ¬ç« å…³é”®æ¦‚å¿µæ€»ç»“",
    "title": "3Â  ç¬¬äºŒç« ï¼šå›å½’ (Regression)",
    "section": "3.17 ğŸ”‘ æœ¬ç« å…³é”®æ¦‚å¿µæ€»ç»“",
    "text": "3.17 ğŸ”‘ æœ¬ç« å…³é”®æ¦‚å¿µæ€»ç»“\n\n\n\næ¦‚å¿µ\nè¯´æ˜\n\n\n\n\nå›å½’\né¢„æµ‹è¿ç»­æ•°å€¼\n\n\nçº¿æ€§å›å½’\ny = b + wÂ·x\n\n\nMSE\nå‡æ–¹è¯¯å·®ï¼Œå¸¸ç”¨æŸå¤±å‡½æ•°\n\n\næ¢¯åº¦ä¸‹é™\næ‰¾æœ€ä¼˜å‚æ•°çš„æ–¹æ³•\n\n\nå­¦ä¹ ç‡\næ§åˆ¶å‚æ•°æ›´æ–°æ­¥é•¿\n\n\nè¿‡æ‹Ÿåˆ\nè®­ç»ƒé›†å¥½ï¼Œæµ‹è¯•é›†å·®\n\n\næ¬ æ‹Ÿåˆ\nè®­ç»ƒé›†å’Œæµ‹è¯•é›†éƒ½å·®\n\n\næ­£åˆ™åŒ–\næƒ©ç½šè¿‡å¤§å‚æ•°ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ\n\n\nç‰¹å¾ç¼©æ”¾\nç»Ÿä¸€ç‰¹å¾å°ºåº¦\n\n\nç‰¹å¾å·¥ç¨‹\nè®¾è®¡å¥½çš„ç‰¹å¾",
    "crumbs": [
      "ç¬¬ä¸€éƒ¨åˆ†ï¼šåŸºç¡€ç¯‡",
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>ç¬¬äºŒç« ï¼šå›å½’ (Regression)</span>"
    ]
  },
  {
    "objectID": "Chapter2.html#ä¸‹ä¸€ç« é¢„å‘Š",
    "href": "Chapter2.html#ä¸‹ä¸€ç« é¢„å‘Š",
    "title": "3Â  ç¬¬äºŒç« ï¼šå›å½’ (Regression)",
    "section": "3.18 ğŸ¯ ä¸‹ä¸€ç« é¢„å‘Š",
    "text": "3.18 ğŸ¯ ä¸‹ä¸€ç« é¢„å‘Š\nç¬¬ä¸‰ç« ï¼šé€»è¾‘å›å½’ä¸åˆ†ç±» (Logistic Regression & Classification) - ä¸ºä»€ä¹ˆä¸èƒ½ç”¨çº¿æ€§å›å½’åšåˆ†ç±»ï¼Ÿ - Sigmoid å‡½æ•°çš„ä½œç”¨ - Cross Entropy Loss - å¤šåˆ†ç±»é—®é¢˜ (Softmax) - å®æˆ˜ï¼šæ‰‹å†™æ•°å­—è¯†åˆ« (MNIST)",
    "crumbs": [
      "ç¬¬ä¸€éƒ¨åˆ†ï¼šåŸºç¡€ç¯‡",
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>ç¬¬äºŒç« ï¼šå›å½’ (Regression)</span>"
    ]
  },
  {
    "objectID": "Chapter3.html",
    "href": "Chapter3.html",
    "title": "4Â  ç¬¬ä¸‰ç« ï¼šåˆ†ç±»ä¸é€»è¾‘å›å½’ (Classification & Logistic Regression)",
    "section": "",
    "text": "4.1 ğŸ“Œ ç« èŠ‚ç›®æ ‡",
    "crumbs": [
      "ç¬¬ä¸€éƒ¨åˆ†ï¼šåŸºç¡€ç¯‡",
      "<span class='chapter-number'>4</span>Â  <span class='chapter-title'>ç¬¬ä¸‰ç« ï¼šåˆ†ç±»ä¸é€»è¾‘å›å½’ (Classification & Logistic Regression)</span>"
    ]
  },
  {
    "objectID": "Chapter3.html#ç« èŠ‚ç›®æ ‡",
    "href": "Chapter3.html#ç« èŠ‚ç›®æ ‡",
    "title": "4Â  ç¬¬ä¸‰ç« ï¼šåˆ†ç±»ä¸é€»è¾‘å›å½’ (Classification & Logistic Regression)",
    "section": "",
    "text": "ç†è§£åˆ†ç±»é—®é¢˜ä¸å›å½’é—®é¢˜çš„åŒºåˆ«\næŒæ¡é€»è¾‘å›å½’çš„åŸç†å’Œå®ç°\nå­¦ä¹  Sigmoid å’Œ Softmax å‡½æ•°\nç†è§£äº¤å‰ç†µæŸå¤±å‡½æ•°\nå®æˆ˜ï¼šæ‰‹å†™æ•°å­—è¯†åˆ«ã€ä¿¡ç”¨å¡æ¬ºè¯ˆæ£€æµ‹",
    "crumbs": [
      "ç¬¬ä¸€éƒ¨åˆ†ï¼šåŸºç¡€ç¯‡",
      "<span class='chapter-number'>4</span>Â  <span class='chapter-title'>ç¬¬ä¸‰ç« ï¼šåˆ†ç±»ä¸é€»è¾‘å›å½’ (Classification & Logistic Regression)</span>"
    ]
  },
  {
    "objectID": "Chapter3.html#ä»€ä¹ˆæ˜¯åˆ†ç±»",
    "href": "Chapter3.html#ä»€ä¹ˆæ˜¯åˆ†ç±»",
    "title": "4Â  ç¬¬ä¸‰ç« ï¼šåˆ†ç±»ä¸é€»è¾‘å›å½’ (Classification & Logistic Regression)",
    "section": "4.2 3.1 ä»€ä¹ˆæ˜¯åˆ†ç±»ï¼Ÿ",
    "text": "4.2 3.1 ä»€ä¹ˆæ˜¯åˆ†ç±»ï¼Ÿ\n\n4.2.1 ğŸ¯ åˆ†ç±» vs å›å½’\nå›å½’ (Regression):\n  é¢„æµ‹è¿ç»­æ•°å€¼\n  ä¾‹ï¼šæˆ¿ä»· $350,000, æ¸©åº¦ 25.3Â°C\n  è¾“å‡ºï¼šå®æ•° â„\n\nåˆ†ç±» (Classification):\n  é¢„æµ‹ç¦»æ•£ç±»åˆ«\n  ä¾‹ï¼šçŒ«/ç‹—, åƒåœ¾é‚®ä»¶/æ­£å¸¸é‚®ä»¶\n  è¾“å‡ºï¼šç±»åˆ«æ ‡ç­¾\n\n\n4.2.2 ğŸ“Š åˆ†ç±»é—®é¢˜çš„ç±»å‹\n\n4.2.2.1 1. äºŒå…ƒåˆ†ç±» (Binary Classification)\nåªæœ‰ä¸¤ä¸ªç±»åˆ«\nä¾‹å­ï¼š - é‚®ä»¶ï¼šåƒåœ¾é‚®ä»¶ (1) / æ­£å¸¸é‚®ä»¶ (0) - åŒ»å­¦ï¼šæœ‰ç—… (1) / æ²¡ç—… (0) - ä¿¡ç”¨å¡ï¼šæ¬ºè¯ˆ (1) / æ­£å¸¸ (0) - å®¢æˆ·ï¼šä¼šè´­ä¹° (1) / ä¸ä¼šè´­ä¹° (0)\n\n\n4.2.2.2 2. å¤šå…ƒåˆ†ç±» (Multi-class Classification)\nå¤šä¸ªç±»åˆ«ï¼ˆä½†åªèƒ½å±äºä¸€ä¸ªï¼‰\nä¾‹å­ï¼š - æ‰‹å†™æ•°å­—è¯†åˆ«ï¼š0, 1, 2, â€¦, 9 - æ–°é—»åˆ†ç±»ï¼šä½“è‚²ã€æ”¿æ²»ã€å¨±ä¹ã€ç§‘æŠ€ - åŠ¨ç‰©åˆ†ç±»ï¼šçŒ«ã€ç‹—ã€é¸Ÿã€é±¼\n\n\n4.2.2.3 3. å¤šæ ‡ç­¾åˆ†ç±» (Multi-label Classification)\nå¯ä»¥åŒæ—¶å±äºå¤šä¸ªç±»åˆ«\nä¾‹å­ï¼š - ç”µå½±æ ‡ç­¾ï¼š[åŠ¨ä½œ, å–œå‰§, çˆ±æƒ…] - æ–‡ç« æ ‡ç­¾ï¼š[æœºå™¨å­¦ä¹ , Python, æ·±åº¦å­¦ä¹ ]",
    "crumbs": [
      "ç¬¬ä¸€éƒ¨åˆ†ï¼šåŸºç¡€ç¯‡",
      "<span class='chapter-number'>4</span>Â  <span class='chapter-title'>ç¬¬ä¸‰ç« ï¼šåˆ†ç±»ä¸é€»è¾‘å›å½’ (Classification & Logistic Regression)</span>"
    ]
  },
  {
    "objectID": "Chapter3.html#ä¸ºä»€ä¹ˆä¸èƒ½ç”¨çº¿æ€§å›å½’åšåˆ†ç±»",
    "href": "Chapter3.html#ä¸ºä»€ä¹ˆä¸èƒ½ç”¨çº¿æ€§å›å½’åšåˆ†ç±»",
    "title": "4Â  ç¬¬ä¸‰ç« ï¼šåˆ†ç±»ä¸é€»è¾‘å›å½’ (Classification & Logistic Regression)",
    "section": "4.3 3.2 ä¸ºä»€ä¹ˆä¸èƒ½ç”¨çº¿æ€§å›å½’åšåˆ†ç±»ï¼Ÿ",
    "text": "4.3 3.2 ä¸ºä»€ä¹ˆä¸èƒ½ç”¨çº¿æ€§å›å½’åšåˆ†ç±»ï¼Ÿ\n\n4.3.1 ğŸ¤” å°è¯•ç”¨å›å½’åšåˆ†ç±»\nå‡è®¾æˆ‘ä»¬è¦åˆ†ç±»ï¼šçŒ« (0) / ç‹— (1)\nè®­ç»ƒæ•°æ®ï¼š\nx (ç‰¹å¾) | y (æ ‡ç­¾)\n  1.0    |   0  (çŒ«)\n  2.0    |   0  (çŒ«)\n  3.0    |   1  (ç‹—)\n  4.0    |   1  (ç‹—)\nçº¿æ€§å›å½’ï¼šy = b + wÂ·x\ny\nâ†‘\n1 |         â—  â—   (ç‹—)\n  |       /\n  |     /\n0 |   â—  â—       (çŒ«)\n  |_____________â†’ x\nçœ‹èµ·æ¥è¿˜ä¸é”™ï¼Ÿ\n\n\n4.3.2 âš ï¸ é—®é¢˜æ¥äº†ï¼\næ–°æ•°æ®ç‚¹ï¼šx = 10\ny\nâ†‘\n3 |               â—  (é¢„æµ‹å€¼ = 3ï¼Ÿï¼Ÿ)\n2 |             /\n1 |         â—  â—\n  |       /\n  |     /\n0 |   â—  â—\n  |_____________â†’ x\n            10\né—®é¢˜ï¼š 1. è¾“å‡ºä¸æ˜¯ 0 æˆ– 1ï¼ˆå¯èƒ½æ˜¯ 3, -1, 0.7â€¦ï¼‰ 2. è¿œç¦»è®­ç»ƒæ•°æ®çš„ç‚¹ä¼šå½±å“å†³ç­–è¾¹ç•Œ 3. æ— æ³•è¡¨ç¤ºæ¦‚ç‡\n\n\n4.3.3 ğŸ’¡ æˆ‘ä»¬éœ€è¦ä»€ä¹ˆï¼Ÿ\nç†æƒ³çš„åˆ†ç±»å™¨ï¼š\n  è¾“å‡ºèŒƒå›´åœ¨ [0, 1]\n  å¯ä»¥è§£é‡Šä¸ºæ¦‚ç‡\n  0.9 â†’ 90% ç¡®å®šæ˜¯ç‹—\n  0.1 â†’ 10% ç¡®å®šæ˜¯ç‹—ï¼ˆ90% æ˜¯çŒ«ï¼‰",
    "crumbs": [
      "ç¬¬ä¸€éƒ¨åˆ†ï¼šåŸºç¡€ç¯‡",
      "<span class='chapter-number'>4</span>Â  <span class='chapter-title'>ç¬¬ä¸‰ç« ï¼šåˆ†ç±»ä¸é€»è¾‘å›å½’ (Classification & Logistic Regression)</span>"
    ]
  },
  {
    "objectID": "Chapter3.html#logistic-regression",
    "href": "Chapter3.html#logistic-regression",
    "title": "4Â  ç¬¬ä¸‰ç« ï¼šåˆ†ç±»ä¸é€»è¾‘å›å½’ (Classification & Logistic Regression)",
    "section": "4.4 3.3 Logistic Regression",
    "text": "4.4 3.3 Logistic Regression\n\n4.4.1 ğŸ”¹ æ ¸å¿ƒæ€æƒ³\næ”¹é€ çº¿æ€§å›å½’ï¼š\nStep 1: è®¡ç®—çº¿æ€§ç»„åˆ\n  z = b + wÂ·x\n\nStep 2: é€šè¿‡ Sigmoid å‡½æ•°å‹ç¼©åˆ° (0, 1)\n  y = Ïƒ(z) = 1 / (1 + e^(-z))\n\n\n4.4.2 ğŸ“ Sigmoid å‡½æ•°\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef sigmoid(z):\n    return 1 / (1 + np.exp(-z))\n\nz = np.linspace(-10, 10, 100)\ny = sigmoid(z)\n\nplt.figure(figsize=(10, 6))\nplt.plot(z, y, 'b-', linewidth=2)\nplt.axhline(y=0.5, color='r', linestyle='--', label='é˜ˆå€¼ = 0.5')\nplt.axvline(x=0, color='g', linestyle='--', alpha=0.5)\nplt.xlabel('z = b + wx')\nplt.ylabel('Ïƒ(z)')\nplt.title('Sigmoid Function')\nplt.grid(True, alpha=0.3)\nplt.legend()\nplt.show()\nå›¾å½¢ï¼š\nÏƒ(z)\n 1 |         ______\n   |       /\n0.5|      * (z=0, Ïƒ=0.5)\n   |     /\n 0 |____/\n   |_____________â†’ z\n  -10      0     10\n\n\n4.4.3 ğŸ” Sigmoid çš„æ€§è´¨\nÏƒ(z) = 1 / (1 + e^(-z))\n\næ€§è´¨ï¼š\n1. è¾“å‡ºèŒƒå›´ï¼š(0, 1)\n2. Ïƒ(0) = 0.5 (ä¸­ç‚¹)\n3. z â†’ +âˆ, Ïƒ(z) â†’ 1\n4. z â†’ -âˆ, Ïƒ(z) â†’ 0\n5. å…³äº (0, 0.5) ä¸­å¿ƒå¯¹ç§°\n6. å¯¼æ•°ï¼šÏƒ'(z) = Ïƒ(z)Â·(1 - Ïƒ(z))\n\n\n4.4.4 ğŸ¯ å†³ç­–è§„åˆ™\nç»™å®šè¾“å…¥ xï¼Œè®¡ç®—ï¼š\n  z = b + wÂ·x\n  P(y=1|x) = Ïƒ(z)\n\nå†³ç­–ï¼š\n  å¦‚æœ P(y=1|x) â‰¥ 0.5  â†’ é¢„æµ‹ä¸ºç±»åˆ« 1\n  å¦‚æœ P(y=1|x) &lt; 0.5  â†’ é¢„æµ‹ä¸ºç±»åˆ« 0\n\nç­‰ä»·äºï¼š\n  å¦‚æœ z â‰¥ 0  â†’ é¢„æµ‹ä¸ºç±»åˆ« 1\n  å¦‚æœ z &lt; 0  â†’ é¢„æµ‹ä¸ºç±»åˆ« 0\n\n\n4.4.5 ğŸ“Š å†³ç­–è¾¹ç•Œ (Decision Boundary)\nä¸€ç»´æƒ…å†µï¼š\nz = b + wÂ·x = 0\nâ†’ x = -b/w  (å†³ç­–è¾¹ç•Œ)\n\nä¾‹ï¼šb = -3, w = 1\n  x &lt; 3 â†’ é¢„æµ‹ç±»åˆ« 0\n  x &gt; 3 â†’ é¢„æµ‹ç±»åˆ« 1\näºŒç»´æƒ…å†µï¼š\nz = b + wâ‚xâ‚ + wâ‚‚xâ‚‚ = 0\nâ†’ xâ‚‚ = -(b + wâ‚xâ‚)/wâ‚‚  (ä¸€æ¡ç›´çº¿)\n\nxâ‚‚\nâ†‘\n|     /\n|    /  ç±»åˆ« 1\n|   /\n|  /_________ å†³ç­–è¾¹ç•Œ\n| /\n|/ ç±»åˆ« 0\n|________â†’ xâ‚",
    "crumbs": [
      "ç¬¬ä¸€éƒ¨åˆ†ï¼šåŸºç¡€ç¯‡",
      "<span class='chapter-number'>4</span>Â  <span class='chapter-title'>ç¬¬ä¸‰ç« ï¼šåˆ†ç±»ä¸é€»è¾‘å›å½’ (Classification & Logistic Regression)</span>"
    ]
  },
  {
    "objectID": "Chapter3.html#loss-function-for-classification",
    "href": "Chapter3.html#loss-function-for-classification",
    "title": "4Â  ç¬¬ä¸‰ç« ï¼šåˆ†ç±»ä¸é€»è¾‘å›å½’ (Classification & Logistic Regression)",
    "section": "4.5 3.4 Loss Function for Classification",
    "text": "4.5 3.4 Loss Function for Classification\n\n4.5.1 ğŸš« ä¸ºä»€ä¹ˆä¸ç”¨ MSEï¼Ÿ\nå°è¯•ï¼šL = (y - Ïƒ(z))Â²\né—®é¢˜ï¼š 1. éå‡¸å‡½æ•°ï¼šæœ‰å¾ˆå¤šå±€éƒ¨æœ€ä¼˜è§£ 2. æ¢¯åº¦æ¶ˆå¤±ï¼šå½“é¢„æµ‹å¾ˆé”™æ—¶ï¼Œæ¢¯åº¦åè€Œå¾ˆå°\nLoss\n â†‘\n |  *     *\n | / \\   / \\   å¤šä¸ªå±€éƒ¨æœ€ä¼˜ï¼\n |/   \\_/   \\\n |___________â†’ w\n\n\n4.5.2 âœ… Cross Entropy Loss\nå…¬å¼ï¼š\nå¯¹äºå•ä¸ªæ ·æœ¬ï¼š\nL(y, Å·) = -[yÂ·log(Å·) + (1-y)Â·log(1-Å·)]\n\nå…¶ä¸­ï¼š\n  y âˆˆ {0, 1}     çœŸå®æ ‡ç­¾\n  Å· = Ïƒ(z)       é¢„æµ‹æ¦‚ç‡\nç†è§£ï¼š\næƒ…å†µ1ï¼šçœŸå®æ ‡ç­¾ y = 1\n  L = -log(Å·)\n  å¦‚æœ Å· â†’ 1 (é¢„æµ‹æ­£ç¡®)  â†’ L â†’ 0   (æŸå¤±å°)\n  å¦‚æœ Å· â†’ 0 (é¢„æµ‹é”™è¯¯)  â†’ L â†’ âˆ   (æŸå¤±å¤§)\n\næƒ…å†µ2ï¼šçœŸå®æ ‡ç­¾ y = 0\n  L = -log(1-Å·)\n  å¦‚æœ Å· â†’ 0 (é¢„æµ‹æ­£ç¡®)  â†’ L â†’ 0   (æŸå¤±å°)\n  å¦‚æœ Å· â†’ 1 (é¢„æµ‹é”™è¯¯)  â†’ L â†’ âˆ   (æŸå¤±å¤§)\n\n\n4.5.3 ğŸ“Š å¯è§†åŒ– Cross Entropy\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ny_pred = np.linspace(0.01, 0.99, 100)\n\n# y = 1 æ—¶çš„æŸå¤±\nloss_y1 = -np.log(y_pred)\n\n# y = 0 æ—¶çš„æŸå¤±\nloss_y0 = -np.log(1 - y_pred)\n\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.plot(y_pred, loss_y1, 'b-', linewidth=2)\nplt.title('çœŸå®æ ‡ç­¾ y = 1')\nplt.xlabel('é¢„æµ‹æ¦‚ç‡ Å·')\nplt.ylabel('Loss = -log(Å·)')\nplt.grid(True, alpha=0.3)\n\nplt.subplot(1, 2, 2)\nplt.plot(y_pred, loss_y0, 'r-', linewidth=2)\nplt.title('çœŸå®æ ‡ç­¾ y = 0')\nplt.xlabel('é¢„æµ‹æ¦‚ç‡ Å·')\nplt.ylabel('Loss = -log(1-Å·)')\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\n4.5.4 ğŸ§® å®Œæ•´çš„ Loss Function\nå¯¹äº N ä¸ªè®­ç»ƒæ ·æœ¬ï¼š\nL(w, b) = -(1/N) Î£[yâ¿Â·log(Å·â¿) + (1-yâ¿)Â·log(1-Å·â¿)]\n\nå…¶ä¸­ï¼š\n  Å·â¿ = Ïƒ(b + wÂ·xâ¿)",
    "crumbs": [
      "ç¬¬ä¸€éƒ¨åˆ†ï¼šåŸºç¡€ç¯‡",
      "<span class='chapter-number'>4</span>Â  <span class='chapter-title'>ç¬¬ä¸‰ç« ï¼šåˆ†ç±»ä¸é€»è¾‘å›å½’ (Classification & Logistic Regression)</span>"
    ]
  },
  {
    "objectID": "Chapter3.html#æ¢¯åº¦ä¸‹é™æ±‚è§£",
    "href": "Chapter3.html#æ¢¯åº¦ä¸‹é™æ±‚è§£",
    "title": "4Â  ç¬¬ä¸‰ç« ï¼šåˆ†ç±»ä¸é€»è¾‘å›å½’ (Classification & Logistic Regression)",
    "section": "4.6 3.5 æ¢¯åº¦ä¸‹é™æ±‚è§£",
    "text": "4.6 3.5 æ¢¯åº¦ä¸‹é™æ±‚è§£\n\n4.6.1 ğŸ“ è®¡ç®—æ¢¯åº¦\nÅ· = Ïƒ(z) = Ïƒ(b + wÂ·x)\n\nL = -[yÂ·log(Å·) + (1-y)Â·log(1-Å·)]\n\næ±‚å¯¼ï¼ˆé“¾å¼æ³•åˆ™ï¼‰ï¼š\n\nâˆ‚L/âˆ‚w = (Å· - y)Â·x\nâˆ‚L/âˆ‚b = (Å· - y)\n\næƒŠå–œï¼å½¢å¼å’Œçº¿æ€§å›å½’ä¸€æ ·ç®€å•ï¼\n\n\n4.6.2 ğŸ’» ä»é›¶å®ç° Logistic Regression\nimport numpy as np\n\nclass LogisticRegression:\n    def __init__(self, learning_rate=0.01, epochs=1000):\n        self.lr = learning_rate\n        self.epochs = epochs\n        self.w = None\n        self.b = None\n        self.losses = []\n\n    def sigmoid(self, z):\n        \"\"\"Sigmoid å‡½æ•°\"\"\"\n        return 1 / (1 + np.exp(-z))\n\n    def fit(self, X, y):\n        \"\"\"è®­ç»ƒæ¨¡å‹\"\"\"\n        n_samples, n_features = X.shape\n\n        # åˆå§‹åŒ–å‚æ•°\n        self.w = np.zeros(n_features)\n        self.b = 0\n\n        # æ¢¯åº¦ä¸‹é™\n        for epoch in range(self.epochs):\n            # å‰å‘ä¼ æ’­\n            z = np.dot(X, self.w) + self.b\n            y_pred = self.sigmoid(z)\n\n            # è®¡ç®—æŸå¤±\n            loss = -np.mean(y * np.log(y_pred + 1e-9) +\n                           (1 - y) * np.log(1 - y_pred + 1e-9))\n            self.losses.append(loss)\n\n            # è®¡ç®—æ¢¯åº¦\n            dw = np.dot(X.T, (y_pred - y)) / n_samples\n            db = np.mean(y_pred - y)\n\n            # æ›´æ–°å‚æ•°\n            self.w -= self.lr * dw\n            self.b -= self.lr * db\n\n            # æ‰“å°è¿›åº¦\n            if epoch % 100 == 0:\n                print(f\"Epoch {epoch}: Loss = {loss:.4f}\")\n\n    def predict_proba(self, X):\n        \"\"\"é¢„æµ‹æ¦‚ç‡\"\"\"\n        z = np.dot(X, self.w) + self.b\n        return self.sigmoid(z)\n\n    def predict(self, X, threshold=0.5):\n        \"\"\"é¢„æµ‹ç±»åˆ«\"\"\"\n        proba = self.predict_proba(X)\n        return (proba &gt;= threshold).astype(int)\n\n# ä½¿ç”¨ç¤ºä¾‹\nif __name__ == \"__main__\":\n    # ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®\n    np.random.seed(42)\n\n    # ç±»åˆ« 0\n    X0 = np.random.randn(100, 2) + np.array([2, 2])\n    y0 = np.zeros(100)\n\n    # ç±»åˆ« 1\n    X1 = np.random.randn(100, 2) + np.array([5, 5])\n    y1 = np.ones(100)\n\n    # åˆå¹¶\n    X = np.vstack([X0, X1])\n    y = np.hstack([y0, y1])\n\n    # è®­ç»ƒ\n    model = LogisticRegression(learning_rate=0.1, epochs=1000)\n    model.fit(X, y)\n\n    # é¢„æµ‹\n    predictions = model.predict(X)\n    accuracy = np.mean(predictions == y)\n    print(f\"\\nå‡†ç¡®ç‡: {accuracy:.4f}\")\nè¾“å‡ºï¼š\nEpoch 0: Loss = 0.6931\nEpoch 100: Loss = 0.2156\nEpoch 200: Loss = 0.1398\nEpoch 300: Loss = 0.1045\nEpoch 400: Loss = 0.0850\nEpoch 500: Loss = 0.0722\nEpoch 600: Loss = 0.0632\nEpoch 700: Loss = 0.0565\nEpoch 800: Loss = 0.0513\nEpoch 900: Loss = 0.0472\n\nå‡†ç¡®ç‡: 1.0000\n\n\n4.6.3 ğŸ“Š å¯è§†åŒ–å†³ç­–è¾¹ç•Œ\nimport matplotlib.pyplot as plt\n\ndef plot_decision_boundary(model, X, y):\n    # åˆ›å»ºç½‘æ ¼\n    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n\n    xx1, xx2 = np.meshgrid(\n        np.linspace(x1_min, x1_max, 100),\n        np.linspace(x2_min, x2_max, 100)\n    )\n\n    # é¢„æµ‹æ¯ä¸ªç½‘æ ¼ç‚¹\n    Z = model.predict(np.c_[xx1.ravel(), xx2.ravel()])\n    Z = Z.reshape(xx1.shape)\n\n    # ç»˜å›¾\n    plt.figure(figsize=(10, 8))\n    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap='RdYlBu')\n    plt.contour(xx1, xx2, Z, levels=[0.5], colors='black', linewidths=2)\n\n    # ç»˜åˆ¶æ•°æ®ç‚¹\n    plt.scatter(X[y==0, 0], X[y==0, 1], c='blue', label='ç±»åˆ« 0',\n                edgecolors='k', s=50)\n    plt.scatter(X[y==1, 0], X[y==1, 1], c='red', label='ç±»åˆ« 1',\n                edgecolors='k', s=50)\n\n    plt.xlabel('ç‰¹å¾ 1')\n    plt.ylabel('ç‰¹å¾ 2')\n    plt.title('é€»è¾‘å›å½’å†³ç­–è¾¹ç•Œ')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    plt.show()\n\n# ç»˜åˆ¶\nplot_decision_boundary(model, X, y)",
    "crumbs": [
      "ç¬¬ä¸€éƒ¨åˆ†ï¼šåŸºç¡€ç¯‡",
      "<span class='chapter-number'>4</span>Â  <span class='chapter-title'>ç¬¬ä¸‰ç« ï¼šåˆ†ç±»ä¸é€»è¾‘å›å½’ (Classification & Logistic Regression)</span>"
    ]
  },
  {
    "objectID": "Chapter3.html#è¯„ä¼°åˆ†ç±»æ¨¡å‹",
    "href": "Chapter3.html#è¯„ä¼°åˆ†ç±»æ¨¡å‹",
    "title": "4Â  ç¬¬ä¸‰ç« ï¼šåˆ†ç±»ä¸é€»è¾‘å›å½’ (Classification & Logistic Regression)",
    "section": "4.7 3.6 è¯„ä¼°åˆ†ç±»æ¨¡å‹",
    "text": "4.7 3.6 è¯„ä¼°åˆ†ç±»æ¨¡å‹\n\n4.7.1 ğŸ“Š Confusion Matrix (æ··æ·†çŸ©é˜µ)\n                é¢„æµ‹\n              0      1\nçœŸ   0      TN     FP\nå®   1      FN     TP\n\nTN (True Negative):  æ­£ç¡®é¢„æµ‹ä¸ºè´Ÿç±»\nTP (True Positive):  æ­£ç¡®é¢„æµ‹ä¸ºæ­£ç±»\nFN (False Negative): é”™è¯¯é¢„æµ‹ä¸ºè´Ÿç±»ï¼ˆæ¼æŠ¥ï¼‰\nFP (False Positive): é”™è¯¯é¢„æµ‹ä¸ºæ­£ç±»ï¼ˆè¯¯æŠ¥ï¼‰\nä¾‹å­ï¼šç™Œç—‡æ£€æµ‹\n                é¢„æµ‹\n           æ²¡ç—…    æœ‰ç—…\nçœŸ æ²¡ç—…    90      10    (10ä¸ªå‡é˜³æ€§)\nå® æœ‰ç—…     5      95    (5ä¸ªå‡é˜´æ€§)\n\n\n4.7.2 ğŸ”¢ è¯„ä¼°æŒ‡æ ‡\n\n4.7.2.1 1. Accuracy (å‡†ç¡®ç‡)\nAccuracy = (TP + TN) / (TP + TN + FP + FN)\n\n= æ­£ç¡®é¢„æµ‹çš„æ•°é‡ / æ€»æ•°é‡\n\nä¾‹ï¼š(90 + 95) / 200 = 0.925 = 92.5%\né—®é¢˜ï¼šç±»åˆ«ä¸å¹³è¡¡æ—¶ä¼šè¯¯å¯¼\nä¾‹ï¼š100ä¸ªæ ·æœ¬ï¼Œ95ä¸ªè´Ÿç±»ï¼Œ5ä¸ªæ­£ç±»\nå¦‚æœå…¨éƒ¨é¢„æµ‹ä¸ºè´Ÿç±»ï¼š\n  Accuracy = 95/100 = 95%  (çœ‹èµ·æ¥å¾ˆå¥½ï¼)\n  ä½†å®Œå…¨æ²¡æœ‰æ£€æµ‹åˆ°æ­£ç±»ï¼\n\n\n4.7.2.2 2. Precision (ç²¾ç¡®ç‡)\nPrecision = TP / (TP + FP)\n\n= é¢„æµ‹ä¸ºæ­£ç±»ä¸­ï¼ŒçœŸæ­£æ˜¯æ­£ç±»çš„æ¯”ä¾‹\n\nä¾‹ï¼š95 / (95 + 10) = 0.905 = 90.5%\n\nç†è§£ï¼šåœ¨æˆ‘è¯´\"æœ‰ç—…\"çš„äººä¸­ï¼ŒçœŸçš„æœ‰ç—…çš„æ¯”ä¾‹\nä½¿ç”¨åœºæ™¯ï¼šå½“ è¯¯æŠ¥ä»£ä»·é«˜ æ—¶ - åƒåœ¾é‚®ä»¶è¿‡æ»¤ï¼šä¸è¦æŠŠæ­£å¸¸é‚®ä»¶æ ‡è®°ä¸ºåƒåœ¾é‚®ä»¶ - ä¿¡ç”¨å¡æ¬ºè¯ˆï¼šä¸è¦è¯¯æŠ¥æ­£å¸¸äº¤æ˜“\n\n\n4.7.2.3 3. Recall (å¬å›ç‡ / çµæ•åº¦)\nRecall = TP / (TP + FN)\n\n= çœŸæ­£çš„æ­£ç±»ä¸­ï¼Œè¢«æ­£ç¡®é¢„æµ‹çš„æ¯”ä¾‹\n\nä¾‹ï¼š95 / (95 + 5) = 0.95 = 95%\n\nç†è§£ï¼šæ‰€æœ‰çœŸæ­£æœ‰ç—…çš„äººä¸­ï¼Œè¢«æ£€æµ‹å‡ºæ¥çš„æ¯”ä¾‹\nä½¿ç”¨åœºæ™¯ï¼šå½“ æ¼æŠ¥ä»£ä»·é«˜ æ—¶ - ç–¾ç—…æ£€æµ‹ï¼šä¸èƒ½æ¼æ‰çœŸæ­£çš„ç—…äºº - æ¬ºè¯ˆæ£€æµ‹ï¼šä¸èƒ½æ¼æ‰çœŸæ­£çš„æ¬ºè¯ˆ\n\n\n4.7.2.4 4. F1 Score\nF1 = 2 Â· (Precision Â· Recall) / (Precision + Recall)\n\n= Precision å’Œ Recall çš„è°ƒå’Œå¹³å‡æ•°\n\nä¾‹ï¼š2 Â· (0.905 Â· 0.95) / (0.905 + 0.95) = 0.927\nç‰¹ç‚¹ï¼šå¹³è¡¡ Precision å’Œ Recall\n\n\n4.7.2.5 5. ROC æ›²çº¿å’Œ AUC\nROC (Receiver Operating Characteristic) æ›²çº¿ï¼š\nTPR (True Positive Rate) = Recall = TP/(TP+FN)\nFPR (False Positive Rate) = FP/(FP+TN)\n\næ¨ªè½´ï¼šFPR (å‡é˜³æ€§ç‡)\nçºµè½´ï¼šTPR (çœŸé˜³æ€§ç‡)\nAUC (Area Under Curve)ï¼šROC æ›²çº¿ä¸‹é¢ç§¯ - AUC = 1: å®Œç¾åˆ†ç±»å™¨ - AUC = 0.5: éšæœºçŒœæµ‹ - AUC &gt; 0.8: é€šå¸¸è®¤ä¸ºä¸é”™\n\n\n\n4.7.3 ğŸ’» è®¡ç®—è¯„ä¼°æŒ‡æ ‡\nfrom sklearn.metrics import (\n    accuracy_score,\n    precision_score,\n    recall_score,\n    f1_score,\n    confusion_matrix,\n    classification_report,\n    roc_curve,\n    roc_auc_score\n)\nimport matplotlib.pyplot as plt\n\n# é¢„æµ‹\ny_pred = model.predict(X)\ny_proba = model.predict_proba(X)\n\n# 1. æ··æ·†çŸ©é˜µ\ncm = confusion_matrix(y, y_pred)\nprint(\"æ··æ·†çŸ©é˜µï¼š\")\nprint(cm)\n\n# 2. åŸºæœ¬æŒ‡æ ‡\naccuracy = accuracy_score(y, y_pred)\nprecision = precision_score(y, y_pred)\nrecall = recall_score(y, y_pred)\nf1 = f1_score(y, y_pred)\n\nprint(f\"\\nAccuracy:  {accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall:    {recall:.4f}\")\nprint(f\"F1 Score:  {f1:.4f}\")\n\n# 3. å®Œæ•´æŠ¥å‘Š\nprint(\"\\nåˆ†ç±»æŠ¥å‘Šï¼š\")\nprint(classification_report(y, y_pred,\n                           target_names=['ç±»åˆ«0', 'ç±»åˆ«1']))\n\n# 4. ROC æ›²çº¿\nfpr, tpr, thresholds = roc_curve(y, y_proba)\nauc = roc_auc_score(y, y_proba)\n\nplt.figure(figsize=(8, 6))\nplt.plot(fpr, tpr, 'b-', linewidth=2, label=f'ROC (AUC = {auc:.3f})')\nplt.plot([0, 1], [0, 1], 'r--', label='éšæœºçŒœæµ‹')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n\n# 5. Precision-Recall æ›²çº¿\nfrom sklearn.metrics import precision_recall_curve\n\nprecision_vals, recall_vals, _ = precision_recall_curve(y, y_proba)\n\nplt.figure(figsize=(8, 6))\nplt.plot(recall_vals, precision_vals, 'b-', linewidth=2)\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall Curve')\nplt.grid(True, alpha=0.3)\nplt.show()",
    "crumbs": [
      "ç¬¬ä¸€éƒ¨åˆ†ï¼šåŸºç¡€ç¯‡",
      "<span class='chapter-number'>4</span>Â  <span class='chapter-title'>ç¬¬ä¸‰ç« ï¼šåˆ†ç±»ä¸é€»è¾‘å›å½’ (Classification & Logistic Regression)</span>"
    ]
  },
  {
    "objectID": "Chapter3.html#å¤šå…ƒåˆ†ç±»-multi-class-classification-1",
    "href": "Chapter3.html#å¤šå…ƒåˆ†ç±»-multi-class-classification-1",
    "title": "4Â  ç¬¬ä¸‰ç« ï¼šåˆ†ç±»ä¸é€»è¾‘å›å½’ (Classification & Logistic Regression)",
    "section": "4.8 3.7 å¤šå…ƒåˆ†ç±» (Multi-class Classification)",
    "text": "4.8 3.7 å¤šå…ƒåˆ†ç±» (Multi-class Classification)\n\n4.8.1 ğŸ¯ ä»äºŒå…ƒåˆ°å¤šå…ƒ\né—®é¢˜ï¼šæ‰‹å†™æ•°å­—è¯†åˆ« (0-9)ï¼Œå…± 10 ä¸ªç±»åˆ«\n\n\n4.8.2 ğŸ”¹ æ–¹æ³• 1ï¼šOne-vs-Rest (OvR)\nç­–ç•¥ï¼šè®­ç»ƒ K ä¸ªäºŒå…ƒåˆ†ç±»å™¨\nåˆ†ç±»å™¨ 1: ç±»åˆ« 0 vs å…¶ä»–\nåˆ†ç±»å™¨ 2: ç±»åˆ« 1 vs å…¶ä»–\n...\nåˆ†ç±»å™¨ K: ç±»åˆ« K-1 vs å…¶ä»–\n\né¢„æµ‹æ—¶ï¼šé€‰æ‹©è¾“å‡ºæ¦‚ç‡æœ€å¤§çš„åˆ†ç±»å™¨\nä¾‹å­ï¼šè¯†åˆ«æ•°å­— 3\nåˆ†ç±»å™¨ 1 (0 vs å…¶ä»–): P = 0.05\nåˆ†ç±»å™¨ 2 (1 vs å…¶ä»–): P = 0.10\nåˆ†ç±»å™¨ 3 (2 vs å…¶ä»–): P = 0.08\nåˆ†ç±»å™¨ 4 (3 vs å…¶ä»–): P = 0.95  âœ“ æœ€å¤§\n...\nåˆ†ç±»å™¨ 10 (9 vs å…¶ä»–): P = 0.03\n\nâ†’ é¢„æµ‹ä¸ºç±»åˆ« 3\n\n\n4.8.3 ğŸ”¹ æ–¹æ³• 2ï¼šSoftmax Regression\næ ¸å¿ƒæ€æƒ³ï¼šæ‰©å±• Sigmoid åˆ°å¤šä¸ªç±»åˆ«\n\n4.8.3.1 Softmax å‡½æ•°\nç»™å®š K ä¸ªç±»åˆ«ï¼Œè®¡ç®— K ä¸ªåˆ†æ•°ï¼š\n\nzâ‚ = bâ‚ + wâ‚áµ€x\nzâ‚‚ = bâ‚‚ + wâ‚‚áµ€x\n...\nzâ‚– = bâ‚– + wâ‚–áµ€x\n\nSoftmax:\nP(y=i|x) = e^(záµ¢) / Î£â±¼ e^(zâ±¼)\n\næ€§è´¨ï¼š\n1. æ‰€æœ‰æ¦‚ç‡å’Œä¸º 1: Î£áµ¢ P(y=i|x) = 1\n2. æ¯ä¸ªæ¦‚ç‡éƒ½åœ¨ (0, 1)\n3. å¦‚æœ K=2ï¼Œé€€åŒ–ä¸º Sigmoid\n\n\n4.8.3.2 å¯è§†åŒ–ç†è§£\nä¾‹ï¼š3ä¸ªç±»åˆ«\n\nzâ‚ = 2.0    â†’  e^2.0 = 7.39\nzâ‚‚ = 1.0    â†’  e^1.0 = 2.72\nzâ‚ƒ = 0.1    â†’  e^0.1 = 1.11\n                ________\n                Sum = 11.22\n\nP(y=1) = 7.39/11.22 = 0.659  (65.9%)\nP(y=2) = 2.72/11.22 = 0.242  (24.2%)\nP(y=3) = 1.11/11.22 = 0.099  (9.9%)\n\né¢„æµ‹ï¼šç±»åˆ« 1 (æ¦‚ç‡æœ€é«˜)\n\n\n\n4.8.4 ğŸ“ Cross Entropy for Multi-class\nå¯¹äºå•ä¸ªæ ·æœ¬ï¼š\nL = -Î£áµ¢ yáµ¢Â·log(Å·áµ¢)\n\nå…¶ä¸­ï¼š\n  yáµ¢: one-hot ç¼–ç çš„çœŸå®æ ‡ç­¾\n  Å·áµ¢: softmax è¾“å‡ºçš„é¢„æµ‹æ¦‚ç‡\n\nä¾‹ï¼šçœŸå®ç±»åˆ«æ˜¯ 2 (å…±3ä¸ªç±»åˆ«)\ny = [0, 1, 0]       (one-hot)\nÅ· = [0.1, 0.7, 0.2]  (é¢„æµ‹)\n\nL = -(0Â·log(0.1) + 1Â·log(0.7) + 0Â·log(0.2))\n  = -log(0.7)\n  = 0.357\n\n\n4.8.5 ğŸ’» å®ç° Softmax Regression\nimport numpy as np\n\nclass SoftmaxRegression:\n    def __init__(self, learning_rate=0.01, epochs=1000):\n        self.lr = learning_rate\n        self.epochs = epochs\n        self.W = None\n        self.b = None\n\n    def softmax(self, z):\n        \"\"\"Softmax å‡½æ•°\"\"\"\n        # å‡å»æœ€å¤§å€¼é˜²æ­¢æ•°å€¼æº¢å‡º\n        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n\n    def fit(self, X, y):\n        \"\"\"è®­ç»ƒæ¨¡å‹\"\"\"\n        n_samples, n_features = X.shape\n        n_classes = len(np.unique(y))\n\n        # One-hot ç¼–ç æ ‡ç­¾\n        y_onehot = np.eye(n_classes)[y]\n\n        # åˆå§‹åŒ–å‚æ•°\n        self.W = np.random.randn(n_features, n_classes) * 0.01\n        self.b = np.zeros(n_classes)\n\n        # æ¢¯åº¦ä¸‹é™\n        for epoch in range(self.epochs):\n            # å‰å‘ä¼ æ’­\n            z = np.dot(X, self.W) + self.b\n            y_pred = self.softmax(z)\n\n            # è®¡ç®—æŸå¤±\n            loss = -np.mean(np.sum(y_onehot * np.log(y_pred + 1e-9), axis=1))\n\n            # è®¡ç®—æ¢¯åº¦\n            dz = y_pred - y_onehot\n            dW = np.dot(X.T, dz) / n_samples\n            db = np.mean(dz, axis=0)\n\n            # æ›´æ–°å‚æ•°\n            self.W -= self.lr * dW\n            self.b -= self.lr * db\n\n            if epoch % 100 == 0:\n                accuracy = np.mean(np.argmax(y_pred, axis=1) == y)\n                print(f\"Epoch {epoch}: Loss = {loss:.4f}, Acc = {accuracy:.4f}\")\n\n    def predict_proba(self, X):\n        \"\"\"é¢„æµ‹æ¦‚ç‡\"\"\"\n        z = np.dot(X, self.W) + self.b\n        return self.softmax(z)\n\n    def predict(self, X):\n        \"\"\"é¢„æµ‹ç±»åˆ«\"\"\"\n        proba = self.predict_proba(X)\n        return np.argmax(proba, axis=1)\n\n# ä½¿ç”¨ç¤ºä¾‹\nif __name__ == \"__main__\":\n    from sklearn.datasets import load_iris\n    from sklearn.model_selection import train_test_split\n\n    # åŠ è½½é¸¢å°¾èŠ±æ•°æ®é›†ï¼ˆ3ä¸ªç±»åˆ«ï¼‰\n    iris = load_iris()\n    X, y = iris.data, iris.target\n\n    # åˆ†å‰²æ•°æ®\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=42\n    )\n\n    # è®­ç»ƒ\n    model = SoftmaxRegression(learning_rate=0.1, epochs=1000)\n    model.fit(X_train, y_train)\n\n    # æµ‹è¯•\n    y_pred = model.predict(X_test)\n    accuracy = np.mean(y_pred == y_test)\n    print(f\"\\næµ‹è¯•é›†å‡†ç¡®ç‡: {accuracy:.4f}\")",
    "crumbs": [
      "ç¬¬ä¸€éƒ¨åˆ†ï¼šåŸºç¡€ç¯‡",
      "<span class='chapter-number'>4</span>Â  <span class='chapter-title'>ç¬¬ä¸‰ç« ï¼šåˆ†ç±»ä¸é€»è¾‘å›å½’ (Classification & Logistic Regression)</span>"
    ]
  },
  {
    "objectID": "Chapter3.html#å®æˆ˜-1ä¿¡ç”¨å¡æ¬ºè¯ˆæ£€æµ‹",
    "href": "Chapter3.html#å®æˆ˜-1ä¿¡ç”¨å¡æ¬ºè¯ˆæ£€æµ‹",
    "title": "4Â  ç¬¬ä¸‰ç« ï¼šåˆ†ç±»ä¸é€»è¾‘å›å½’ (Classification & Logistic Regression)",
    "section": "4.9 3.8 å®æˆ˜ 1ï¼šä¿¡ç”¨å¡æ¬ºè¯ˆæ£€æµ‹",
    "text": "4.9 3.8 å®æˆ˜ 1ï¼šä¿¡ç”¨å¡æ¬ºè¯ˆæ£€æµ‹\n\n4.9.1 ğŸ“‹ é—®é¢˜æè¿°\næ•°æ®é›†ï¼šKaggle Credit Card Fraud Detection - 284,807 ç¬”äº¤æ˜“è®°å½• - 492 ç¬”æ¬ºè¯ˆï¼ˆ0.172%ï¼‰â† æåº¦ä¸å¹³è¡¡ï¼ - 30 ä¸ªç‰¹å¾ï¼ˆPCA å¤„ç†è¿‡ï¼Œå·²è„±æ•ï¼‰\n\n\n4.9.2 âš ï¸ ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜\næ­£å¸¸äº¤æ˜“: 284,315  (99.83%)\næ¬ºè¯ˆäº¤æ˜“:     492  (0.17%)\n\nå¦‚æœå…¨éƒ¨é¢„æµ‹ä¸º\"æ­£å¸¸\"ï¼š\n  Accuracy = 99.83%  (çœ‹èµ·æ¥å¾ˆé«˜ï¼)\n  ä½†å®Œå…¨æ²¡æœ‰æ£€æµ‹åˆ°æ¬ºè¯ˆï¼\n\n\n4.9.3 ğŸ’¡ å¤„ç†æ–¹æ³•\n\n4.9.3.1 æ–¹æ³• 1ï¼šé‡é‡‡æ ·\næ¬ é‡‡æ · (Under-sampling)ï¼š\nå‡å°‘å¤šæ•°ç±»æ ·æœ¬\næ­£å¸¸: 284,315 â†’ 492\næ¬ºè¯ˆ:     492 â†’ 492\n\nä¼˜ç‚¹ï¼šå¹³è¡¡æ•°æ®é›†\nç¼ºç‚¹ï¼šä¸¢å¤±å¤§é‡ä¿¡æ¯\nè¿‡é‡‡æ · (Over-sampling)ï¼š\nå¢åŠ å°‘æ•°ç±»æ ·æœ¬ï¼ˆå¤åˆ¶æˆ–ç”Ÿæˆï¼‰\næ­£å¸¸: 284,315 â†’ 284,315\næ¬ºè¯ˆ:     492 â†’ 284,315\n\nä¼˜ç‚¹ï¼šä¿ç•™æ‰€æœ‰ä¿¡æ¯\nç¼ºç‚¹ï¼šå¯èƒ½è¿‡æ‹Ÿåˆ\nSMOTE (Synthetic Minority Over-sampling)ï¼š\nåˆæˆæ–°çš„å°‘æ•°ç±»æ ·æœ¬\nä¸æ˜¯ç®€å•å¤åˆ¶ï¼Œè€Œæ˜¯åœ¨ç‰¹å¾ç©ºé—´ä¸­æ’å€¼ç”Ÿæˆ\n\n\n4.9.3.2 æ–¹æ³• 2ï¼šè°ƒæ•´ç±»åˆ«æƒé‡\nfrom sklearn.linear_model import LogisticRegression\n\n# è‡ªåŠ¨è®¡ç®—æƒé‡\nmodel = LogisticRegression(class_weight='balanced')\n\n# æ‰‹åŠ¨è®¾ç½®\nmodel = LogisticRegression(class_weight={0: 1, 1: 100})\n\n\n4.9.3.3 æ–¹æ³• 3ï¼šè°ƒæ•´å†³ç­–é˜ˆå€¼\n# é»˜è®¤é˜ˆå€¼ 0.5\ny_pred = (y_proba &gt;= 0.5).astype(int)\n\n# é™ä½é˜ˆå€¼ï¼Œæé«˜å¬å›ç‡\ny_pred = (y_proba &gt;= 0.3).astype(int)\n\n# æé«˜é˜ˆå€¼ï¼Œæé«˜ç²¾ç¡®ç‡\ny_pred = (y_proba &gt;= 0.7).astype(int)\n\n\n4.9.3.4 æ–¹æ³• 4ï¼šä½¿ç”¨é€‚å½“çš„è¯„ä¼°æŒ‡æ ‡\nä¸è¦ç”¨ Accuracyï¼\nåº”è¯¥ç”¨ï¼š\n  - F1 Score\n  - Precision-Recall AUC\n  - ROC AUC\n\n\n\n4.9.4 ğŸ’» å®Œæ•´ä»£ç \nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import (\n    classification_report,\n    confusion_matrix,\n    roc_auc_score,\n    precision_recall_curve\n)\nfrom imblearn.over_sampling import SMOTE\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# 1. åŠ è½½æ•°æ®\ndf = pd.read_csv('creditcard.csv')\n\nprint(\"æ•°æ®å½¢çŠ¶:\", df.shape)\nprint(\"\\nç±»åˆ«åˆ†å¸ƒ:\")\nprint(df['Class'].value_counts())\nprint(f\"\\næ¬ºè¯ˆæ¯”ä¾‹: {df['Class'].mean():.4%}\")\n\n# 2. å‡†å¤‡æ•°æ®\nX = df.drop('Class', axis=1)\ny = df['Class']\n\n# 3. åˆ†å‰²æ•°æ®\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\n# 4. ç‰¹å¾ç¼©æ”¾\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# 5. å¤„ç†ä¸å¹³è¡¡ï¼šSMOTE\nsmote = SMOTE(random_state=42)\nX_train_resampled, y_train_resampled = smote.fit_resample(\n    X_train_scaled, y_train\n)\n\nprint(f\"\\nSMOTE å‰: {len(y_train)} æ ·æœ¬\")\nprint(f\"SMOTE å: {len(y_train_resampled)} æ ·æœ¬\")\nprint(f\"æ–°çš„ç±»åˆ«åˆ†å¸ƒ:\\n{pd.Series(y_train_resampled).value_counts()}\")\n\n# 6. è®­ç»ƒæ¨¡å‹\nmodel = LogisticRegression(max_iter=1000, random_state=42)\nmodel.fit(X_train_resampled, y_train_resampled)\n\n# 7. é¢„æµ‹\ny_pred = model.predict(X_test_scaled)\ny_proba = model.predict_proba(X_test_scaled)[:, 1]\n\n# 8. è¯„ä¼°\nprint(\"\\næ··æ·†çŸ©é˜µ:\")\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\n\nprint(\"\\nåˆ†ç±»æŠ¥å‘Š:\")\nprint(classification_report(y_test, y_pred,\n                           target_names=['æ­£å¸¸', 'æ¬ºè¯ˆ']))\n\nprint(f\"\\nROC AUC: {roc_auc_score(y_test, y_proba):.4f}\")\n\n# 9. å¯è§†åŒ–æ··æ·†çŸ©é˜µ\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\nplt.xlabel('é¢„æµ‹æ ‡ç­¾')\nplt.ylabel('çœŸå®æ ‡ç­¾')\nplt.title('æ··æ·†çŸ©é˜µ')\nplt.show()\n\n# 10. Precision-Recall æ›²çº¿\nprecision, recall, thresholds = precision_recall_curve(y_test, y_proba)\n\nplt.figure(figsize=(10, 6))\nplt.plot(thresholds, precision[:-1], 'b-', label='Precision')\nplt.plot(thresholds, recall[:-1], 'r-', label='Recall')\nplt.xlabel('é˜ˆå€¼')\nplt.ylabel('åˆ†æ•°')\nplt.title('Precision vs Recall vs é˜ˆå€¼')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n\n# 11. æ‰¾æœ€ä¼˜é˜ˆå€¼\nf1_scores = 2 * (precision * recall) / (precision + recall + 1e-9)\noptimal_idx = np.argmax(f1_scores[:-1])\noptimal_threshold = thresholds[optimal_idx]\n\nprint(f\"\\næœ€ä¼˜é˜ˆå€¼: {optimal_threshold:.4f}\")\nprint(f\"å¯¹åº” F1 Score: {f1_scores[optimal_idx]:.4f}\")\n\n# 12. ä½¿ç”¨æœ€ä¼˜é˜ˆå€¼é‡æ–°é¢„æµ‹\ny_pred_optimal = (y_proba &gt;= optimal_threshold).astype(int)\n\nprint(\"\\nä½¿ç”¨æœ€ä¼˜é˜ˆå€¼çš„ç»“æœ:\")\nprint(classification_report(y_test, y_pred_optimal,\n                           target_names=['æ­£å¸¸', 'æ¬ºè¯ˆ']))",
    "crumbs": [
      "ç¬¬ä¸€éƒ¨åˆ†ï¼šåŸºç¡€ç¯‡",
      "<span class='chapter-number'>4</span>Â  <span class='chapter-title'>ç¬¬ä¸‰ç« ï¼šåˆ†ç±»ä¸é€»è¾‘å›å½’ (Classification & Logistic Regression)</span>"
    ]
  },
  {
    "objectID": "Chapter3.html#å®æˆ˜-2æ‰‹å†™æ•°å­—è¯†åˆ«-mnist",
    "href": "Chapter3.html#å®æˆ˜-2æ‰‹å†™æ•°å­—è¯†åˆ«-mnist",
    "title": "4Â  ç¬¬ä¸‰ç« ï¼šåˆ†ç±»ä¸é€»è¾‘å›å½’ (Classification & Logistic Regression)",
    "section": "4.10 3.9 å®æˆ˜ 2ï¼šæ‰‹å†™æ•°å­—è¯†åˆ« (MNIST)",
    "text": "4.10 3.9 å®æˆ˜ 2ï¼šæ‰‹å†™æ•°å­—è¯†åˆ« (MNIST)\n\n4.10.1 ğŸ“‹ MNIST æ•°æ®é›†\n70,000 å¼ æ‰‹å†™æ•°å­—å›¾ç‰‡\n  - 60,000 è®­ç»ƒé›†\n  - 10,000 æµ‹è¯•é›†\n\næ¯å¼ å›¾ç‰‡ï¼š\n  - 28Ã—28 åƒç´ \n  - ç°åº¦å€¼ 0-255\n  - æ ‡ç­¾ï¼š0-9\n\n\n4.10.2 ğŸ’» å®Œæ•´ä»£ç \nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nimport seaborn as sns\n\n# 1. åŠ è½½æ•°æ®\nprint(\"åŠ è½½ MNIST æ•°æ®...\")\nmnist = fetch_openml('mnist_784', version=1, parser='auto')\nX, y = mnist['data'], mnist['target'].astype(int)\n\nprint(f\"æ•°æ®å½¢çŠ¶: {X.shape}\")\nprint(f\"æ ‡ç­¾å½¢çŠ¶: {y.shape}\")\n\n# 2. å¯è§†åŒ–ä¸€äº›æ ·æœ¬\nfig, axes = plt.subplots(2, 5, figsize=(12, 5))\nfor i, ax in enumerate(axes.flat):\n    ax.imshow(X.iloc[i].values.reshape(28, 28), cmap='gray')\n    ax.set_title(f'æ ‡ç­¾: {y.iloc[i]}')\n    ax.axis('off')\nplt.tight_layout()\nplt.show()\n\n# 3. æ•°æ®é¢„å¤„ç†\n# å½’ä¸€åŒ–åˆ° [0, 1]\nX = X / 255.0\n\n# ä½¿ç”¨éƒ¨åˆ†æ•°æ®ï¼ˆåŠ å¿«è®­ç»ƒï¼‰\nX_subset = X[:10000]\ny_subset = y[:10000]\n\n# åˆ†å‰²æ•°æ®\nX_train, X_test, y_train, y_test = train_test_split(\n    X_subset, y_subset, test_size=0.2, random_state=42\n)\n\n# 4. è®­ç»ƒæ¨¡å‹\nprint(\"\\nè®­ç»ƒé€»è¾‘å›å½’æ¨¡å‹...\")\nmodel = LogisticRegression(\n    max_iter=100,\n    multi_class='multinomial',  # Softmax\n    solver='lbfgs',             # ä¼˜åŒ–ç®—æ³•\n    random_state=42\n)\nmodel.fit(X_train, y_train)\n\n# 5. é¢„æµ‹\ny_pred = model.predict(X_test)\n\n# 6. è¯„ä¼°\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"\\nå‡†ç¡®ç‡: {accuracy:.4f}\")\n\nprint(\"\\nåˆ†ç±»æŠ¥å‘Š:\")\nprint(classification_report(y_test, y_pred))\n\n# 7. æ··æ·†çŸ©é˜µ\ncm = confusion_matrix(y_test, y_pred)\n\nplt.figure(figsize=(10, 8))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\nplt.xlabel('é¢„æµ‹æ ‡ç­¾')\nplt.ylabel('çœŸå®æ ‡ç­¾')\nplt.title('æ··æ·†çŸ©é˜µ')\nplt.show()\n\n# 8. å¯è§†åŒ–ä¸€äº›é¢„æµ‹ç»“æœ\nfig, axes = plt.subplots(3, 5, figsize=(12, 7))\nindices = np.random.choice(len(X_test), 15, replace=False)\n\nfor i, (ax, idx) in enumerate(zip(axes.flat, indices)):\n    image = X_test.iloc[idx].values.reshape(28, 28)\n    true_label = y_test.iloc[idx]\n    pred_label = y_pred[idx]\n\n    ax.imshow(image, cmap='gray')\n    color = 'green' if true_label == pred_label else 'red'\n    ax.set_title(f'çœŸå®: {true_label}, é¢„æµ‹: {pred_label}', color=color)\n    ax.axis('off')\n\nplt.tight_layout()\nplt.show()\n\n# 9. æŸ¥çœ‹æ¨¡å‹å­¦åˆ°çš„æƒé‡\n# æ¯ä¸ªæ•°å­—çš„æƒé‡å¯ä»¥çœ‹ä½œä¸€ä¸ª\"æ¨¡æ¿\"\nfig, axes = plt.subplots(2, 5, figsize=(12, 5))\nfor digit, ax in enumerate(axes.flat):\n    weight = model.coef_[digit].reshape(28, 28)\n    ax.imshow(weight, cmap='RdBu', vmin=-weight.max(), vmax=weight.max())\n    ax.set_title(f'æ•°å­— {digit}çš„æƒé‡')\n    ax.axis('off')\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "ç¬¬ä¸€éƒ¨åˆ†ï¼šåŸºç¡€ç¯‡",
      "<span class='chapter-number'>4</span>Â  <span class='chapter-title'>ç¬¬ä¸‰ç« ï¼šåˆ†ç±»ä¸é€»è¾‘å›å½’ (Classification & Logistic Regression)</span>"
    ]
  },
  {
    "objectID": "Chapter3.html#é€»è¾‘å›å½’çš„ä¼˜ç¼ºç‚¹",
    "href": "Chapter3.html#é€»è¾‘å›å½’çš„ä¼˜ç¼ºç‚¹",
    "title": "4Â  ç¬¬ä¸‰ç« ï¼šåˆ†ç±»ä¸é€»è¾‘å›å½’ (Classification & Logistic Regression)",
    "section": "4.11 3.10 é€»è¾‘å›å½’çš„ä¼˜ç¼ºç‚¹",
    "text": "4.11 3.10 é€»è¾‘å›å½’çš„ä¼˜ç¼ºç‚¹\n\n4.11.1 âœ… ä¼˜ç‚¹\n\nç®€å•é«˜æ•ˆ\n\næ˜“äºå®ç°å’Œç†è§£\nè®­ç»ƒé€Ÿåº¦å¿«\né¢„æµ‹é€Ÿåº¦å¿«\n\nå¯è§£é‡Šæ€§å¼º\n\nå¯ä»¥çœ‹æƒé‡äº†è§£ç‰¹å¾é‡è¦æ€§\nè¾“å‡ºæ¦‚ç‡ï¼Œæ–¹ä¾¿å†³ç­–\n\nä¸éœ€è¦å¤ªå¤šæ•°æ®\n\nç›¸æ¯”æ·±åº¦å­¦ä¹ ï¼Œæ•°æ®éœ€æ±‚å°‘\n\nä¸å®¹æ˜“è¿‡æ‹Ÿåˆ\n\næ¨¡å‹ç®€å•ï¼Œæ³›åŒ–èƒ½åŠ›å¥½\nå¯ä»¥ç”¨æ­£åˆ™åŒ–è¿›ä¸€æ­¥æ§åˆ¶\n\n\n\n\n4.11.2 âŒ ç¼ºç‚¹\n\nçº¿æ€§æ¨¡å‹\n\nåªèƒ½å­¦ä¹ çº¿æ€§å†³ç­–è¾¹ç•Œ\næ— æ³•å¤„ç†å¤æ‚çš„éçº¿æ€§å…³ç³»\n\nç‰¹å¾å·¥ç¨‹ä¾èµ–\n\néœ€è¦æ‰‹åŠ¨è®¾è®¡å¥½çš„ç‰¹å¾\nç‰¹å¾è´¨é‡å†³å®šæ¨¡å‹ä¸Šé™\n\nå¤šé‡å…±çº¿æ€§æ•æ„Ÿ\n\nç‰¹å¾é«˜åº¦ç›¸å…³æ—¶ï¼Œæ¨¡å‹ä¸ç¨³å®š\n\nç±»åˆ«ä¸å¹³è¡¡é—®é¢˜\n\néœ€è¦ç‰¹æ®Šå¤„ç†",
    "crumbs": [
      "ç¬¬ä¸€éƒ¨åˆ†ï¼šåŸºç¡€ç¯‡",
      "<span class='chapter-number'>4</span>Â  <span class='chapter-title'>ç¬¬ä¸‰ç« ï¼šåˆ†ç±»ä¸é€»è¾‘å›å½’ (Classification & Logistic Regression)</span>"
    ]
  },
  {
    "objectID": "Chapter3.html#æœ¬ç« ä½œä¸š",
    "href": "Chapter3.html#æœ¬ç« ä½œä¸š",
    "title": "4Â  ç¬¬ä¸‰ç« ï¼šåˆ†ç±»ä¸é€»è¾‘å›å½’ (Classification & Logistic Regression)",
    "section": "4.12 ğŸ“ æœ¬ç« ä½œä¸š",
    "text": "4.12 ğŸ“ æœ¬ç« ä½œä¸š\n\n4.12.1 ä½œä¸š 1ï¼šæ¦‚å¿µé¢˜\n\nä¸ºä»€ä¹ˆä¸èƒ½ç”¨çº¿æ€§å›å½’åšåˆ†ç±»ï¼Ÿ\n\nä¸¾ä¾‹è¯´æ˜é—®é¢˜\nç”»å›¾è§£é‡Š\n\nSigmoid vs Softmax\n\nä»€ä¹ˆæ—¶å€™ç”¨ Sigmoidï¼Ÿ\nä»€ä¹ˆæ—¶å€™ç”¨ Softmaxï¼Ÿ\nå®ƒä»¬çš„å…³ç³»æ˜¯ä»€ä¹ˆï¼Ÿ\n\nè¯„ä¼°æŒ‡æ ‡é€‰æ‹©\n\nç™Œç—‡æ£€æµ‹åº”è¯¥å…³æ³¨ Precision è¿˜æ˜¯ Recallï¼Ÿä¸ºä»€ä¹ˆï¼Ÿ\nåƒåœ¾é‚®ä»¶è¿‡æ»¤åº”è¯¥å…³æ³¨å“ªä¸ªï¼Ÿ\nç»™å‡º 3 ä¸ªåœºæ™¯å’Œå¯¹åº”çš„æœ€é‡è¦æŒ‡æ ‡\n\n\n\n\n4.12.2 ä½œä¸š 2ï¼šç¼–ç¨‹å®è·µ\n\n4.12.2.1 ä»»åŠ¡ 1ï¼šä¹³è…ºç™Œæ£€æµ‹\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report\n\n# åŠ è½½æ•°æ®\ndata = load_breast_cancer()\nX, y = data.data, data.target\n\n# TODO:\n# 1. åˆ†å‰²æ•°æ®ï¼ˆ80/20ï¼‰\n# 2. ç‰¹å¾ç¼©æ”¾\n# 3. è®­ç»ƒé€»è¾‘å›å½’æ¨¡å‹\n# 4. è¯„ä¼°æ¨¡å‹ï¼ˆæ··æ·†çŸ©é˜µã€ROCæ›²çº¿ï¼‰\n# 5. è°ƒæ•´å†³ç­–é˜ˆå€¼ï¼Œä¼˜åŒ– Recall\n# 6. æ¯”è¾ƒä¸åŒæ­£åˆ™åŒ–å¼ºåº¦ï¼ˆCå‚æ•°ï¼‰çš„æ•ˆæœ\n\n\n4.12.2.2 ä»»åŠ¡ 2ï¼šå¤šåˆ†ç±»å®æˆ˜\nä½¿ç”¨ Scikit-learn çš„ load_digits æ•°æ®é›†ï¼ˆ8Ã—8 æ‰‹å†™æ•°å­—ï¼‰\nfrom sklearn.datasets import load_digits\n\n# TODO:\n# 1. åŠ è½½æ•°æ®å¹¶å¯è§†åŒ–\n# 2. è®­ç»ƒ Softmax Regression\n# 3. åˆ†æå“ªäº›æ•°å­—å®¹æ˜“æ··æ·†ï¼ˆçœ‹æ··æ·†çŸ©é˜µï¼‰\n# 4. å¯è§†åŒ–æ¨¡å‹å­¦åˆ°çš„æƒé‡\n# 5. å®ç° One-vs-Rest æ–¹æ³•å¹¶å¯¹æ¯”æ€§èƒ½\n\n\n\n4.12.3 ä½œä¸š 3ï¼šKaggle ç«èµ›\nå‚åŠ  â€œTitanic - Machine Learning from Disasterâ€\nè¦æ±‚ï¼š 1. æ•°æ®æ¢ç´¢å’Œå¯è§†åŒ– 2. ç‰¹å¾å·¥ç¨‹ï¼ˆå¤„ç†ç¼ºå¤±å€¼ã€åˆ›å»ºæ–°ç‰¹å¾ï¼‰ 3. è®­ç»ƒé€»è¾‘å›å½’æ¨¡å‹ 4. è°ƒæ•´è¶…å‚æ•° 5. æäº¤é¢„æµ‹ç»“æœ 6. å†™ä¸€ä»½å®Œæ•´æŠ¥å‘Š",
    "crumbs": [
      "ç¬¬ä¸€éƒ¨åˆ†ï¼šåŸºç¡€ç¯‡",
      "<span class='chapter-number'>4</span>Â  <span class='chapter-title'>ç¬¬ä¸‰ç« ï¼šåˆ†ç±»ä¸é€»è¾‘å›å½’ (Classification & Logistic Regression)</span>"
    ]
  },
  {
    "objectID": "Chapter3.html#æœ¬ç« å…³é”®æ¦‚å¿µæ€»ç»“",
    "href": "Chapter3.html#æœ¬ç« å…³é”®æ¦‚å¿µæ€»ç»“",
    "title": "4Â  ç¬¬ä¸‰ç« ï¼šåˆ†ç±»ä¸é€»è¾‘å›å½’ (Classification & Logistic Regression)",
    "section": "4.13 ğŸ”‘ æœ¬ç« å…³é”®æ¦‚å¿µæ€»ç»“",
    "text": "4.13 ğŸ”‘ æœ¬ç« å…³é”®æ¦‚å¿µæ€»ç»“\n\n\n\næ¦‚å¿µ\nè¯´æ˜\n\n\n\n\nåˆ†ç±»\né¢„æµ‹ç¦»æ•£ç±»åˆ«\n\n\nSigmoid\nå°†å®æ•°æ˜ å°„åˆ° (0,1)\n\n\né€»è¾‘å›å½’\nç”¨äºäºŒå…ƒåˆ†ç±»\n\n\nCross Entropy\nåˆ†ç±»é—®é¢˜çš„æŸå¤±å‡½æ•°\n\n\nSoftmax\nå¤šå…ƒåˆ†ç±»çš„æ¿€æ´»å‡½æ•°\n\n\nConfusion Matrix\nè¯„ä¼°åˆ†ç±»æ€§èƒ½\n\n\nPrecision\né¢„æµ‹ä¸ºæ­£çš„å‡†ç¡®ç‡\n\n\nRecall\næ‰¾å‡ºæ‰€æœ‰æ­£ä¾‹çš„èƒ½åŠ›\n\n\nF1 Score\nPrecision å’Œ Recall çš„è°ƒå’Œå¹³å‡\n\n\nROC/AUC\nè¯„ä¼°åˆ†ç±»å™¨æ€§èƒ½\n\n\nç±»åˆ«ä¸å¹³è¡¡\nç±»åˆ«æ ·æœ¬æ•°å·®å¼‚å¤§\n\n\nSMOTE\nåˆæˆå°‘æ•°ç±»æ ·æœ¬",
    "crumbs": [
      "ç¬¬ä¸€éƒ¨åˆ†ï¼šåŸºç¡€ç¯‡",
      "<span class='chapter-number'>4</span>Â  <span class='chapter-title'>ç¬¬ä¸‰ç« ï¼šåˆ†ç±»ä¸é€»è¾‘å›å½’ (Classification & Logistic Regression)</span>"
    ]
  },
  {
    "objectID": "Chapter3.html#ä¸‹ä¸€ç« é¢„å‘Š",
    "href": "Chapter3.html#ä¸‹ä¸€ç« é¢„å‘Š",
    "title": "4Â  ç¬¬ä¸‰ç« ï¼šåˆ†ç±»ä¸é€»è¾‘å›å½’ (Classification & Logistic Regression)",
    "section": "4.14 ğŸ¯ ä¸‹ä¸€ç« é¢„å‘Š",
    "text": "4.14 ğŸ¯ ä¸‹ä¸€ç« é¢„å‘Š\nç¬¬å››ç« ï¼šæ·±åº¦ç¥ç»ç½‘ç»œåŸºç¡€ (Deep Neural Networks) - ä»é€»è¾‘å›å½’åˆ°ç¥ç»ç½‘ç»œ - æ¿€æ´»å‡½æ•° (ReLU, Tanh, etc.) - åå‘ä¼ æ’­ç®—æ³• - æ·±åº¦ç½‘ç»œçš„è®­ç»ƒæŠ€å·§ - å®æˆ˜ï¼šç”¨ç¥ç»ç½‘ç»œæ”¹è¿› MNIST",
    "crumbs": [
      "ç¬¬ä¸€éƒ¨åˆ†ï¼šåŸºç¡€ç¯‡",
      "<span class='chapter-number'>4</span>Â  <span class='chapter-title'>ç¬¬ä¸‰ç« ï¼šåˆ†ç±»ä¸é€»è¾‘å›å½’ (Classification & Logistic Regression)</span>"
    ]
  },
  {
    "objectID": "Chapter4.html",
    "href": "Chapter4.html",
    "title": "5Â  ç¬¬å››ç« ï¼šæ·±åº¦ç¥ç»ç½‘ç»œåŸºç¡€ (Deep Neural Networks)",
    "section": "",
    "text": "5.1 ğŸ“Œ ç« èŠ‚ç›®æ ‡",
    "crumbs": [
      "ç¬¬ä¸€éƒ¨åˆ†ï¼šåŸºç¡€ç¯‡",
      "<span class='chapter-number'>5</span>Â  <span class='chapter-title'>ç¬¬å››ç« ï¼šæ·±åº¦ç¥ç»ç½‘ç»œåŸºç¡€ (Deep Neural Networks)</span>"
    ]
  },
  {
    "objectID": "Chapter4.html#ç« èŠ‚ç›®æ ‡",
    "href": "Chapter4.html#ç« èŠ‚ç›®æ ‡",
    "title": "5Â  ç¬¬å››ç« ï¼šæ·±åº¦ç¥ç»ç½‘ç»œåŸºç¡€ (Deep Neural Networks)",
    "section": "",
    "text": "ç†è§£ä»é€»è¾‘å›å½’åˆ°ç¥ç»ç½‘ç»œçš„æ¼”è¿›\næŒæ¡å‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­ç®—æ³•\näº†è§£å„ç§æ¿€æ´»å‡½æ•°åŠå…¶ä½œç”¨\nå­¦ä¹ æ·±åº¦ç½‘ç»œçš„åˆå§‹åŒ–å’Œè®­ç»ƒæŠ€å·§\nå®æˆ˜ï¼šç”¨ PyTorch/TensorFlow æ„å»ºç¥ç»ç½‘ç»œ",
    "crumbs": [
      "ç¬¬ä¸€éƒ¨åˆ†ï¼šåŸºç¡€ç¯‡",
      "<span class='chapter-number'>5</span>Â  <span class='chapter-title'>ç¬¬å››ç« ï¼šæ·±åº¦ç¥ç»ç½‘ç»œåŸºç¡€ (Deep Neural Networks)</span>"
    ]
  },
  {
    "objectID": "Chapter4.html#ä»é€»è¾‘å›å½’åˆ°ç¥ç»ç½‘ç»œ",
    "href": "Chapter4.html#ä»é€»è¾‘å›å½’åˆ°ç¥ç»ç½‘ç»œ",
    "title": "5Â  ç¬¬å››ç« ï¼šæ·±åº¦ç¥ç»ç½‘ç»œåŸºç¡€ (Deep Neural Networks)",
    "section": "5.2 4.1 ä»é€»è¾‘å›å½’åˆ°ç¥ç»ç½‘ç»œ",
    "text": "5.2 4.1 ä»é€»è¾‘å›å½’åˆ°ç¥ç»ç½‘ç»œ\n\n5.2.1 ğŸ”„ å›é¡¾ï¼šé€»è¾‘å›å½’\nè¾“å…¥ x â†’ çº¿æ€§ç»„åˆ z = wÂ·x + b â†’ Sigmoid Ïƒ(z) â†’ è¾“å‡º Å·\nå±€é™æ€§ï¼šåªèƒ½å­¦ä¹ çº¿æ€§å†³ç­–è¾¹ç•Œ\nç¤ºä¾‹ï¼šXOR é—®é¢˜\n\nè¾“å…¥          è¾“å‡º\nxâ‚  xâ‚‚       y\n0   0    â†’   0\n0   1    â†’   1\n1   0    â†’   1\n1   1    â†’   0\n\nxâ‚‚\nâ†‘\n1 | 0   1    æ— æ³•ç”¨ä¸€æ¡ç›´çº¿åˆ†å¼€ï¼\n0 | 1   0\n  |_____â†’ xâ‚\n  0     1\n\n\n5.2.2 ğŸ’¡ è§£å†³æ–¹æ¡ˆï¼šå †å å¤šå±‚\nç¥ç»ç½‘ç»œçš„æ ¸å¿ƒæ€æƒ³ï¼š\n\nä¸€å±‚é€»è¾‘å›å½’å¤ªç®€å•ï¼Ÿé‚£å°±å å¾ˆå¤šå±‚ï¼\n\nè¾“å…¥å±‚ â†’ éšè—å±‚1 â†’ éšè—å±‚2 â†’ ... â†’ è¾“å‡ºå±‚\n\n\n5.2.3 ğŸ§  ç¥ç»å…ƒ (Neuron)\nå•ä¸ªç¥ç»å…ƒ = é€»è¾‘å›å½’å•å…ƒ\n       xâ‚ â”€â”\n       xâ‚‚ â”€â”¤\n       xâ‚ƒ â”€â”¼â†’ z = Î£wáµ¢xáµ¢ + b â†’ a = Ïƒ(z) â†’ è¾“å‡º\n      ...  â”¤\n       xâ‚™ â”€â”˜\n\nè¾“å…¥ â†’ åŠ æƒæ±‚å’Œ â†’ æ¿€æ´»å‡½æ•° â†’ è¾“å‡º\næ•°å­¦è¡¨ç¤ºï¼š\nz = wâ‚xâ‚ + wâ‚‚xâ‚‚ + ... + wâ‚™xâ‚™ + b\na = Ïƒ(z)\n\nå‘é‡å½¢å¼ï¼š\nz = wáµ€x + b\na = Ïƒ(z)\n\n\n5.2.4 ğŸ•¸ï¸ ç¥ç»ç½‘ç»œ = å¾ˆå¤šç¥ç»å…ƒçš„ç»„åˆ\nè¾“å…¥å±‚        éšè—å±‚1         éšè—å±‚2        è¾“å‡ºå±‚\n  xâ‚ â”€â”€â”€â”¬â”€â”€â†’ hâ‚â½Â¹â¾ â”€â”€â”€â”¬â”€â”€â†’ hâ‚â½Â²â¾ â”€â”€â”€â”¬â”€â”€â†’ Å·â‚\n        â”‚              â”‚              â”‚\n  xâ‚‚ â”€â”€â”€â”¼â”€â”€â†’ hâ‚‚â½Â¹â¾ â”€â”€â”€â”¼â”€â”€â†’ hâ‚‚â½Â²â¾ â”€â”€â”€â”¼â”€â”€â†’ Å·â‚‚\n        â”‚              â”‚              â”‚\n  xâ‚ƒ â”€â”€â”€â”¼â”€â”€â†’ hâ‚ƒâ½Â¹â¾ â”€â”€â”€â”¼â”€â”€â†’ hâ‚ƒâ½Â²â¾ â”€â”€â”€â”¼â”€â”€â†’ Å·â‚ƒ\n        â”‚              â”‚              â”‚\n  ...   â””â”€â”€â†’ ...   â”€â”€â”€â””â”€â”€â†’ ...   â”€â”€â”€â”˜\n\nç¬¬0å±‚         ç¬¬1å±‚           ç¬¬2å±‚         ç¬¬3å±‚\n(è¾“å…¥)       (éšè—)         (éšè—)       (è¾“å‡º)",
    "crumbs": [
      "ç¬¬ä¸€éƒ¨åˆ†ï¼šåŸºç¡€ç¯‡",
      "<span class='chapter-number'>5</span>Â  <span class='chapter-title'>ç¬¬å››ç« ï¼šæ·±åº¦ç¥ç»ç½‘ç»œåŸºç¡€ (Deep Neural Networks)</span>"
    ]
  },
  {
    "objectID": "Chapter4.html#å‰å‘ä¼ æ’­-forward-propagation",
    "href": "Chapter4.html#å‰å‘ä¼ æ’­-forward-propagation",
    "title": "5Â  ç¬¬å››ç« ï¼šæ·±åº¦ç¥ç»ç½‘ç»œåŸºç¡€ (Deep Neural Networks)",
    "section": "5.3 4.2 å‰å‘ä¼ æ’­ (Forward Propagation)",
    "text": "5.3 4.2 å‰å‘ä¼ æ’­ (Forward Propagation)\n\n5.3.1 ğŸ“ æ•°å­¦æ¨å¯¼\nç¬¦å·å®šä¹‰ï¼š\nL: ç½‘ç»œå±‚æ•°\nnâ½Ë¡â¾: ç¬¬ l å±‚çš„ç¥ç»å…ƒæ•°é‡\nwâ½Ë¡â¾: ç¬¬ l å±‚çš„æƒé‡çŸ©é˜µ\nbâ½Ë¡â¾: ç¬¬ l å±‚çš„åç½®å‘é‡\naâ½Ë¡â¾: ç¬¬ l å±‚çš„æ¿€æ´»å€¼ï¼ˆè¾“å‡ºï¼‰\nzâ½Ë¡â¾: ç¬¬ l å±‚çš„åŠ æƒè¾“å…¥\nå•å±‚è®¡ç®—ï¼š\nzâ½Ë¡â¾ = Wâ½Ë¡â¾Â·aâ½Ë¡â»Â¹â¾ + bâ½Ë¡â¾\naâ½Ë¡â¾ = Ïƒ(zâ½Ë¡â¾)\n\n\n5.3.2 ğŸ”¢ å…·ä½“ä¾‹å­\nç½‘ç»œç»“æ„ï¼š2 â†’ 3 â†’ 1\nè¾“å…¥å±‚ï¼š2ä¸ªç¥ç»å…ƒ [xâ‚, xâ‚‚]\néšè—å±‚ï¼š3ä¸ªç¥ç»å…ƒ [hâ‚, hâ‚‚, hâ‚ƒ]\nè¾“å‡ºå±‚ï¼š1ä¸ªç¥ç»å…ƒ [Å·]\nç¬¬1å±‚ï¼ˆè¾“å…¥â†’éšè—ï¼‰ï¼š\nzâ‚â½Â¹â¾ = wâ‚â‚â½Â¹â¾xâ‚ + wâ‚â‚‚â½Â¹â¾xâ‚‚ + bâ‚â½Â¹â¾\nzâ‚‚â½Â¹â¾ = wâ‚‚â‚â½Â¹â¾xâ‚ + wâ‚‚â‚‚â½Â¹â¾xâ‚‚ + bâ‚‚â½Â¹â¾\nzâ‚ƒâ½Â¹â¾ = wâ‚ƒâ‚â½Â¹â¾xâ‚ + wâ‚ƒâ‚‚â½Â¹â¾xâ‚‚ + bâ‚ƒâ½Â¹â¾\n\naâ‚â½Â¹â¾ = Ïƒ(zâ‚â½Â¹â¾)\naâ‚‚â½Â¹â¾ = Ïƒ(zâ‚‚â½Â¹â¾)\naâ‚ƒâ½Â¹â¾ = Ïƒ(zâ‚ƒâ½Â¹â¾)\n\nçŸ©é˜µå½¢å¼ï¼š\nzâ½Â¹â¾ = Wâ½Â¹â¾Â·x + bâ½Â¹â¾\n\nå…¶ä¸­ï¼š\n     [wâ‚â‚ wâ‚â‚‚]       [bâ‚]\nWâ½Â¹â¾=[wâ‚‚â‚ wâ‚‚â‚‚]  bâ½Â¹â¾=[bâ‚‚]\n     [wâ‚ƒâ‚ wâ‚ƒâ‚‚]       [bâ‚ƒ]\nç¬¬2å±‚ï¼ˆéšè—â†’è¾“å‡ºï¼‰ï¼š\nzâ½Â²â¾ = Wâ½Â²â¾Â·aâ½Â¹â¾ + bâ½Â²â¾\nÅ· = aâ½Â²â¾ = Ïƒ(zâ½Â²â¾)\n\nå…¶ä¸­ï¼š\nWâ½Â²â¾ = [wâ‚ wâ‚‚ wâ‚ƒ]  (1Ã—3 çŸ©é˜µ)\nbâ½Â²â¾ = [b]          (æ ‡é‡)\n\n\n5.3.3 ğŸ’» ä»£ç å®ç°\nimport numpy as np\n\ndef sigmoid(z):\n    \"\"\"Sigmoid æ¿€æ´»å‡½æ•°\"\"\"\n    return 1 / (1 + np.exp(-z))\n\ndef forward_propagation(X, parameters):\n    \"\"\"\n    å‰å‘ä¼ æ’­\n\n    å‚æ•°ï¼š\n        X: è¾“å…¥æ•°æ® (n_features, m_samples)\n        parameters: å­—å…¸ï¼ŒåŒ…å« W1, b1, W2, b2\n\n    è¿”å›ï¼š\n        A2: è¾“å‡ºå±‚æ¿€æ´»å€¼\n        cache: ä¸­é—´å€¼ï¼Œç”¨äºåå‘ä¼ æ’­\n    \"\"\"\n    # è·å–å‚æ•°\n    W1 = parameters['W1']\n    b1 = parameters['b1']\n    W2 = parameters['W2']\n    b2 = parameters['b2']\n\n    # ç¬¬1å±‚\n    Z1 = np.dot(W1, X) + b1\n    A1 = sigmoid(Z1)\n\n    # ç¬¬2å±‚\n    Z2 = np.dot(W2, A1) + b2\n    A2 = sigmoid(Z2)\n\n    # ä¿å­˜ä¸­é—´å€¼\n    cache = {\n        'Z1': Z1,\n        'A1': A1,\n        'Z2': Z2,\n        'A2': A2\n    }\n\n    return A2, cache\n\n# ç¤ºä¾‹\nnp.random.seed(42)\n\n# åˆå§‹åŒ–å‚æ•°\nparameters = {\n    'W1': np.random.randn(3, 2) * 0.01,  # 3Ã—2\n    'b1': np.zeros((3, 1)),              # 3Ã—1\n    'W2': np.random.randn(1, 3) * 0.01,  # 1Ã—3\n    'b2': np.zeros((1, 1))               # 1Ã—1\n}\n\n# è¾“å…¥æ•°æ®\nX = np.array([[1.0, 2.0],\n              [0.5, 1.5]]).T  # 2Ã—2 (2ä¸ªæ ·æœ¬)\n\n# å‰å‘ä¼ æ’­\nA2, cache = forward_propagation(X, parameters)\n\nprint(\"è¾“å…¥ X:\")\nprint(X)\nprint(\"\\nè¾“å‡º A2:\")\nprint(A2)\nprint(\"\\néšè—å±‚æ¿€æ´» A1:\")\nprint(cache['A1'])",
    "crumbs": [
      "ç¬¬ä¸€éƒ¨åˆ†ï¼šåŸºç¡€ç¯‡",
      "<span class='chapter-number'>5</span>Â  <span class='chapter-title'>ç¬¬å››ç« ï¼šæ·±åº¦ç¥ç»ç½‘ç»œåŸºç¡€ (Deep Neural Networks)</span>"
    ]
  },
  {
    "objectID": "Chapter4.html#æ¿€æ´»å‡½æ•°-activation-functions",
    "href": "Chapter4.html#æ¿€æ´»å‡½æ•°-activation-functions",
    "title": "5Â  ç¬¬å››ç« ï¼šæ·±åº¦ç¥ç»ç½‘ç»œåŸºç¡€ (Deep Neural Networks)",
    "section": "5.4 4.3 æ¿€æ´»å‡½æ•° (Activation Functions)",
    "text": "5.4 4.3 æ¿€æ´»å‡½æ•° (Activation Functions)\n\n5.4.1 ğŸ¤” ä¸ºä»€ä¹ˆéœ€è¦æ¿€æ´»å‡½æ•°ï¼Ÿ\nå¦‚æœæ²¡æœ‰æ¿€æ´»å‡½æ•°ï¼ˆæˆ–ä½¿ç”¨çº¿æ€§æ¿€æ´»ï¼‰ï¼š\nzâ½Â¹â¾ = Wâ½Â¹â¾x + bâ½Â¹â¾\naâ½Â¹â¾ = zâ½Â¹â¾              â† çº¿æ€§\n\nzâ½Â²â¾ = Wâ½Â²â¾aâ½Â¹â¾ + bâ½Â²â¾\n    = Wâ½Â²â¾(Wâ½Â¹â¾x + bâ½Â¹â¾) + bâ½Â²â¾\n    = (Wâ½Â²â¾Wâ½Â¹â¾)x + (Wâ½Â²â¾bâ½Â¹â¾ + bâ½Â²â¾)\n    = W'x + b'           â† è¿˜æ˜¯çº¿æ€§ï¼\n\nå¤šå±‚çº¿æ€§å˜æ¢ = å•å±‚çº¿æ€§å˜æ¢\næ·±åº¦ç½‘ç»œé€€åŒ–æˆæµ…å±‚ç½‘ç»œï¼\nç»“è®ºï¼šæ¿€æ´»å‡½æ•°å¼•å…¥éçº¿æ€§ï¼Œè®©ç¥ç»ç½‘ç»œèƒ½å­¦ä¹ å¤æ‚å‡½æ•°\n\n\n\n5.4.2 ğŸ”¹ å¸¸è§æ¿€æ´»å‡½æ•°\n\n5.4.2.1 1. Sigmoid\nÏƒ(z) = 1 / (1 + eâ»á¶»)\n\nèŒƒå›´ï¼š(0, 1)\nå¯¼æ•°ï¼šÏƒ'(z) = Ïƒ(z)Â·(1 - Ïƒ(z))\nå›¾å½¢ï¼š\n  1 |         ____\n    |       /\n0.5 |      /\n    |     /\n  0 |____/\n    |___________ z\n   -5  0   5\nä¼˜ç‚¹ï¼š - è¾“å‡ºèŒƒå›´ (0,1)ï¼Œé€‚åˆè¡¨ç¤ºæ¦‚ç‡ - å¹³æ»‘å¯å¯¼\nç¼ºç‚¹ï¼š - æ¢¯åº¦æ¶ˆå¤±ï¼šz å¾ˆå¤§æˆ–å¾ˆå°æ—¶ï¼Œæ¢¯åº¦æ¥è¿‘ 0 - è¾“å‡ºä¸æ˜¯é›¶ä¸­å¿ƒï¼šéƒ½æ˜¯æ­£æ•°ï¼Œå¯¼è‡´æƒé‡æ›´æ–°æ•ˆç‡ä½ - è®¡ç®—é‡å¤§ï¼šæœ‰æŒ‡æ•°è¿ç®—\nä½¿ç”¨åœºæ™¯ï¼š - è¾“å‡ºå±‚ï¼ˆäºŒå…ƒåˆ†ç±»ï¼‰ - ä¸æ¨èåœ¨éšè—å±‚ä½¿ç”¨\n\n\n5.4.2.2 2. Tanh (åŒæ›²æ­£åˆ‡)\ntanh(z) = (eá¶» - eâ»á¶») / (eá¶» + eâ»á¶»)\n        = 2Â·Ïƒ(2z) - 1\n\nèŒƒå›´ï¼š(-1, 1)\nå¯¼æ•°ï¼štanh'(z) = 1 - tanhÂ²(z)\nå›¾å½¢ï¼š\n  1 |         ____\n    |       /\n  0 |      /\n    |     /\n -1 |____/\n    |___________ z\n   -5  0   5\nä¼˜ç‚¹ï¼š - é›¶ä¸­å¿ƒè¾“å‡º - æ¯” Sigmoid å¥½\nç¼ºç‚¹ï¼š - ä»æœ‰æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ - è®¡ç®—é‡å¤§\nä½¿ç”¨åœºæ™¯ï¼š - RNN/LSTM ä¸­å¸¸ç”¨ - éšè—å±‚ï¼ˆä½†ç°åœ¨æ›´æ¨è ReLUï¼‰\n\n\n5.4.2.3 3. ReLU (Rectified Linear Unit) â­\nReLU(z) = max(0, z)\n\n       â§ z,  å¦‚æœ z &gt; 0\n     = â¨\n       â© 0,  å¦‚æœ z â‰¤ 0\n\nå¯¼æ•°ï¼šReLU'(z) = â§ 1, å¦‚æœ z &gt; 0\n                 â© 0, å¦‚æœ z â‰¤ 0\nå›¾å½¢ï¼š\n    |    /\n    |   /\n    |  /\n    | /\n____|/_______ z\n    0\nä¼˜ç‚¹ï¼š - âœ… è®¡ç®—ç®€å•ï¼šä¸æ¶‰åŠæŒ‡æ•°è¿ç®— - âœ… ç¼“è§£æ¢¯åº¦æ¶ˆå¤±ï¼šæ­£åŒºåŸŸæ¢¯åº¦æ’ä¸º1 - âœ… æ”¶æ•›å¿«ï¼šæ¯” Sigmoid/Tanh å¿«å¾ˆå¤š - âœ… ç¨€ç–æ¿€æ´»ï¼šçº¦50%ç¥ç»å…ƒè¢«æ¿€æ´»\nç¼ºç‚¹ï¼š - âŒ Dead ReLUï¼šè´ŸåŒºåŸŸæ¢¯åº¦ä¸º0ï¼Œç¥ç»å…ƒå¯èƒ½â€æ­»äº¡â€ - âŒ è¾“å‡ºä¸æ˜¯é›¶ä¸­å¿ƒ\nä½¿ç”¨åœºæ™¯ï¼š - ğŸŒŸ é»˜è®¤é¦–é€‰ï¼éšè—å±‚çš„æ ‡å‡†é€‰æ‹© - CNN ä¸­å¹¿æ³›ä½¿ç”¨\n\n\n5.4.2.4 4. Leaky ReLU\nLeaky ReLU(z) = max(Î±z, z)\n\n              â§ z,   å¦‚æœ z &gt; 0\n            = â¨\n              â© Î±z,  å¦‚æœ z â‰¤ 0\n\né€šå¸¸ Î± = 0.01\nå›¾å½¢ï¼š\n    |    /\n    |   /\n    |  /\n    | /\n___/|_______ z\n  / 0\nä¼˜ç‚¹ï¼š - è§£å†³ Dead ReLU é—®é¢˜ - è´ŸåŒºåŸŸä»æœ‰å°æ¢¯åº¦\nå˜ç§ï¼š - PReLU (Parametric ReLU)ï¼šÎ± æ˜¯å¯å­¦ä¹ çš„å‚æ•° - ELU (Exponential Linear Unit)\n\n\n5.4.2.5 5. Softmax (è¾“å‡ºå±‚)\nå¯¹äº K ä¸ªç±»åˆ«ï¼š\n\nSoftmax(záµ¢) = e^záµ¢ / Î£â±¼ e^zâ±¼\n\næ€§è´¨ï¼š\n- Î£áµ¢ Softmax(záµ¢) = 1\n- è¾“å‡ºå¯è§£é‡Šä¸ºæ¦‚ç‡\nä½¿ç”¨åœºæ™¯ï¼š - å¤šåˆ†ç±»é—®é¢˜çš„è¾“å‡ºå±‚\n\n\n5.4.2.6 6. å…¶ä»–æ¿€æ´»å‡½æ•°\nSwish (Google 2017):\nSwish(z) = zÂ·Ïƒ(z)\n\nä¼˜ç‚¹ï¼šæ— ç•Œã€å¹³æ»‘ã€éå•è°ƒ\nGELU (Gaussian Error Linear Unit):\nGELU(z) â‰ˆ 0.5z(1 + tanh(âˆš(2/Ï€)(z + 0.044715zÂ³)))\n\nç”¨äº BERTã€GPT\n\n\n\n\n5.4.3 ğŸ“Š æ¿€æ´»å‡½æ•°å¯¹æ¯”\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef sigmoid(z):\n    return 1 / (1 + np.exp(-z))\n\ndef tanh(z):\n    return np.tanh(z)\n\ndef relu(z):\n    return np.maximum(0, z)\n\ndef leaky_relu(z, alpha=0.01):\n    return np.maximum(alpha * z, z)\n\ndef elu(z, alpha=1.0):\n    return np.where(z &gt; 0, z, alpha * (np.exp(z) - 1))\n\n# ç”Ÿæˆæ•°æ®\nz = np.linspace(-5, 5, 1000)\n\n# ç»˜å›¾\nplt.figure(figsize=(15, 10))\n\n# æ¿€æ´»å‡½æ•°\nplt.subplot(2, 2, 1)\nplt.plot(z, sigmoid(z), label='Sigmoid', linewidth=2)\nplt.plot(z, tanh(z), label='Tanh', linewidth=2)\nplt.plot(z, relu(z), label='ReLU', linewidth=2)\nplt.plot(z, leaky_relu(z), label='Leaky ReLU', linewidth=2)\nplt.axhline(y=0, color='k', linestyle='--', alpha=0.3)\nplt.axvline(x=0, color='k', linestyle='--', alpha=0.3)\nplt.xlabel('z')\nplt.ylabel('Activation')\nplt.title('æ¿€æ´»å‡½æ•°')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\n# å¯¼æ•°\nplt.subplot(2, 2, 2)\nsigmoid_derivative = sigmoid(z) * (1 - sigmoid(z))\ntanh_derivative = 1 - tanh(z)**2\nrelu_derivative = (z &gt; 0).astype(float)\nleaky_relu_derivative = np.where(z &gt; 0, 1, 0.01)\n\nplt.plot(z, sigmoid_derivative, label='Sigmoid\\'', linewidth=2)\nplt.plot(z, tanh_derivative, label='Tanh\\'', linewidth=2)\nplt.plot(z, relu_derivative, label='ReLU\\'', linewidth=2)\nplt.plot(z, leaky_relu_derivative, label='Leaky ReLU\\'', linewidth=2)\nplt.axhline(y=0, color='k', linestyle='--', alpha=0.3)\nplt.axvline(x=0, color='k', linestyle='--', alpha=0.3)\nplt.xlabel('z')\nplt.ylabel('Derivative')\nplt.title('æ¿€æ´»å‡½æ•°çš„å¯¼æ•°')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\n# ReLU å˜ç§å¯¹æ¯”\nplt.subplot(2, 2, 3)\nplt.plot(z, relu(z), label='ReLU', linewidth=2)\nplt.plot(z, leaky_relu(z), label='Leaky ReLU', linewidth=2)\nplt.plot(z, elu(z), label='ELU', linewidth=2)\nplt.axhline(y=0, color='k', linestyle='--', alpha=0.3)\nplt.axvline(x=0, color='k', linestyle='--', alpha=0.3)\nplt.xlabel('z')\nplt.ylabel('Activation')\nplt.title('ReLU å˜ç§å¯¹æ¯”')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\n# æ¢¯åº¦æ¶ˆå¤±é—®é¢˜æ¼”ç¤º\nplt.subplot(2, 2, 4)\nz_grad = np.linspace(-10, 10, 1000)\nsigmoid_grad = sigmoid(z_grad) * (1 - sigmoid(z_grad))\ntanh_grad = 1 - tanh(z_grad)**2\nrelu_grad = (z_grad &gt; 0).astype(float)\n\nplt.plot(z_grad, sigmoid_grad, label='Sigmoid', linewidth=2)\nplt.plot(z_grad, tanh_grad, label='Tanh', linewidth=2)\nplt.plot(z_grad, relu_grad, label='ReLU', linewidth=2)\nplt.axhline(y=0, color='k', linestyle='--', alpha=0.3)\nplt.xlabel('z')\nplt.ylabel('Gradient')\nplt.title('æ¢¯åº¦æ¶ˆå¤±é—®é¢˜')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.ylim(-0.1, 1.1)\n\nplt.tight_layout()\nplt.show()\n\n\n5.4.4 ğŸ’¡ æ¿€æ´»å‡½æ•°é€‰æ‹©æŒ‡å—\néšè—å±‚ï¼š\n  â”œâ”€ é»˜è®¤ï¼šReLU â­\n  â”œâ”€ å°è¯•ï¼šLeaky ReLU / PReLU / ELU\n  â””â”€ é¿å…ï¼šSigmoid / Tanh (é™¤éç‰¹æ®Šéœ€æ±‚)\n\nè¾“å‡ºå±‚ï¼š\n  â”œâ”€ äºŒå…ƒåˆ†ç±»ï¼šSigmoid\n  â”œâ”€ å¤šå…ƒåˆ†ç±»ï¼šSoftmax\n  â”œâ”€ å›å½’ï¼šLinear (æ— æ¿€æ´»å‡½æ•°)\n  â””â”€ ç‰¹æ®ŠèŒƒå›´ï¼šTanh (è¾“å‡º[-1,1])",
    "crumbs": [
      "ç¬¬ä¸€éƒ¨åˆ†ï¼šåŸºç¡€ç¯‡",
      "<span class='chapter-number'>5</span>Â  <span class='chapter-title'>ç¬¬å››ç« ï¼šæ·±åº¦ç¥ç»ç½‘ç»œåŸºç¡€ (Deep Neural Networks)</span>"
    ]
  },
  {
    "objectID": "Chapter4.html#æŸå¤±å‡½æ•°-loss-functions",
    "href": "Chapter4.html#æŸå¤±å‡½æ•°-loss-functions",
    "title": "5Â  ç¬¬å››ç« ï¼šæ·±åº¦ç¥ç»ç½‘ç»œåŸºç¡€ (Deep Neural Networks)",
    "section": "5.5 4.4 æŸå¤±å‡½æ•° (Loss Functions)",
    "text": "5.5 4.4 æŸå¤±å‡½æ•° (Loss Functions)\n\n5.5.1 ğŸ¯ å¸¸è§æŸå¤±å‡½æ•°\n\n5.5.1.1 1. å›å½’é—®é¢˜\nMean Squared Error (MSE):\nL = (1/N) Î£(Å·â¿ - yâ¿)Â²\nMean Absolute Error (MAE):\nL = (1/N) Î£|Å·â¿ - yâ¿|\nHuber Loss (ç»“åˆ MSE å’Œ MAE):\n        â§ 0.5Â·(y - Å·)Â²,           if |y - Å·| â‰¤ Î´\nL_Î´(y,Å·)=â¨\n        â© Î´Â·(|y - Å·| - 0.5Â·Î´),   otherwise\n\n\n5.5.1.2 2. äºŒå…ƒåˆ†ç±»\nBinary Cross Entropy:\nL = -(1/N) Î£[yâ¿Â·log(Å·â¿) + (1-yâ¿)Â·log(1-Å·â¿)]\n\n\n5.5.1.3 3. å¤šå…ƒåˆ†ç±»\nCategorical Cross Entropy:\nL = -(1/N) Î£Î£ yáµ¢â¿Â·log(Å·áµ¢â¿)\n           n i\n\nå…¶ä¸­ yáµ¢â¿ æ˜¯ one-hot ç¼–ç ",
    "crumbs": [
      "ç¬¬ä¸€éƒ¨åˆ†ï¼šåŸºç¡€ç¯‡",
      "<span class='chapter-number'>5</span>Â  <span class='chapter-title'>ç¬¬å››ç« ï¼šæ·±åº¦ç¥ç»ç½‘ç»œåŸºç¡€ (Deep Neural Networks)</span>"
    ]
  },
  {
    "objectID": "Chapter4.html#åå‘ä¼ æ’­-backpropagation",
    "href": "Chapter4.html#åå‘ä¼ æ’­-backpropagation",
    "title": "5Â  ç¬¬å››ç« ï¼šæ·±åº¦ç¥ç»ç½‘ç»œåŸºç¡€ (Deep Neural Networks)",
    "section": "5.6 4.5 åå‘ä¼ æ’­ (Backpropagation)",
    "text": "5.6 4.5 åå‘ä¼ æ’­ (Backpropagation)\n\n5.6.1 ğŸ¯ æ ¸å¿ƒæ€æƒ³\n\nåå‘ä¼ æ’­ = é“¾å¼æ³•åˆ™ + ä»åå¾€å‰è®¡ç®—æ¢¯åº¦\n\nç›®æ ‡ï¼šè®¡ç®— âˆ‚L/âˆ‚Wâ½Ë¡â¾ å’Œ âˆ‚L/âˆ‚bâ½Ë¡â¾\n\n\n5.6.2 ğŸ“ é“¾å¼æ³•åˆ™å›é¡¾\nå¦‚æœ y = f(u) ä¸” u = g(x)\nåˆ™ dy/dx = (dy/du)Â·(du/dx)\n\nä¾‹å­ï¼š\ny = (xÂ² + 1)Â³\nä»¤ u = xÂ² + 1, åˆ™ y = uÂ³\n\ndy/dx = (dy/du)Â·(du/dx)\n      = 3uÂ²Â·2x\n      = 3(xÂ²+1)Â²Â·2x\n\n\n5.6.3 ğŸ”„ åå‘ä¼ æ’­æ¨å¯¼\nç½‘ç»œç»“æ„ï¼šè¾“å…¥ â†’ éšè—å±‚ â†’ è¾“å‡ºå±‚\nå‰å‘ä¼ æ’­ï¼š\n  zâ½Â¹â¾ = Wâ½Â¹â¾x + bâ½Â¹â¾\n  aâ½Â¹â¾ = Ïƒ(zâ½Â¹â¾)\n  zâ½Â²â¾ = Wâ½Â²â¾aâ½Â¹â¾ + bâ½Â²â¾\n  aâ½Â²â¾ = Ïƒ(zâ½Â²â¾) = Å·\n\næŸå¤±ï¼š\n  L = (y - Å·)Â²\nåå‘ä¼ æ’­ï¼š\nè¾“å‡ºå±‚ï¼š\nâˆ‚L/âˆ‚Å· = -2(y - Å·)\n\nâˆ‚L/âˆ‚zâ½Â²â¾ = âˆ‚L/âˆ‚Å· Â· âˆ‚Å·/âˆ‚zâ½Â²â¾\n         = âˆ‚L/âˆ‚Å· Â· Ïƒ'(zâ½Â²â¾)\n\nâˆ‚L/âˆ‚Wâ½Â²â¾ = âˆ‚L/âˆ‚zâ½Â²â¾ Â· âˆ‚zâ½Â²â¾/âˆ‚Wâ½Â²â¾\n         = âˆ‚L/âˆ‚zâ½Â²â¾ Â· aâ½Â¹â¾áµ€\n\nâˆ‚L/âˆ‚bâ½Â²â¾ = âˆ‚L/âˆ‚zâ½Â²â¾\néšè—å±‚ï¼š\nâˆ‚L/âˆ‚aâ½Â¹â¾ = (Wâ½Â²â¾)áµ€ Â· âˆ‚L/âˆ‚zâ½Â²â¾\n\nâˆ‚L/âˆ‚zâ½Â¹â¾ = âˆ‚L/âˆ‚aâ½Â¹â¾ âŠ™ Ïƒ'(zâ½Â¹â¾)\n         (âŠ™ è¡¨ç¤ºé€å…ƒç´ ä¹˜æ³•)\n\nâˆ‚L/âˆ‚Wâ½Â¹â¾ = âˆ‚L/âˆ‚zâ½Â¹â¾ Â· xáµ€\n\nâˆ‚L/âˆ‚bâ½Â¹â¾ = âˆ‚L/âˆ‚zâ½Â¹â¾\n\n\n5.6.4 ğŸ’» ä»£ç å®ç°\ndef backward_propagation(X, Y, parameters, cache):\n    \"\"\"\n    åå‘ä¼ æ’­\n\n    å‚æ•°ï¼š\n        X: è¾“å…¥ (n_features, m_samples)\n        Y: çœŸå®æ ‡ç­¾ (1, m_samples)\n        parameters: æƒé‡å’Œåç½®\n        cache: å‰å‘ä¼ æ’­çš„ä¸­é—´å€¼\n\n    è¿”å›ï¼š\n        gradients: æ¢¯åº¦å­—å…¸\n    \"\"\"\n    m = X.shape[1]  # æ ·æœ¬æ•°\n\n    # è·å–å‚æ•°\n    W1 = parameters['W1']\n    W2 = parameters['W2']\n\n    # è·å–å‰å‘ä¼ æ’­çš„å€¼\n    A1 = cache['A1']\n    A2 = cache['A2']\n\n    # è¾“å‡ºå±‚æ¢¯åº¦\n    dZ2 = A2 - Y  # å¯¹äº sigmoid + MSE\n    dW2 = (1/m) * np.dot(dZ2, A1.T)\n    db2 = (1/m) * np.sum(dZ2, axis=1, keepdims=True)\n\n    # éšè—å±‚æ¢¯åº¦\n    dA1 = np.dot(W2.T, dZ2)\n    dZ1 = dA1 * A1 * (1 - A1)  # sigmoid çš„å¯¼æ•°\n    dW1 = (1/m) * np.dot(dZ1, X.T)\n    db1 = (1/m) * np.sum(dZ1, axis=1, keepdims=True)\n\n    gradients = {\n        'dW1': dW1,\n        'db1': db1,\n        'dW2': dW2,\n        'db2': db2\n    }\n\n    return gradients\n\ndef update_parameters(parameters, gradients, learning_rate):\n    \"\"\"\n    æ›´æ–°å‚æ•°\n    \"\"\"\n    parameters['W1'] -= learning_rate * gradients['dW1']\n    parameters['b1'] -= learning_rate * gradients['db1']\n    parameters['W2'] -= learning_rate * gradients['dW2']\n    parameters['b2'] -= learning_rate * gradients['db2']\n\n    return parameters\n\n\n5.6.5 ğŸ” å®Œæ•´è®­ç»ƒå¾ªç¯\ndef train_neural_network(X, Y, hidden_size=4, learning_rate=0.01, epochs=10000):\n    \"\"\"\n    è®­ç»ƒç¥ç»ç½‘ç»œ\n    \"\"\"\n    n_x = X.shape[0]  # è¾“å…¥ç‰¹å¾æ•°\n    n_y = Y.shape[0]  # è¾“å‡ºæ•°\n\n    # åˆå§‹åŒ–å‚æ•°\n    np.random.seed(42)\n    parameters = {\n        'W1': np.random.randn(hidden_size, n_x) * 0.01,\n        'b1': np.zeros((hidden_size, 1)),\n        'W2': np.random.randn(n_y, hidden_size) * 0.01,\n        'b2': np.zeros((n_y, 1))\n    }\n\n    losses = []\n\n    for epoch in range(epochs):\n        # å‰å‘ä¼ æ’­\n        A2, cache = forward_propagation(X, parameters)\n\n        # è®¡ç®—æŸå¤±\n        loss = np.mean((A2 - Y) ** 2)\n        losses.append(loss)\n\n        # åå‘ä¼ æ’­\n        gradients = backward_propagation(X, Y, parameters, cache)\n\n        # æ›´æ–°å‚æ•°\n        parameters = update_parameters(parameters, gradients, learning_rate)\n\n        # æ‰“å°è¿›åº¦\n        if epoch % 1000 == 0:\n            print(f\"Epoch {epoch}: Loss = {loss:.6f}\")\n\n    return parameters, losses\n\n# ç¤ºä¾‹ï¼šè§£å†³ XOR é—®é¢˜\nX = np.array([[0, 0, 1, 1],\n              [0, 1, 0, 1]])\nY = np.array([[0, 1, 1, 0]])\n\nparameters, losses = train_neural_network(\n    X, Y,\n    hidden_size=4,\n    learning_rate=0.5,\n    epochs=10000\n)\n\n# æµ‹è¯•\nA2, _ = forward_propagation(X, parameters)\nprint(\"\\nè¾“å…¥:\")\nprint(X.T)\nprint(\"\\né¢„æµ‹:\")\nprint(A2.T)\nprint(\"\\nçœŸå®æ ‡ç­¾:\")\nprint(Y.T)\nè¾“å‡ºï¼š\nEpoch 0: Loss = 0.250615\nEpoch 1000: Loss = 0.062439\nEpoch 2000: Loss = 0.013152\nEpoch 3000: Loss = 0.005862\nEpoch 4000: Loss = 0.003494\nEpoch 5000: Loss = 0.002388\nEpoch 6000: Loss = 0.001756\nEpoch 7000: Loss = 0.001353\nEpoch 8000: Loss = 0.001081\nEpoch 9000: Loss = 0.000887\n\nè¾“å…¥:\n[[0 0]\n [0 1]\n [1 0]\n [1 1]]\n\né¢„æµ‹:\n[[0.02458917]\n [0.97201347]\n [0.97412658]\n [0.02907213]]\n\nçœŸå®æ ‡ç­¾:\n[[0]\n [1]\n [1]\n [0]]\næˆåŠŸè§£å†³äº† XOR é—®é¢˜ï¼ğŸ‰",
    "crumbs": [
      "ç¬¬ä¸€éƒ¨åˆ†ï¼šåŸºç¡€ç¯‡",
      "<span class='chapter-number'>5</span>Â  <span class='chapter-title'>ç¬¬å››ç« ï¼šæ·±åº¦ç¥ç»ç½‘ç»œåŸºç¡€ (Deep Neural Networks)</span>"
    ]
  },
  {
    "objectID": "Chapter4.html#åˆå§‹åŒ–ç­–ç•¥",
    "href": "Chapter4.html#åˆå§‹åŒ–ç­–ç•¥",
    "title": "5Â  ç¬¬å››ç« ï¼šæ·±åº¦ç¥ç»ç½‘ç»œåŸºç¡€ (Deep Neural Networks)",
    "section": "5.7 4.6 åˆå§‹åŒ–ç­–ç•¥",
    "text": "5.7 4.6 åˆå§‹åŒ–ç­–ç•¥\n\n5.7.1 ğŸ¤” ä¸ºä»€ä¹ˆåˆå§‹åŒ–é‡è¦ï¼Ÿ\nå…¨é›¶åˆå§‹åŒ–ï¼š\nW = np.zeros((n_out, n_in))\né—®é¢˜ï¼šæ‰€æœ‰ç¥ç»å…ƒå­¦åˆ°ç›¸åŒçš„ç‰¹å¾ï¼ˆå¯¹ç§°æ€§é—®é¢˜ï¼‰\néšæœºåˆå§‹åŒ–ï¼š\nW = np.random.randn(n_out, n_in)\né—®é¢˜ï¼šæ–¹å·®å¯èƒ½å¤ªå¤§æˆ–å¤ªå°\n\n\n5.7.2 âœ… å¥½çš„åˆå§‹åŒ–æ–¹æ³•\n\n5.7.2.1 1. Xavier åˆå§‹åŒ– (Glorot)\né€‚ç”¨äºï¼šSigmoid / Tanh\nW = np.random.randn(n_out, n_in) * np.sqrt(1 / n_in)\n\n# æˆ–\n\nW = np.random.randn(n_out, n_in) * np.sqrt(2 / (n_in + n_out))\nåŸç†ï¼šä¿æŒæ–¹å·®åœ¨å„å±‚ä¹‹é—´å¹³è¡¡\n\n\n5.7.2.2 2. He åˆå§‹åŒ–\né€‚ç”¨äºï¼šReLU åŠå…¶å˜ç§\nW = np.random.randn(n_out, n_in) * np.sqrt(2 / n_in)\nåŸç†ï¼šè€ƒè™‘ ReLU ä¼šâ€æ€æ­»â€ä¸€åŠç¥ç»å…ƒ\n\n\n\n5.7.3 ğŸ’» å®ç°\ndef initialize_parameters_xavier(layer_dims):\n    \"\"\"\n    Xavier åˆå§‹åŒ–\n\n    å‚æ•°ï¼š\n        layer_dims: åˆ—è¡¨ï¼Œæ¯å±‚çš„ç¥ç»å…ƒæ•°é‡\n                   ä¾‹å¦‚ [784, 128, 64, 10]\n    \"\"\"\n    parameters = {}\n    L = len(layer_dims)\n\n    for l in range(1, L):\n        parameters[f'W{l}'] = np.random.randn(\n            layer_dims[l],\n            layer_dims[l-1]\n        ) * np.sqrt(1 / layer_dims[l-1])\n\n        parameters[f'b{l}'] = np.zeros((layer_dims[l], 1))\n\n    return parameters\n\ndef initialize_parameters_he(layer_dims):\n    \"\"\"\n    He åˆå§‹åŒ–\n    \"\"\"\n    parameters = {}\n    L = len(layer_dims)\n\n    for l in range(1, L):\n        parameters[f'W{l}'] = np.random.randn(\n            layer_dims[l],\n            layer_dims[l-1]\n        ) * np.sqrt(2 / layer_dims[l-1])\n\n        parameters[f'b{l}'] = np.zeros((layer_dims[l], 1))\n\n    return parameters",
    "crumbs": [
      "ç¬¬ä¸€éƒ¨åˆ†ï¼šåŸºç¡€ç¯‡",
      "<span class='chapter-number'>5</span>Â  <span class='chapter-title'>ç¬¬å››ç« ï¼šæ·±åº¦ç¥ç»ç½‘ç»œåŸºç¡€ (Deep Neural Networks)</span>"
    ]
  },
  {
    "objectID": "Chapter4.html#æ¢¯åº¦æ£€éªŒ-gradient-checking",
    "href": "Chapter4.html#æ¢¯åº¦æ£€éªŒ-gradient-checking",
    "title": "5Â  ç¬¬å››ç« ï¼šæ·±åº¦ç¥ç»ç½‘ç»œåŸºç¡€ (Deep Neural Networks)",
    "section": "5.8 4.7 æ¢¯åº¦æ£€éªŒ (Gradient Checking)",
    "text": "5.8 4.7 æ¢¯åº¦æ£€éªŒ (Gradient Checking)\n\n5.8.1 ğŸ¯ ç›®çš„\néªŒè¯åå‘ä¼ æ’­çš„å®ç°æ˜¯å¦æ­£ç¡®\n\n\n5.8.2 ğŸ“ æ•°å€¼æ¢¯åº¦\nf'(Î¸) â‰ˆ [f(Î¸ + Îµ) - f(Î¸ - Îµ)] / (2Îµ)\n\né€šå¸¸ Îµ = 10â»â·\n\n\n5.8.3 ğŸ’» å®ç°\ndef gradient_check(parameters, gradients, X, Y, epsilon=1e-7):\n    \"\"\"\n    æ¢¯åº¦æ£€éªŒ\n\n    è¿”å›ï¼š\n        difference: æ•°å€¼æ¢¯åº¦å’Œè§£ææ¢¯åº¦çš„ç›¸å¯¹å·®å¼‚\n    \"\"\"\n    # å°†å‚æ•°å±•å¹³ä¸ºå‘é‡\n    parameters_values, _ = dictionary_to_vector(parameters)\n    grad = gradients_to_vector(gradients)\n    num_parameters = parameters_values.shape[0]\n\n    # è®¡ç®—æ•°å€¼æ¢¯åº¦\n    J_plus = np.zeros((num_parameters, 1))\n    J_minus = np.zeros((num_parameters, 1))\n    gradapprox = np.zeros((num_parameters, 1))\n\n    for i in range(num_parameters):\n        # è®¡ç®— J_plus[i]\n        thetaplus = np.copy(parameters_values)\n        thetaplus[i][0] += epsilon\n        J_plus[i] = forward_propagation_cost(X, Y, vector_to_dictionary(thetaplus))\n\n        # è®¡ç®— J_minus[i]\n        thetaminus = np.copy(parameters_values)\n        thetaminus[i][0] -= epsilon\n        J_minus[i] = forward_propagation_cost(X, Y, vector_to_dictionary(thetaminus))\n\n        # è®¡ç®—æ•°å€¼æ¢¯åº¦\n        gradapprox[i] = (J_plus[i] - J_minus[i]) / (2 * epsilon)\n\n    # è®¡ç®—ç›¸å¯¹å·®å¼‚\n    numerator = np.linalg.norm(grad - gradapprox)\n    denominator = np.linalg.norm(grad) + np.linalg.norm(gradapprox)\n    difference = numerator / denominator\n\n    if difference &gt; 2e-7:\n        print(f\"âš ï¸  æ¢¯åº¦æ£€éªŒå¤±è´¥ï¼å·®å¼‚ = {difference}\")\n    else:\n        print(f\"âœ… æ¢¯åº¦æ£€éªŒé€šè¿‡ï¼å·®å¼‚ = {difference}\")\n\n    return difference",
    "crumbs": [
      "ç¬¬ä¸€éƒ¨åˆ†ï¼šåŸºç¡€ç¯‡",
      "<span class='chapter-number'>5</span>Â  <span class='chapter-title'>ç¬¬å››ç« ï¼šæ·±åº¦ç¥ç»ç½‘ç»œåŸºç¡€ (Deep Neural Networks)</span>"
    ]
  },
  {
    "objectID": "Chapter4.html#ä½¿ç”¨æ·±åº¦å­¦ä¹ æ¡†æ¶",
    "href": "Chapter4.html#ä½¿ç”¨æ·±åº¦å­¦ä¹ æ¡†æ¶",
    "title": "5Â  ç¬¬å››ç« ï¼šæ·±åº¦ç¥ç»ç½‘ç»œåŸºç¡€ (Deep Neural Networks)",
    "section": "5.9 4.8 ä½¿ç”¨æ·±åº¦å­¦ä¹ æ¡†æ¶",
    "text": "5.9 4.8 ä½¿ç”¨æ·±åº¦å­¦ä¹ æ¡†æ¶\n\n5.9.1 ğŸ”¥ PyTorch å®ç°\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# 1. å®šä¹‰ç½‘ç»œ\nclass NeuralNetwork(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(NeuralNetwork, self).__init__()\n        self.fc1 = nn.Linear(input_size, hidden_size)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(hidden_size, output_size)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        x = self.sigmoid(x)\n        return x\n\n# 2. å‡†å¤‡æ•°æ®\nX_train = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32)\ny_train = torch.tensor([[0], [1], [1], [0]], dtype=torch.float32)\n\n# 3. åˆ›å»ºæ¨¡å‹\nmodel = NeuralNetwork(input_size=2, hidden_size=4, output_size=1)\n\n# 4. å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=0.1)\n\n# 5. è®­ç»ƒ\nepochs = 5000\nfor epoch in range(epochs):\n    # å‰å‘ä¼ æ’­\n    outputs = model(X_train)\n    loss = criterion(outputs, y_train)\n\n    # åå‘ä¼ æ’­\n    optimizer.zero_grad()  # æ¸…é›¶æ¢¯åº¦\n    loss.backward()        # è®¡ç®—æ¢¯åº¦\n    optimizer.step()       # æ›´æ–°å‚æ•°\n\n    if epoch % 500 == 0:\n        print(f'Epoch [{epoch}/{epochs}], Loss: {loss.item():.6f}')\n\n# 6. æµ‹è¯•\nwith torch.no_grad():\n    predictions = model(X_train)\n    print(\"\\né¢„æµ‹ç»“æœ:\")\n    print(predictions.numpy())\n\n\n5.9.2 ğŸŒ TensorFlow/Keras å®ç°\nimport tensorflow as tf\nfrom tensorflow import keras\nimport numpy as np\n\n# 1. å‡†å¤‡æ•°æ®\nX_train = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\ny_train = np.array([[0], [1], [1], [0]])\n\n# 2. æ„å»ºæ¨¡å‹\nmodel = keras.Sequential([\n    keras.layers.Dense(4, activation='relu', input_shape=(2,)),\n    keras.layers.Dense(1, activation='sigmoid')\n])\n\n# 3. ç¼–è¯‘æ¨¡å‹\nmodel.compile(\n    optimizer='adam',\n    loss='mse',\n    metrics=['accuracy']\n)\n\n# 4. è®­ç»ƒ\nhistory = model.fit(\n    X_train, y_train,\n    epochs=5000,\n    verbose=0  # ä¸æ‰“å°è®­ç»ƒè¿‡ç¨‹\n)\n\n# 5. è¯„ä¼°\nloss, accuracy = model.evaluate(X_train, y_train, verbose=0)\nprint(f'Loss: {loss:.6f}')\n\n# 6. é¢„æµ‹\npredictions = model.predict(X_train)\nprint(\"\\né¢„æµ‹ç»“æœ:\")\nprint(predictions)\n\n# 7. æŸ¥çœ‹æ¨¡å‹ç»“æ„\nmodel.summary()",
    "crumbs": [
      "ç¬¬ä¸€éƒ¨åˆ†ï¼šåŸºç¡€ç¯‡",
      "<span class='chapter-number'>5</span>Â  <span class='chapter-title'>ç¬¬å››ç« ï¼šæ·±åº¦ç¥ç»ç½‘ç»œåŸºç¡€ (Deep Neural Networks)</span>"
    ]
  },
  {
    "objectID": "Chapter4.html#å®æˆ˜mnist-æ‰‹å†™æ•°å­—è¯†åˆ«æ·±åº¦ç½‘ç»œ",
    "href": "Chapter4.html#å®æˆ˜mnist-æ‰‹å†™æ•°å­—è¯†åˆ«æ·±åº¦ç½‘ç»œ",
    "title": "5Â  ç¬¬å››ç« ï¼šæ·±åº¦ç¥ç»ç½‘ç»œåŸºç¡€ (Deep Neural Networks)",
    "section": "5.10 4.9 å®æˆ˜ï¼šMNIST æ‰‹å†™æ•°å­—è¯†åˆ«ï¼ˆæ·±åº¦ç½‘ç»œï¼‰",
    "text": "5.10 4.9 å®æˆ˜ï¼šMNIST æ‰‹å†™æ•°å­—è¯†åˆ«ï¼ˆæ·±åº¦ç½‘ç»œï¼‰\n\n5.10.1 ğŸ’» å®Œæ•´å®ç°\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\nimport matplotlib.pyplot as plt\n\n# 1. æ•°æ®åŠ è½½å’Œé¢„å¤„ç†\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.1307,), (0.3081,))  # MNIST çš„å‡å€¼å’Œæ ‡å‡†å·®\n])\n\ntrain_dataset = datasets.MNIST(\n    root='./data',\n    train=True,\n    download=True,\n    transform=transform\n)\n\ntest_dataset = datasets.MNIST(\n    root='./data',\n    train=False,\n    download=True,\n    transform=transform\n)\n\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n\n# 2. å®šä¹‰æ·±åº¦ç¥ç»ç½‘ç»œ\nclass DeepNN(nn.Module):\n    def __init__(self):\n        super(DeepNN, self).__init__()\n        self.fc1 = nn.Linear(28*28, 512)\n        self.fc2 = nn.Linear(512, 256)\n        self.fc3 = nn.Linear(256, 128)\n        self.fc4 = nn.Linear(128, 10)\n\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(0.2)\n\n    def forward(self, x):\n        x = x.view(-1, 28*28)  # å±•å¹³\n\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.dropout(x)\n\n        x = self.fc2(x)\n        x = self.relu(x)\n        x = self.dropout(x)\n\n        x = self.fc3(x)\n        x = self.relu(x)\n        x = self.dropout(x)\n\n        x = self.fc4(x)\n        return x\n\n# 3. åˆå§‹åŒ–æ¨¡å‹\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = DeepNN().to(device)\n\n# 4. å®šä¹‰æŸå¤±å’Œä¼˜åŒ–å™¨\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# 5. è®­ç»ƒå‡½æ•°\ndef train(model, device, train_loader, optimizer, epoch):\n    model.train()\n    train_loss = 0\n    correct = 0\n\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n\n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n\n        train_loss += loss.item()\n        pred = output.argmax(dim=1, keepdim=True)\n        correct += pred.eq(target.view_as(pred)).sum().item()\n\n    train_loss /= len(train_loader)\n    accuracy = 100. * correct / len(train_loader.dataset)\n\n    print(f'Epoch: {epoch}, Loss: {train_loss:.4f}, Accuracy: {accuracy:.2f}%')\n    return train_loss, accuracy\n\n# 6. æµ‹è¯•å‡½æ•°\ndef test(model, device, test_loader):\n    model.eval()\n    test_loss = 0\n    correct = 0\n\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            test_loss += criterion(output, target).item()\n            pred = output.argmax(dim=1, keepdim=True)\n            correct += pred.eq(target.view_as(pred)).sum().item()\n\n    test_loss /= len(test_loader)\n    accuracy = 100. * correct / len(test_loader.dataset)\n\n    print(f'Test Loss: {test_loss:.4f}, Accuracy: {accuracy:.2f}%\\n')\n    return test_loss, accuracy\n\n# 7. è®­ç»ƒæ¨¡å‹\nepochs = 10\ntrain_losses, train_accs = [], []\ntest_losses, test_accs = [], []\n\nfor epoch in range(1, epochs + 1):\n    train_loss, train_acc = train(model, device, train_loader, optimizer, epoch)\n    test_loss, test_acc = test(model, device, test_loader)\n\n    train_losses.append(train_loss)\n    train_accs.append(train_acc)\n    test_losses.append(test_loss)\n    test_accs.append(test_acc)\n\n# 8. å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n\nax1.plot(train_losses, label='Train Loss')\nax1.plot(test_losses, label='Test Loss')\nax1.set_xlabel('Epoch')\nax1.set_ylabel('Loss')\nax1.set_title('Training and Test Loss')\nax1.legend()\nax1.grid(True)\n\nax2.plot(train_accs, label='Train Accuracy')\nax2.plot(test_accs, label='Test Accuracy')\nax2.set_xlabel('Epoch')\nax2.set_ylabel('Accuracy (%)')\nax2.set_title('Training and Test Accuracy')\nax2.legend()\nax2.grid(True)\n\nplt.tight_layout()\nplt.show()\n\n# 9. å¯è§†åŒ–ä¸€äº›é¢„æµ‹ç»“æœ\nmodel.eval()\nwith torch.no_grad():\n    data, target = next(iter(test_loader))\n    data, target = data.to(device), target.to(device)\n    output = model(data)\n    pred = output.argmax(dim=1)\n\n    # æ˜¾ç¤ºå‰16ä¸ªæ ·æœ¬\n    fig, axes = plt.subplots(4, 4, figsize=(10, 10))\n    for i, ax in enumerate(axes.flat):\n        image = data[i].cpu().squeeze()\n        true_label = target[i].item()\n        pred_label = pred[i].item()\n\n        ax.imshow(image, cmap='gray')\n        color = 'green' if true_label == pred_label else 'red'\n        ax.set_title(f'True: {true_label}, Pred: {pred_label}', color=color)\n        ax.axis('off')\n\n    plt.tight_layout()\n    plt.show()",
    "crumbs": [
      "ç¬¬ä¸€éƒ¨åˆ†ï¼šåŸºç¡€ç¯‡",
      "<span class='chapter-number'>5</span>Â  <span class='chapter-title'>ç¬¬å››ç« ï¼šæ·±åº¦ç¥ç»ç½‘ç»œåŸºç¡€ (Deep Neural Networks)</span>"
    ]
  },
  {
    "objectID": "Chapter4.html#æœ¬ç« ä½œä¸š",
    "href": "Chapter4.html#æœ¬ç« ä½œä¸š",
    "title": "5Â  ç¬¬å››ç« ï¼šæ·±åº¦ç¥ç»ç½‘ç»œåŸºç¡€ (Deep Neural Networks)",
    "section": "5.11 ğŸ“ æœ¬ç« ä½œä¸š",
    "text": "5.11 ğŸ“ æœ¬ç« ä½œä¸š\n\n5.11.1 ä½œä¸š 1ï¼šç†è®ºé¢˜\n\næ¿€æ´»å‡½æ•°é€‰æ‹©\n\nä¸ºä»€ä¹ˆ ReLU æ¯” Sigmoid æ›´å¸¸ç”¨ï¼Ÿ\nä»€ä¹ˆæ˜¯ â€œDead ReLUâ€ é—®é¢˜ï¼Ÿå¦‚ä½•è§£å†³ï¼Ÿ\nåœ¨ä»€ä¹ˆæƒ…å†µä¸‹ä»ç„¶ä½¿ç”¨ Sigmoidï¼Ÿ\n\nåå‘ä¼ æ’­ç†è§£\n\nç”¨è‡ªå·±çš„è¯è§£é‡Šåå‘ä¼ æ’­\nä¸ºä»€ä¹ˆå«â€åå‘â€ä¼ æ’­ï¼Ÿ\nç”»å‡ºä¸€ä¸ª3å±‚ç½‘ç»œçš„è®¡ç®—å›¾\n\nåˆå§‹åŒ–ç­–ç•¥\n\nä¸ºä»€ä¹ˆä¸èƒ½å…¨é›¶åˆå§‹åŒ–ï¼Ÿ\nXavier å’Œ He åˆå§‹åŒ–çš„åŒºåˆ«ï¼Ÿ\nåç½®éœ€è¦ç‰¹æ®Šåˆå§‹åŒ–å—ï¼Ÿ\n\n\n\n\n5.11.2 ä½œä¸š 2ï¼šç¼–ç¨‹å®è·µ\n\n5.11.2.1 ä»»åŠ¡ 1ï¼šä»é›¶å®ç°å¤šå±‚ç½‘ç»œ\n# å®ç°ä¸€ä¸ªLå±‚å…¨è¿æ¥ç½‘ç»œ\nclass DeepNeuralNetwork:\n    def __init__(self, layer_dims):\n        \"\"\"\n        layer_dims: æ¯å±‚çš„ç¥ç»å…ƒæ•°é‡\n                   ä¾‹å¦‚ [784, 128, 64, 10]\n        \"\"\"\n        pass\n\n    def forward(self, X):\n        \"\"\"å‰å‘ä¼ æ’­\"\"\"\n        pass\n\n    def backward(self, X, Y):\n        \"\"\"åå‘ä¼ æ’­\"\"\"\n        pass\n\n    def train(self, X, Y, epochs, learning_rate):\n        \"\"\"è®­ç»ƒ\"\"\"\n        pass\n\n# TODO:\n# 1. æ”¯æŒä»»æ„å±‚æ•°\n# 2. æ”¯æŒä¸åŒæ¿€æ´»å‡½æ•°ï¼ˆReLU, Sigmoid, Tanhï¼‰\n# 3. å®ç°æ¢¯åº¦æ£€éªŒ\n# 4. åœ¨ MNIST ä¸Šæµ‹è¯•\n\n\n5.11.2.2 ä»»åŠ¡ 2ï¼šæ¿€æ´»å‡½æ•°å¯¹æ¯”å®éªŒ\n# åœ¨ç›¸åŒæ•°æ®é›†ä¸Šå¯¹æ¯”ä¸åŒæ¿€æ´»å‡½æ•°çš„æ•ˆæœ\n# 1. Sigmoid\n# 2. Tanh\n# 3. ReLU\n# 4. Leaky ReLU\n\n# è®°å½•ï¼š\n# - è®­ç»ƒé€Ÿåº¦ï¼ˆè¾¾åˆ°90%å‡†ç¡®ç‡éœ€è¦çš„epochæ•°ï¼‰\n# - æœ€ç»ˆå‡†ç¡®ç‡\n# - è®­ç»ƒç¨³å®šæ€§\n\n# ç”»å‡ºè®­ç»ƒæ›²çº¿å¯¹æ¯”å›¾\n\n\n5.11.2.3 ä»»åŠ¡ 3ï¼šæ·±åº¦ç½‘ç»œå®éªŒ\n# åœ¨ Fashion-MNIST ä¸Šå¯¹æ¯”ä¸åŒæ·±åº¦çš„ç½‘ç»œ\n# 1. 2å±‚ï¼š784 â†’ 128 â†’ 10\n# 2. 3å±‚ï¼š784 â†’ 256 â†’ 128 â†’ 10\n# 3. 4å±‚ï¼š784 â†’ 512 â†’ 256 â†’ 128 â†’ 10\n# 4. 5å±‚ï¼š784 â†’ 512 â†’ 256 â†’ 128 â†’ 64 â†’ 10\n\n# åˆ†æï¼š\n# - æ˜¯å¦è¶Šæ·±è¶Šå¥½ï¼Ÿ\n# - è§‚å¯Ÿæ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸ç°è±¡\n# - å°è¯•ä¸åŒçš„åˆå§‹åŒ–æ–¹æ³•",
    "crumbs": [
      "ç¬¬ä¸€éƒ¨åˆ†ï¼šåŸºç¡€ç¯‡",
      "<span class='chapter-number'>5</span>Â  <span class='chapter-title'>ç¬¬å››ç« ï¼šæ·±åº¦ç¥ç»ç½‘ç»œåŸºç¡€ (Deep Neural Networks)</span>"
    ]
  },
  {
    "objectID": "Chapter4.html#æœ¬ç« å…³é”®æ¦‚å¿µ",
    "href": "Chapter4.html#æœ¬ç« å…³é”®æ¦‚å¿µ",
    "title": "5Â  ç¬¬å››ç« ï¼šæ·±åº¦ç¥ç»ç½‘ç»œåŸºç¡€ (Deep Neural Networks)",
    "section": "5.12 ğŸ”‘ æœ¬ç« å…³é”®æ¦‚å¿µ",
    "text": "5.12 ğŸ”‘ æœ¬ç« å…³é”®æ¦‚å¿µ\n\n\n\næ¦‚å¿µ\nè¯´æ˜\n\n\n\n\nç¥ç»ç½‘ç»œ\nå¤šå±‚æ„ŸçŸ¥æœºçš„å †å \n\n\nå‰å‘ä¼ æ’­\nä»è¾“å…¥è®¡ç®—åˆ°è¾“å‡º\n\n\nåå‘ä¼ æ’­\nç”¨é“¾å¼æ³•åˆ™è®¡ç®—æ¢¯åº¦\n\n\næ¿€æ´»å‡½æ•°\nå¼•å…¥éçº¿æ€§\n\n\nReLU\næœ€å¸¸ç”¨çš„æ¿€æ´»å‡½æ•°\n\n\nSigmoid\nè¾“å‡ºå±‚ï¼ˆäºŒå…ƒåˆ†ç±»ï¼‰\n\n\nSoftmax\nè¾“å‡ºå±‚ï¼ˆå¤šå…ƒåˆ†ç±»ï¼‰\n\n\nXavieråˆå§‹åŒ–\né€‚ç”¨äº Sigmoid/Tanh\n\n\nHeåˆå§‹åŒ–\né€‚ç”¨äº ReLU\n\n\næ¢¯åº¦æ£€éªŒ\néªŒè¯åå‘ä¼ æ’­æ­£ç¡®æ€§",
    "crumbs": [
      "ç¬¬ä¸€éƒ¨åˆ†ï¼šåŸºç¡€ç¯‡",
      "<span class='chapter-number'>5</span>Â  <span class='chapter-title'>ç¬¬å››ç« ï¼šæ·±åº¦ç¥ç»ç½‘ç»œåŸºç¡€ (Deep Neural Networks)</span>"
    ]
  },
  {
    "objectID": "Chapter4.html#ä¸‹ä¸€ç« é¢„å‘Š",
    "href": "Chapter4.html#ä¸‹ä¸€ç« é¢„å‘Š",
    "title": "5Â  ç¬¬å››ç« ï¼šæ·±åº¦ç¥ç»ç½‘ç»œåŸºç¡€ (Deep Neural Networks)",
    "section": "5.13 ğŸ¯ ä¸‹ä¸€ç« é¢„å‘Š",
    "text": "5.13 ğŸ¯ ä¸‹ä¸€ç« é¢„å‘Š\nç¬¬äº”ç« ï¼šä¼˜åŒ–ç®—æ³•ä¸è®­ç»ƒæŠ€å·§ - Mini-batch æ¢¯åº¦ä¸‹é™ - åŠ¨é‡ (Momentum) - RMSprop, Adam - Learning Rate Scheduling - Batch Normalization - Dropout - æ­£åˆ™åŒ–æŠ€æœ¯ - è°ƒå‚æŠ€å·§",
    "crumbs": [
      "ç¬¬ä¸€éƒ¨åˆ†ï¼šåŸºç¡€ç¯‡",
      "<span class='chapter-number'>5</span>Â  <span class='chapter-title'>ç¬¬å››ç« ï¼šæ·±åº¦ç¥ç»ç½‘ç»œåŸºç¡€ (Deep Neural Networks)</span>"
    ]
  },
  {
    "objectID": "Chapter5.html",
    "href": "Chapter5.html",
    "title": "6Â  ç¬¬äº”ç« ï¼šä¼˜åŒ–ç®—æ³•ä¸è®­ç»ƒæŠ€å·§ (Optimization & Training Tricks)",
    "section": "",
    "text": "6.1 ğŸ“Œ ç« èŠ‚ç›®æ ‡",
    "crumbs": [
      "ç¬¬äºŒéƒ¨åˆ†ï¼šè¿›é˜¶ç¯‡",
      "<span class='chapter-number'>6</span>Â  <span class='chapter-title'>ç¬¬äº”ç« ï¼šä¼˜åŒ–ç®—æ³•ä¸è®­ç»ƒæŠ€å·§ (Optimization & Training Tricks)</span>"
    ]
  },
  {
    "objectID": "Chapter5.html#ç« èŠ‚ç›®æ ‡",
    "href": "Chapter5.html#ç« èŠ‚ç›®æ ‡",
    "title": "6Â  ç¬¬äº”ç« ï¼šä¼˜åŒ–ç®—æ³•ä¸è®­ç»ƒæŠ€å·§ (Optimization & Training Tricks)",
    "section": "",
    "text": "ç†è§£ä¸åŒæ¢¯åº¦ä¸‹é™å˜ç§çš„åŸç†\næŒæ¡ç°ä»£ä¼˜åŒ–ç®—æ³•ï¼ˆMomentumã€Adamç­‰ï¼‰\nå­¦ä¹ æ­£åˆ™åŒ–æŠ€æœ¯é˜²æ­¢è¿‡æ‹Ÿåˆ\näº†è§£æ‰¹å½’ä¸€åŒ–å’Œå­¦ä¹ ç‡è°ƒåº¦\næŒæ¡æ·±åº¦å­¦ä¹ è®­ç»ƒçš„å®ç”¨æŠ€å·§",
    "crumbs": [
      "ç¬¬äºŒéƒ¨åˆ†ï¼šè¿›é˜¶ç¯‡",
      "<span class='chapter-number'>6</span>Â  <span class='chapter-title'>ç¬¬äº”ç« ï¼šä¼˜åŒ–ç®—æ³•ä¸è®­ç»ƒæŠ€å·§ (Optimization & Training Tricks)</span>"
    ]
  },
  {
    "objectID": "Chapter5.html#æ¢¯åº¦ä¸‹é™çš„å˜ç§",
    "href": "Chapter5.html#æ¢¯åº¦ä¸‹é™çš„å˜ç§",
    "title": "6Â  ç¬¬äº”ç« ï¼šä¼˜åŒ–ç®—æ³•ä¸è®­ç»ƒæŠ€å·§ (Optimization & Training Tricks)",
    "section": "6.2 5.1 æ¢¯åº¦ä¸‹é™çš„å˜ç§",
    "text": "6.2 5.1 æ¢¯åº¦ä¸‹é™çš„å˜ç§\n\n6.2.1 ğŸ”„ ä¸‰ç§æ¢¯åº¦ä¸‹é™\n\n6.2.1.1 1. Batch Gradient Descent (æ‰¹é‡æ¢¯åº¦ä¸‹é™)\næ¯æ¬¡ä½¿ç”¨å…¨éƒ¨è®­ç»ƒæ•°æ®\nfor epoch in range(epochs):\n    # ä½¿ç”¨æ‰€æœ‰æ•°æ®è®¡ç®—æ¢¯åº¦\n    gradients = compute_gradients(X_train, y_train, parameters)\n    parameters = update_parameters(parameters, gradients, lr)\nä¼˜ç‚¹ï¼š - âœ… æ”¶æ•›ç¨³å®š - âœ… å¯ä»¥åˆ©ç”¨çŸ©é˜µè¿ç®—åŠ é€Ÿ\nç¼ºç‚¹ï¼š - âŒ æ•°æ®é‡å¤§æ—¶è®¡ç®—æ…¢ - âŒ å†…å­˜å ç”¨å¤§ - âŒ å®¹æ˜“é™·å…¥å±€éƒ¨æœ€ä¼˜\n\n\n\n6.2.1.2 2. Stochastic Gradient Descent (éšæœºæ¢¯åº¦ä¸‹é™ï¼ŒSGD)\næ¯æ¬¡ä½¿ç”¨ä¸€ä¸ªæ ·æœ¬\nfor epoch in range(epochs):\n    # éšæœºæ‰“ä¹±æ•°æ®\n    indices = np.random.permutation(len(X_train))\n\n    for i in indices:\n        # æ¯æ¬¡ç”¨ä¸€ä¸ªæ ·æœ¬\n        gradients = compute_gradients(X_train[i:i+1], y_train[i:i+1], parameters)\n        parameters = update_parameters(parameters, gradients, lr)\nä¼˜ç‚¹ï¼š - âœ… æ›´æ–°é¢‘ç¹ï¼Œæ”¶æ•›å¿« - âœ… å¯ä»¥é€ƒç¦»å±€éƒ¨æœ€ä¼˜ - âœ… å¯ä»¥åœ¨çº¿å­¦ä¹ \nç¼ºç‚¹ï¼š - âŒ æ³¢åŠ¨å¤§ï¼Œä¸ç¨³å®š - âŒ éš¾ä»¥å¹¶è¡ŒåŒ– - âŒ å¯èƒ½ä¸æ”¶æ•›\nLoss æ›²çº¿å¯¹æ¯”ï¼š\nBatch GD:        SGD:\nLoss             Loss\n â†“                â†“\n |\\                |  *\n | \\               | * *\n |  \\              |*   *\n |   \\___          | *   *\n |_______â†’        |___*___â†’\n   Epoch            Epoch\n (å¹³æ»‘ä¸‹é™)        (éœ‡è¡ä¸‹é™)\n\n\n\n6.2.1.3 3. Mini-batch Gradient Descent (å°æ‰¹é‡æ¢¯åº¦ä¸‹é™) â­\næ¯æ¬¡ä½¿ç”¨ä¸€å°æ‰¹æ•°æ®ï¼ˆé€šå¸¸ 32-256ï¼‰\nbatch_size = 64\n\nfor epoch in range(epochs):\n    # éšæœºæ‰“ä¹±\n    indices = np.random.permutation(len(X_train))\n\n    # åˆ†æ‰¹å¤„ç†\n    for i in range(0, len(X_train), batch_size):\n        batch_indices = indices[i:i+batch_size]\n        X_batch = X_train[batch_indices]\n        y_batch = y_train[batch_indices]\n\n        # è®¡ç®—æ¢¯åº¦å’Œæ›´æ–°\n        gradients = compute_gradients(X_batch, y_batch, parameters)\n        parameters = update_parameters(parameters, gradients, lr)\nä¼˜ç‚¹ï¼š - âœ… å¹³è¡¡äº†é€Ÿåº¦å’Œç¨³å®šæ€§ - âœ… å¯ä»¥åˆ©ç”¨ GPU å¹¶è¡Œ - âœ… æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›\nç¼ºç‚¹ï¼š - éœ€è¦è°ƒæ•´ batch_size\nBatch Size é€‰æ‹©ï¼š\nå° batch (16-32):\n  + æ³›åŒ–èƒ½åŠ›å¼º\n  + é€‚åˆå°æ•°æ®é›†\n  - è®­ç»ƒä¸ç¨³å®š\n  - é€Ÿåº¦æ…¢\n\nå¤§ batch (256-512):\n  + è®­ç»ƒç¨³å®š\n  + å……åˆ†åˆ©ç”¨ GPU\n  + é€Ÿåº¦å¿«\n  - å¯èƒ½è¿‡æ‹Ÿåˆ\n  - æ³›åŒ–èƒ½åŠ›è¾ƒå¼±\n\nå¸¸ç”¨: 32, 64, 128\n\n\n\n\n6.2.2 ğŸ’» å®ç° Mini-batch\ndef create_mini_batches(X, y, batch_size):\n    \"\"\"\n    åˆ›å»º mini-batches\n\n    è¿”å›ï¼š\n        mini_batches: åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ (X_batch, y_batch)\n    \"\"\"\n    m = X.shape[0]\n    mini_batches = []\n\n    # éšæœºæ‰“ä¹±\n    permutation = np.random.permutation(m)\n    shuffled_X = X[permutation]\n    shuffled_y = y[permutation]\n\n    # åˆ†æ‰¹\n    num_complete_batches = m // batch_size\n\n    for k in range(num_complete_batches):\n        X_batch = shuffled_X[k*batch_size:(k+1)*batch_size]\n        y_batch = shuffled_y[k*batch_size:(k+1)*batch_size]\n        mini_batches.append((X_batch, y_batch))\n\n    # å¤„ç†å‰©ä½™çš„æ•°æ®\n    if m % batch_size != 0:\n        X_batch = shuffled_X[num_complete_batches*batch_size:]\n        y_batch = shuffled_y[num_complete_batches*batch_size:]\n        mini_batches.append((X_batch, y_batch))\n\n    return mini_batches\n\n# ä½¿ç”¨\ndef train_with_mini_batch(X, y, parameters, epochs, batch_size, learning_rate):\n    \"\"\"ä½¿ç”¨ mini-batch è®­ç»ƒ\"\"\"\n    losses = []\n\n    for epoch in range(epochs):\n        epoch_loss = 0\n        mini_batches = create_mini_batches(X, y, batch_size)\n\n        for X_batch, y_batch in mini_batches:\n            # å‰å‘ä¼ æ’­\n            y_pred, cache = forward_propagation(X_batch, parameters)\n\n            # è®¡ç®—æŸå¤±\n            loss = compute_loss(y_pred, y_batch)\n            epoch_loss += loss\n\n            # åå‘ä¼ æ’­\n            gradients = backward_propagation(X_batch, y_batch, parameters, cache)\n\n            # æ›´æ–°å‚æ•°\n            parameters = update_parameters(parameters, gradients, learning_rate)\n\n        # å¹³å‡æŸå¤±\n        avg_loss = epoch_loss / len(mini_batches)\n        losses.append(avg_loss)\n\n        if epoch % 100 == 0:\n            print(f\"Epoch {epoch}: Loss = {avg_loss:.6f}\")\n\n    return parameters, losses",
    "crumbs": [
      "ç¬¬äºŒéƒ¨åˆ†ï¼šè¿›é˜¶ç¯‡",
      "<span class='chapter-number'>6</span>Â  <span class='chapter-title'>ç¬¬äº”ç« ï¼šä¼˜åŒ–ç®—æ³•ä¸è®­ç»ƒæŠ€å·§ (Optimization & Training Tricks)</span>"
    ]
  },
  {
    "objectID": "Chapter5.html#åŠ¨é‡æ³•-momentum",
    "href": "Chapter5.html#åŠ¨é‡æ³•-momentum",
    "title": "6Â  ç¬¬äº”ç« ï¼šä¼˜åŒ–ç®—æ³•ä¸è®­ç»ƒæŠ€å·§ (Optimization & Training Tricks)",
    "section": "6.3 5.2 åŠ¨é‡æ³• (Momentum)",
    "text": "6.3 5.2 åŠ¨é‡æ³• (Momentum)\n\n6.3.1 ğŸ¯ é—®é¢˜ï¼šæ¢¯åº¦ä¸‹é™çš„éœ‡è¡\nLoss\n â†‘\n |     *\n |    * *\n |   *   *     â† å‚ç›´æ–¹å‘éœ‡è¡\n |  *     *\n | *       *\n |__________â†’\n   å‚æ•°ç©ºé—´\n\nç†æƒ³ï¼šæ¨ªå‘å¿«é€Ÿå‰è¿›ï¼Œçºµå‘å‡å°‘éœ‡è¡\n\n\n6.3.2 ğŸ’¡ åŠ¨é‡æ³•åŸç†\nç‰©ç†ç±»æ¯”ï¼šæ»šä¸‹å±±çš„å°çƒ\nå°çƒä¸ä¼šç«‹åˆ»æ”¹å˜æ–¹å‘\nè€Œæ˜¯ç´¯ç§¯åŠ¨é‡ï¼Œå¹³æ»‘åœ°æ»šåŠ¨\næ•°å­¦å…¬å¼ï¼š\nv_t = Î²Â·v_{t-1} + (1-Î²)Â·âˆ‡L_t\n\nÎ¸_t = Î¸_{t-1} - Î±Â·v_t\n\nå…¶ä¸­ï¼š\n  v_t: é€Ÿåº¦ï¼ˆåŠ¨é‡ï¼‰\n  Î²: åŠ¨é‡ç³»æ•°ï¼ˆé€šå¸¸ 0.9ï¼‰\n  âˆ‡L_t: å½“å‰æ¢¯åº¦\n  Î±: å­¦ä¹ ç‡\næŒ‡æ•°åŠ æƒç§»åŠ¨å¹³å‡ï¼š\nv_t = Î²Â·v_{t-1} + (1-Î²)Â·g_t\n    = (1-Î²)Â·g_t + Î²Â·(1-Î²)Â·g_{t-1} + Î²Â²Â·(1-Î²)Â·g_{t-2} + ...\n\næƒé‡ï¼š\n  g_t:   (1-Î²) = 0.1\n  g_{t-1}: Î²(1-Î²) = 0.09\n  g_{t-2}: Î²Â²(1-Î²) = 0.081\n  ...\n\nè¶Šè¿‘çš„æ¢¯åº¦æƒé‡è¶Šå¤§\n\n\n6.3.3 ğŸ“Š æ•ˆæœå¯¹æ¯”\nä¸ä½¿ç”¨åŠ¨é‡:        ä½¿ç”¨åŠ¨é‡:\n    *                 â”€â”€â†’\n   * *               â”€â”€â†’\n  *   *             â”€â”€â†’\n *     *           â”€â”€â†’\n*éœ‡è¡  *          å¹³æ»‘\n\n\n6.3.4 ğŸ’» å®ç°\ndef initialize_momentum(parameters):\n    \"\"\"\n    åˆå§‹åŒ–åŠ¨é‡\n\n    è¿”å›ï¼š\n        v: å­—å…¸ï¼Œä¸ parameters ç»“æ„ç›¸åŒï¼Œåˆå§‹åŒ–ä¸º 0\n    \"\"\"\n    v = {}\n    L = len(parameters) // 2  # W å’Œ b çš„å¯¹æ•°\n\n    for l in range(1, L + 1):\n        v[f'dW{l}'] = np.zeros_like(parameters[f'W{l}'])\n        v[f'db{l}'] = np.zeros_like(parameters[f'b{l}'])\n\n    return v\n\ndef update_parameters_with_momentum(parameters, gradients, v, learning_rate, beta=0.9):\n    \"\"\"\n    ä½¿ç”¨åŠ¨é‡æ›´æ–°å‚æ•°\n\n    å‚æ•°ï¼š\n        parameters: å½“å‰å‚æ•°\n        gradients: æ¢¯åº¦\n        v: åŠ¨é‡\n        learning_rate: å­¦ä¹ ç‡\n        beta: åŠ¨é‡ç³»æ•°\n    \"\"\"\n    L = len(parameters) // 2\n\n    for l in range(1, L + 1):\n        # æ›´æ–°åŠ¨é‡\n        v[f'dW{l}'] = beta * v[f'dW{l}'] + (1 - beta) * gradients[f'dW{l}']\n        v[f'db{l}'] = beta * v[f'db{l}'] + (1 - beta) * gradients[f'db{l}']\n\n        # æ›´æ–°å‚æ•°\n        parameters[f'W{l}'] -= learning_rate * v[f'dW{l}']\n        parameters[f'b{l}'] -= learning_rate * v[f'db{l}']\n\n    return parameters, v\n\n# ä½¿ç”¨ç¤ºä¾‹\nv = initialize_momentum(parameters)\n\nfor epoch in range(epochs):\n    for X_batch, y_batch in mini_batches:\n        # å‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­\n        y_pred, cache = forward_propagation(X_batch, parameters)\n        gradients = backward_propagation(X_batch, y_batch, parameters, cache)\n\n        # ä½¿ç”¨åŠ¨é‡æ›´æ–°\n        parameters, v = update_parameters_with_momentum(\n            parameters, gradients, v, learning_rate, beta=0.9\n        )",
    "crumbs": [
      "ç¬¬äºŒéƒ¨åˆ†ï¼šè¿›é˜¶ç¯‡",
      "<span class='chapter-number'>6</span>Â  <span class='chapter-title'>ç¬¬äº”ç« ï¼šä¼˜åŒ–ç®—æ³•ä¸è®­ç»ƒæŠ€å·§ (Optimization & Training Tricks)</span>"
    ]
  },
  {
    "objectID": "Chapter5.html#rmsprop-root-mean-square-propagation",
    "href": "Chapter5.html#rmsprop-root-mean-square-propagation",
    "title": "6Â  ç¬¬äº”ç« ï¼šä¼˜åŒ–ç®—æ³•ä¸è®­ç»ƒæŠ€å·§ (Optimization & Training Tricks)",
    "section": "6.4 5.3 RMSprop (Root Mean Square Propagation)",
    "text": "6.4 5.3 RMSprop (Root Mean Square Propagation)\n\n6.4.1 ğŸ¯ é—®é¢˜ï¼šä¸åŒå‚æ•°éœ€è¦ä¸åŒå­¦ä¹ ç‡\nå‚æ•° wâ‚: æ¢¯åº¦èŒƒå›´ [-100, 100]  â† éœ€è¦å°å­¦ä¹ ç‡\nå‚æ•° wâ‚‚: æ¢¯åº¦èŒƒå›´ [-0.01, 0.01] â† éœ€è¦å¤§å­¦ä¹ ç‡\n\nå›ºå®šå­¦ä¹ ç‡æ— æ³•åŒæ—¶æ»¡è¶³\n\n\n6.4.2 ğŸ’¡ RMSprop åŸç†\nè‡ªé€‚åº”è°ƒæ•´æ¯ä¸ªå‚æ•°çš„å­¦ä¹ ç‡\ns_t = Î²Â·s_{t-1} + (1-Î²)Â·(âˆ‡L_t)Â²\n\nÎ¸_t = Î¸_{t-1} - Î±Â·âˆ‡L_t / âˆš(s_t + Îµ)\n\nå…¶ä¸­ï¼š\n  s_t: æ¢¯åº¦å¹³æ–¹çš„æŒ‡æ•°åŠ æƒç§»åŠ¨å¹³å‡\n  Î²: è¡°å‡ç‡ï¼ˆé€šå¸¸ 0.999ï¼‰\n  Îµ: é˜²æ­¢é™¤é›¶ï¼ˆé€šå¸¸ 10â»â¸ï¼‰\nç›´è§‰ï¼š - æ¢¯åº¦å¤§ â†’ s å¤§ â†’ æ­¥é•¿å°ï¼ˆé™¤ä»¥å¤§æ•°ï¼‰ - æ¢¯åº¦å° â†’ s å° â†’ æ­¥é•¿å¤§ï¼ˆé™¤ä»¥å°æ•°ï¼‰\n\n\n6.4.3 ğŸ’» å®ç°\ndef initialize_rmsprop(parameters):\n    \"\"\"åˆå§‹åŒ– RMSprop\"\"\"\n    s = {}\n    L = len(parameters) // 2\n\n    for l in range(1, L + 1):\n        s[f'dW{l}'] = np.zeros_like(parameters[f'W{l}'])\n        s[f'db{l}'] = np.zeros_like(parameters[f'b{l}'])\n\n    return s\n\ndef update_parameters_with_rmsprop(parameters, gradients, s, learning_rate,\n                                   beta=0.999, epsilon=1e-8):\n    \"\"\"ä½¿ç”¨ RMSprop æ›´æ–°å‚æ•°\"\"\"\n    L = len(parameters) // 2\n\n    for l in range(1, L + 1):\n        # æ›´æ–°å¹³æ–¹æ¢¯åº¦çš„ç§»åŠ¨å¹³å‡\n        s[f'dW{l}'] = beta * s[f'dW{l}'] + (1 - beta) * gradients[f'dW{l}']**2\n        s[f'db{l}'] = beta * s[f'db{l}'] + (1 - beta) * gradients[f'db{l}']**2\n\n        # æ›´æ–°å‚æ•°\n        parameters[f'W{l}'] -= learning_rate * gradients[f'dW{l}'] / (np.sqrt(s[f'dW{l}']) + epsilon)\n        parameters[f'b{l}'] -= learning_rate * gradients[f'db{l}'] / (np.sqrt(s[f'db{l}']) + epsilon)\n\n    return parameters, s",
    "crumbs": [
      "ç¬¬äºŒéƒ¨åˆ†ï¼šè¿›é˜¶ç¯‡",
      "<span class='chapter-number'>6</span>Â  <span class='chapter-title'>ç¬¬äº”ç« ï¼šä¼˜åŒ–ç®—æ³•ä¸è®­ç»ƒæŠ€å·§ (Optimization & Training Tricks)</span>"
    ]
  },
  {
    "objectID": "Chapter5.html#adam-adaptive-moment-estimation",
    "href": "Chapter5.html#adam-adaptive-moment-estimation",
    "title": "6Â  ç¬¬äº”ç« ï¼šä¼˜åŒ–ç®—æ³•ä¸è®­ç»ƒæŠ€å·§ (Optimization & Training Tricks)",
    "section": "6.5 5.4 Adam (Adaptive Moment Estimation) â­",
    "text": "6.5 5.4 Adam (Adaptive Moment Estimation) â­\n\n6.5.1 ğŸ¯ Adam = Momentum + RMSprop\nç»“åˆä¸¤è€…ä¼˜ç‚¹ï¼š - Momentumï¼šå¹³æ»‘æ¢¯åº¦æ–¹å‘ - RMSpropï¼šè‡ªé€‚åº”å­¦ä¹ ç‡\n\n\n6.5.2 ğŸ“ ç®—æ³•\nåˆå§‹åŒ–ï¼š\n  vâ‚€ = 0  (ä¸€é˜¶çŸ©ä¼°è®¡ï¼ŒåŠ¨é‡)\n  sâ‚€ = 0  (äºŒé˜¶çŸ©ä¼°è®¡ï¼ŒRMSprop)\n\næ¯æ¬¡è¿­ä»£ï¼š\n  1. è®¡ç®—æ¢¯åº¦ g_t = âˆ‡L_t\n\n  2. æ›´æ–°åŠ¨é‡ï¼š\n     v_t = Î²â‚Â·v_{t-1} + (1-Î²â‚)Â·g_t\n\n  3. æ›´æ–°å¹³æ–¹æ¢¯åº¦ï¼š\n     s_t = Î²â‚‚Â·s_{t-1} + (1-Î²â‚‚)Â·g_tÂ²\n\n  4. åå·®ä¿®æ­£ï¼š\n     vÌ‚_t = v_t / (1 - Î²â‚áµ—)\n     Å_t = s_t / (1 - Î²â‚‚áµ—)\n\n  5. æ›´æ–°å‚æ•°ï¼š\n     Î¸_t = Î¸_{t-1} - Î±Â·vÌ‚_t / (âˆšÅ_t + Îµ)\n\né»˜è®¤è¶…å‚æ•°ï¼š\n  Î± = 0.001\n  Î²â‚ = 0.9\n  Î²â‚‚ = 0.999\n  Îµ = 10â»â¸\n\n\n6.5.3 ğŸ¤” ä¸ºä»€ä¹ˆéœ€è¦åå·®ä¿®æ­£ï¼Ÿ\nåˆå§‹æ—¶ vâ‚€ = 0, sâ‚€ = 0\n\nç¬¬ä¸€æ­¥ï¼š\n  vâ‚ = 0.9Â·0 + 0.1Â·gâ‚ = 0.1Â·gâ‚\n\né—®é¢˜ï¼švâ‚ è¿œå°äºçœŸå®æœŸæœ›ï¼\n  (å› ä¸ºåˆå§‹åŒ–ä¸º 0ï¼Œæœ‰åå·®)\n\nä¿®æ­£ï¼š\n  vÌ‚â‚ = vâ‚ / (1 - 0.9Â¹) = 0.1Â·gâ‚ / 0.1 = gâ‚  âœ“\n\néšç€ t å¢å¤§ï¼š\n  (1 - Î²â‚áµ—) â†’ 1\n  ä¿®æ­£æ•ˆæœé€æ¸æ¶ˆå¤±\n\n\n6.5.4 ğŸ’» å®Œæ•´å®ç°\ndef initialize_adam(parameters):\n    \"\"\"\n    åˆå§‹åŒ– Adam ä¼˜åŒ–å™¨\n\n    è¿”å›ï¼š\n        v: ä¸€é˜¶çŸ©ä¼°è®¡ï¼ˆåŠ¨é‡ï¼‰\n        s: äºŒé˜¶çŸ©ä¼°è®¡ï¼ˆRMSpropï¼‰\n    \"\"\"\n    v = {}\n    s = {}\n    L = len(parameters) // 2\n\n    for l in range(1, L + 1):\n        v[f'dW{l}'] = np.zeros_like(parameters[f'W{l}'])\n        v[f'db{l}'] = np.zeros_like(parameters[f'b{l}'])\n        s[f'dW{l}'] = np.zeros_like(parameters[f'W{l}'])\n        s[f'db{l}'] = np.zeros_like(parameters[f'b{l}'])\n\n    return v, s\n\ndef update_parameters_with_adam(parameters, gradients, v, s, t,\n                                learning_rate=0.001,\n                                beta1=0.9, beta2=0.999, epsilon=1e-8):\n    \"\"\"\n    ä½¿ç”¨ Adam æ›´æ–°å‚æ•°\n\n    å‚æ•°ï¼š\n        t: å½“å‰è¿­ä»£æ¬¡æ•°ï¼ˆä» 1 å¼€å§‹ï¼‰\n    \"\"\"\n    L = len(parameters) // 2\n    v_corrected = {}\n    s_corrected = {}\n\n    for l in range(1, L + 1):\n        # æ›´æ–°åŠ¨é‡\n        v[f'dW{l}'] = beta1 * v[f'dW{l}'] + (1 - beta1) * gradients[f'dW{l}']\n        v[f'db{l}'] = beta1 * v[f'db{l}'] + (1 - beta1) * gradients[f'db{l}']\n\n        # æ›´æ–°å¹³æ–¹æ¢¯åº¦\n        s[f'dW{l}'] = beta2 * s[f'dW{l}'] + (1 - beta2) * (gradients[f'dW{l}']**2)\n        s[f'db{l}'] = beta2 * s[f'db{l}'] + (1 - beta2) * (gradients[f'db{l}']**2)\n\n        # åå·®ä¿®æ­£\n        v_corrected[f'dW{l}'] = v[f'dW{l}'] / (1 - beta1**t)\n        v_corrected[f'db{l}'] = v[f'db{l}'] / (1 - beta1**t)\n        s_corrected[f'dW{l}'] = s[f'dW{l}'] / (1 - beta2**t)\n        s_corrected[f'db{l}'] = s[f'db{l}'] / (1 - beta2**t)\n\n        # æ›´æ–°å‚æ•°\n        parameters[f'W{l}'] -= learning_rate * v_corrected[f'dW{l}'] / (np.sqrt(s_corrected[f'dW{l}']) + epsilon)\n        parameters[f'b{l}'] -= learning_rate * v_corrected[f'db{l}'] / (np.sqrt(s_corrected[f'db{l}']) + epsilon)\n\n    return parameters, v, s\n\n# ä½¿ç”¨ç¤ºä¾‹\ndef train_with_adam(X_train, y_train, layer_dims, epochs=1000, batch_size=64):\n    \"\"\"ä½¿ç”¨ Adam è®­ç»ƒç½‘ç»œ\"\"\"\n    # åˆå§‹åŒ–å‚æ•°\n    parameters = initialize_parameters(layer_dims)\n    v, s = initialize_adam(parameters)\n\n    losses = []\n    t = 0  # å…¨å±€è¿­ä»£è®¡æ•°å™¨\n\n    for epoch in range(epochs):\n        mini_batches = create_mini_batches(X_train, y_train, batch_size)\n        epoch_loss = 0\n\n        for X_batch, y_batch in mini_batches:\n            t += 1  # æ¯ä¸ª mini-batch å¢åŠ è®¡æ•°\n\n            # å‰å‘ä¼ æ’­\n            AL, caches = forward_propagation_deep(X_batch, parameters)\n\n            # è®¡ç®—æŸå¤±\n            loss = compute_loss(AL, y_batch)\n            epoch_loss += loss\n\n            # åå‘ä¼ æ’­\n            gradients = backward_propagation_deep(AL, y_batch, caches)\n\n            # Adam æ›´æ–°\n            parameters, v, s = update_parameters_with_adam(\n                parameters, gradients, v, s, t\n            )\n\n        avg_loss = epoch_loss / len(mini_batches)\n        losses.append(avg_loss)\n\n        if epoch % 100 == 0:\n            print(f\"Epoch {epoch}: Loss = {avg_loss:.6f}\")\n\n    return parameters, losses",
    "crumbs": [
      "ç¬¬äºŒéƒ¨åˆ†ï¼šè¿›é˜¶ç¯‡",
      "<span class='chapter-number'>6</span>Â  <span class='chapter-title'>ç¬¬äº”ç« ï¼šä¼˜åŒ–ç®—æ³•ä¸è®­ç»ƒæŠ€å·§ (Optimization & Training Tricks)</span>"
    ]
  },
  {
    "objectID": "Chapter5.html#ä¼˜åŒ–å™¨å¯¹æ¯”",
    "href": "Chapter5.html#ä¼˜åŒ–å™¨å¯¹æ¯”",
    "title": "6Â  ç¬¬äº”ç« ï¼šä¼˜åŒ–ç®—æ³•ä¸è®­ç»ƒæŠ€å·§ (Optimization & Training Tricks)",
    "section": "6.6 5.5 ä¼˜åŒ–å™¨å¯¹æ¯”",
    "text": "6.6 5.5 ä¼˜åŒ–å™¨å¯¹æ¯”\n\n6.6.1 ğŸ“Š å¯è§†åŒ–å¯¹æ¯”\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.animation import FuncAnimation\n\n# å®šä¹‰ä¸€ä¸ªéå‡¸å‡½æ•°ï¼ˆç±»ä¼¼ Rosenbrockï¼‰\ndef f(x, y):\n    return (1 - x)**2 + 100 * (y - x**2)**2\n\ndef grad_f(x, y):\n    \"\"\"è®¡ç®—æ¢¯åº¦\"\"\"\n    dx = -2 * (1 - x) - 400 * x * (y - x**2)\n    dy = 200 * (y - x**2)\n    return np.array([dx, dy])\n\n# åˆ›å»ºç­‰é«˜çº¿å›¾\nx = np.linspace(-2, 2, 100)\ny = np.linspace(-1, 3, 100)\nX, Y = np.meshgrid(x, y)\nZ = f(X, Y)\n\n# æµ‹è¯•ä¸åŒä¼˜åŒ–å™¨\ndef sgd(pos, grad, lr=0.001):\n    return pos - lr * grad\n\ndef momentum(pos, grad, v, beta=0.9, lr=0.001):\n    v = beta * v + (1 - beta) * grad\n    return pos - lr * v, v\n\ndef rmsprop(pos, grad, s, beta=0.999, lr=0.001, eps=1e-8):\n    s = beta * s + (1 - beta) * grad**2\n    return pos - lr * grad / (np.sqrt(s) + eps), s\n\ndef adam(pos, grad, v, s, t, beta1=0.9, beta2=0.999, lr=0.001, eps=1e-8):\n    v = beta1 * v + (1 - beta1) * grad\n    s = beta2 * s + (1 - beta2) * grad**2\n    v_hat = v / (1 - beta1**t)\n    s_hat = s / (1 - beta2**t)\n    return pos - lr * v_hat / (np.sqrt(s_hat) + eps), v, s\n\n# è¿è¡Œä¼˜åŒ–\ndef run_optimizer(optimizer_name, steps=200):\n    pos = np.array([-1.5, 2.5])\n    trajectory = [pos.copy()]\n\n    if optimizer_name == 'SGD':\n        for _ in range(steps):\n            grad = grad_f(pos[0], pos[1])\n            pos = sgd(pos, grad)\n            trajectory.append(pos.copy())\n\n    elif optimizer_name == 'Momentum':\n        v = np.zeros(2)\n        for _ in range(steps):\n            grad = grad_f(pos[0], pos[1])\n            pos, v = momentum(pos, grad, v)\n            trajectory.append(pos.copy())\n\n    elif optimizer_name == 'RMSprop':\n        s = np.zeros(2)\n        for _ in range(steps):\n            grad = grad_f(pos[0], pos[1])\n            pos, s = rmsprop(pos, grad, s)\n            trajectory.append(pos.copy())\n\n    elif optimizer_name == 'Adam':\n        v = np.zeros(2)\n        s = np.zeros(2)\n        for t in range(1, steps + 1):\n            grad = grad_f(pos[0], pos[1])\n            pos, v, s = adam(pos, grad, v, s, t)\n            trajectory.append(pos.copy())\n\n    return np.array(trajectory)\n\n# ç»˜åˆ¶å¯¹æ¯”å›¾\nfig, axes = plt.subplots(2, 2, figsize=(14, 12))\noptimizers = ['SGD', 'Momentum', 'RMSprop', 'Adam']\n\nfor ax, opt_name in zip(axes.flat, optimizers):\n    ax.contour(X, Y, Z, levels=np.logspace(-1, 3, 20), cmap='viridis', alpha=0.3)\n\n    trajectory = run_optimizer(opt_name, steps=200)\n    ax.plot(trajectory[:, 0], trajectory[:, 1], 'r-', linewidth=2, alpha=0.7)\n    ax.plot(trajectory[0, 0], trajectory[0, 1], 'go', markersize=10, label='èµ·ç‚¹')\n    ax.plot(1, 1, 'r*', markersize=15, label='æœ€ä¼˜ç‚¹')\n\n    ax.set_title(f'{opt_name} Optimizer', fontsize=14, fontweight='bold')\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\n6.6.2 ğŸ“ˆ æ€§èƒ½å¯¹æ¯”è¡¨\n\n\n\nä¼˜åŒ–å™¨\næ”¶æ•›é€Ÿåº¦\nç¨³å®šæ€§\nå†…å­˜å¼€é”€\nè¶…å‚æ•°æ•æ„Ÿåº¦\næ¨èåº¦\n\n\n\n\nSGD\nâ˜…â˜…â˜†â˜†â˜†\nâ˜…â˜…â˜†â˜†â˜†\nâ˜…â˜…â˜…â˜…â˜…\nâ˜…â˜…â˜…â˜…â˜†\nâ˜…â˜…â˜†â˜†â˜†\n\n\nSGD+Momentum\nâ˜…â˜…â˜…â˜…â˜†\nâ˜…â˜…â˜…â˜…â˜†\nâ˜…â˜…â˜…â˜…â˜†\nâ˜…â˜…â˜…â˜†â˜†\nâ˜…â˜…â˜…â˜…â˜†\n\n\nRMSprop\nâ˜…â˜…â˜…â˜…â˜†\nâ˜…â˜…â˜…â˜…â˜†\nâ˜…â˜…â˜…â˜…â˜†\nâ˜…â˜…â˜…â˜†â˜†\nâ˜…â˜…â˜…â˜…â˜†\n\n\nAdam\nâ˜…â˜…â˜…â˜…â˜…\nâ˜…â˜…â˜…â˜…â˜…\nâ˜…â˜…â˜…â˜†â˜†\nâ˜…â˜…â˜…â˜…â˜…\nâ˜…â˜…â˜…â˜…â˜…\n\n\nAdaGrad\nâ˜…â˜…â˜…â˜†â˜†\nâ˜…â˜…â˜…â˜†â˜†\nâ˜…â˜…â˜…â˜…â˜†\nâ˜…â˜…â˜…â˜…â˜†\nâ˜…â˜…â˜†â˜†â˜†\n\n\nAdamW\nâ˜…â˜…â˜…â˜…â˜…\nâ˜…â˜…â˜…â˜…â˜…\nâ˜…â˜…â˜…â˜†â˜†\nâ˜…â˜…â˜…â˜…â˜…\nâ˜…â˜…â˜…â˜…â˜…\n\n\n\n\n\n6.6.3 ğŸ’¡ é€‰æ‹©æŒ‡å—\né»˜è®¤é€‰æ‹©ï¼šAdam â­\n  - å‡ ä¹é€‚ç”¨äºæ‰€æœ‰åœºæ™¯\n  - ä¸éœ€è¦å¤ªå¤šè°ƒå‚\n  - æ”¶æ•›å¿«ä¸”ç¨³å®š\n\néœ€è¦æœ€ä½³æ€§èƒ½ï¼šSGD + Momentum\n  - è®­ç»ƒæ—¶é—´è¶³å¤Ÿæ—¶\n  - é…åˆ Learning Rate Schedule\n  - é€šå¸¸æ³›åŒ–èƒ½åŠ›æ›´å¥½\n\nè®¡ç®—æœºè§†è§‰ï¼šSGD + Momentum\n  - ResNet, VGG ç­‰ç»å…¸æ¨¡å‹\n  - éœ€è¦ä»”ç»†è°ƒæ•´å­¦ä¹ ç‡\n\nNLP / Transformerï¼šAdam / AdamW\n  - BERT, GPT æ ‡é…\n  - AdamW åŠ å…¥æƒé‡è¡°å‡\n\nå†…å­˜å—é™ï¼šSGD\n  - ä¸éœ€è¦é¢å¤–å­˜å‚¨åŠ¨é‡",
    "crumbs": [
      "ç¬¬äºŒéƒ¨åˆ†ï¼šè¿›é˜¶ç¯‡",
      "<span class='chapter-number'>6</span>Â  <span class='chapter-title'>ç¬¬äº”ç« ï¼šä¼˜åŒ–ç®—æ³•ä¸è®­ç»ƒæŠ€å·§ (Optimization & Training Tricks)</span>"
    ]
  },
  {
    "objectID": "Chapter5.html#å­¦ä¹ ç‡è°ƒåº¦-learning-rate-scheduling",
    "href": "Chapter5.html#å­¦ä¹ ç‡è°ƒåº¦-learning-rate-scheduling",
    "title": "6Â  ç¬¬äº”ç« ï¼šä¼˜åŒ–ç®—æ³•ä¸è®­ç»ƒæŠ€å·§ (Optimization & Training Tricks)",
    "section": "6.7 5.6 å­¦ä¹ ç‡è°ƒåº¦ (Learning Rate Scheduling)",
    "text": "6.7 5.6 å­¦ä¹ ç‡è°ƒåº¦ (Learning Rate Scheduling)\n\n6.7.1 ğŸ¯ ä¸ºä»€ä¹ˆéœ€è¦è°ƒæ•´å­¦ä¹ ç‡ï¼Ÿ\nè®­ç»ƒåˆæœŸï¼š\n  - ç¦»æœ€ä¼˜ç‚¹è¿œ\n  - å¯ä»¥ç”¨å¤§å­¦ä¹ ç‡å¿«é€Ÿæ¥è¿‘\n\nè®­ç»ƒåæœŸï¼š\n  - æ¥è¿‘æœ€ä¼˜ç‚¹\n  - éœ€è¦å°å­¦ä¹ ç‡ç²¾ç»†è°ƒæ•´\n\nå›ºå®šå­¦ä¹ ç‡ï¼š\n  å¤ªå¤§ â†’ éœ‡è¡ï¼Œä¸æ”¶æ•›\n  å¤ªå° â†’ è®­ç»ƒæ…¢\n\n\n6.7.2 ğŸ“Š å¸¸è§è°ƒåº¦ç­–ç•¥\n\n6.7.2.1 1. Step Decay (é˜¶æ¢¯è¡°å‡)\næ¯éš”å›ºå®š epochï¼Œå­¦ä¹ ç‡ä¹˜ä»¥è¡°å‡å› å­\n\nlr_t = lr_0 Â· Î³^âŒŠepoch/step_sizeâŒ‹\n\nä¾‹ï¼š\n  lr_0 = 0.1\n  Î³ = 0.1\n  step_size = 30\n\n  epoch 0-29:  lr = 0.1\n  epoch 30-59: lr = 0.01\n  epoch 60-89: lr = 0.001\ndef step_decay_schedule(epoch, lr, drop=0.5, epochs_drop=10):\n    \"\"\"é˜¶æ¢¯è¡°å‡\"\"\"\n    return lr * (drop ** (epoch // epochs_drop))\n\n\n6.7.2.2 2. Exponential Decay (æŒ‡æ•°è¡°å‡)\nlr_t = lr_0 Â· e^(-Î»t)\n\næˆ–\n\nlr_t = lr_0 Â· Î³^t\ndef exponential_decay(epoch, lr_0, decay_rate=0.96):\n    \"\"\"æŒ‡æ•°è¡°å‡\"\"\"\n    return lr_0 * np.exp(-decay_rate * epoch)\n\n\n6.7.2.3 3. Cosine Annealing (ä½™å¼¦é€€ç«)\nlr_t = lr_min + (lr_max - lr_min) Â· (1 + cos(Ï€t/T)) / 2\n\nå¹³æ»‘ä¸‹é™ï¼Œå¸¸ç”¨äºè®­ç»ƒåæœŸ fine-tune\ndef cosine_annealing(epoch, lr_max, lr_min, T_max):\n    \"\"\"ä½™å¼¦é€€ç«\"\"\"\n    return lr_min + (lr_max - lr_min) * (1 + np.cos(np.pi * epoch / T_max)) / 2\n\n\n6.7.2.4 4. Warm-up + Cosine (ç°ä»£ Transformer æ ‡é…)\nWarm-up é˜¶æ®µï¼ˆå‰å‡ ä¸ª epochï¼‰ï¼š\n  çº¿æ€§å¢åŠ å­¦ä¹ ç‡ 0 â†’ lr_max\n\nä¸»è®­ç»ƒé˜¶æ®µï¼š\n  ä½™å¼¦é€€ç« lr_max â†’ lr_min\ndef warmup_cosine_schedule(epoch, lr_max, warmup_epochs, total_epochs, lr_min=0):\n    \"\"\"Warm-up + Cosine\"\"\"\n    if epoch &lt; warmup_epochs:\n        # Warm-up é˜¶æ®µ\n        return lr_max * (epoch + 1) / warmup_epochs\n    else:\n        # Cosine é˜¶æ®µ\n        progress = (epoch - warmup_epochs) / (total_epochs - warmup_epochs)\n        return lr_min + (lr_max - lr_min) * 0.5 * (1 + np.cos(np.pi * progress))\n\n\n6.7.2.5 5. Reduce on Plateau (åŸºäºéªŒè¯é›†)\nç›‘æ§éªŒè¯é›† Lossï¼š\n\nå¦‚æœ N ä¸ª epoch æ²¡æœ‰æ”¹è¿›ï¼š\n  lr = lr * factor\n\nå¾ˆå®ç”¨ï¼\nclass ReduceLROnPlateau:\n    def __init__(self, lr_init, factor=0.1, patience=10, min_lr=1e-7):\n        self.lr = lr_init\n        self.factor = factor\n        self.patience = patience\n        self.min_lr = min_lr\n        self.best_loss = float('inf')\n        self.counter = 0\n\n    def step(self, val_loss):\n        \"\"\"\n        æ ¹æ®éªŒè¯é›† Loss è°ƒæ•´å­¦ä¹ ç‡\n        \"\"\"\n        if val_loss &lt; self.best_loss:\n            self.best_loss = val_loss\n            self.counter = 0\n        else:\n            self.counter += 1\n            if self.counter &gt;= self.patience:\n                self.lr *= self.factor\n                self.lr = max(self.lr, self.min_lr)\n                self.counter = 0\n                print(f\"å­¦ä¹ ç‡å·²é™ä½åˆ° {self.lr:.6f}\")\n\n        return self.lr\n\n\n\n6.7.3 ğŸ“ˆ å¯è§†åŒ–å¯¹æ¯”\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nepochs = 200\nlr_0 = 0.1\n\n# ç”Ÿæˆä¸åŒè°ƒåº¦çš„å­¦ä¹ ç‡\nepochs_arr = np.arange(epochs)\n\nlr_constant = np.ones(epochs) * lr_0\nlr_step = np.array([step_decay_schedule(e, lr_0, drop=0.5, epochs_drop=50)\n                    for e in epochs_arr])\nlr_exp = np.array([exponential_decay(e, lr_0, decay_rate=0.02)\n                   for e in epochs_arr])\nlr_cosine = np.array([cosine_annealing(e, lr_0, 1e-5, 200)\n                      for e in epochs_arr])\nlr_warmup_cosine = np.array([warmup_cosine_schedule(e, lr_0, 10, 200, 1e-5)\n                             for e in epochs_arr])\n\n# ç»˜å›¾\nplt.figure(figsize=(12, 6))\nplt.semilogy(epochs_arr, lr_constant, label='Constant', linewidth=2)\nplt.semilogy(epochs_arr, lr_step, label='Step Decay', linewidth=2)\nplt.semilogy(epochs_arr, lr_exp, label='Exponential Decay', linewidth=2)\nplt.semilogy(epochs_arr, lr_cosine, label='Cosine Annealing', linewidth=2)\nplt.semilogy(epochs_arr, lr_warmup_cosine, label='Warm-up + Cosine', linewidth=2)\n\nplt.xlabel('Epoch')\nplt.ylabel('Learning Rate')\nplt.title('Learning Rate Schedules Comparison')\nplt.legend(fontsize=11)\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "ç¬¬äºŒéƒ¨åˆ†ï¼šè¿›é˜¶ç¯‡",
      "<span class='chapter-number'>6</span>Â  <span class='chapter-title'>ç¬¬äº”ç« ï¼šä¼˜åŒ–ç®—æ³•ä¸è®­ç»ƒæŠ€å·§ (Optimization & Training Tricks)</span>"
    ]
  },
  {
    "objectID": "Chapter5.html#æ­£åˆ™åŒ–æŠ€æœ¯-regularization",
    "href": "Chapter5.html#æ­£åˆ™åŒ–æŠ€æœ¯-regularization",
    "title": "6Â  ç¬¬äº”ç« ï¼šä¼˜åŒ–ç®—æ³•ä¸è®­ç»ƒæŠ€å·§ (Optimization & Training Tricks)",
    "section": "6.8 5.7 æ­£åˆ™åŒ–æŠ€æœ¯ (Regularization)",
    "text": "6.8 5.7 æ­£åˆ™åŒ–æŠ€æœ¯ (Regularization)\n\n6.8.1 ğŸ¯ é—®é¢˜ï¼šè¿‡æ‹Ÿåˆ\nè®­ç»ƒé›† Loss: 0.01  âœ“\næµ‹è¯•é›† Loss: 0.5   âœ—\n\næ¨¡å‹è¿‡åº¦å­¦ä¹ äº†è®­ç»ƒæ•°æ®çš„å™ªå£°\n\n\n6.8.2 ğŸ”¹ L1 å’Œ L2 æ­£åˆ™åŒ–\nL2 æ­£åˆ™åŒ–ï¼ˆæƒé‡è¡°å‡ï¼‰ï¼š\nL_total = L_origin + Î»Â·(1/2)Â·Î£wÂ²\n\næ•ˆæœï¼šå€¾å‘äºè®©æƒé‡å˜å°\n# PyTorch ä¸­ç›´æ¥åœ¨ä¼˜åŒ–å™¨ä¸­æŒ‡å®š\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.01)\n\n# æˆ–æ‰‹åŠ¨æ·»åŠ åˆ° Loss\nL_total = L_origin + weight_decay * sum(p**2 for p in model.parameters())\nL1 æ­£åˆ™åŒ–ï¼š\nL_total = L_origin + Î»Â·Î£|w|\n\næ•ˆæœï¼šè®©ä¸€äº›æƒé‡å˜æˆ 0ï¼ˆç¨€ç–æ€§ï¼‰\n\n\n\n6.8.3 ğŸ”¹ Dropout â­\næ ¸å¿ƒæ€æƒ³ï¼šè®­ç»ƒæ—¶éšæœºâ€å…³é—­â€ä¸€äº›ç¥ç»å…ƒ\nè®­ç»ƒæ—¶ (dropout = 0.5):\n  xâ‚ â”€â”€ wâ‚ â”€â”€â”\n  xâ‚‚ â”€â”€ âœ—   â”œâ”€ z  (éšæœºå…³é—­éƒ¨åˆ†è¿æ¥)\n  xâ‚ƒ â”€â”€ wâ‚ƒ â”€â”€â”˜\n\né¢„æµ‹æ—¶ï¼š\n  ä½¿ç”¨æ‰€æœ‰è¿æ¥ï¼Œä½†æƒé‡ä¹˜ä»¥ (1-p)\n  æˆ–ä½¿ç”¨ inverted dropoutï¼Œè®­ç»ƒæ—¶å°±è°ƒæ•´\nInverted Dropout (æ¨è):\nè®­ç»ƒæ—¶ï¼š\n  a_dropped = a / (1 - p)  with probability (1-p)\n              0             with probability p\n\né¢„æµ‹æ—¶ï¼š\n  ä½¿ç”¨ a ç›´æ¥ï¼ˆä¸éœ€è¦è°ƒæ•´ï¼‰\næ•ˆæœï¼š - é˜²æ­¢å…±é€‚åº”ï¼ˆco-adaptationï¼‰ - å‡å°‘è¿‡æ‹Ÿåˆ - é›†æˆæ•ˆæœ\n# PyTorch\nclass NeuralNetworkWithDropout(nn.Module):\n    def __init__(self, input_size, hidden_size, dropout_rate=0.5):\n        super().__init__()\n        self.fc1 = nn.Linear(input_size, hidden_size)\n        self.dropout1 = nn.Dropout(dropout_rate)\n        self.fc2 = nn.Linear(hidden_size, hidden_size)\n        self.dropout2 = nn.Dropout(dropout_rate)\n        self.fc3 = nn.Linear(hidden_size, 10)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = self.dropout1(x)  # è®­ç»ƒæ—¶éšæœºå…³é—­ï¼Œé¢„æµ‹æ—¶è‡ªåŠ¨è°ƒæ•´\n\n        x = torch.relu(self.fc2(x))\n        x = self.dropout2(x)\n\n        x = self.fc3(x)\n        return x\n\n# ä½¿ç”¨\nmodel = NeuralNetworkWithDropout(784, 128, dropout_rate=0.5)\nmodel.train()   # è®­ç»ƒæ¨¡å¼ï¼ŒDropout æœ‰æ•ˆ\nmodel.eval()    # è¯„ä¼°æ¨¡å¼ï¼ŒDropout æ— æ•ˆ\n\n\n6.8.4 ğŸ“Š Dropout æ•ˆæœ\nä¸ä½¿ç”¨ Dropout:          ä½¿ç”¨ Dropout:\nè®­ç»ƒLoss â†“              è®­ç»ƒLoss â†˜\næµ‹è¯•Loss â†—              æµ‹è¯•Loss â†˜\n\nè¿‡æ‹Ÿåˆ                  æ³›åŒ–æ›´å¥½\n\n\n\n6.8.5 ğŸ”¹ Early Stopping\næ€æƒ³ï¼šç›‘æ§éªŒè¯é›†ï¼Œå½“éªŒè¯é›† Loss åœæ­¢æ”¹è¿›æ—¶åœæ­¢è®­ç»ƒ\nclass EarlyStopping:\n    def __init__(self, patience=10, min_delta=0, restore_best_weights=True):\n        self.patience = patience\n        self.min_delta = min_delta\n        self.counter = 0\n        self.best_loss = float('inf')\n        self.restore_best_weights = restore_best_weights\n        self.best_weights = None\n\n    def __call__(self, val_loss, model):\n        \"\"\"\n        è¿”å› True è¡¨ç¤ºåº”è¯¥åœæ­¢è®­ç»ƒ\n        \"\"\"\n        if val_loss &lt; self.best_loss - self.min_delta:\n            self.best_loss = val_loss\n            self.counter = 0\n            if self.restore_best_weights:\n                self.best_weights = model.state_dict().copy()\n        else:\n            self.counter += 1\n            if self.counter &gt;= self.patience:\n                if self.restore_best_weights and self.best_weights is not None:\n                    model.load_state_dict(self.best_weights)\n                return True  # åœæ­¢è®­ç»ƒ\n        return False\n\n# ä½¿ç”¨\nearly_stopping = EarlyStopping(patience=10)\n\nfor epoch in range(num_epochs):\n    # è®­ç»ƒ\n    train_loss = train_one_epoch()\n\n    # éªŒè¯\n    val_loss = validate()\n\n    # æ£€æŸ¥æ˜¯å¦åœæ­¢\n    if early_stopping(val_loss, model):\n        print(f\"åœ¨ç¬¬ {epoch} ä¸ª epoch åœæ­¢\")\n        break",
    "crumbs": [
      "ç¬¬äºŒéƒ¨åˆ†ï¼šè¿›é˜¶ç¯‡",
      "<span class='chapter-number'>6</span>Â  <span class='chapter-title'>ç¬¬äº”ç« ï¼šä¼˜åŒ–ç®—æ³•ä¸è®­ç»ƒæŠ€å·§ (Optimization & Training Tricks)</span>"
    ]
  },
  {
    "objectID": "Chapter5.html#batch-normalization-bn",
    "href": "Chapter5.html#batch-normalization-bn",
    "title": "6Â  ç¬¬äº”ç« ï¼šä¼˜åŒ–ç®—æ³•ä¸è®­ç»ƒæŠ€å·§ (Optimization & Training Tricks)",
    "section": "6.9 5.8 Batch Normalization (BN)",
    "text": "6.9 5.8 Batch Normalization (BN)\n\n6.9.1 ğŸ¯ ä¸ºä»€ä¹ˆéœ€è¦ BNï¼Ÿ\né—®é¢˜ï¼šå†…éƒ¨åå˜é‡è½¬ç§» (Internal Covariate Shift)\nç¬¬1å±‚çš„è¾“å‡ºå˜åŒ– â†’ ç¬¬2å±‚çš„è¾“å…¥åˆ†å¸ƒå˜åŒ–\nâ†’ ç¬¬2å±‚éœ€è¦ä¸æ–­é€‚åº” â†’ è®­ç»ƒå˜æ…¢\n\n\n6.9.2 ğŸ’¡ Batch Normalization åŸç†\næ ‡å‡†åŒ–æ¯ä¸ª batchï¼š\nå¯¹æ¯ä¸ªç‰¹å¾ï¼š\n  1. è®¡ç®—å‡å€¼ï¼šÎ¼_B = (1/m)Â·Î£xáµ¢\n  2. è®¡ç®—æ–¹å·®ï¼šÏƒ_BÂ² = (1/m)Â·Î£(xáµ¢ - Î¼_B)Â²\n  3. æ ‡å‡†åŒ–ï¼šxÌ‚áµ¢ = (xáµ¢ - Î¼_B) / âˆš(Ïƒ_BÂ² + Îµ)\n  4. å°ºåº¦å’Œå¹³ç§»ï¼šyáµ¢ = Î³Â·xÌ‚áµ¢ + Î²\n\nå…¶ä¸­ Î³ å’Œ Î² æ˜¯å¯å­¦ä¹ çš„å‚æ•°\n\n\n6.9.3 ğŸ“ BN åœ¨å“ªé‡Œæ”¾ï¼Ÿ\nå¸¸è§ä½ç½®ï¼š\n  1. Linear â†’ BN â†’ Activation\n  2. Linear â†’ Activation â†’ BN\n  3. Conv â†’ BN â†’ ReLU (æ¨è)\n\nä¸€èˆ¬ï¼šBN åœ¨æ¿€æ´»å‡½æ•°ä¹‹å‰\n\n\n6.9.4 ğŸ’» PyTorch å®ç°\nclass NeuralNetworkWithBN(nn.Module):\n    def __init__(self, input_size, hidden_size):\n        super().__init__()\n        self.fc1 = nn.Linear(input_size, hidden_size)\n        self.bn1 = nn.BatchNorm1d(hidden_size)\n\n        self.fc2 = nn.Linear(hidden_size, hidden_size)\n        self.bn2 = nn.BatchNorm1d(hidden_size)\n\n        self.fc3 = nn.Linear(hidden_size, 10)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.bn1(x)\n        x = torch.relu(x)\n\n        x = self.fc2(x)\n        x = self.bn2(x)\n        x = torch.relu(x)\n\n        x = self.fc3(x)\n        return x\n\n# è®­ç»ƒæ—¶ BN ä½¿ç”¨ batch ç»Ÿè®¡\n# æ¨ç†æ—¶ BN ä½¿ç”¨è¿è¡Œå‡å€¼å’Œæ–¹å·®\nmodel.train()    # ä½¿ç”¨ batch ç»Ÿè®¡\nmodel.eval()     # ä½¿ç”¨è¿è¡Œç»Ÿè®¡\n\n\n6.9.5 âœ… Batch Normalization çš„ä¼˜ç‚¹\n\nåŠ é€Ÿæ”¶æ•›ï¼šå‡å°‘å†…éƒ¨åå˜é‡è½¬ç§»\nå…è®¸æ›´å¤§å­¦ä¹ ç‡ï¼šæ›´ç¨³å®šçš„æ¢¯åº¦\nå‡å°‘åˆå§‹åŒ–æ•æ„Ÿæ€§ï¼šå¯¹åˆå§‹åŒ–ä¸æ•æ„Ÿ\nè½»å¾®çš„æ­£åˆ™åŒ–æ•ˆæœ\nç®€åŒ–åç»­ç½‘ç»œï¼šå¯ä»¥ç§»é™¤ Dropout\n\n\n\n6.9.6 âš ï¸ BN çš„ç¼ºç‚¹å’Œé™åˆ¶\né—®é¢˜ï¼š\n  - Batch size å¤ªå°æ—¶æ•ˆæœå·®\n  - è®­ç»ƒå’Œæ¨ç†æ—¶è¡Œä¸ºä¸åŒ\n  - åœ¨ RNN ä¸­ä½¿ç”¨å›°éš¾\n\nè§£å†³ï¼š\n  - Layer Normalization (LN)\n  - Group Normalization (GN)\n  - Instance Normalization (IN)",
    "crumbs": [
      "ç¬¬äºŒéƒ¨åˆ†ï¼šè¿›é˜¶ç¯‡",
      "<span class='chapter-number'>6</span>Â  <span class='chapter-title'>ç¬¬äº”ç« ï¼šä¼˜åŒ–ç®—æ³•ä¸è®­ç»ƒæŠ€å·§ (Optimization & Training Tricks)</span>"
    ]
  },
  {
    "objectID": "Chapter5.html#å®æˆ˜å®Œæ•´è®­ç»ƒæµç¨‹",
    "href": "Chapter5.html#å®æˆ˜å®Œæ•´è®­ç»ƒæµç¨‹",
    "title": "6Â  ç¬¬äº”ç« ï¼šä¼˜åŒ–ç®—æ³•ä¸è®­ç»ƒæŠ€å·§ (Optimization & Training Tricks)",
    "section": "6.10 5.9 å®æˆ˜ï¼šå®Œæ•´è®­ç»ƒæµç¨‹",
    "text": "6.10 5.9 å®æˆ˜ï¼šå®Œæ•´è®­ç»ƒæµç¨‹\n\n6.10.1 ğŸ’» PyTorch å®Œæ•´ç¤ºä¾‹\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\nfrom tqdm import tqdm\n\n# ==================== è¶…å‚æ•° ====================\nBATCH_SIZE = 128\nLEARNING_RATE = 0.001\nWEIGHT_DECAY = 1e-5\nEPOCHS = 50\nDROPOUT_RATE = 0.3\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# ==================== æ•°æ®åŠ è½½ ====================\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.1307,), (0.3081,))\n])\n\ntrain_dataset = datasets.MNIST(root='./data', train=True,\n                               download=True, transform=transform)\nval_dataset = datasets.MNIST(root='./data', train=False,\n                             download=True, transform=transform)\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n                          shuffle=True, num_workers=2)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE,\n                        shuffle=False, num_workers=2)\n\n# ==================== å®šä¹‰æ¨¡å‹ ====================\nclass ImprovedNN(nn.Module):\n    def __init__(self, dropout_rate=0.3):\n        super().__init__()\n\n        # ç¬¬1å±‚\n        self.fc1 = nn.Linear(28*28, 512)\n        self.bn1 = nn.BatchNorm1d(512)\n        self.dropout1 = nn.Dropout(dropout_rate)\n\n        # ç¬¬2å±‚\n        self.fc2 = nn.Linear(512, 256)\n        self.bn2 = nn.BatchNorm1d(256)\n        self.dropout2 = nn.Dropout(dropout_rate)\n\n        # ç¬¬3å±‚\n        self.fc3 = nn.Linear(256, 128)\n        self.bn3 = nn.BatchNorm1d(128)\n        self.dropout3 = nn.Dropout(dropout_rate)\n\n        # è¾“å‡ºå±‚\n        self.fc4 = nn.Linear(128, 10)\n\n    def forward(self, x):\n        # å±•å¹³\n        x = x.view(-1, 28*28)\n\n        # ç¬¬1å±‚\n        x = self.fc1(x)\n        x = self.bn1(x)\n        x = torch.relu(x)\n        x = self.dropout1(x)\n\n        # ç¬¬2å±‚\n        x = self.fc2(x)\n        x = self.bn2(x)\n        x = torch.relu(x)\n        x = self.dropout2(x)\n\n        # ç¬¬3å±‚\n        x = self.fc3(x)\n        x = self.bn3(x)\n        x = torch.relu(x)\n        x = self.dropout3(x)\n\n        # è¾“å‡ºå±‚\n        x = self.fc4(x)\n        return x\n\nmodel = ImprovedNN(dropout_rate=DROPOUT_RATE).to(DEVICE)\n\n# ==================== æŸå¤±å’Œä¼˜åŒ–å™¨ ====================\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(),\n                       lr=LEARNING_RATE,\n                       weight_decay=WEIGHT_DECAY)\n\n# å­¦ä¹ ç‡è°ƒåº¦\nscheduler = optim.lr_scheduler.CosineAnnealingLR(\n    optimizer, T_max=EPOCHS, eta_min=1e-6\n)\n\n# Early Stopping\nearly_stopping = EarlyStopping(patience=10)\n\n# ==================== è®­ç»ƒå‡½æ•° ====================\ndef train_epoch(model, train_loader, criterion, optimizer, device):\n    \"\"\"è®­ç»ƒä¸€ä¸ª epoch\"\"\"\n    model.train()\n    total_loss = 0\n    correct = 0\n    total = 0\n\n    progress_bar = tqdm(train_loader, desc='Training')\n\n    for images, labels in progress_bar:\n        images, labels = images.to(device), labels.to(device)\n\n        # å‰å‘ä¼ æ’­\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n\n        # åå‘ä¼ æ’­\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # ç»Ÿè®¡\n        total_loss += loss.item()\n        _, predicted = outputs.max(1)\n        correct += predicted.eq(labels).sum().item()\n        total += labels.size(0)\n\n        # æ›´æ–°è¿›åº¦æ¡\n        accuracy = 100. * correct / total\n        progress_bar.set_postfix({\n            'loss': f'{total_loss/(total):.3f}',\n            'acc': f'{accuracy:.2f}%'\n        })\n\n    avg_loss = total_loss / len(train_loader)\n    accuracy = 100. * correct / total\n\n    return avg_loss, accuracy\n\n# ==================== éªŒè¯å‡½æ•° ====================\ndef validate(model, val_loader, criterion, device):\n    \"\"\"éªŒè¯\"\"\"\n    model.eval()\n    total_loss = 0\n    correct = 0\n    total = 0\n\n    with torch.no_grad():\n        for images, labels in tqdm(val_loader, desc='Validation'):\n            images, labels = images.to(device), labels.to(device)\n\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n\n            total_loss += loss.item()\n            _, predicted = outputs.max(1)\n            correct += predicted.eq(labels).sum().item()\n            total += labels.size(0)\n\n    avg_loss = total_loss / len(val_loader)\n    accuracy = 100. * correct / total\n\n    return avg_loss, accuracy\n\n# ==================== è®­ç»ƒå¾ªç¯ ====================\ntrain_losses = []\ntrain_accs = []\nval_losses = []\nval_accs = []\n\nfor epoch in range(EPOCHS):\n    print(f'\\n=== Epoch {epoch+1}/{EPOCHS} ===')\n    print(f'å­¦ä¹ ç‡: {optimizer.param_groups[0][\"lr\"]:.6f}')\n\n    # è®­ç»ƒ\n    train_loss, train_acc = train_epoch(\n        model, train_loader, criterion, optimizer, DEVICE\n    )\n    train_losses.append(train_loss)\n    train_accs.append(train_acc)\n\n    # éªŒè¯\n    val_loss, val_acc = validate(model, val_loader, criterion, DEVICE)\n    val_losses.append(val_loss)\n    val_accs.append(val_acc)\n\n    print(f'è®­ç»ƒ: Loss={train_loss:.4f}, Acc={train_acc:.2f}%')\n    print(f'éªŒè¯: Loss={val_loss:.4f}, Acc={val_acc:.2f}%')\n\n    # å­¦ä¹ ç‡è°ƒåº¦\n    scheduler.step()\n\n    # Early Stopping\n    if early_stopping(val_loss, model):\n        print(f'\\nåœ¨ç¬¬ {epoch+1} ä¸ª epoch åœæ­¢è®­ç»ƒï¼ˆéªŒè¯é›†æ— æ”¹è¿›ï¼‰')\n        break\n\n# ==================== å¯è§†åŒ– ====================\nimport matplotlib.pyplot as plt\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n\nepochs_range = range(1, len(train_losses) + 1)\n\nax1.plot(epochs_range, train_losses, label='Train Loss', marker='o')\nax1.plot(epochs_range, val_losses, label='Val Loss', marker='o')\nax1.set_xlabel('Epoch')\nax1.set_ylabel('Loss')\nax1.set_title('Loss Over Epochs')\nax1.legend()\nax1.grid(True, alpha=0.3)\n\nax2.plot(epochs_range, train_accs, label='Train Acc', marker='o')\nax2.plot(epochs_range, val_accs, label='Val Acc', marker='o')\nax2.set_xlabel('Epoch')\nax2.set_ylabel('Accuracy (%)')\nax2.set_title('Accuracy Over Epochs')\nax2.legend()\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# ==================== æœ€ç»ˆè¯„ä¼° ====================\nprint('\\n' + '='*50)\nprint('æœ€ç»ˆç»“æœï¼š')\nprint(f'æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {max(val_accs):.2f}% (Epoch {val_accs.index(max(val_accs))+1})')\nprint('='*50)",
    "crumbs": [
      "ç¬¬äºŒéƒ¨åˆ†ï¼šè¿›é˜¶ç¯‡",
      "<span class='chapter-number'>6</span>Â  <span class='chapter-title'>ç¬¬äº”ç« ï¼šä¼˜åŒ–ç®—æ³•ä¸è®­ç»ƒæŠ€å·§ (Optimization & Training Tricks)</span>"
    ]
  },
  {
    "objectID": "Chapter5.html#æœ¬ç« ä½œä¸š",
    "href": "Chapter5.html#æœ¬ç« ä½œä¸š",
    "title": "6Â  ç¬¬äº”ç« ï¼šä¼˜åŒ–ç®—æ³•ä¸è®­ç»ƒæŠ€å·§ (Optimization & Training Tricks)",
    "section": "6.11 ğŸ“ æœ¬ç« ä½œä¸š",
    "text": "6.11 ğŸ“ æœ¬ç« ä½œä¸š\n\n6.11.1 ä½œä¸š 1ï¼šä¼˜åŒ–å™¨å¯¹æ¯”\nå®ç°ä»¥ä¸‹ä¼˜åŒ–å™¨ï¼Œåœ¨ MNIST ä¸Šå¯¹æ¯”ï¼š\n# TODO:\n# 1. SGD\n# 2. SGD + Momentum\n# 3. RMSprop\n# 4. Adam\n\n# è®°å½•ï¼š\n#   - è¾¾åˆ° 95% å‡†ç¡®ç‡éœ€è¦çš„ epoch æ•°\n#   - æœ€ç»ˆå‡†ç¡®ç‡\n#   - è®­ç»ƒæ—¶é—´\n#   - Loss æ›²çº¿å¹³æ»‘åº¦\n\n# ç»˜åˆ¶å¯¹æ¯”å›¾è¡¨\n\n\n6.11.2 ä½œä¸š 2ï¼šå­¦ä¹ ç‡è°ƒåº¦å®éªŒ\n# åœ¨åŒä¸€ä¸ªæ¨¡å‹ä¸Šå¯¹æ¯”ä¸åŒçš„å­¦ä¹ ç‡è°ƒåº¦ï¼š\n\n# 1. å›ºå®šå­¦ä¹ ç‡\n# 2. Step Decay\n# 3. Exponential Decay\n# 4. Cosine Annealing\n# 5. Warm-up + Cosine\n\n# åˆ†æï¼š\n#   - æœ€ç»ˆå‡†ç¡®ç‡\n#   - æ”¶æ•›é€Ÿåº¦\n#   - è®­ç»ƒç¨³å®šæ€§\n\n\n6.11.3 ä½œä¸š 3ï¼šæ­£åˆ™åŒ–æŠ€æœ¯å¯¹æ¯”\n# åœ¨ç›¸åŒæ¶æ„ä¸Šå¯¹æ¯”ï¼š\n\n# 1. æ— æ­£åˆ™åŒ–\n# 2. L2 æ­£åˆ™åŒ– (Î»=0.001, 0.01, 0.1)\n# 3. Dropout (p=0.3, 0.5)\n# 4. Batch Normalization\n# 5. ç»„åˆï¼ˆBN + Dropoutï¼‰\n\n# è§‚å¯Ÿï¼š\n#   - è®­ç»ƒé›† vs æµ‹è¯•é›†æ€§èƒ½å·®è·\n#   - è¿‡æ‹Ÿåˆæƒ…å†µ\n#   - æ¯ç§æ–¹æ³•çš„å½±å“\n\n\n6.11.4 ä½œä¸š 4ï¼šå®Œæ•´é¡¹ç›®\nåœ¨ CIFAR-10 æ•°æ®é›†ä¸Šæ„å»ºå®Œæ•´çš„è®­ç»ƒæµç¨‹\nè¦æ±‚ï¼š 1. å®ç°ä¸€ä¸ª 4-5 å±‚çš„æ·±åº¦ç½‘ç»œ 2. ä½¿ç”¨ Batch Normalization 3. ä½¿ç”¨ Dropout 4. é€‰æ‹©åˆé€‚çš„ä¼˜åŒ–å™¨ï¼ˆAdam æˆ– SGD+Momentumï¼‰ 5. å®ç°å­¦ä¹ ç‡è°ƒåº¦ 6. å®ç° Early Stopping 7. è®°å½•è®­ç»ƒè¿‡ç¨‹å’Œè¯„ä¼°ç»“æœ 8. å¯è§†åŒ–è®­ç»ƒæ›²çº¿ 9. åˆ†ææ¨¡å‹æ€§èƒ½å’Œæ”¹è¿›æ–¹å‘",
    "crumbs": [
      "ç¬¬äºŒéƒ¨åˆ†ï¼šè¿›é˜¶ç¯‡",
      "<span class='chapter-number'>6</span>Â  <span class='chapter-title'>ç¬¬äº”ç« ï¼šä¼˜åŒ–ç®—æ³•ä¸è®­ç»ƒæŠ€å·§ (Optimization & Training Tricks)</span>"
    ]
  },
  {
    "objectID": "Chapter5.html#æœ¬ç« å…³é”®æ¦‚å¿µ",
    "href": "Chapter5.html#æœ¬ç« å…³é”®æ¦‚å¿µ",
    "title": "6Â  ç¬¬äº”ç« ï¼šä¼˜åŒ–ç®—æ³•ä¸è®­ç»ƒæŠ€å·§ (Optimization & Training Tricks)",
    "section": "6.12 ğŸ”‘ æœ¬ç« å…³é”®æ¦‚å¿µ",
    "text": "6.12 ğŸ”‘ æœ¬ç« å…³é”®æ¦‚å¿µ\n\n\n\næ¦‚å¿µ\nè¯´æ˜\n\n\n\n\nMini-batch GD\næ‰¹é‡æ¢¯åº¦ä¸‹é™çš„æŠ˜ä¸­\n\n\nMomentum\nä½¿ç”¨å†å²æ¢¯åº¦åŠ é€Ÿæ”¶æ•›\n\n\nRMSprop\nè‡ªé€‚åº”å­¦ä¹ ç‡\n\n\nAdam\nMomentum + RMSprop çš„ç»„åˆ\n\n\nLearning Rate Schedule\nåŠ¨æ€è°ƒæ•´å­¦ä¹ ç‡\n\n\nBatch Normalization\næ ‡å‡†åŒ–ä¸­é—´å±‚è¾“å‡º\n\n\nDropout\néšæœºç¦ç”¨ç¥ç»å…ƒé˜²æ­¢è¿‡æ‹Ÿåˆ\n\n\nL1/L2 æ­£åˆ™åŒ–\næƒ©ç½šå¤§æƒé‡\n\n\nEarly Stopping\nç›‘æ§éªŒè¯é›†æå‰åœæ­¢\n\n\nWeight Decay\næƒé‡è¡°å‡ï¼ˆç­‰åŒ L2ï¼‰",
    "crumbs": [
      "ç¬¬äºŒéƒ¨åˆ†ï¼šè¿›é˜¶ç¯‡",
      "<span class='chapter-number'>6</span>Â  <span class='chapter-title'>ç¬¬äº”ç« ï¼šä¼˜åŒ–ç®—æ³•ä¸è®­ç»ƒæŠ€å·§ (Optimization & Training Tricks)</span>"
    ]
  },
  {
    "objectID": "Chapter5.html#ä¸‹ä¸€ç« é¢„å‘Š",
    "href": "Chapter5.html#ä¸‹ä¸€ç« é¢„å‘Š",
    "title": "6Â  ç¬¬äº”ç« ï¼šä¼˜åŒ–ç®—æ³•ä¸è®­ç»ƒæŠ€å·§ (Optimization & Training Tricks)",
    "section": "6.13 ğŸ¯ ä¸‹ä¸€ç« é¢„å‘Š",
    "text": "6.13 ğŸ¯ ä¸‹ä¸€ç« é¢„å‘Š\nç¬¬å…­ç« ï¼šå·ç§¯ç¥ç»ç½‘ç»œ (Convolutional Neural Networks) - å·ç§¯æ“ä½œçš„åŸç† - æ„Ÿå—é‡å’Œå‚æ•°å…±äº« - æ± åŒ–å’Œç‰¹å¾å›¾ - ç»å…¸ CNN æ¶æ„ï¼ˆLeNet, AlexNet, VGG, ResNetï¼‰ - å®æˆ˜ï¼šå›¾åƒåˆ†ç±»",
    "crumbs": [
      "ç¬¬äºŒéƒ¨åˆ†ï¼šè¿›é˜¶ç¯‡",
      "<span class='chapter-number'>6</span>Â  <span class='chapter-title'>ç¬¬äº”ç« ï¼šä¼˜åŒ–ç®—æ³•ä¸è®­ç»ƒæŠ€å·§ (Optimization & Training Tricks)</span>"
    ]
  },
  {
    "objectID": "Chapter6.html",
    "href": "Chapter6.html",
    "title": "7Â  ç¬¬å…­ç« ï¼šå·ç§¯ç¥ç»ç½‘ç»œ (Convolutional Neural Networks)",
    "section": "",
    "text": "7.1 ğŸ“Œ ç« èŠ‚ç›®æ ‡",
    "crumbs": [
      "ç¬¬äºŒéƒ¨åˆ†ï¼šè¿›é˜¶ç¯‡",
      "<span class='chapter-number'>7</span>Â  <span class='chapter-title'>ç¬¬å…­ç« ï¼šå·ç§¯ç¥ç»ç½‘ç»œ (Convolutional Neural Networks)</span>"
    ]
  },
  {
    "objectID": "Chapter6.html#ç« èŠ‚ç›®æ ‡",
    "href": "Chapter6.html#ç« èŠ‚ç›®æ ‡",
    "title": "7Â  ç¬¬å…­ç« ï¼šå·ç§¯ç¥ç»ç½‘ç»œ (Convolutional Neural Networks)",
    "section": "",
    "text": "ç†è§£å·ç§¯æ“ä½œçš„åŸç†\næŒæ¡ CNN çš„å…³é”®æ¦‚å¿µï¼ˆå·ç§¯æ ¸ã€æ± åŒ–ã€æ„Ÿå—é‡ï¼‰\nå­¦ä¹ ç»å…¸ CNN æ¶æ„\näº†è§£ CNN çš„ç‰¹æ€§å’Œä¼˜åŠ¿\nå®æˆ˜ï¼šå›¾åƒåˆ†ç±»å’Œç‰©ä½“æ£€æµ‹",
    "crumbs": [
      "ç¬¬äºŒéƒ¨åˆ†ï¼šè¿›é˜¶ç¯‡",
      "<span class='chapter-number'>7</span>Â  <span class='chapter-title'>ç¬¬å…­ç« ï¼šå·ç§¯ç¥ç»ç½‘ç»œ (Convolutional Neural Networks)</span>"
    ]
  },
  {
    "objectID": "Chapter6.html#ä¸ºä»€ä¹ˆéœ€è¦-cnn",
    "href": "Chapter6.html#ä¸ºä»€ä¹ˆéœ€è¦-cnn",
    "title": "7Â  ç¬¬å…­ç« ï¼šå·ç§¯ç¥ç»ç½‘ç»œ (Convolutional Neural Networks)",
    "section": "7.2 6.1 ä¸ºä»€ä¹ˆéœ€è¦ CNNï¼Ÿ",
    "text": "7.2 6.1 ä¸ºä»€ä¹ˆéœ€è¦ CNNï¼Ÿ\n\n7.2.1 ğŸ¯ å…¨è¿æ¥ç½‘ç»œçš„é—®é¢˜\nå›¾åƒåˆ†ç±»ä»»åŠ¡ï¼š28Ã—28 åƒç´ çš„æ‰‹å†™æ•°å­—\nè¾“å…¥å±‚ï¼š28Ã—28 = 784 ä¸ªç¥ç»å…ƒ\néšè—å±‚ï¼š512 ä¸ªç¥ç»å…ƒ\n\næƒé‡æ•°é‡ï¼š784 Ã— 512 = 401,408 ä¸ªï¼\né—®é¢˜ï¼š 1. å‚æ•°å¤ªå¤šï¼šå®¹æ˜“è¿‡æ‹Ÿåˆ 2. è®¡ç®—é‡å¤§ï¼šè®­ç»ƒæ…¢ 3. ç©ºé—´ç»“æ„ä¸¢å¤±ï¼šç›¸é‚»åƒç´ çš„å…³ç³»è¢«å¿½è§† 4. ä¸ç¨³å®šï¼šå¹³ç§»å›¾åƒä¼šå¾—åˆ°ä¸åŒç»“æœ\n\n\n7.2.2 ğŸ’¡ CNN çš„æ ¸å¿ƒæ€æƒ³\nè§‚å¯Ÿï¼šå›¾åƒå…·æœ‰å±€éƒ¨ç»“æ„ç‰¹æ€§\nä¸€ä¸ªå°çš„å·ç§¯æ ¸å¯ä»¥æ£€æµ‹ï¼š\n  - è¾¹ç¼˜\n  - è§’\n  - çº¹ç†\n  - ...\nç­–ç•¥ï¼š 1. ç”¨å°å·ç§¯æ ¸æ‰«ææ•´ä¸ªå›¾åƒï¼ˆå‚æ•°å…±äº«ï¼‰ 2. æå–å±€éƒ¨ç‰¹å¾ 3. é€å±‚æŠ½è±¡ï¼ˆä»ä½çº§ç‰¹å¾åˆ°é«˜çº§è¯­ä¹‰ï¼‰",
    "crumbs": [
      "ç¬¬äºŒéƒ¨åˆ†ï¼šè¿›é˜¶ç¯‡",
      "<span class='chapter-number'>7</span>Â  <span class='chapter-title'>ç¬¬å…­ç« ï¼šå·ç§¯ç¥ç»ç½‘ç»œ (Convolutional Neural Networks)</span>"
    ]
  },
  {
    "objectID": "Chapter6.html#å·ç§¯æ“ä½œ-convolution",
    "href": "Chapter6.html#å·ç§¯æ“ä½œ-convolution",
    "title": "7Â  ç¬¬å…­ç« ï¼šå·ç§¯ç¥ç»ç½‘ç»œ (Convolutional Neural Networks)",
    "section": "7.3 6.2 å·ç§¯æ“ä½œ (Convolution)",
    "text": "7.3 6.2 å·ç§¯æ“ä½œ (Convolution)\n\n7.3.1 ğŸ“ å•é€šé“å·ç§¯\nè¾“å…¥ï¼š5Ã—5 çš„å›¾åƒ\n1 0 1 0 1\n0 1 0 1 0\n1 0 1 0 1\n0 1 0 1 0\n1 0 1 0 1\nå·ç§¯æ ¸ï¼š3Ã—3\n1 0 -1\n1 0 -1\n1 0 -1\nå·ç§¯è¿‡ç¨‹ï¼š\nç¬¬1ä¸ªä½ç½®ï¼š\n  1 0 1       1 0 -1\n  0 1 0   âŠ—   1 0 -1   = 1Ã—1 + 0Ã—0 + 1Ã—(-1) + 0Ã—1 + 1Ã—0 + 0Ã—(-1)\n  1 0 1       1 0 -1     + 1Ã—1 + 0Ã—0 + 1Ã—(-1)\n                       = 1 + 0 - 1 + 0 + 0 + 0 + 1 + 0 - 1 = 0\nè¾“å‡ºï¼šç‰¹å¾å›¾ï¼ˆfeature mapï¼‰\n0  2  0\n2  0  2\n0  2  0  (3Ã—3 çš„è¾“å‡º)\n\n\n7.3.2 ğŸ“ å¤šé€šé“å·ç§¯ (å½©è‰²å›¾åƒ)\nè¾“å…¥ï¼š5Ã—5Ã—3 (RGB å›¾åƒ)\nRé€šé“ã€Gé€šé“ã€Bé€šé“\nå·ç§¯æ ¸ï¼š3Ã—3Ã—3 (é’ˆå¯¹æ¯ä¸ªé€šé“å„æœ‰ä¸€ä¸ªæ ¸)\nå·ç§¯è¿‡ç¨‹ï¼š\n  å¯¹æ¯ä¸ªé€šé“åˆ†åˆ«åšå·ç§¯\n  ç„¶åæ±‚å’Œ\n\n\n7.3.3 ğŸ“ æ•°å­¦è¡¨ç¤º\ny[i,j] = Î£_m Î£_n x[i+m, j+n] Â· w[m,n] + b\n\nå…¶ä¸­ï¼š\n  x: è¾“å…¥\n  w: å·ç§¯æ ¸æƒé‡\n  b: åç½®\næ•ˆç‡é«˜çš„åŸå› ï¼š - å‚æ•°å…±äº«ï¼šåŒä¸€ä¸ªå·ç§¯æ ¸ç”¨äºæ•´ä¸ªå›¾åƒ - ç›¸æ¯”å…¨è¿æ¥ï¼šå‚æ•°é‡å¤§å¹…å‡å°‘\n\n\n\n7.3.4 ğŸ’» Python å®ç°\nimport numpy as np\n\ndef convolve2d(image, kernel, padding=0, stride=1):\n    \"\"\"\n    2D å·ç§¯\n\n    å‚æ•°ï¼š\n        image: è¾“å…¥ (H, W)\n        kernel: å·ç§¯æ ¸ (K, K)\n        padding: å¡«å……\n        stride: æ­¥é•¿\n    \"\"\"\n    H, W = image.shape\n    K, _ = kernel.shape\n\n    # åŠ  padding\n    if padding &gt; 0:\n        image = np.pad(image, padding, mode='constant')\n\n    # è¾“å‡ºå¤§å°\n    H_out = (H + 2*padding - K) // stride + 1\n    W_out = (W + 2*padding - K) // stride + 1\n\n    # è¾“å‡ºç‰¹å¾å›¾\n    output = np.zeros((H_out, W_out))\n\n    # å·ç§¯æ“ä½œ\n    for i in range(H_out):\n        for j in range(W_out):\n            # æå–åŒºåŸŸ\n            region = image[i*stride:i*stride+K, j*stride:j*stride+K]\n            # é€å…ƒç´ ç›¸ä¹˜åæ±‚å’Œ\n            output[i, j] = np.sum(region * kernel)\n\n    return output\n\n# ç¤ºä¾‹\nimage = np.array([\n    [1, 0, 1, 0, 1],\n    [0, 1, 0, 1, 0],\n    [1, 0, 1, 0, 1],\n    [0, 1, 0, 1, 0],\n    [1, 0, 1, 0, 1]\n], dtype=float)\n\nkernel = np.array([\n    [1, 0, -1],\n    [1, 0, -1],\n    [1, 0, -1]\n], dtype=float)\n\noutput = convolve2d(image, kernel, padding=0, stride=1)\nprint(\"å·ç§¯è¾“å‡ºï¼š\")\nprint(output)",
    "crumbs": [
      "ç¬¬äºŒéƒ¨åˆ†ï¼šè¿›é˜¶ç¯‡",
      "<span class='chapter-number'>7</span>Â  <span class='chapter-title'>ç¬¬å…­ç« ï¼šå·ç§¯ç¥ç»ç½‘ç»œ (Convolutional Neural Networks)</span>"
    ]
  },
  {
    "objectID": "Chapter6.html#å…³é”®æ¦‚å¿µ",
    "href": "Chapter6.html#å…³é”®æ¦‚å¿µ",
    "title": "7Â  ç¬¬å…­ç« ï¼šå·ç§¯ç¥ç»ç½‘ç»œ (Convolutional Neural Networks)",
    "section": "7.4 6.3 å…³é”®æ¦‚å¿µ",
    "text": "7.4 6.3 å…³é”®æ¦‚å¿µ\n\n7.4.1 ğŸ”¹ Padding (å¡«å……)\né—®é¢˜ï¼šå·ç§¯ä¼šå‡å°å›¾åƒå°ºå¯¸\nè¾“å…¥ï¼š5Ã—5\nå·ç§¯æ ¸ï¼š3Ã—3ï¼Œstride=1\nè¾“å‡ºï¼š3Ã—3 (ç¼©å°äº†)\nè§£å†³ï¼šåœ¨è¾¹ç•Œæ·»åŠ é›¶\nä¸ padding:           ä½¿ç”¨ padding=1:\n1 0 1 0 1           0 0 0 0 0 0\n0 1 0 1 0           0 1 0 1 0 1\n1 0 1 0 1    â†’      0 0 1 0 1 0\n0 1 0 1 0           0 1 0 1 0 1\n1 0 1 0 1           0 0 1 0 1 0\n                    0 0 0 0 0 0\n\nè¾“å‡ºï¼š3Ã—3             è¾“å‡ºï¼š5Ã—5\nâ€˜Sameâ€™ paddingï¼špadding = (kernel_size - 1) / 2 - ä¿æŒè¾“å…¥è¾“å‡ºå°ºå¯¸ç›¸åŒ\nâ€˜Validâ€™ paddingï¼šæ—  padding - è¾“å‡ºå°ºå¯¸ = (input_size - kernel_size) / stride + 1\n\n\n\n7.4.2 ğŸ”¹ Stride (æ­¥é•¿)\nä¸€æ¬¡å·ç§¯æ ¸ç§»åŠ¨çš„è·ç¦»\nstride=1ï¼š\n[â—]âš¬âš¬âš¬âš¬\nâš¬â—âš¬âš¬\nâš¬âš¬â—âš¬âš¬\nâš¬âš¬âš¬â—âš¬\nâš¬âš¬âš¬âš¬â—\n\nstride=2ï¼š\n[â—]âš¬[â—]âš¬[â—]\nâš¬âš¬âš¬âš¬âš¬\n[â—]âš¬[â—]âš¬[â—]\nè¾“å‡ºå°ºå¯¸è®¡ç®—ï¼š\nH_out = floor((H_in + 2Ã—padding - kernel_size) / stride) + 1\nW_out = floor((W_in + 2Ã—padding - kernel_size) / stride) + 1\n\n\n\n7.4.3 ğŸ”¹ æ„Ÿå—é‡ (Receptive Field)\nå®šä¹‰ï¼šè¾“å‡ºç‰¹å¾å›¾çš„ä¸€ä¸ªåƒç´ èƒ½â€çœ‹åˆ°â€çš„è¾“å…¥åŒºåŸŸå¤§å°\nå•å±‚ 3Ã—3 å·ç§¯ï¼š\n  æ„Ÿå—é‡ = 3Ã—3\n\nä¸¤å±‚ 3Ã—3 å·ç§¯ï¼š\n  æ„Ÿå—é‡ = 5Ã—5\n\nä¸‰å±‚ 3Ã—3 å·ç§¯ï¼š\n  æ„Ÿå—é‡ = 7Ã—7\nè®¡ç®—å…¬å¼ï¼š\nRF_l = RF_{l-1} + (kernel_size - 1) Ã— Î (stride_i)\næ„ä¹‰ï¼š - æ·±å±‚ç¥ç»å…ƒèƒ½çœ‹åˆ°æ›´å¤§èŒƒå›´ - å¯ä»¥æ•è·æ›´é«˜çº§çš„ç‰¹å¾\n\n\n\n7.4.4 ğŸ”¹ æ± åŒ– (Pooling)\nç›®çš„ï¼šé™ä½ç‰¹å¾å›¾å°ºå¯¸ï¼Œå‡å°‘è®¡ç®—é‡\n\n7.4.4.1 Max Poolingï¼ˆæœ€å¤§æ± åŒ–ï¼‰\nè¾“å…¥ 4Ã—4:\n1  3  2  4\n5  6  7  8\n9  10 11 12\n13 14 15 16\n\nMax Pooling 2Ã—2, stride=2:\n[1  3]  [2  4]      6   8\n[5  6]  [7  8]  â†’\n                    14  16\n[9  10] [11 12]\n[13 14] [15 16]\n\nè¾“å‡º 2Ã—2:\n6  8\n14 16\n\n\n7.4.4.2 Average Poolingï¼ˆå¹³å‡æ± åŒ–ï¼‰\nå¯¹æ¯ä¸ªåŒºåŸŸå–å¹³å‡å€¼\nä»£ç ï¼š\ndef max_pool2d(input, pool_size=2, stride=2):\n    \"\"\"Max Pooling\"\"\"\n    H, W = input.shape\n    H_out = (H - pool_size) // stride + 1\n    W_out = (W - pool_size) // stride + 1\n\n    output = np.zeros((H_out, W_out))\n\n    for i in range(H_out):\n        for j in range(W_out):\n            region = input[i*stride:i*stride+pool_size,\n                          j*stride:j*stride+pool_size]\n            output[i, j] = np.max(region)\n\n    return output\n\n# PyTorch\nimport torch.nn as nn\nmax_pool = nn.MaxPool2d(kernel_size=2, stride=2)\navg_pool = nn.AvgPool2d(kernel_size=2, stride=2)\nç‰¹ç‚¹ï¼š - âœ… å‡å°‘å‚æ•°å’Œè®¡ç®—é‡ - âœ… å¢åŠ å¹³ç§»ä¸å˜æ€§ - âœ… æ‰©å¤§æ„Ÿå—é‡ - âŒ ä¸¢å¤±ä½ç½®ä¿¡æ¯",
    "crumbs": [
      "ç¬¬äºŒéƒ¨åˆ†ï¼šè¿›é˜¶ç¯‡",
      "<span class='chapter-number'>7</span>Â  <span class='chapter-title'>ç¬¬å…­ç« ï¼šå·ç§¯ç¥ç»ç½‘ç»œ (Convolutional Neural Networks)</span>"
    ]
  },
  {
    "objectID": "Chapter6.html#å…¸å‹-cnn-æ¶æ„",
    "href": "Chapter6.html#å…¸å‹-cnn-æ¶æ„",
    "title": "7Â  ç¬¬å…­ç« ï¼šå·ç§¯ç¥ç»ç½‘ç»œ (Convolutional Neural Networks)",
    "section": "7.5 6.4 å…¸å‹ CNN æ¶æ„",
    "text": "7.5 6.4 å…¸å‹ CNN æ¶æ„\n\n7.5.1 ğŸ—ï¸ åŸºæœ¬ç»“æ„\nè¾“å…¥å›¾åƒ\n    â†“\n[å·ç§¯ â†’ ReLU â†’ æ± åŒ–] Ã— N å±‚\n    â†“\nå±•å¹³\n    â†“\nå…¨è¿æ¥å±‚ Ã— M å±‚\n    â†“\nSoftmax è¾“å‡º\n\n\n\n7.5.2 ğŸ”¹ LeNet-5 (1998) - CNN çš„å¼€å±±ä¹‹ä½œ\næ¶æ„ï¼š\nè¾“å…¥: 32Ã—32Ã—1 (ç°åº¦å›¾)\n    â†“\nConv1: 6ä¸ª 5Ã—5 å·ç§¯æ ¸ â†’ 28Ã—28Ã—6\n    â†“\nAvgPool1: 2Ã—2 â†’ 14Ã—14Ã—6\n    â†“\nConv2: 16ä¸ª 5Ã—5 å·ç§¯æ ¸ â†’ 10Ã—10Ã—16\n    â†“\nAvgPool2: 2Ã—2 â†’ 5Ã—5Ã—16\n    â†“\nFlatten â†’ 400\n    â†“\nFC1: 120\n    â†“\nFC2: 84\n    â†“\nFC3: 10 (è¾“å‡º)\nä»£ç å®ç°ï¼š\nimport torch\nimport torch.nn as nn\n\nclass LeNet5(nn.Module):\n    def __init__(self):\n        super(LeNet5, self).__init__()\n\n        # ç‰¹å¾æå–éƒ¨åˆ†\n        self.conv1 = nn.Conv2d(1, 6, kernel_size=5, padding=2)  # 32â†’32\n        self.pool1 = nn.AvgPool2d(kernel_size=2, stride=2)       # 32â†’16\n\n        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)             # 16â†’12\n        self.pool2 = nn.AvgPool2d(kernel_size=2, stride=2)       # 12â†’6\n\n        # åˆ†ç±»éƒ¨åˆ†\n        self.fc1 = nn.Linear(16 * 6 * 6, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n    def forward(self, x):\n        # å·ç§¯å±‚\n        x = torch.relu(self.conv1(x))\n        x = self.pool1(x)\n\n        x = torch.relu(self.conv2(x))\n        x = self.pool2(x)\n\n        # å±•å¹³\n        x = x.view(-1, 16 * 6 * 6)\n\n        # å…¨è¿æ¥å±‚\n        x = torch.relu(self.fc1(x))\n        x = torch.relu(self.fc2(x))\n        x = self.fc3(x)\n\n        return x\n\nmodel = LeNet5()\nprint(model)\n\n\n\n7.5.3 ğŸ”¹ AlexNet (2012) - æ·±åº¦å­¦ä¹ å¤å…´\nåˆ›æ–°ç‚¹ï¼š - ä½¿ç”¨ ReLU æ›¿ä»£ Sigmoid/Tanh - ä½¿ç”¨ Dropout é˜²æ­¢è¿‡æ‹Ÿåˆ - æ•°æ®å¢å¼º - GPU åŠ é€Ÿè®­ç»ƒ - æ›´æ·±çš„ç½‘ç»œï¼ˆ8å±‚ï¼‰\næ¶æ„ï¼š\nè¾“å…¥: 224Ã—224Ã—3\n    â†“\nConv1: 96ä¸ª 11Ã—11, stride=4 â†’ 55Ã—55Ã—96\nMaxPool1: 3Ã—3, stride=2 â†’ 27Ã—27Ã—96\n    â†“\nConv2: 256ä¸ª 5Ã—5 â†’ 27Ã—27Ã—256\nMaxPool2: 3Ã—3, stride=2 â†’ 13Ã—13Ã—256\n    â†“\nConv3: 384ä¸ª 3Ã—3 â†’ 13Ã—13Ã—384\nConv4: 384ä¸ª 3Ã—3 â†’ 13Ã—13Ã—384\nConv5: 256ä¸ª 3Ã—3 â†’ 13Ã—13Ã—256\nMaxPool3: 3Ã—3, stride=2 â†’ 6Ã—6Ã—256\n    â†“\nFC1: 4096 + Dropout\nFC2: 4096 + Dropout\nFC3: 1000 (ImageNet)\nä»£ç ï¼š\nclass AlexNet(nn.Module):\n    def __init__(self, num_classes=1000):\n        super(AlexNet, self).__init__()\n\n        self.features = nn.Sequential(\n            # Conv1\n            nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n\n            # Conv2\n            nn.Conv2d(96, 256, kernel_size=5, padding=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n\n            # Conv3\n            nn.Conv2d(256, 384, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n\n            # Conv4\n            nn.Conv2d(384, 384, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n\n            # Conv5\n            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n        )\n\n        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n\n        self.classifier = nn.Sequential(\n            nn.Dropout(0.5),\n            nn.Linear(256 * 6 * 6, 4096),\n            nn.ReLU(inplace=True),\n\n            nn.Dropout(0.5),\n            nn.Linear(4096, 4096),\n            nn.ReLU(inplace=True),\n\n            nn.Linear(4096, num_classes),\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.classifier(x)\n        return x\n\n\n\n7.5.4 ğŸ”¹ VGGNet (2014) - æ›´æ·±æ›´è§„æ•´\næ ¸å¿ƒæ€æƒ³ï¼š - åªä½¿ç”¨ 3Ã—3 å°å·ç§¯æ ¸ - ç½‘ç»œæ›´æ·±ï¼ˆ16-19å±‚ï¼‰ - ç»“æ„è§„æ•´ï¼Œæ˜“äºç†è§£\nä¸ºä»€ä¹ˆ 3Ã—3ï¼Ÿ\nä¸¤ä¸ª 3Ã—3 å·ç§¯ = ä¸€ä¸ª 5Ã—5 æ„Ÿå—é‡\nä¸‰ä¸ª 3Ã—3 å·ç§¯ = ä¸€ä¸ª 7Ã—7 æ„Ÿå—é‡\n\nä½†å‚æ•°æ›´å°‘ï¼š\n  7Ã—7: 49 ä¸ªå‚æ•°\n  3ä¸ª3Ã—3: 27 ä¸ªå‚æ•°\nVGG-16 æ¶æ„ï¼š\nè¾“å…¥: 224Ã—224Ã—3\n\nBlock 1:\n  Conv3-64 Ã— 2\n  MaxPool\n\nBlock 2:\n  Conv3-128 Ã— 2\n  MaxPool\n\nBlock 3:\n  Conv3-256 Ã— 3\n  MaxPool\n\nBlock 4:\n  Conv3-512 Ã— 3\n  MaxPool\n\nBlock 5:\n  Conv3-512 Ã— 3\n  MaxPool\n\nFC: 4096 â†’ 4096 â†’ 1000\nä»£ç ï¼š\nclass VGG16(nn.Module):\n    def __init__(self, num_classes=1000):\n        super(VGG16, self).__init__()\n\n        self.features = nn.Sequential(\n            # Block 1\n            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n\n            # Block 2\n            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n\n            # Block 3\n            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n\n            # Block 4\n            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n\n            # Block 5\n            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n        )\n\n        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n\n        self.classifier = nn.Sequential(\n            nn.Linear(512 * 7 * 7, 4096),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.5),\n\n            nn.Linear(4096, 4096),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.5),\n\n            nn.Linear(4096, num_classes),\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.classifier(x)\n        return x\n\n\n\n7.5.5 ğŸ”¹ ResNet (2015) - æ®‹å·®ç½‘ç»œ â­\né—®é¢˜ï¼šç½‘ç»œè¶Šæ·±ï¼Œæ€§èƒ½è¶Šå·®ï¼Ÿ\n56å±‚ç½‘ç»œçš„è®­ç»ƒè¯¯å·® &gt; 20å±‚ç½‘ç»œï¼Ÿ\nè¿™ä¸æ˜¯è¿‡æ‹Ÿåˆï¼Œè€Œæ˜¯ä¼˜åŒ–é—®é¢˜ï¼\nè§£å†³ï¼šæ®‹å·®è¿æ¥ï¼ˆSkip Connectionï¼‰\nä¼ ç»Ÿï¼š\n  x â†’ Conv â†’ ReLU â†’ Conv â†’ ReLU â†’ output\n\nResNetï¼š\n  x â†’ Conv â†’ ReLU â†’ Conv â”€â”¬â†’ ReLU â†’ output\n  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n       (ç›´æ¥è¿æ¥)\n\nè¾“å‡º = F(x) + x\nä¼˜åŠ¿ï¼š - è§£å†³æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸ - å…è®¸è®­ç»ƒè¶…æ·±ç½‘ç»œï¼ˆ152å±‚ï¼Œç”šè‡³1000å±‚ï¼‰ - æ€§èƒ½æ˜¾è‘—æå‡\nResidual Blockï¼š\nclass ResidualBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, stride=1):\n        super(ResidualBlock, self).__init__()\n\n        # ä¸»è·¯å¾„\n        self.conv1 = nn.Conv2d(in_channels, out_channels,\n                               kernel_size=3, stride=stride, padding=1)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n\n        self.conv2 = nn.Conv2d(out_channels, out_channels,\n                               kernel_size=3, stride=1, padding=1)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n\n        # æ·å¾„è¿æ¥\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_channels != out_channels:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels,\n                         kernel_size=1, stride=stride),\n                nn.BatchNorm2d(out_channels)\n            )\n\n    def forward(self, x):\n        # ä¸»è·¯å¾„\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = torch.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        # æ®‹å·®è¿æ¥\n        out += self.shortcut(x)\n        out = torch.relu(out)\n\n        return out\nResNet-34 æ¶æ„ï¼š\nclass ResNet34(nn.Module):\n    def __init__(self, num_classes=1000):\n        super(ResNet34, self).__init__()\n\n        # åˆå§‹å±‚\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n\n        # Residual blocks\n        self.layer1 = self._make_layer(64, 64, 3, stride=1)\n        self.layer2 = self._make_layer(64, 128, 4, stride=2)\n        self.layer3 = self._make_layer(128, 256, 6, stride=2)\n        self.layer4 = self._make_layer(256, 512, 3, stride=2)\n\n        # åˆ†ç±»å±‚\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(512, num_classes)\n\n    def _make_layer(self, in_channels, out_channels, num_blocks, stride):\n        layers = []\n\n        # ç¬¬ä¸€ä¸ª block å¯èƒ½æ”¹å˜å°ºå¯¸\n        layers.append(ResidualBlock(in_channels, out_channels, stride))\n\n        # å‰©ä½™çš„ blocks\n        for _ in range(1, num_blocks):\n            layers.append(ResidualBlock(out_channels, out_channels, 1))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        # åˆå§‹å·ç§¯\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = torch.relu(x)\n        x = self.maxpool(x)\n\n        # Residual blocks\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        # åˆ†ç±»\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.fc(x)\n\n        return x\nResNet å˜ç§ï¼š\n\n\n\næ¨¡å‹\nå±‚æ•°\nTop-5 é”™è¯¯ç‡ (ImageNet)\n\n\n\n\nResNet-18\n18\n~10%\n\n\nResNet-34\n34\n~8%\n\n\nResNet-50\n50\n~6.7%\n\n\nResNet-101\n101\n~6.4%\n\n\nResNet-152\n152\n~6.2%",
    "crumbs": [
      "ç¬¬äºŒéƒ¨åˆ†ï¼šè¿›é˜¶ç¯‡",
      "<span class='chapter-number'>7</span>Â  <span class='chapter-title'>ç¬¬å…­ç« ï¼šå·ç§¯ç¥ç»ç½‘ç»œ (Convolutional Neural Networks)</span>"
    ]
  },
  {
    "objectID": "Chapter6.html#ç°ä»£-cnn-æŠ€å·§",
    "href": "Chapter6.html#ç°ä»£-cnn-æŠ€å·§",
    "title": "7Â  ç¬¬å…­ç« ï¼šå·ç§¯ç¥ç»ç½‘ç»œ (Convolutional Neural Networks)",
    "section": "7.6 6.5 ç°ä»£ CNN æŠ€å·§",
    "text": "7.6 6.5 ç°ä»£ CNN æŠ€å·§\n\n7.6.1 ğŸ”¹ 1Ã—1 å·ç§¯\nä½œç”¨ï¼š - æ”¹å˜é€šé“æ•°ï¼ˆé™ç»´/å‡ç»´ï¼‰ - å¢åŠ éçº¿æ€§ - å‚æ•°å°‘\nè¾“å…¥: 56Ã—56Ã—192\n1Ã—1 å·ç§¯ï¼Œ64ä¸ªæ ¸\nè¾“å‡º: 56Ã—56Ã—64\n\nä½œç”¨ç±»ä¼¼å…¨è¿æ¥ï¼Œä½†ä¿æŒç©ºé—´ç»“æ„\n# é™ç»´ç¤ºä¾‹\nnn.Conv2d(192, 64, kernel_size=1)  # 192é€šé“ â†’ 64é€šé“\n\n\n\n7.6.2 ğŸ”¹ å…¨å±€å¹³å‡æ± åŒ– (Global Average Pooling)\næ›¿ä»£å…¨è¿æ¥å±‚\nä¼ ç»Ÿï¼š\n  7Ã—7Ã—512 â†’ Flatten â†’ FC(4096) â†’ FC(1000)\n  å‚æ•°é‡ï¼š7Ã—7Ã—512Ã—4096 â‰ˆ 102M\n\nGAPï¼š\n  7Ã—7Ã—512 â†’ GAP â†’ 512 â†’ FC(1000)\n  å‚æ•°é‡ï¼š512Ã—1000 â‰ˆ 512K\n# PyTorch\nself.gap = nn.AdaptiveAvgPool2d((1, 1))  # è¾“å‡º 1Ã—1Ã—C\n\n# ä½¿ç”¨\nx = self.gap(x)  # (B, C, H, W) â†’ (B, C, 1, 1)\nx = x.view(x.size(0), -1)  # (B, C)\nä¼˜åŠ¿ï¼š - å‚æ•°å¤§å¹…å‡å°‘ - æ›´å¼ºçš„ç©ºé—´ä¸å˜æ€§ - å‡å°‘è¿‡æ‹Ÿåˆ\n\n\n\n7.6.3 ğŸ”¹ æ·±åº¦å¯åˆ†ç¦»å·ç§¯ (Depthwise Separable Convolution)\nMobileNet çš„æ ¸å¿ƒ\næ ‡å‡†å·ç§¯ï¼š\nè¾“å…¥: HÃ—WÃ—C_in\nå·ç§¯æ ¸: KÃ—KÃ—C_inÃ—C_out\nå‚æ•°é‡: KÃ—KÃ—C_inÃ—C_out\næ·±åº¦å¯åˆ†ç¦»å·ç§¯ï¼šåˆ†ä¸¤æ­¥\n1. Depthwise å·ç§¯ï¼š\n   æ¯ä¸ªè¾“å…¥é€šé“å•ç‹¬å·ç§¯\n   å‚æ•°: KÃ—KÃ—C_in\n\n2. Pointwise å·ç§¯ï¼š\n   1Ã—1 å·ç§¯æ··åˆé€šé“\n   å‚æ•°: 1Ã—1Ã—C_inÃ—C_out\n\næ€»å‚æ•°: KÃ—KÃ—C_in + C_inÃ—C_out\nå‚æ•°å‡å°‘æ¯”ä¾‹ï¼š\n(KÂ²Ã—C_in + C_inÃ—C_out) / (KÂ²Ã—C_inÃ—C_out)\n= 1/C_out + 1/KÂ²\nä»£ç ï¼š\nclass DepthwiseSeparableConv(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size=3):\n        super().__init__()\n\n        # Depthwise\n        self.depthwise = nn.Conv2d(\n            in_channels, in_channels,\n            kernel_size=kernel_size,\n            padding=kernel_size//2,\n            groups=in_channels  # å…³é”®ï¼æ¯ç»„ä¸€ä¸ªé€šé“\n        )\n\n        # Pointwise\n        self.pointwise = nn.Conv2d(\n            in_channels, out_channels,\n            kernel_size=1\n        )\n\n    def forward(self, x):\n        x = self.depthwise(x)\n        x = self.pointwise(x)\n        return x",
    "crumbs": [
      "ç¬¬äºŒéƒ¨åˆ†ï¼šè¿›é˜¶ç¯‡",
      "<span class='chapter-number'>7</span>Â  <span class='chapter-title'>ç¬¬å…­ç« ï¼šå·ç§¯ç¥ç»ç½‘ç»œ (Convolutional Neural Networks)</span>"
    ]
  },
  {
    "objectID": "Chapter6.html#å®æˆ˜cifar-10-å›¾åƒåˆ†ç±»",
    "href": "Chapter6.html#å®æˆ˜cifar-10-å›¾åƒåˆ†ç±»",
    "title": "7Â  ç¬¬å…­ç« ï¼šå·ç§¯ç¥ç»ç½‘ç»œ (Convolutional Neural Networks)",
    "section": "7.7 6.6 å®æˆ˜ï¼šCIFAR-10 å›¾åƒåˆ†ç±»",
    "text": "7.7 6.6 å®æˆ˜ï¼šCIFAR-10 å›¾åƒåˆ†ç±»\n\n7.7.1 ğŸ“‹ æ•°æ®é›†ä»‹ç»\nCIFAR-10:\n  - 60,000 å¼  32Ã—32 å½©è‰²å›¾åƒ\n  - 10 ä¸ªç±»åˆ«ï¼ˆé£æœºã€æ±½è½¦ã€é¸Ÿ...ï¼‰\n  - 50,000 è®­ç»ƒ + 10,000 æµ‹è¯•\n\n\n7.7.2 ğŸ’» å®Œæ•´å®ç°\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\n\n# ==================== è¶…å‚æ•° ====================\nBATCH_SIZE = 128\nLEARNING_RATE = 0.001\nEPOCHS = 50\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# ==================== æ•°æ®å¢å¼º ====================\ntransform_train = transforms.Compose([\n    transforms.RandomCrop(32, padding=4),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize((0.4914, 0.4822, 0.4465),\n                        (0.2023, 0.1994, 0.2010))\n])\n\ntransform_test = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.4914, 0.4822, 0.4465),\n                        (0.2023, 0.1994, 0.2010))\n])\n\n# ==================== åŠ è½½æ•°æ® ====================\ntrain_dataset = datasets.CIFAR10(\n    root='./data', train=True, download=True, transform=transform_train\n)\ntest_dataset = datasets.CIFAR10(\n    root='./data', train=False, download=True, transform=transform_test\n)\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n                          shuffle=True, num_workers=2)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n                         shuffle=False, num_workers=2)\n\n# ==================== å®šä¹‰æ¨¡å‹ ====================\nclass SimpleCNN(nn.Module):\n    def __init__(self):\n        super(SimpleCNN, self).__init__()\n\n        # Block 1\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n        self.bn2 = nn.BatchNorm2d(64)\n        self.pool1 = nn.MaxPool2d(2, 2)\n        self.dropout1 = nn.Dropout(0.2)\n\n        # Block 2\n        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n        self.bn3 = nn.BatchNorm2d(128)\n        self.conv4 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n        self.bn4 = nn.BatchNorm2d(128)\n        self.pool2 = nn.MaxPool2d(2, 2)\n        self.dropout2 = nn.Dropout(0.3)\n\n        # Block 3\n        self.conv5 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n        self.bn5 = nn.BatchNorm2d(256)\n        self.conv6 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n        self.bn6 = nn.BatchNorm2d(256)\n        self.pool3 = nn.MaxPool2d(2, 2)\n        self.dropout3 = nn.Dropout(0.4)\n\n        # å…¨è¿æ¥å±‚\n        self.fc1 = nn.Linear(256 * 4 * 4, 512)\n        self.dropout4 = nn.Dropout(0.5)\n        self.fc2 = nn.Linear(512, 10)\n\n    def forward(self, x):\n        # Block 1\n        x = torch.relu(self.bn1(self.conv1(x)))\n        x = torch.relu(self.bn2(self.conv2(x)))\n        x = self.pool1(x)\n        x = self.dropout1(x)\n\n        # Block 2\n        x = torch.relu(self.bn3(self.conv3(x)))\n        x = torch.relu(self.bn4(self.conv4(x)))\n        x = self.pool2(x)\n        x = self.dropout2(x)\n\n        # Block 3\n        x = torch.relu(self.bn5(self.conv5(x)))\n        x = torch.relu(self.bn6(self.conv6(x)))\n        x = self.pool3(x)\n        x = self.dropout3(x)\n\n        # å±•å¹³å’Œå…¨è¿æ¥\n        x = x.view(-1, 256 * 4 * 4)\n        x = torch.relu(self.fc1(x))\n        x = self.dropout4(x)\n        x = self.fc2(x)\n\n        return x\n\nmodel = SimpleCNN().to(DEVICE)\n\n# æŸ¥çœ‹æ¨¡å‹ç»“æ„\nfrom torchsummary import summary\nsummary(model, (3, 32, 32))\n\n# ==================== æŸå¤±å’Œä¼˜åŒ–å™¨ ====================\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\nscheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n\n# ==================== è®­ç»ƒå‡½æ•° ====================\ndef train_epoch(model, train_loader, criterion, optimizer, device):\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n\n    progress_bar = tqdm(train_loader, desc='Training')\n\n    for images, labels in progress_bar:\n        images, labels = images.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        _, predicted = outputs.max(1)\n        total += labels.size(0)\n        correct += predicted.eq(labels).sum().item()\n\n        progress_bar.set_postfix({\n            'loss': f'{running_loss/len(train_loader):.3f}',\n            'acc': f'{100.*correct/total:.2f}%'\n        })\n\n    return running_loss / len(train_loader), 100. * correct / total\n\n# ==================== æµ‹è¯•å‡½æ•° ====================\ndef test(model, test_loader, criterion, device):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    total = 0\n\n    with torch.no_grad():\n        for images, labels in tqdm(test_loader, desc='Testing'):\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n\n            test_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n\n    return test_loss / len(test_loader), 100. * correct / total\n\n# ==================== è®­ç»ƒå¾ªç¯ ====================\ntrain_losses, train_accs = [], []\ntest_losses, test_accs = [], []\nbest_acc = 0\n\nfor epoch in range(EPOCHS):\n    print(f'\\n=== Epoch {epoch+1}/{EPOCHS} ===')\n\n    train_loss, train_acc = train_epoch(model, train_loader, criterion,\n                                        optimizer, DEVICE)\n    test_loss, test_acc = test(model, test_loader, criterion, DEVICE)\n\n    train_losses.append(train_loss)\n    train_accs.append(train_acc)\n    test_losses.append(test_loss)\n    test_accs.append(test_acc)\n\n    print(f'Train Loss: {train_loss:.4f}, Acc: {train_acc:.2f}%')\n    print(f'Test  Loss: {test_loss:.4f}, Acc: {test_acc:.2f}%')\n\n    # ä¿å­˜æœ€ä½³æ¨¡å‹\n    if test_acc &gt; best_acc:\n        best_acc = test_acc\n        torch.save(model.state_dict(), 'best_model.pth')\n        print(f'âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Acc: {best_acc:.2f}%)')\n\n    scheduler.step()\n\n# ==================== å¯è§†åŒ– ====================\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n\nepochs_range = range(1, len(train_losses) + 1)\n\nax1.plot(epochs_range, train_losses, 'b-', label='Train Loss')\nax1.plot(epochs_range, test_losses, 'r-', label='Test Loss')\nax1.set_xlabel('Epoch')\nax1.set_ylabel('Loss')\nax1.set_title('Training and Test Loss')\nax1.legend()\nax1.grid(True, alpha=0.3)\n\nax2.plot(epochs_range, train_accs, 'b-', label='Train Acc')\nax2.plot(epochs_range, test_accs, 'r-', label='Test Acc')\nax2.set_xlabel('Epoch')\nax2.set_ylabel('Accuracy (%)')\nax2.set_title('Training and Test Accuracy')\nax2.legend()\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# ==================== æœ€ç»ˆè¯„ä¼° ====================\nmodel.load_state_dict(torch.load('best_model.pth'))\n_, final_acc = test(model, test_loader, criterion, DEVICE)\nprint(f'\\næœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡: {final_acc:.2f}%')",
    "crumbs": [
      "ç¬¬äºŒéƒ¨åˆ†ï¼šè¿›é˜¶ç¯‡",
      "<span class='chapter-number'>7</span>Â  <span class='chapter-title'>ç¬¬å…­ç« ï¼šå·ç§¯ç¥ç»ç½‘ç»œ (Convolutional Neural Networks)</span>"
    ]
  },
  {
    "objectID": "Chapter6.html#å·ç§¯ç¥ç»ç½‘ç»œçš„å¯è§†åŒ–",
    "href": "Chapter6.html#å·ç§¯ç¥ç»ç½‘ç»œçš„å¯è§†åŒ–",
    "title": "7Â  ç¬¬å…­ç« ï¼šå·ç§¯ç¥ç»ç½‘ç»œ (Convolutional Neural Networks)",
    "section": "7.8 6.7 å·ç§¯ç¥ç»ç½‘ç»œçš„å¯è§†åŒ–",
    "text": "7.8 6.7 å·ç§¯ç¥ç»ç½‘ç»œçš„å¯è§†åŒ–\n\n7.8.1 ğŸ¨ ç‰¹å¾å›¾å¯è§†åŒ–\ndef visualize_feature_maps(model, image, device):\n    \"\"\"å¯è§†åŒ–ä¸­é—´å±‚çš„ç‰¹å¾å›¾\"\"\"\n    model.eval()\n\n    # æå–ç‰¹å¾å›¾çš„é’©å­\n    activations = {}\n\n    def get_activation(name):\n        def hook(model, input, output):\n            activations[name] = output.detach()\n        return hook\n\n    # æ³¨å†Œé’©å­\n    model.conv1.register_forward_hook(get_activation('conv1'))\n    model.conv3.register_forward_hook(get_activation('conv3'))\n    model.conv5.register_forward_hook(get_activation('conv5'))\n\n    # å‰å‘ä¼ æ’­\n    with torch.no_grad():\n        output = model(image.to(device))\n\n    # å¯è§†åŒ–\n    fig, axes = plt.subplots(3, 8, figsize=(16, 6))\n\n    # Conv1 ç‰¹å¾å›¾\n    feat1 = activations['conv1'][0]\n    for i in range(8):\n        ax = axes[0, i]\n        ax.imshow(feat1[i].cpu().numpy(), cmap='gray')\n        ax.set_title(f'Conv1-{i}')\n        ax.axis('off')\n\n    # Conv3 ç‰¹å¾å›¾\n    feat3 = activations['conv3'][0]\n    for i in range(8):\n        ax = axes[1, i]\n        ax.imshow(feat3[i].cpu().numpy(), cmap='gray')\n        ax.set_title(f'Conv3-{i}')\n        ax.axis('off')\n\n    # Conv5 ç‰¹å¾å›¾\n    feat5 = activations['conv5'][0]\n    for i in range(8):\n        ax = axes[2, i]\n        ax.imshow(feat5[i].cpu().numpy(), cmap='gray')\n        ax.set_title(f'Conv5-{i}')\n        ax.axis('off')\n\n    plt.tight_layout()\n    plt.show()\n\n# ä½¿ç”¨\ntest_image, _ = test_dataset[0]\ntest_image = test_image.unsqueeze(0)\nvisualize_feature_maps(model, test_image, DEVICE)\n\n\n\n7.8.2 ğŸ¯ å·ç§¯æ ¸å¯è§†åŒ–\ndef visualize_kernels(model):\n    \"\"\"å¯è§†åŒ–ç¬¬ä¸€å±‚å·ç§¯æ ¸\"\"\"\n    conv1_weight = model.conv1.weight.data\n\n    fig, axes = plt.subplots(8, 8, figsize=(12, 12))\n\n    for i in range(64):\n        ax = axes[i // 8, i % 8]\n\n        # å¹³å‡RGBä¸‰ä¸ªé€šé“\n        kernel = conv1_weight[i].mean(dim=0)\n\n        # æ ‡å‡†åŒ–åˆ° [0, 1]\n        kernel = (kernel - kernel.min()) / (kernel.max() - kernel.min())\n\n        ax.imshow(kernel.cpu().numpy(), cmap='gray')\n        ax.set_title(f'Filter {i}')\n        ax.axis('off')\n\n    plt.tight_layout()\n    plt.show()\n\n# ä½¿ç”¨\nvisualize_kernels(model)",
    "crumbs": [
      "ç¬¬äºŒéƒ¨åˆ†ï¼šè¿›é˜¶ç¯‡",
      "<span class='chapter-number'>7</span>Â  <span class='chapter-title'>ç¬¬å…­ç« ï¼šå·ç§¯ç¥ç»ç½‘ç»œ (Convolutional Neural Networks)</span>"
    ]
  },
  {
    "objectID": "Chapter6.html#cnn-vs-vitå±•æœ›",
    "href": "Chapter6.html#cnn-vs-vitå±•æœ›",
    "title": "7Â  ç¬¬å…­ç« ï¼šå·ç§¯ç¥ç»ç½‘ç»œ (Convolutional Neural Networks)",
    "section": "7.9 6.8 CNN vs ViTï¼ˆå±•æœ›ï¼‰",
    "text": "7.9 6.8 CNN vs ViTï¼ˆå±•æœ›ï¼‰\n\n7.9.1 ğŸ”„ ä» CNN åˆ° Vision Transformer\nCNN çš„å±€é™ï¼š - æ„Ÿå—é‡å±€é™ï¼ˆé€å±‚æ‰©å¤§ï¼‰ - ç©ºé—´å½’çº³åç½®å¼ºï¼ˆå¯èƒ½é™åˆ¶æ€§èƒ½ï¼‰ - éœ€è¦æ›´å¤šæ•°æ®\nVision Transformer çš„ä¼˜åŠ¿ï¼š - å…¨å±€æ„Ÿå—é‡ï¼ˆä»ç¬¬ä¸€å±‚ï¼‰ - æ›´çµæ´»çš„ç‰¹å¾æå– - æ‰©å±•æ€§å¥½\nCNN æ¶æ„ï¼š\n  å·ç§¯ â†’ å·ç§¯ â†’ å·ç§¯ â†’ ç‰¹å¾\n  (é€å±‚èšåˆå±€éƒ¨ä¿¡æ¯)\n\nViT æ¶æ„ï¼š\n  Patch Embedding â†’ Transformer â†’ ç‰¹å¾\n  (ç›´æ¥æ•è·å…¨å±€å…³ç³»)",
    "crumbs": [
      "ç¬¬äºŒéƒ¨åˆ†ï¼šè¿›é˜¶ç¯‡",
      "<span class='chapter-number'>7</span>Â  <span class='chapter-title'>ç¬¬å…­ç« ï¼šå·ç§¯ç¥ç»ç½‘ç»œ (Convolutional Neural Networks)</span>"
    ]
  },
  {
    "objectID": "Chapter6.html#æœ¬ç« ä½œä¸š",
    "href": "Chapter6.html#æœ¬ç« ä½œä¸š",
    "title": "7Â  ç¬¬å…­ç« ï¼šå·ç§¯ç¥ç»ç½‘ç»œ (Convolutional Neural Networks)",
    "section": "7.10 ğŸ“ æœ¬ç« ä½œä¸š",
    "text": "7.10 ğŸ“ æœ¬ç« ä½œä¸š\n\n7.10.1 ä½œä¸š 1ï¼šä»é›¶å®ç°å·ç§¯\n# TODO:\n# 1. å®ç°å‰å‘å·ç§¯ï¼ˆå·²æä¾›ï¼‰\n# 2. å®ç°åå‘ä¼ æ’­ï¼šæ¢¯åº¦w.r.t è¾“å…¥ã€æƒé‡ã€åç½®\n# 3. å®ç°æ± åŒ–å‰å‘å’Œåå‘\n# 4. ç»„åˆæˆå®Œæ•´ CNN å±‚\n# 5. åœ¨å°æ•°æ®é›†ä¸Šæµ‹è¯•\n\n\n7.10.2 ä½œä¸š 2ï¼šCNN æ¶æ„å¯¹æ¯”\nåœ¨ CIFAR-10 ä¸Šå®ç°å¹¶å¯¹æ¯”ï¼š\n# 1. LeNet-5\n# 2. ç®€å• CNN (3å±‚å·ç§¯)\n# 3. VGG-16 (ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹)\n# 4. ResNet-18 (ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹)\n\n# è®°å½•ï¼š\n#   - å‚æ•°æ•°é‡\n#   - è®­ç»ƒæ—¶é—´\n#   - æœ€ç»ˆå‡†ç¡®ç‡\n#   - æ¨¡å‹å¤§å°\n\n# åˆ†æä¼˜ç¼ºç‚¹\n\n\n7.10.3 ä½œä¸š 3ï¼šç‰¹å¾å¯è§†åŒ–\n# å¯¹è®­ç»ƒå¥½çš„ CNN æ¨¡å‹ï¼š\n\n# 1. å¯è§†åŒ–ä¸åŒå±‚çš„ç‰¹å¾å›¾\n# 2. ç»˜åˆ¶å·ç§¯æ ¸\n# 3. å°è¯• DeconvNet æˆ– Grad-CAM è¿›è¡Œå¯è§†åŒ–\n# 4. åˆ†æä¸åŒå±‚å­¦åˆ°äº†ä»€ä¹ˆ\n\n# å†™ä¸€ä»½åˆ†ææŠ¥å‘Š\n\n\n7.10.4 ä½œä¸š 4ï¼šæ•°æ®å¢å¼ºå®éªŒ\n# åœ¨ CIFAR-10 ä¸Šæµ‹è¯•ä¸åŒçš„æ•°æ®å¢å¼ºæ–¹æ³•ï¼š\n\n# 1. æ— å¢å¼º\n# 2. éšæœºè£å‰ª + ç¿»è½¬\n# 3. + é¢œè‰²æŠ–åŠ¨\n# 4. + Cutout\n# 5. + MixUp / CutMix\n\n# è®°å½•æ€§èƒ½å·®å¼‚ï¼Œåˆ†ææ¯ç§å¢å¼ºçš„ä½œç”¨",
    "crumbs": [
      "ç¬¬äºŒéƒ¨åˆ†ï¼šè¿›é˜¶ç¯‡",
      "<span class='chapter-number'>7</span>Â  <span class='chapter-title'>ç¬¬å…­ç« ï¼šå·ç§¯ç¥ç»ç½‘ç»œ (Convolutional Neural Networks)</span>"
    ]
  },
  {
    "objectID": "Chapter6.html#æœ¬ç« å…³é”®æ¦‚å¿µ",
    "href": "Chapter6.html#æœ¬ç« å…³é”®æ¦‚å¿µ",
    "title": "7Â  ç¬¬å…­ç« ï¼šå·ç§¯ç¥ç»ç½‘ç»œ (Convolutional Neural Networks)",
    "section": "7.11 ğŸ”‘ æœ¬ç« å…³é”®æ¦‚å¿µ",
    "text": "7.11 ğŸ”‘ æœ¬ç« å…³é”®æ¦‚å¿µ\n\n\n\næ¦‚å¿µ\nè¯´æ˜\n\n\n\n\nå·ç§¯\nç‰¹å¾æå–çš„æ ¸å¿ƒæ“ä½œ\n\n\næ„Ÿå—é‡\nè¾“å‡ºèƒ½çœ‹åˆ°çš„è¾“å…¥åŒºåŸŸ\n\n\næ± åŒ–\né™ç»´å’Œç‰¹å¾èšåˆ\n\n\nå¡«å……\næ§åˆ¶è¾“å‡ºå°ºå¯¸\n\n\næ­¥é•¿\nå·ç§¯æ ¸ç§»åŠ¨è·ç¦»\n\n\nå‚æ•°å…±äº«\nCNN çš„æ ¸å¿ƒä¼˜åŠ¿\n\n\nLeNet\nCNN çš„å¼€åˆ›è€…\n\n\nAlexNet\næ·±åº¦å­¦ä¹ å¤å…´\n\n\nVGG\nè§„æ•´æ·±å±‚è®¾è®¡\n\n\nResNet\næ®‹å·®è¿æ¥è§£å†³æ·±åº¦é—®é¢˜\n\n\næ·±åº¦å¯åˆ†ç¦»å·ç§¯\nå‚æ•°é«˜æ•ˆè®¾è®¡",
    "crumbs": [
      "ç¬¬äºŒéƒ¨åˆ†ï¼šè¿›é˜¶ç¯‡",
      "<span class='chapter-number'>7</span>Â  <span class='chapter-title'>ç¬¬å…­ç« ï¼šå·ç§¯ç¥ç»ç½‘ç»œ (Convolutional Neural Networks)</span>"
    ]
  },
  {
    "objectID": "Chapter6.html#åç»­ç« èŠ‚é¢„å‘Š",
    "href": "Chapter6.html#åç»­ç« èŠ‚é¢„å‘Š",
    "title": "7Â  ç¬¬å…­ç« ï¼šå·ç§¯ç¥ç»ç½‘ç»œ (Convolutional Neural Networks)",
    "section": "7.12 ğŸ¯ åç»­ç« èŠ‚é¢„å‘Š",
    "text": "7.12 ğŸ¯ åç»­ç« èŠ‚é¢„å‘Š\nç¬¬ä¸ƒç« ï¼šå¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNN & LSTMï¼‰ - åºåˆ—æ•°æ®å¤„ç† - RNN çš„æ¢¯åº¦é—®é¢˜ - LSTM å’Œ GRU - åŒå‘ RNN\nç¬¬å…«ç« ï¼šAttention ä¸ Transformer - Self-Attention æœºåˆ¶ - Transformer æ¶æ„ - BERT å’Œ GPT\nç¬¬ä¹ç« ï¼šè¿ç§»å­¦ä¹ ä¸å¾®è°ƒ - é¢„è®­ç»ƒæ¨¡å‹ - ç‰¹å¾æå– - Fine-tuning ç­–ç•¥\n\nè¿™æ˜¯ä¸€éƒ¨ç³»ç»Ÿã€è¯¦ç»†ä¸”æ˜“æ‡‚çš„æ·±åº¦å­¦ä¹ æ•™ç¨‹ï¼Œæ¶µç›–äº†ä»åŸºç¡€åˆ°è¿›é˜¶çš„å®Œæ•´å†…å®¹ã€‚æ¯ç« éƒ½åŒ…å«ï¼š\nâœ… ç†è®ºè®²è§£ - ç›´è§‚æ˜“æ‡‚ï¼Œé…å›¾è¯´æ˜ âœ… æ•°å­¦æ¨å¯¼ - å…³é”®å…¬å¼è¯¦ç»†æ¨å¯¼ âœ… ä»£ç å®ç° - å®Œæ•´å¯è¿è¡Œçš„ç¤ºä¾‹ âœ… å®æˆ˜é¡¹ç›® - çœŸå®æ•°æ®é›†çš„ç«¯åˆ°ç«¯æµç¨‹ âœ… ä½œä¸šç»ƒä¹  - å¸®åŠ©å·©å›ºçŸ¥è¯† âœ… å¯è§†åŒ– - å¸®åŠ©ç†è§£å¤æ‚æ¦‚å¿µ",
    "crumbs": [
      "ç¬¬äºŒéƒ¨åˆ†ï¼šè¿›é˜¶ç¯‡",
      "<span class='chapter-number'>7</span>Â  <span class='chapter-title'>ç¬¬å…­ç« ï¼šå·ç§¯ç¥ç»ç½‘ç»œ (Convolutional Neural Networks)</span>"
    ]
  },
  {
    "objectID": "Chapter7.html",
    "href": "Chapter7.html",
    "title": "8Â  ç¬¬ä¸ƒç« ï¼šå¾ªç¯ç¥ç»ç½‘ç»œ (Recurrent Neural Networks)",
    "section": "",
    "text": "8.1 ğŸ“Œ ç« èŠ‚ç›®æ ‡",
    "crumbs": [
      "ç¬¬äºŒéƒ¨åˆ†ï¼šè¿›é˜¶ç¯‡",
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>ç¬¬ä¸ƒç« ï¼šå¾ªç¯ç¥ç»ç½‘ç»œ (Recurrent Neural Networks)</span>"
    ]
  },
  {
    "objectID": "Chapter7.html#ç« èŠ‚ç›®æ ‡",
    "href": "Chapter7.html#ç« èŠ‚ç›®æ ‡",
    "title": "8Â  ç¬¬ä¸ƒç« ï¼šå¾ªç¯ç¥ç»ç½‘ç»œ (Recurrent Neural Networks)",
    "section": "",
    "text": "ç†è§£åºåˆ—æ•°æ®å’Œå¾ªç¯ç»“æ„\næŒæ¡åŸºæœ¬ RNN åŠå…¶æ¢¯åº¦é—®é¢˜\næ·±å…¥å­¦ä¹  LSTM å’Œ GRU çš„è®¾è®¡\näº†è§£åŒå‘ RNN å’Œå¤šå±‚ RNN\nå®æˆ˜ï¼šæ–‡æœ¬åˆ†ç±»ã€æƒ…æ„Ÿåˆ†æã€åºåˆ—é¢„æµ‹",
    "crumbs": [
      "ç¬¬äºŒéƒ¨åˆ†ï¼šè¿›é˜¶ç¯‡",
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>ç¬¬ä¸ƒç« ï¼šå¾ªç¯ç¥ç»ç½‘ç»œ (Recurrent Neural Networks)</span>"
    ]
  },
  {
    "objectID": "Chapter7.html#ä¸ºä»€ä¹ˆéœ€è¦-rnn",
    "href": "Chapter7.html#ä¸ºä»€ä¹ˆéœ€è¦-rnn",
    "title": "8Â  ç¬¬ä¸ƒç« ï¼šå¾ªç¯ç¥ç»ç½‘ç»œ (Recurrent Neural Networks)",
    "section": "8.2 7.1 ä¸ºä»€ä¹ˆéœ€è¦ RNNï¼Ÿ",
    "text": "8.2 7.1 ä¸ºä»€ä¹ˆéœ€è¦ RNNï¼Ÿ\n\n8.2.1 ğŸ¯ åºåˆ—æ•°æ®çš„ç‰¹æ€§\nä»€ä¹ˆæ˜¯åºåˆ—æ•°æ®ï¼Ÿ\næ™®é€šæ•°æ®ï¼ˆç‹¬ç«‹ï¼‰ï¼š\n  å›¾ç‰‡ã€æˆ¿ä»·ã€åŒ»å­¦è¯Šæ–­\n  æ¯ä¸ªæ ·æœ¬æ˜¯ç‹¬ç«‹çš„\n\nåºåˆ—æ•°æ®ï¼ˆæœ‰ä¾èµ–ï¼‰ï¼š\n  æ–‡æœ¬ï¼šä»Šå¤©å¤©æ°”â†’å¾ˆå¥½â†’é€‚åˆâ†’å‡ºå»\n  è¯­éŸ³ï¼šéŸ³é¢‘å¸§ tâ‚, tâ‚‚, ..., tâ‚™\n  æ—¶é—´åºåˆ—ï¼šè‚¡ç¥¨ä»·æ ¼ã€æ¸©åº¦è®°å½•\n\nç‰¹æ€§ï¼š\n  âœ“ é•¿åº¦å¯å˜\n  âœ“ å‰åæœ‰ä¾èµ–å…³ç³»\n  âœ“ é¡ºåºå¾ˆé‡è¦\n\n\n8.2.2 âŒ CNN å’Œ FC çš„å±€é™\nå…¨è¿æ¥ç½‘ç»œï¼š - å›ºå®šè¾“å…¥å¤§å° - å¿½è§†åºåˆ—é¡ºåº - æ— æ³•å¤„ç†å¯å˜é•¿åº¦\nCNNï¼š - è™½ç„¶æœ‰å±€éƒ¨è¿æ¥ï¼Œä½†æ„Ÿå—é‡æœ‰é™ - éœ€è¦å¾ˆå¤šå±‚æ‰èƒ½æ•è·é•¿è·ç¦»ä¾èµ– - ä¸å¤Ÿè‡ªç„¶\n\n\n8.2.3 âœ… RNN çš„ä¼˜åŠ¿\nè®¾è®¡ç”¨äºåºåˆ—æ•°æ®ï¼š\n  âœ“ å¯å˜é•¿åº¦è¾“å…¥\n  âœ“ å¾ªç¯ç»“æ„ä¿ç•™åºåˆ—ä¿¡æ¯\n  âœ“ å‚æ•°å…±äº«ï¼ˆæ‰€æœ‰æ—¶é—´æ­¥å…±ç”¨ï¼‰\n  âœ“ å¯ä»¥å»ºæ¨¡é•¿æœŸä¾èµ–ï¼ˆç†è®ºä¸Šï¼‰",
    "crumbs": [
      "ç¬¬äºŒéƒ¨åˆ†ï¼šè¿›é˜¶ç¯‡",
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>ç¬¬ä¸ƒç« ï¼šå¾ªç¯ç¥ç»ç½‘ç»œ (Recurrent Neural Networks)</span>"
    ]
  },
  {
    "objectID": "Chapter7.html#åŸºæœ¬-rnn-vanilla-rnn",
    "href": "Chapter7.html#åŸºæœ¬-rnn-vanilla-rnn",
    "title": "8Â  ç¬¬ä¸ƒç« ï¼šå¾ªç¯ç¥ç»ç½‘ç»œ (Recurrent Neural Networks)",
    "section": "8.3 7.2 åŸºæœ¬ RNN (Vanilla RNN)",
    "text": "8.3 7.2 åŸºæœ¬ RNN (Vanilla RNN)\n\n8.3.1 ğŸ”„ RNN çš„å¾ªç¯ç»“æ„\nå±•å¼€è§†å›¾ï¼š\nyâ‚      yâ‚‚      yâ‚ƒ      yâ‚„\nâ†‘       â†‘       â†‘       â†‘\nhâ‚      hâ‚‚      hâ‚ƒ      hâ‚„\nâ†‘       â†‘       â†‘       â†‘\nxâ‚  â†’  hâ‚  â†’  hâ‚‚  â†’  hâ‚ƒ  â†’  hâ‚„\n        â†“       â†“       â†“\n        (å¾ªç¯)\n\néšè—çŠ¶æ€ä½œä¸ºä¿¡æ¯è½½ä½“ä¼ é€’\næŠ˜å è§†å›¾ï¼ˆå‚æ•°å…±äº«ï¼‰ï¼š\n      x(t)\n        â†“\n    [U]  [W]  [V]\n      â†“    â†“    â†“\n   h(t-1) â†’ RNN â†’ h(t) â†’ y(t)\n            å•å…ƒ\n\n\n8.3.2 ğŸ“ RNN è®¡ç®—\nå•æ—¶åˆ»è®¡ç®—ï¼š\nh(t) = tanh(UÂ·x(t) + WÂ·h(t-1) + b)\ny(t) = VÂ·h(t) + c\n\næˆ–ç”¨æ¿€æ´»å‡½æ•° Ïƒï¼š\nh(t) = Ïƒ(UÂ·x(t) + WÂ·h(t-1) + b)\nç¬¦å·è¯´æ˜ï¼š - x(t): t æ—¶åˆ»çš„è¾“å…¥ - h(t): t æ—¶åˆ»çš„éšè—çŠ¶æ€ - y(t): t æ—¶åˆ»çš„è¾“å‡º - U: è¾“å…¥åˆ°éšè—çš„æƒé‡ - W: éšè—åˆ°éšè—çš„æƒé‡ï¼ˆå¾ªç¯ï¼‰ - V: éšè—åˆ°è¾“å‡ºçš„æƒé‡ - b, c: åç½®\n\n\n8.3.3 ğŸ’» ä»é›¶å®ç° RNN\nimport numpy as np\n\nclass VanillaRNN:\n    def __init__(self, input_size, hidden_size, output_size,\n                 learning_rate=0.01):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        self.learning_rate = learning_rate\n\n        # åˆå§‹åŒ–æƒé‡\n        self.U = np.random.randn(hidden_size, input_size) * 0.01\n        self.W = np.random.randn(hidden_size, hidden_size) * 0.01\n        self.V = np.random.randn(output_size, hidden_size) * 0.01\n        self.b = np.zeros((hidden_size, 1))\n        self.c = np.zeros((output_size, 1))\n\n    def forward(self, X):\n        \"\"\"\n        å‰å‘ä¼ æ’­\n\n        å‚æ•°ï¼š\n            X: åºåˆ—è¾“å…¥ [(seq_len, input_size), ...]\n\n        è¿”å›ï¼š\n            Y: è¾“å‡ºåºåˆ—\n            cache: ç”¨äºåå‘ä¼ æ’­çš„ä¸­é—´å€¼\n        \"\"\"\n        seq_len = len(X)\n\n        # åˆå§‹åŒ–éšè—çŠ¶æ€\n        h = np.zeros((self.hidden_size, 1))\n\n        # å­˜å‚¨ä¸­é—´å€¼\n        cache = {\n            'X': X,\n            'h': [h],  # åŒ…æ‹¬åˆå§‹ h\n            'z': [],\n            'y': []\n        }\n\n        # å‰å‘ä¼ æ’­\n        for t in range(seq_len):\n            # éšè—çŠ¶æ€è®¡ç®—\n            z = self.U @ X[t] + self.W @ h + self.b\n            h = np.tanh(z)\n\n            # è¾“å‡ºè®¡ç®—\n            y = self.V @ h + self.c\n\n            # ä¿å­˜ä¸­é—´å€¼\n            cache['z'].append(z)\n            cache['h'].append(h)\n            cache['y'].append(y)\n\n        return np.array(cache['y']), cache\n\n    def backward(self, dY, cache):\n        \"\"\"\n        åå‘ä¼ æ’­ï¼ˆBPTTï¼‰\n\n        å‚æ•°ï¼š\n            dY: è¾“å‡ºæ¢¯åº¦ [(output_size, 1), ...]\n            cache: å‰å‘ä¼ æ’­çš„ä¸­é—´å€¼\n        \"\"\"\n        seq_len = len(dY)\n\n        # åˆå§‹åŒ–æ¢¯åº¦\n        dU = np.zeros_like(self.U)\n        dW = np.zeros_like(self.W)\n        dV = np.zeros_like(self.V)\n        db = np.zeros_like(self.b)\n        dc = np.zeros_like(self.c)\n\n        # åˆå§‹éšè—çŠ¶æ€æ¢¯åº¦\n        dh_next = np.zeros((self.hidden_size, 1))\n\n        # åå‘éå†æ—¶é—´æ­¥\n        for t in reversed(range(seq_len)):\n            # è¾“å‡ºå±‚æ¢¯åº¦\n            dV += dY[t] @ cache['h'][t+1].T\n            dc += dY[t]\n\n            # éšè—å±‚æ¢¯åº¦\n            dh = self.V.T @ dY[t] + dh_next\n\n            # tanh çš„æ¢¯åº¦\n            dz = dh * (1 - np.tanh(cache['z'][t])**2)\n\n            # æƒé‡æ¢¯åº¦\n            dU += dz @ cache['X'][t].T\n            dW += dz @ cache['h'][t].T\n            db += dz\n\n            # ä¼ é€’åˆ°å‰ä¸€æ—¶åˆ»\n            dh_next = self.W.T @ dz\n\n        # æ¢¯åº¦è£å‰ªï¼ˆé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼‰\n        for dparam in [dU, dW, dV, db, dc]:\n            np.clip(dparam, -5, 5, out=dparam)\n\n        return dU, dW, dV, db, dc\n\n    def update_parameters(self, dU, dW, dV, db, dc):\n        \"\"\"æ›´æ–°å‚æ•°\"\"\"\n        self.U -= self.learning_rate * dU\n        self.W -= self.learning_rate * dW\n        self.V -= self.learning_rate * dV\n        self.b -= self.learning_rate * db\n        self.c -= self.learning_rate * dc\n\n# ç¤ºä¾‹ï¼šé¢„æµ‹æ•°å­—åºåˆ—\ndef train_rnn():\n    # è¶…å‚æ•°\n    input_size = 1\n    hidden_size = 10\n    output_size = 1\n    seq_len = 5\n\n    rnn = VanillaRNN(input_size, hidden_size, output_size,\n                     learning_rate=0.01)\n\n    # ç”Ÿæˆç®€å•æ•°æ®ï¼ˆt æ—¶åˆ»é¢„æµ‹ t+1ï¼‰\n    X_train = [np.array([[i]]) for i in range(5)]\n    y_train = [np.array([[i+1]]) for i in range(5)]\n\n    # è®­ç»ƒ\n    for epoch in range(100):\n        Y_pred, cache = rnn.forward(X_train)\n\n        # è®¡ç®—æŸå¤±\n        loss = np.mean((Y_pred - np.array(y_train))**2)\n\n        # æ¢¯åº¦è®¡ç®—\n        dY = 2 * (Y_pred - np.array(y_train)) / len(y_train)\n        dU, dW, dV, db, dc = rnn.backward(dY, cache)\n\n        # æ›´æ–°å‚æ•°\n        rnn.update_parameters(dU, dW, dV, db, dc)\n\n        if epoch % 20 == 0:\n            print(f\"Epoch {epoch}: Loss = {loss:.6f}\")\n\ntrain_rnn()",
    "crumbs": [
      "ç¬¬äºŒéƒ¨åˆ†ï¼šè¿›é˜¶ç¯‡",
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>ç¬¬ä¸ƒç« ï¼šå¾ªç¯ç¥ç»ç½‘ç»œ (Recurrent Neural Networks)</span>"
    ]
  },
  {
    "objectID": "Chapter7.html#æ¢¯åº¦é—®é¢˜æ¶ˆå¤±å’Œçˆ†ç‚¸",
    "href": "Chapter7.html#æ¢¯åº¦é—®é¢˜æ¶ˆå¤±å’Œçˆ†ç‚¸",
    "title": "8Â  ç¬¬ä¸ƒç« ï¼šå¾ªç¯ç¥ç»ç½‘ç»œ (Recurrent Neural Networks)",
    "section": "8.4 7.3 æ¢¯åº¦é—®é¢˜ï¼šæ¶ˆå¤±å’Œçˆ†ç‚¸",
    "text": "8.4 7.3 æ¢¯åº¦é—®é¢˜ï¼šæ¶ˆå¤±å’Œçˆ†ç‚¸\n\n8.4.1 ğŸš¨ æ¢¯åº¦æ¶ˆå¤± (Vanishing Gradient)\né—®é¢˜ï¼šé•¿æœŸä¾èµ–éš¾ä»¥å­¦ä¹ \nh(t) = tanh(UÂ·x(t) + WÂ·h(t-1) + b)\n\nâˆ‚h(t)/âˆ‚h(t-1) = WÂ·diag(1 - tanhÂ²(...))\n\nå¯¹ t æ­¥ä¹‹å‰çš„æ¢¯åº¦ï¼š\nâˆ‚h(T)/âˆ‚h(t) = âˆ(Ï„=t+1 to T) [WÂ·diag(1-tanhÂ²(...))]\n\nå¦‚æœ ||W|| &lt; 1ï¼Œåˆ™ï¼š\n||âˆ‚h(T)/âˆ‚h(t)|| â‰ˆ ||W||^(T-t)\n\nT-t å¾ˆå¤§æ—¶ï¼Œæ¢¯åº¦æ¥è¿‘ 0 â†’ æ¢¯åº¦æ¶ˆå¤±\nåæœï¼š - æ—©æœŸæƒé‡å‡ ä¹ä¸æ›´æ–° - æ— æ³•å­¦ä¹ é•¿æœŸä¾èµ–\n\n\n8.4.2 ğŸ’¥ æ¢¯åº¦çˆ†ç‚¸ (Exploding Gradient)\né—®é¢˜ï¼šå¦‚æœ ||W|| &gt; 1\næ¢¯åº¦ âˆ ||W||^(T-t) â†’ âˆ\n\nå¯¼è‡´ï¼š\n- å‚æ•°æ›´æ–°ä¸ç¨³å®š\n- NaN/Inf å€¼\n- è®­ç»ƒå´©æºƒ\n\n\n8.4.3 âœ… è§£å†³æ–¹æ¡ˆ\n\n8.4.3.1 1. æ¢¯åº¦è£å‰ª (Gradient Clipping)\né˜²æ­¢æ¢¯åº¦çˆ†ç‚¸\ndef clip_gradients(gradients, max_norm=5):\n    \"\"\"\n    L2 èŒƒæ•°è£å‰ª\n    \"\"\"\n    total_norm = 0\n    for g in gradients:\n        total_norm += np.sum(g**2)\n    total_norm = np.sqrt(total_norm)\n\n    clip_ratio = max_norm / (total_norm + 1e-8)\n    clip_ratio = min(clip_ratio, 1.0)\n\n    clipped_grads = []\n    for g in gradients:\n        clipped_grads.append(g * clip_ratio)\n\n    return clipped_grads\n\n\n8.4.3.2 2. æƒé‡åˆå§‹åŒ–\n# æ­£äº¤åˆå§‹åŒ–\ndef orthogonal_init(shape):\n    \"\"\"æ­£äº¤åˆå§‹åŒ– W çŸ©é˜µ\"\"\"\n    Q, R = np.linalg.qr(np.random.randn(*shape))\n    return Q\n\n# ä½¿ç”¨\nW = orthogonal_init((hidden_size, hidden_size))\n\n\n8.4.3.3 3. æ¿€æ´»å‡½æ•°é€‰æ‹©\nReLU çš„æ¢¯åº¦æ’ä¸º 1ï¼ˆåœ¨æ­£åŒºåŸŸï¼‰\nä¸å®¹æ˜“æ¢¯åº¦æ¶ˆå¤±\n\ntanh çš„æ¢¯åº¦æœ€å¤§ä¸º 0.25\nå®¹æ˜“æ¢¯åº¦æ¶ˆå¤±",
    "crumbs": [
      "ç¬¬äºŒéƒ¨åˆ†ï¼šè¿›é˜¶ç¯‡",
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>ç¬¬ä¸ƒç« ï¼šå¾ªç¯ç¥ç»ç½‘ç»œ (Recurrent Neural Networks)</span>"
    ]
  },
  {
    "objectID": "Chapter7.html#lstm-long-short-term-memory",
    "href": "Chapter7.html#lstm-long-short-term-memory",
    "title": "8Â  ç¬¬ä¸ƒç« ï¼šå¾ªç¯ç¥ç»ç½‘ç»œ (Recurrent Neural Networks)",
    "section": "8.5 7.4 LSTM (Long Short-Term Memory) â­",
    "text": "8.5 7.4 LSTM (Long Short-Term Memory) â­\n\n8.5.1 ğŸ¯ æ ¸å¿ƒæ€æƒ³\nä½¿ç”¨â€è®°å¿†å•å…ƒâ€ä»£æ›¿éšè—çŠ¶æ€\nä¼ ç»Ÿ RNNï¼š\n  ä¿¡æ¯é€šè¿‡éšè—çŠ¶æ€ä¼ é€’\n  æ¯æ­¥éƒ½è¢«ç ´åæ€§åœ°æ”¹å˜\n\nLSTMï¼š\n  æœ‰ä¸“é—¨çš„\"ç»†èƒçŠ¶æ€\"C(t)\n  ä¿¡æ¯å¯ä»¥é•¿æœŸä¿ç•™\n  é€šè¿‡é—¨æ§æœºåˆ¶æœ‰é€‰æ‹©åœ°æ›´æ–°\n\n\n8.5.2 ğŸ“ LSTM çš„å››ä¸ªé—¨\n1. é—å¿˜é—¨ (Forget Gate)\nf(t) = Ïƒ(W_fÂ·[h(t-1), x(t)] + b_f)\n\nä½œç”¨ï¼šå†³å®šå“ªäº›ä¿¡æ¯è¢«ä¸¢å¼ƒ\nf(t) â‰ˆ 0: ä¸¢å¼ƒ\nf(t) â‰ˆ 1: ä¿ç•™\n2. è¾“å…¥é—¨ (Input Gate)\ni(t) = Ïƒ(W_iÂ·[h(t-1), x(t)] + b_i)\nCÌƒ(t) = tanh(W_cÂ·[h(t-1), x(t)] + b_c)\n\nä½œç”¨ï¼šå†³å®šæ–°ä¿¡æ¯\ni(t): æœ‰å¤šå°‘æ–°ä¿¡æ¯è¿›å…¥\nCÌƒ(t): æ–°ä¿¡æ¯çš„å†…å®¹\n3. ç»†èƒçŠ¶æ€æ›´æ–° (Cell State Update)\nC(t) = f(t) âŠ™ C(t-1) + i(t) âŠ™ CÌƒ(t)\n\nâŠ™ è¡¨ç¤ºé€å…ƒç´ ä¹˜æ³•ï¼ˆHadamard ç§¯ï¼‰\n\nè¿‡ç¨‹ï¼š\n  å‰ä¸€ä¸ªç»†èƒçŠ¶æ€ Ã— é—å¿˜é—¨\n  + æ–°ä¿¡æ¯ Ã— è¾“å…¥é—¨\n4. è¾“å‡ºé—¨ (Output Gate)\no(t) = Ïƒ(W_oÂ·[h(t-1), x(t)] + b_o)\nh(t) = o(t) âŠ™ tanh(C(t))\n\nä½œç”¨ï¼šå†³å®šè¾“å‡ºå¤šå°‘ä¿¡æ¯\n\n\n8.5.3 ğŸ“Š LSTM å•å…ƒå›¾\n        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n        â”‚  ç»†èƒçŠ¶æ€ C(t)      â”‚ â† é•¿æœŸè®°å¿†\n        â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n               â”‚\n        â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”\n        â”‚   âŠ™ f(t)    â”‚ â† é—å¿˜é—¨ï¼ˆä¿ç•™å¤šå°‘ï¼‰\n        â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜\n               â”‚\n        â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n        â”‚ âŠ™ + âŠ™ i(t) CÌƒ(t) â”‚ â† æ–°ä¿¡æ¯åŠ å…¥\n        â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n               â”‚\n               â–¼\n        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n        â”‚ tanh âŠ™ o(t) â”‚ â† è¾“å‡ºé—¨\n        â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜\n               â”‚\n               â–¼\n            h(t) â† çŸ­æœŸè®°å¿†\n\n\n8.5.4 ğŸ’» PyTorch å®ç°\nimport torch\nimport torch.nn as nn\n\n# æ–¹å¼1ï¼šä½¿ç”¨é«˜çº§ LSTM\nclass LSTMModel(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, output_size):\n        super(LSTMModel, self).__init__()\n\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        # LSTM å±‚\n        self.lstm = nn.LSTM(\n            input_size=input_size,\n            hidden_size=hidden_size,\n            num_layers=num_layers,\n            batch_first=True,  # è¾“å…¥æ ¼å¼ (batch, seq, feature)\n            dropout=0.3 if num_layers &gt; 1 else 0\n        )\n\n        # è¾“å‡ºå±‚\n        self.fc = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x):\n        \"\"\"\n        å‚æ•°ï¼š\n            x: (batch_size, seq_len, input_size)\n        \"\"\"\n        # LSTM è¾“å‡º\n        lstm_out, (h_n, c_n) = self.lstm(x)\n        # lstm_out: (batch, seq_len, hidden_size)\n        # h_n: (num_layers, batch, hidden_size)\n        # c_n: (num_layers, batch, hidden_size)\n\n        # ä½¿ç”¨æœ€åä¸€ä¸ªæ—¶åˆ»çš„è¾“å‡º\n        last_out = lstm_out[:, -1, :]  # (batch, hidden_size)\n\n        # å…¨è¿æ¥å±‚\n        output = self.fc(last_out)  # (batch, output_size)\n\n        return output\n\n# æ–¹å¼2ï¼šä»é›¶å®ç° LSTM å•å…ƒ\nclass LSTMCell(nn.Module):\n    def __init__(self, input_size, hidden_size):\n        super(LSTMCell, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n\n        # å››ä¸ªé—¨çš„æƒé‡çŸ©é˜µ\n        self.W_f = nn.Linear(input_size + hidden_size, hidden_size)\n        self.W_i = nn.Linear(input_size + hidden_size, hidden_size)\n        self.W_c = nn.Linear(input_size + hidden_size, hidden_size)\n        self.W_o = nn.Linear(input_size + hidden_size, hidden_size)\n\n    def forward(self, x, h_prev, c_prev):\n        \"\"\"\n        å‚æ•°ï¼š\n            x: (batch, input_size)\n            h_prev: (batch, hidden_size)\n            c_prev: (batch, hidden_size)\n\n        è¿”å›ï¼š\n            h: (batch, hidden_size)\n            c: (batch, hidden_size)\n        \"\"\"\n        # æ‹¼æ¥è¾“å…¥å’Œå‰ä¸€éšè—çŠ¶æ€\n        combined = torch.cat([x, h_prev], dim=1)\n\n        # å››ä¸ªé—¨\n        f = torch.sigmoid(self.W_f(combined))  # é—å¿˜é—¨\n        i = torch.sigmoid(self.W_i(combined))  # è¾“å…¥é—¨\n        c_tilde = torch.tanh(self.W_c(combined))  # å€™é€‰å€¼\n        o = torch.sigmoid(self.W_o(combined))  # è¾“å‡ºé—¨\n\n        # æ›´æ–°ç»†èƒçŠ¶æ€\n        c = f * c_prev + i * c_tilde\n\n        # è®¡ç®—éšè—çŠ¶æ€\n        h = o * torch.tanh(c)\n\n        return h, c\n\n# ä½¿ç”¨è‡ªå®šä¹‰ LSTM Cell\nclass CustomLSTM(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n        super(CustomLSTM, self).__init__()\n\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        # å¤šå±‚ LSTM\n        self.lstm_cells = nn.ModuleList([\n            LSTMCell(\n                input_size if layer == 0 else hidden_size,\n                hidden_size\n            )\n            for layer in range(num_layers)\n        ])\n\n        self.fc = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x):\n        \"\"\"\n        å‚æ•°ï¼š\n            x: (batch, seq_len, input_size)\n        \"\"\"\n        batch_size, seq_len, _ = x.size()\n\n        # åˆå§‹åŒ–éšè—çŠ¶æ€å’Œç»†èƒçŠ¶æ€\n        h = [torch.zeros(batch_size, self.hidden_size)\n             for _ in range(self.num_layers)]\n        c = [torch.zeros(batch_size, self.hidden_size)\n             for _ in range(self.num_layers)]\n\n        # å‰å‘ä¼ æ’­\n        for t in range(seq_len):\n            x_t = x[:, t, :]  # (batch, input_size)\n\n            for layer in range(self.num_layers):\n                h[layer], c[layer] = self.lstm_cells[layer](\n                    x_t, h[layer], c[layer]\n                )\n                x_t = h[layer]\n\n        # ä½¿ç”¨æœ€åä¸€ä¸ªæ—¶åˆ»çš„éšè—çŠ¶æ€\n        output = self.fc(h[-1])\n\n        return output",
    "crumbs": [
      "ç¬¬äºŒéƒ¨åˆ†ï¼šè¿›é˜¶ç¯‡",
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>ç¬¬ä¸ƒç« ï¼šå¾ªç¯ç¥ç»ç½‘ç»œ (Recurrent Neural Networks)</span>"
    ]
  },
  {
    "objectID": "Chapter7.html#gru-gated-recurrent-unit",
    "href": "Chapter7.html#gru-gated-recurrent-unit",
    "title": "8Â  ç¬¬ä¸ƒç« ï¼šå¾ªç¯ç¥ç»ç½‘ç»œ (Recurrent Neural Networks)",
    "section": "8.6 7.5 GRU (Gated Recurrent Unit)",
    "text": "8.6 7.5 GRU (Gated Recurrent Unit)\n\n8.6.1 ğŸ¯ ç®€åŒ–çš„ LSTM\nLSTM é—®é¢˜ï¼šå‚æ•°å¤šï¼Œè®¡ç®—å¤æ‚\nGRU è§£å†³ï¼šåªç”¨ä¸¤ä¸ªé—¨ï¼Œç»“æ„æ›´ç®€æ´\n\n\n8.6.2 ğŸ“ GRU çš„ä¸¤ä¸ªé—¨\n1. é‡ç½®é—¨ (Reset Gate)\nr(t) = Ïƒ(W_rÂ·[h(t-1), x(t)] + b_r)\n\nä½œç”¨ï¼šå†³å®šæœ‰å¤šå°‘å†å²ä¿¡æ¯è¢«é—å¿˜\n2. æ›´æ–°é—¨ (Update Gate)\nz(t) = Ïƒ(W_zÂ·[h(t-1), x(t)] + b_z)\n\nä½œç”¨ï¼šå†³å®šæ–°æ—§ä¿¡æ¯çš„æ¯”ä¾‹\n3. å€™é€‰éšè—çŠ¶æ€\nhÌƒ(t) = tanh(WÂ·[r(t) âŠ™ h(t-1), x(t)] + b)\n\nä½¿ç”¨é‡ç½®é—¨æ¥é€‰æ‹©å†å²ä¿¡æ¯\n4. éšè—çŠ¶æ€æ›´æ–°\nh(t) = (1 - z(t)) âŠ™ hÌƒ(t) + z(t) âŠ™ h(t-1)\n\n= æ–°ä¿¡æ¯æ¯”ä¾‹ Ã— å€™é€‰å€¼ + å†å²ä¿¡æ¯æ¯”ä¾‹ Ã— å‰å€¼\n\n\n8.6.3 ğŸ“Š LSTM vs GRU\nLSTMï¼š\n  - ç»†èƒçŠ¶æ€ C(t) ç”¨äºé•¿æœŸè®°å¿†\n  - éšè—çŠ¶æ€ h(t) ç”¨äºçŸ­æœŸè¾“å‡º\n  - 3ä¸ªé—¨ï¼ˆé—å¿˜ã€è¾“å…¥ã€è¾“å‡ºï¼‰\n  - å‚æ•°å¤šï¼Œè¡¨è¾¾èƒ½åŠ›å¼º\n\nGRUï¼š\n  - ç»†èƒçŠ¶æ€å’Œéšè—çŠ¶æ€åˆå¹¶\n  - 2ä¸ªé—¨ï¼ˆé‡ç½®ã€æ›´æ–°ï¼‰\n  - å‚æ•°å°‘ï¼ˆçº¦ LSTM çš„ 2/3ï¼‰\n  - è®¡ç®—é€Ÿåº¦å¿«\n  - åœ¨å¤§å¤šæ•°ä»»åŠ¡ä¸Šæ€§èƒ½ç›¸å½“\n\n\n8.6.4 ğŸ’» å®ç°\nclass GRUCell(nn.Module):\n    def __init__(self, input_size, hidden_size):\n        super(GRUCell, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n\n        # é‡ç½®é—¨å’Œæ›´æ–°é—¨\n        self.W_r = nn.Linear(input_size + hidden_size, hidden_size)\n        self.W_z = nn.Linear(input_size + hidden_size, hidden_size)\n        # å€™é€‰éšè—çŠ¶æ€\n        self.W_h = nn.Linear(input_size + hidden_size, hidden_size)\n\n    def forward(self, x, h_prev):\n        \"\"\"\n        å‚æ•°ï¼š\n            x: (batch, input_size)\n            h_prev: (batch, hidden_size)\n\n        è¿”å›ï¼š\n            h: (batch, hidden_size)\n        \"\"\"\n        combined = torch.cat([x, h_prev], dim=1)\n\n        # é‡ç½®é—¨\n        r = torch.sigmoid(self.W_r(combined))\n\n        # æ›´æ–°é—¨\n        z = torch.sigmoid(self.W_z(combined))\n\n        # å€™é€‰éšè—çŠ¶æ€\n        combined_reset = torch.cat([x, r * h_prev], dim=1)\n        h_tilde = torch.tanh(self.W_h(combined_reset))\n\n        # æ›´æ–°éšè—çŠ¶æ€\n        h = (1 - z) * h_tilde + z * h_prev\n\n        return h\n\n# PyTorch é«˜çº§æ¥å£\nmodel = nn.GRU(\n    input_size=10,\n    hidden_size=20,\n    num_layers=2,\n    batch_first=True\n)",
    "crumbs": [
      "ç¬¬äºŒéƒ¨åˆ†ï¼šè¿›é˜¶ç¯‡",
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>ç¬¬ä¸ƒç« ï¼šå¾ªç¯ç¥ç»ç½‘ç»œ (Recurrent Neural Networks)</span>"
    ]
  },
  {
    "objectID": "Chapter7.html#åŒå‘-rnn-bidirectional-rnn",
    "href": "Chapter7.html#åŒå‘-rnn-bidirectional-rnn",
    "title": "8Â  ç¬¬ä¸ƒç« ï¼šå¾ªç¯ç¥ç»ç½‘ç»œ (Recurrent Neural Networks)",
    "section": "8.7 7.6 åŒå‘ RNN (Bidirectional RNN)",
    "text": "8.7 7.6 åŒå‘ RNN (Bidirectional RNN)\n\n8.7.1 ğŸ¯ é—®é¢˜ï¼šå‰å‘ RNN çš„å±€é™\nå‰å‘ RNNï¼š\n  xâ‚ â†’ xâ‚‚ â†’ xâ‚ƒ â†’ xâ‚„\n\nhâ‚ƒ æ— æ³•çœ‹åˆ° xâ‚„ çš„ä¿¡æ¯\nä½†æœ‰äº›ä»»åŠ¡éœ€è¦å®Œæ•´çš„ä¸Šä¸‹æ–‡ï¼\n\n\n8.7.2 âœ… åŒå‘ RNN è§£å†³\nåŒæ—¶è¿è¡Œå‰å‘å’Œåå‘ RNN\nå‰å‘ï¼šxâ‚ â†’ xâ‚‚ â†’ xâ‚ƒ â†’ xâ‚„\n      â†’hâ‚â†’ â†’hâ‚‚â†’ â†’hâ‚ƒâ†’ â†’hâ‚„\n\nåå‘ï¼š        â† â† â† â†\n      â†hÌ„â‚â†  â†hÌ„â‚‚â†  â†hÌ„â‚ƒâ†  â†hÌ„â‚„â†\n\nè¾“å‡ºï¼š[hâ‚ƒ, hÌ„â‚ƒ] = ç»“åˆä¸¤ä¸ªæ–¹å‘çš„ä¿¡æ¯\nè®¡ç®—ï¼š\nhâ‚ƒ = [hâ‚ƒ_forward, hâ‚ƒ_backward]\n   = [LSTM_fwd(xâ‚:xâ‚ƒ), LSTM_bwd(xâ‚ƒ:xâ‚)]\n\n\n8.7.3 ğŸ’» å®ç°\nclass BiLSTM(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, output_size):\n        super(BiLSTM, self).__init__()\n\n        self.lstm = nn.LSTM(\n            input_size=input_size,\n            hidden_size=hidden_size,\n            num_layers=num_layers,\n            batch_first=True,\n            bidirectional=True  # å…³é”®ï¼\n        )\n\n        # åŒå‘ LSTM è¾“å‡ºå¤§å°æ˜¯ 2Ã—hidden_size\n        self.fc = nn.Linear(hidden_size * 2, output_size)\n\n    def forward(self, x):\n        \"\"\"\n        å‚æ•°ï¼š\n            x: (batch, seq_len, input_size)\n        \"\"\"\n        # LSTM è¾“å‡º\n        lstm_out, _ = self.lstm(x)\n        # lstm_out: (batch, seq_len, 2Ã—hidden_size)\n\n        # ä½¿ç”¨æœ€åæ—¶åˆ»\n        last_out = lstm_out[:, -1, :]\n\n        output = self.fc(last_out)\n        return output\n\n# æˆ–è€…ç”¨æ‰€æœ‰æ—¶åˆ»ï¼ˆå¦‚ NER æ ‡ç­¾ï¼‰\nclass BiLSTMSequence(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super().__init__()\n        self.lstm = nn.LSTM(input_size, hidden_size,\n                           batch_first=True, bidirectional=True)\n        self.fc = nn.Linear(hidden_size * 2, output_size)\n\n    def forward(self, x):\n        lstm_out, _ = self.lstm(x)  # (batch, seq, 2Ã—hidden)\n\n        # å¯¹æ¯ä¸ªæ—¶åˆ»åšåˆ†ç±»\n        output = self.fc(lstm_out)  # (batch, seq, output_size)\n        return output",
    "crumbs": [
      "ç¬¬äºŒéƒ¨åˆ†ï¼šè¿›é˜¶ç¯‡",
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>ç¬¬ä¸ƒç« ï¼šå¾ªç¯ç¥ç»ç½‘ç»œ (Recurrent Neural Networks)</span>"
    ]
  },
  {
    "objectID": "Chapter7.html#å®æˆ˜-1æ–‡æœ¬æƒ…æ„Ÿåˆ†æ",
    "href": "Chapter7.html#å®æˆ˜-1æ–‡æœ¬æƒ…æ„Ÿåˆ†æ",
    "title": "8Â  ç¬¬ä¸ƒç« ï¼šå¾ªç¯ç¥ç»ç½‘ç»œ (Recurrent Neural Networks)",
    "section": "8.8 7.7 å®æˆ˜ 1ï¼šæ–‡æœ¬æƒ…æ„Ÿåˆ†æ",
    "text": "8.8 7.7 å®æˆ˜ 1ï¼šæ–‡æœ¬æƒ…æ„Ÿåˆ†æ\n\n8.8.1 ğŸ“‹ ä»»åŠ¡è®¾å®š\næ•°æ®ï¼šç”µå½±è¯„è®º\n\"è¿™éƒ¨ç”µå½±å¤ªæ£’äº†ï¼\" â†’ æ­£é¢ (1)\n\"å®Œå…¨æ˜¯æµªè´¹æ—¶é—´ã€‚\" â†’ è´Ÿé¢ (0)\n\n\n8.8.2 ğŸ’» å®Œæ•´å®ç°\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nimport numpy as np\nfrom collections import Counter\nimport re\n\n# ==================== æ•°æ®é¢„å¤„ç† ====================\n\nclass Tokenizer:\n    def __init__(self, vocab_size=5000):\n        self.vocab_size = vocab_size\n        self.word2idx = {'&lt;PAD&gt;': 0, '&lt;UNK&gt;': 1}\n        self.idx2word = {0: '&lt;PAD&gt;', 1: '&lt;UNK&gt;'}\n\n    def build_vocab(self, texts):\n        \"\"\"æ„å»ºè¯æ±‡è¡¨\"\"\"\n        word_freq = Counter()\n        for text in texts:\n            words = self.tokenize(text)\n            word_freq.update(words)\n\n        idx = 2\n        for word, freq in word_freq.most_common(self.vocab_size - 2):\n            self.word2idx[word] = idx\n            self.idx2word[idx] = word\n            idx += 1\n\n    def tokenize(self, text):\n        \"\"\"åˆ†è¯\"\"\"\n        text = text.lower()\n        text = re.sub(r'[^a-z\\s]', '', text)\n        return text.split()\n\n    def encode(self, text, max_len=100):\n        \"\"\"ç¼–ç æ–‡æœ¬\"\"\"\n        words = self.tokenize(text)\n        ids = [self.word2idx.get(w, 1) for w in words]\n\n        # Padding æˆ–æˆªæ–­\n        if len(ids) &lt; max_len:\n            ids = ids + [0] * (max_len - len(ids))\n        else:\n            ids = ids[:max_len]\n\n        return ids\n\n    def decode(self, ids):\n        \"\"\"è§£ç \"\"\"\n        return ' '.join([self.idx2word.get(i, '&lt;UNK&gt;') for i in ids])\n\n# ==================== æ¨¡å‹å®šä¹‰ ====================\n\nclass SentimentLSTM(nn.Module):\n    def __init__(self, vocab_size, embedding_size, hidden_size,\n                 num_layers=2, dropout=0.3):\n        super(SentimentLSTM, self).__init__()\n\n        # Embedding å±‚\n        self.embedding = nn.Embedding(vocab_size, embedding_size,\n                                     padding_idx=0)\n\n        # LSTM å±‚\n        self.lstm = nn.LSTM(\n            input_size=embedding_size,\n            hidden_size=hidden_size,\n            num_layers=num_layers,\n            batch_first=True,\n            dropout=dropout,\n            bidirectional=True\n        )\n\n        # æ³¨æ„åŠ›æœºåˆ¶ï¼ˆå¯é€‰ï¼‰\n        self.attention = nn.MultiheadAttention(\n            embed_dim=hidden_size * 2,\n            num_heads=4,\n            batch_first=True,\n            dropout=dropout\n        )\n\n        # å…¨è¿æ¥å±‚\n        self.fc1 = nn.Linear(hidden_size * 2, 128)\n        self.dropout = nn.Dropout(dropout)\n        self.fc2 = nn.Linear(128, 1)\n\n        # Batch Normalization\n        self.bn = nn.BatchNorm1d(128)\n\n    def forward(self, x):\n        \"\"\"\n        å‚æ•°ï¼š\n            x: (batch, seq_len)\n        \"\"\"\n        # Embedding\n        embedded = self.embedding(x)  # (batch, seq_len, embedding_size)\n\n        # LSTM\n        lstm_out, (h_n, c_n) = self.lstm(embedded)\n        # lstm_out: (batch, seq_len, hidden_size*2)\n\n        # ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶\n        attn_out, attn_weights = self.attention(\n            lstm_out, lstm_out, lstm_out\n        )\n\n        # å–æœ€åä¸€ä¸ªæ—¶åˆ»æˆ–æ± åŒ–\n        # æ–¹å¼1ï¼šæœ€åæ—¶åˆ»\n        # last_hidden = lstm_out[:, -1, :]\n\n        # æ–¹å¼2ï¼šå¹³å‡æ± åŒ–\n        # last_hidden = torch.mean(lstm_out, dim=1)\n\n        # æ–¹å¼3ï¼šæœ€å¤§æ± åŒ–\n        # last_hidden, _ = torch.max(lstm_out, dim=1)\n\n        # æ–¹å¼4ï¼šä½¿ç”¨æ³¨æ„åŠ›è¾“å‡º\n        last_hidden = torch.mean(attn_out, dim=1)\n\n        # å…¨è¿æ¥å±‚\n        x = torch.relu(self.fc1(last_hidden))\n        x = self.bn(x)\n        x = self.dropout(x)\n        x = self.fc2(x)\n\n        return torch.sigmoid(x)\n\n# ==================== è®­ç»ƒä»£ç  ====================\n\ndef train_sentiment_model():\n    # è¶…å‚æ•°\n    VOCAB_SIZE = 5000\n    EMBEDDING_SIZE = 128\n    HIDDEN_SIZE = 64\n    NUM_LAYERS = 2\n    MAX_LEN = 100\n    BATCH_SIZE = 64\n    EPOCHS = 20\n    LEARNING_RATE = 0.001\n\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n    # ç¤ºä¾‹æ•°æ®ï¼ˆå®é™…åº”ä½¿ç”¨çœŸå®æ•°æ®é›†å¦‚ IMDBï¼‰\n    texts_train = [\n        \"This movie is great and wonderful\",\n        \"I love this film so much\",\n        \"Amazing performance by the actors\",\n        \"Terrible waste of time\",\n        \"Boring and dull movie\",\n        \"I hate this film\"\n    ] * 100  # å¤åˆ¶ä»¥å¢åŠ æ•°æ®é‡\n\n    labels_train = [1, 1, 1, 0, 0, 0] * 100\n\n    # æ„å»ºè¯æ±‡è¡¨\n    tokenizer = Tokenizer(vocab_size=VOCAB_SIZE)\n    tokenizer.build_vocab(texts_train)\n\n    # ç¼–ç æ•°æ®\n    X_train = torch.tensor([tokenizer.encode(t, MAX_LEN) for t in texts_train])\n    y_train = torch.tensor(labels_train, dtype=torch.float32).unsqueeze(1)\n\n    # æ•°æ®åŠ è½½å™¨\n    train_dataset = TensorDataset(X_train, y_train)\n    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n\n    # æ¨¡å‹\n    model = SentimentLSTM(\n        vocab_size=VOCAB_SIZE,\n        embedding_size=EMBEDDING_SIZE,\n        hidden_size=HIDDEN_SIZE,\n        num_layers=NUM_LAYERS\n    ).to(device)\n\n    # æŸå¤±å’Œä¼˜åŒ–å™¨\n    criterion = nn.BCELoss()\n    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, mode='min', patience=3, factor=0.5\n    )\n\n    # è®­ç»ƒ\n    for epoch in range(EPOCHS):\n        model.train()\n        total_loss = 0\n        correct = 0\n        total = 0\n\n        for X_batch, y_batch in train_loader:\n            X_batch = X_batch.to(device)\n            y_batch = y_batch.to(device)\n\n            # å‰å‘ä¼ æ’­\n            outputs = model(X_batch)\n            loss = criterion(outputs, y_batch)\n\n            # åå‘ä¼ æ’­\n            optimizer.zero_grad()\n            loss.backward()\n\n            # æ¢¯åº¦è£å‰ª\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5)\n\n            optimizer.step()\n\n            # ç»Ÿè®¡\n            total_loss += loss.item()\n            predicted = (outputs &gt; 0.5).float()\n            correct += (predicted == y_batch).sum().item()\n            total += y_batch.size(0)\n\n        avg_loss = total_loss / len(train_loader)\n        accuracy = 100 * correct / total\n\n        print(f'Epoch [{epoch+1}/{EPOCHS}], Loss: {avg_loss:.4f}, Acc: {accuracy:.2f}%')\n\n        scheduler.step(avg_loss)\n\n    # ä¿å­˜æ¨¡å‹\n    torch.save(model.state_dict(), 'sentiment_lstm.pth')\n\n    return model, tokenizer\n\n# ==================== é¢„æµ‹å‡½æ•° ====================\n\ndef predict_sentiment(model, tokenizer, text, device):\n    \"\"\"é¢„æµ‹å•ä¸ªæ–‡æœ¬çš„æƒ…æ„Ÿ\"\"\"\n    model.eval()\n\n    # ç¼–ç \n    encoded = tokenizer.encode(text, max_len=100)\n    x = torch.tensor([encoded]).to(device)\n\n    # é¢„æµ‹\n    with torch.no_grad():\n        output = model(x)\n        prob = output.item()\n        sentiment = \"æ­£é¢\" if prob &gt; 0.5 else \"è´Ÿé¢\"\n\n    return sentiment, prob\n\n# ä½¿ç”¨\nif __name__ == \"__main__\":\n    model, tokenizer = train_sentiment_model()\n\n    # æµ‹è¯•\n    test_texts = [\n        \"This is an amazing movie!\",\n        \"Terrible and boring film.\",\n        \"Not bad, quite enjoyable.\"\n    ]\n\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model.to(device)\n\n    for text in test_texts:\n        sentiment, prob = predict_sentiment(model, tokenizer, text, device)\n        print(f\"\\næ–‡æœ¬: {text}\")\n        print(f\"æƒ…æ„Ÿ: {sentiment} (ç½®ä¿¡åº¦: {prob:.4f})\")",
    "crumbs": [
      "ç¬¬äºŒéƒ¨åˆ†ï¼šè¿›é˜¶ç¯‡",
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>ç¬¬ä¸ƒç« ï¼šå¾ªç¯ç¥ç»ç½‘ç»œ (Recurrent Neural Networks)</span>"
    ]
  },
  {
    "objectID": "Chapter7.html#å®æˆ˜-2æ—¶é—´åºåˆ—é¢„æµ‹",
    "href": "Chapter7.html#å®æˆ˜-2æ—¶é—´åºåˆ—é¢„æµ‹",
    "title": "8Â  ç¬¬ä¸ƒç« ï¼šå¾ªç¯ç¥ç»ç½‘ç»œ (Recurrent Neural Networks)",
    "section": "8.9 7.8 å®æˆ˜ 2ï¼šæ—¶é—´åºåˆ—é¢„æµ‹",
    "text": "8.9 7.8 å®æˆ˜ 2ï¼šæ—¶é—´åºåˆ—é¢„æµ‹\n\n8.9.1 ğŸ“‹ ä»»åŠ¡ï¼šé¢„æµ‹è‚¡ç¥¨ä»·æ ¼\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\n\n# ==================== æ•°æ®å‡†å¤‡ ====================\n\ndef create_sequences(data, seq_length):\n    \"\"\"\n    åˆ›å»ºåºåˆ—æ•°æ®\n\n    å‚æ•°ï¼š\n        data: åŸå§‹æ•°æ® (n_samples,)\n        seq_length: åºåˆ—é•¿åº¦\n\n    è¿”å›ï¼š\n        X: (n_samples - seq_length, seq_length, 1)\n        y: (n_samples - seq_length, 1)\n    \"\"\"\n    X, y = [], []\n    for i in range(len(data) - seq_length):\n        X.append(data[i:i+seq_length])\n        y.append(data[i+seq_length])\n\n    return np.array(X), np.array(y)\n\n# ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®ï¼ˆå®é™…åº”ä½¿ç”¨çœŸå®è‚¡ç¥¨æ•°æ®ï¼‰\ndef generate_stock_data(n_samples=1000):\n    \"\"\"ç”Ÿæˆæ¨¡æ‹Ÿè‚¡ç¥¨ä»·æ ¼\"\"\"\n    t = np.linspace(0, 100, n_samples)\n    # è¶‹åŠ¿ + å­£èŠ‚æ€§ + å™ªå£°\n    trend = 0.02 * t\n    seasonal = 10 * np.sin(2 * np.pi * t / 50)\n    noise = np.random.randn(n_samples) * 2\n\n    price = 100 + trend + seasonal + noise\n    return price\n\n# ==================== æ¨¡å‹å®šä¹‰ ====================\n\nclass StockLSTM(nn.Module):\n    def __init__(self, input_size=1, hidden_size=64, num_layers=2, dropout=0.2):\n        super(StockLSTM, self).__init__()\n\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        # LSTM å±‚\n        self.lstm = nn.LSTM(\n            input_size=input_size,\n            hidden_size=hidden_size,\n            num_layers=num_layers,\n            batch_first=True,\n            dropout=dropout if num_layers &gt; 1 else 0\n        )\n\n        # å…¨è¿æ¥å±‚\n        self.fc = nn.Sequential(\n            nn.Linear(hidden_size, 32),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(32, 1)\n        )\n\n    def forward(self, x):\n        \"\"\"\n        å‚æ•°ï¼š\n            x: (batch, seq_len, input_size)\n        \"\"\"\n        # LSTM\n        lstm_out, _ = self.lstm(x)\n\n        # å–æœ€åä¸€ä¸ªæ—¶åˆ»\n        last_out = lstm_out[:, -1, :]\n\n        # é¢„æµ‹\n        output = self.fc(last_out)\n\n        return output\n\n# ==================== è®­ç»ƒä»£ç  ====================\n\ndef train_stock_predictor():\n    # è¶…å‚æ•°\n    SEQ_LENGTH = 30  # ä½¿ç”¨30å¤©é¢„æµ‹ä¸‹ä¸€å¤©\n    BATCH_SIZE = 32\n    EPOCHS = 100\n    LEARNING_RATE = 0.001\n\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n    # ç”Ÿæˆæ•°æ®\n    data = generate_stock_data(n_samples=1000)\n\n    # å½’ä¸€åŒ–\n    scaler = MinMaxScaler()\n    data_normalized = scaler.fit_transform(data.reshape(-1, 1)).flatten()\n\n    # åˆ›å»ºåºåˆ—\n    X, y = create_sequences(data_normalized, SEQ_LENGTH)\n    X = X.reshape(-1, SEQ_LENGTH, 1)\n\n    # è®­ç»ƒé›†å’Œæµ‹è¯•é›†åˆ†å‰²\n    train_size = int(0.8 * len(X))\n    X_train, X_test = X[:train_size], X[train_size:]\n    y_train, y_test = y[:train_size], y[train_size:]\n\n    # è½¬æ¢ä¸º Tensor\n    X_train = torch.FloatTensor(X_train)\n    y_train = torch.FloatTensor(y_train).unsqueeze(1)\n    X_test = torch.FloatTensor(X_test)\n    y_test = torch.FloatTensor(y_test).unsqueeze(1)\n\n    # æ•°æ®åŠ è½½å™¨\n    train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n    train_loader = torch.utils.data.DataLoader(\n        train_dataset, batch_size=BATCH_SIZE, shuffle=True\n    )\n\n    # æ¨¡å‹\n    model = StockLSTM(\n        input_size=1,\n        hidden_size=64,\n        num_layers=2,\n        dropout=0.2\n    ).to(device)\n\n    # æŸå¤±å’Œä¼˜åŒ–å™¨\n    criterion = nn.MSELoss()\n    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, patience=5, factor=0.5, verbose=True\n    )\n\n    # è®­ç»ƒ\n    train_losses = []\n\n    for epoch in range(EPOCHS):\n        model.train()\n        epoch_loss = 0\n\n        for X_batch, y_batch in train_loader:\n            X_batch = X_batch.to(device)\n            y_batch = y_batch.to(device)\n\n            # å‰å‘ä¼ æ’­\n            outputs = model(X_batch)\n            loss = criterion(outputs, y_batch)\n\n            # åå‘ä¼ æ’­\n            optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n            optimizer.step()\n\n            epoch_loss += loss.item()\n\n        avg_loss = epoch_loss / len(train_loader)\n        train_losses.append(avg_loss)\n\n        # éªŒè¯\n        model.eval()\n        with torch.no_grad():\n            X_test_device = X_test.to(device)\n            test_pred = model(X_test_device)\n            test_loss = criterion(test_pred, y_test.to(device))\n\n        scheduler.step(test_loss)\n\n        if (epoch + 1) % 10 == 0:\n            print(f'Epoch [{epoch+1}/{EPOCHS}], '\n                  f'Train Loss: {avg_loss:.6f}, Test Loss: {test_loss:.6f}')\n\n    # ==================== å¯è§†åŒ–é¢„æµ‹ç»“æœ ====================\n\n    model.eval()\n    with torch.no_grad():\n        train_pred = model(X_train.to(device)).cpu().numpy()\n        test_pred = model(X_test.to(device)).cpu().numpy()\n\n    # åå½’ä¸€åŒ–\n    train_pred = scaler.inverse_transform(train_pred)\n    test_pred = scaler.inverse_transform(test_pred)\n    y_train_actual = scaler.inverse_transform(y_train.numpy())\n    y_test_actual = scaler.inverse_transform(y_test.numpy())\n\n    # ç»˜å›¾\n    plt.figure(figsize=(15, 6))\n\n    # è®­ç»ƒé›†\n    plt.subplot(1, 2, 1)\n    plt.plot(y_train_actual, label='çœŸå®å€¼', alpha=0.7)\n    plt.plot(train_pred, label='é¢„æµ‹å€¼', alpha=0.7)\n    plt.title('è®­ç»ƒé›†é¢„æµ‹')\n    plt.xlabel('æ—¶é—´æ­¥')\n    plt.ylabel('ä»·æ ¼')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n\n    # æµ‹è¯•é›†\n    plt.subplot(1, 2, 2)\n    plt.plot(y_test_actual, label='çœŸå®å€¼', alpha=0.7)\n    plt.plot(test_pred, label='é¢„æµ‹å€¼', alpha=0.7)\n    plt.title('æµ‹è¯•é›†é¢„æµ‹')\n    plt.xlabel('æ—¶é—´æ­¥')\n    plt.ylabel('ä»·æ ¼')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n\n    plt.tight_layout()\n    plt.show()\n\n    # è®¡ç®—æŒ‡æ ‡\n    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n\n    test_mse = mean_squared_error(y_test_actual, test_pred)\n    test_mae = mean_absolute_error(y_test_actual, test_pred)\n    test_r2 = r2_score(y_test_actual, test_pred)\n\n    print(f\"\\næµ‹è¯•é›†æŒ‡æ ‡ï¼š\")\n    print(f\"MSE: {test_mse:.4f}\")\n    print(f\"MAE: {test_mae:.4f}\")\n    print(f\"RÂ²: {test_r2:.4f}\")\n\n    return model, scaler\n\n# è¿è¡Œ\nif __name__ == \"__main__\":\n    model, scaler = train_stock_predictor()",
    "crumbs": [
      "ç¬¬äºŒéƒ¨åˆ†ï¼šè¿›é˜¶ç¯‡",
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>ç¬¬ä¸ƒç« ï¼šå¾ªç¯ç¥ç»ç½‘ç»œ (Recurrent Neural Networks)</span>"
    ]
  },
  {
    "objectID": "Chapter7.html#å®æˆ˜-3åºåˆ—åˆ°åºåˆ—-seq2seq",
    "href": "Chapter7.html#å®æˆ˜-3åºåˆ—åˆ°åºåˆ—-seq2seq",
    "title": "8Â  ç¬¬ä¸ƒç« ï¼šå¾ªç¯ç¥ç»ç½‘ç»œ (Recurrent Neural Networks)",
    "section": "8.10 7.9 å®æˆ˜ 3ï¼šåºåˆ—åˆ°åºåˆ— (Seq2Seq)",
    "text": "8.10 7.9 å®æˆ˜ 3ï¼šåºåˆ—åˆ°åºåˆ— (Seq2Seq)\n\n8.10.1 ğŸ“‹ ä»»åŠ¡ï¼šæœºå™¨ç¿»è¯‘\nclass Seq2SeqLSTM(nn.Module):\n    def __init__(self, input_vocab_size, output_vocab_size,\n                 embedding_size, hidden_size, num_layers=2):\n        super(Seq2SeqLSTM, self).__init__()\n\n        # Encoder\n        self.encoder_embedding = nn.Embedding(input_vocab_size, embedding_size)\n        self.encoder = nn.LSTM(\n            embedding_size, hidden_size, num_layers,\n            batch_first=True\n        )\n\n        # Decoder\n        self.decoder_embedding = nn.Embedding(output_vocab_size, embedding_size)\n        self.decoder = nn.LSTM(\n            embedding_size, hidden_size, num_layers,\n            batch_first=True\n        )\n\n        # è¾“å‡ºå±‚\n        self.fc = nn.Linear(hidden_size, output_vocab_size)\n\n    def forward(self, src, tgt, teacher_forcing_ratio=0.5):\n        \"\"\"\n        å‚æ•°ï¼š\n            src: (batch, src_seq_len) æºè¯­è¨€\n            tgt: (batch, tgt_seq_len) ç›®æ ‡è¯­è¨€\n            teacher_forcing_ratio: ä½¿ç”¨çœŸå®ç›®æ ‡çš„æ¦‚ç‡\n        \"\"\"\n        batch_size = src.size(0)\n        tgt_len = tgt.size(1)\n        tgt_vocab_size = self.fc.out_features\n\n        # ç¼–ç å™¨\n        embedded_src = self.encoder_embedding(src)\n        encoder_outputs, (hidden, cell) = self.encoder(embedded_src)\n\n        # è§£ç å™¨åˆå§‹è¾“å…¥ï¼ˆ&lt;SOS&gt; tokenï¼‰\n        decoder_input = tgt[:, 0].unsqueeze(1)\n\n        # å­˜å‚¨è¾“å‡º\n        outputs = torch.zeros(batch_size, tgt_len, tgt_vocab_size)\n\n        # é€æ­¥è§£ç \n        for t in range(1, tgt_len):\n            # è§£ç ä¸€æ­¥\n            embedded_tgt = self.decoder_embedding(decoder_input)\n            decoder_output, (hidden, cell) = self.decoder(\n                embedded_tgt, (hidden, cell)\n            )\n\n            # é¢„æµ‹\n            output = self.fc(decoder_output.squeeze(1))\n            outputs[:, t, :] = output\n\n            # Teacher forcing\n            use_teacher_forcing = np.random.random() &lt; teacher_forcing_ratio\n            top1 = output.argmax(1)\n\n            decoder_input = tgt[:, t].unsqueeze(1) if use_teacher_forcing else top1.unsqueeze(1)\n\n        return outputs\n\n# ä½¿ç”¨\nmodel = Seq2SeqLSTM(\n    input_vocab_size=5000,\n    output_vocab_size=5000,\n    embedding_size=256,\n    hidden_size=512,\n    num_layers=2\n)",
    "crumbs": [
      "ç¬¬äºŒéƒ¨åˆ†ï¼šè¿›é˜¶ç¯‡",
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>ç¬¬ä¸ƒç« ï¼šå¾ªç¯ç¥ç»ç½‘ç»œ (Recurrent Neural Networks)</span>"
    ]
  },
  {
    "objectID": "Chapter7.html#æœ¬ç« ä½œä¸š",
    "href": "Chapter7.html#æœ¬ç« ä½œä¸š",
    "title": "8Â  ç¬¬ä¸ƒç« ï¼šå¾ªç¯ç¥ç»ç½‘ç»œ (Recurrent Neural Networks)",
    "section": "8.11 ğŸ“ æœ¬ç« ä½œä¸š",
    "text": "8.11 ğŸ“ æœ¬ç« ä½œä¸š\n\n8.11.1 ä½œä¸š 1ï¼šRNN æ¢¯åº¦åˆ†æ\n# TODO:\n# 1. å®ç° vanilla RNN\n# 2. åœ¨é•¿åºåˆ—ä¸Šè®­ç»ƒ\n# 3. å¯è§†åŒ–æ¢¯åº¦æµ\n# 4. è§‚å¯Ÿæ¢¯åº¦æ¶ˆå¤±ç°è±¡\n# 5. å¯¹æ¯” LSTM çš„æ¢¯åº¦æµ\n\n\n8.11.2 ä½œä¸š 2ï¼šæƒ…æ„Ÿåˆ†æå®Œæ•´é¡¹ç›®\n# ä½¿ç”¨çœŸå®æ•°æ®é›†ï¼ˆIMDB æˆ–ä¸­æ–‡è¯„è®ºï¼‰\n# è¦æ±‚ï¼š\n# 1. æ•°æ®é¢„å¤„ç†å’Œ EDA\n# 2. å®ç° LSTM å’Œ GRU æ¨¡å‹\n# 3. å¯¹æ¯”åŒå‘å’Œå•å‘\n# 4. åŠ å…¥æ³¨æ„åŠ›æœºåˆ¶\n# 5. è¶…å‚æ•°è°ƒä¼˜\n# 6. å¯è§†åŒ–æ³¨æ„åŠ›æƒé‡\n# 7. ç¼–å†™å®Œæ•´æŠ¥å‘Š\n\n\n8.11.3 ä½œä¸š 3ï¼šæ–‡æœ¬ç”Ÿæˆ\n# å­—ç¬¦çº§è¯­è¨€æ¨¡å‹\n# 1. ä½¿ç”¨èå£«æ¯”äºšæ–‡æœ¬è®­ç»ƒ\n# 2. å®ç° LSTM ç”Ÿæˆæ¨¡å‹\n# 3. å°è¯•ä¸åŒçš„é‡‡æ ·ç­–ç•¥ï¼ˆè´ªå¿ƒã€top-kã€nucleusï¼‰\n# 4. ç”Ÿæˆæ–°æ–‡æœ¬å¹¶è¯„ä¼°è´¨é‡",
    "crumbs": [
      "ç¬¬äºŒéƒ¨åˆ†ï¼šè¿›é˜¶ç¯‡",
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>ç¬¬ä¸ƒç« ï¼šå¾ªç¯ç¥ç»ç½‘ç»œ (Recurrent Neural Networks)</span>"
    ]
  },
  {
    "objectID": "Chapter7.html#æœ¬ç« å…³é”®æ¦‚å¿µ",
    "href": "Chapter7.html#æœ¬ç« å…³é”®æ¦‚å¿µ",
    "title": "8Â  ç¬¬ä¸ƒç« ï¼šå¾ªç¯ç¥ç»ç½‘ç»œ (Recurrent Neural Networks)",
    "section": "8.12 ğŸ”‘ æœ¬ç« å…³é”®æ¦‚å¿µ",
    "text": "8.12 ğŸ”‘ æœ¬ç« å…³é”®æ¦‚å¿µ\n\n\n\næ¦‚å¿µ\nè¯´æ˜\n\n\n\n\nåºåˆ—æ•°æ®\nå‰åæœ‰ä¾èµ–å…³ç³»çš„æ•°æ®\n\n\nRNN\nå¾ªç¯ç¥ç»ç½‘ç»œï¼Œå¤„ç†åºåˆ—\n\n\néšè—çŠ¶æ€\nåºåˆ—ä¿¡æ¯çš„è½½ä½“\n\n\nBPTT\nåå‘ä¼ æ’­ç©¿è¶Šæ—¶é—´\n\n\næ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸\nRNN çš„æ ¸å¿ƒé—®é¢˜\n\n\nLSTM\né•¿çŸ­æœŸè®°å¿†ç½‘ç»œ\n\n\né—¨æ§æœºåˆ¶\næ§åˆ¶ä¿¡æ¯æµ\n\n\nç»†èƒçŠ¶æ€\nLSTM çš„é•¿æœŸè®°å¿†\n\n\nGRU\nç®€åŒ–çš„ LSTM\n\n\nåŒå‘ RNN\nåŒæ—¶çœ‹å‰åæ–‡",
    "crumbs": [
      "ç¬¬äºŒéƒ¨åˆ†ï¼šè¿›é˜¶ç¯‡",
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>ç¬¬ä¸ƒç« ï¼šå¾ªç¯ç¥ç»ç½‘ç»œ (Recurrent Neural Networks)</span>"
    ]
  },
  {
    "objectID": "Chapter8.html",
    "href": "Chapter8.html",
    "title": "9Â  ç¬¬å…«ç« ï¼šAttention ä¸ Transformer",
    "section": "",
    "text": "9.1 ğŸ“Œ ç« èŠ‚ç›®æ ‡",
    "crumbs": [
      "ç¬¬äºŒéƒ¨åˆ†ï¼šè¿›é˜¶ç¯‡",
      "<span class='chapter-number'>9</span>Â  <span class='chapter-title'>ç¬¬å…«ç« ï¼šAttention ä¸ Transformer</span>"
    ]
  },
  {
    "objectID": "Chapter8.html#ç« èŠ‚ç›®æ ‡",
    "href": "Chapter8.html#ç« èŠ‚ç›®æ ‡",
    "title": "9Â  ç¬¬å…«ç« ï¼šAttention ä¸ Transformer",
    "section": "",
    "text": "ç†è§£æ³¨æ„åŠ›æœºåˆ¶çš„åŠ¨æœºå’ŒåŸç†\næŒæ¡ Self-Attention çš„è®¡ç®—\næ·±å…¥å­¦ä¹  Transformer æ¶æ„\näº†è§£ BERT å’Œ GPT çš„è®¾è®¡\nå®æˆ˜ï¼šæ–‡æœ¬åˆ†ç±»ã€ç¿»è¯‘",
    "crumbs": [
      "ç¬¬äºŒéƒ¨åˆ†ï¼šè¿›é˜¶ç¯‡",
      "<span class='chapter-number'>9</span>Â  <span class='chapter-title'>ç¬¬å…«ç« ï¼šAttention ä¸ Transformer</span>"
    ]
  },
  {
    "objectID": "Chapter8.html#ä¸ºä»€ä¹ˆéœ€è¦-attention",
    "href": "Chapter8.html#ä¸ºä»€ä¹ˆéœ€è¦-attention",
    "title": "9Â  ç¬¬å…«ç« ï¼šAttention ä¸ Transformer",
    "section": "9.2 8.1 ä¸ºä»€ä¹ˆéœ€è¦ Attentionï¼Ÿ",
    "text": "9.2 8.1 ä¸ºä»€ä¹ˆéœ€è¦ Attentionï¼Ÿ\n\n9.2.1 ğŸš¨ Seq2Seq çš„ç“¶é¢ˆ\nä¼ ç»Ÿ Seq2Seqï¼š\nç¼–ç å™¨ï¼šæºå¥å­ â†’ å›ºå®šé•¿åº¦å‘é‡ h\n          â†“\nè§£ç å™¨ï¼šh â†’ ç›®æ ‡å¥å­\n\né—®é¢˜ï¼šæ‰€æœ‰ä¿¡æ¯å‹ç¼©åˆ°å›ºå®šå‘é‡ h\n     é•¿å¥å­ä¿¡æ¯ä¸¢å¤±ï¼\nä¾‹å­ï¼šç¿»è¯‘é•¿å¥\nè‹±æ–‡ï¼šThe quick brown fox jumps over the lazy dog.\n     (9ä¸ªè¯)\n\nç¼–ç æˆå•ä¸ªå‘é‡ h (512ç»´)\n     â†“\nè§£ç æ—¶ï¼Œæ—©æœŸè¯çš„ä¿¡æ¯å·²ç»æ¨¡ç³Š\n\n\n9.2.2 âœ… Attention è§£å†³æ–¹æ¡ˆ\næ ¸å¿ƒæ€æƒ³ï¼šè§£ç æ¯ä¸ªè¯æ—¶ï¼ŒåŠ¨æ€å…³æ³¨æºå¥å­çš„ä¸åŒéƒ¨åˆ†\nç¼–ç å™¨ï¼šäº§ç”Ÿæ‰€æœ‰æ—¶åˆ»çš„éšè—çŠ¶æ€ hâ‚, hâ‚‚, ..., hâ‚™\n\nè§£ç ç¬¬ t ä¸ªè¯æ—¶ï¼š\n  1. è®¡ç®—ä¸æ¯ä¸ª háµ¢ çš„ç›¸å…³æ€§\n  2. åŠ æƒæ±‚å’Œå¾—åˆ° context vector c_t\n  3. ä½¿ç”¨ c_t ç”Ÿæˆè¾“å‡º",
    "crumbs": [
      "ç¬¬äºŒéƒ¨åˆ†ï¼šè¿›é˜¶ç¯‡",
      "<span class='chapter-number'>9</span>Â  <span class='chapter-title'>ç¬¬å…«ç« ï¼šAttention ä¸ Transformer</span>"
    ]
  },
  {
    "objectID": "Chapter8.html#attention-æœºåˆ¶",
    "href": "Chapter8.html#attention-æœºåˆ¶",
    "title": "9Â  ç¬¬å…«ç« ï¼šAttention ä¸ Transformer",
    "section": "9.3 8.2 Attention æœºåˆ¶",
    "text": "9.3 8.2 Attention æœºåˆ¶\n\n9.3.1 ğŸ“ è®¡ç®—è¿‡ç¨‹\n1. è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°\nscore(h_t, h_s) = h_t^T Â· W Â· h_s\n\næˆ–ç®€åŒ–ç‰ˆï¼š\nscore(h_t, h_s) = h_t^T Â· h_s  (ç‚¹ç§¯)\n2. å½’ä¸€åŒ–ï¼ˆSoftmaxï¼‰\nÎ±_t,s = exp(score(h_t, h_s)) / Î£_i exp(score(h_t, h_i))\n\nÎ±_t = [Î±_t,1, Î±_t,2, ..., Î±_t,n]  (æ³¨æ„åŠ›æƒé‡)\n3. åŠ æƒæ±‚å’Œ\nc_t = Î£_s Î±_t,s Â· h_s\n\nc_t: context vectorï¼ˆä¸Šä¸‹æ–‡å‘é‡ï¼‰\n4. ç”Ÿæˆè¾“å‡º\noutput_t = f(h_t, c_t)\n\n\n9.3.2 ğŸ’» å®ç°\nclass BahdanauAttention(nn.Module):\n    \"\"\"Bahdanau (Additive) Attention\"\"\"\n\n    def __init__(self, hidden_size):\n        super(BahdanauAttention, self).__init__()\n        self.W_h = nn.Linear(hidden_size, hidden_size)\n        self.W_s = nn.Linear(hidden_size, hidden_size)\n        self.v = nn.Linear(hidden_size, 1)\n\n    def forward(self, query, keys, values):\n        \"\"\"\n        å‚æ•°ï¼š\n            query: (batch, hidden_size) è§£ç å™¨å½“å‰çŠ¶æ€\n            keys: (batch, seq_len, hidden_size) ç¼–ç å™¨æ‰€æœ‰çŠ¶æ€\n            values: (batch, seq_len, hidden_size) åŒ keys\n        \"\"\"\n        # query: (batch, 1, hidden_size)\n        query = query.unsqueeze(1)\n\n        # è®¡ç®—åˆ†æ•°\n        score = self.v(torch.tanh(\n            self.W_h(keys) + self.W_s(query)\n        ))  # (batch, seq_len, 1)\n\n        # æ³¨æ„åŠ›æƒé‡\n        attention_weights = torch.softmax(score, dim=1)\n\n        # åŠ æƒæ±‚å’Œ\n        context = torch.sum(attention_weights * values, dim=1)\n        # (batch, hidden_size)\n\n        return context, attention_weights\n\nclass LuongAttention(nn.Module):\n    \"\"\"Luong (Multiplicative) Attention\"\"\"\n\n    def __init__(self, hidden_size, method='dot'):\n        super(LuongAttention, self).__init__()\n        self.method = method\n\n        if method == 'general':\n            self.W = nn.Linear(hidden_size, hidden_size, bias=False)\n        elif method == 'concat':\n            self.W = nn.Linear(hidden_size * 2, hidden_size)\n            self.v = nn.Linear(hidden_size, 1)\n\n    def forward(self, query, keys, values):\n        \"\"\"\n        å‚æ•°åŒä¸Š\n        \"\"\"\n        query = query.unsqueeze(1)  # (batch, 1, hidden)\n\n        if self.method == 'dot':\n            # ç‚¹ç§¯æ³¨æ„åŠ›\n            score = torch.bmm(query, keys.transpose(1, 2))\n        elif self.method == 'general':\n            # ä¸€èˆ¬æ³¨æ„åŠ›\n            score = torch.bmm(self.W(query), keys.transpose(1, 2))\n        elif self.method == 'concat':\n            # æ‹¼æ¥æ³¨æ„åŠ›\n            query_expanded = query.expand(-1, keys.size(1), -1)\n            score = self.v(torch.tanh(\n                self.W(torch.cat([query_expanded, keys], dim=2))\n            ))\n\n        # (batch, 1, seq_len)\n        attention_weights = torch.softmax(score, dim=2)\n\n        # åŠ æƒæ±‚å’Œ\n        context = torch.bmm(attention_weights, values)\n        # (batch, 1, hidden_size)\n\n        return context.squeeze(1), attention_weights.squeeze(1)",
    "crumbs": [
      "ç¬¬äºŒéƒ¨åˆ†ï¼šè¿›é˜¶ç¯‡",
      "<span class='chapter-number'>9</span>Â  <span class='chapter-title'>ç¬¬å…«ç« ï¼šAttention ä¸ Transformer</span>"
    ]
  },
  {
    "objectID": "Chapter8.html#self-attention",
    "href": "Chapter8.html#self-attention",
    "title": "9Â  ç¬¬å…«ç« ï¼šAttention ä¸ Transformer",
    "section": "9.4 8.3 Self-Attention â­",
    "text": "9.4 8.3 Self-Attention â­\n\n9.4.1 ğŸ¯ åŠ¨æœº\næ™®é€š Attentionï¼šæŸ¥è¯¢å’Œé”®æ¥è‡ªä¸åŒåºåˆ—ï¼ˆç¼–ç å™¨å’Œè§£ç å™¨ï¼‰\nSelf-Attentionï¼šæŸ¥è¯¢ã€é”®ã€å€¼éƒ½æ¥è‡ªåŒä¸€åºåˆ—\nä½œç”¨ï¼š - æ•æ‰åºåˆ—å†…éƒ¨çš„ä¾èµ–å…³ç³» - å¹¶è¡Œè®¡ç®—ï¼ˆä¸åƒ RNN éœ€è¦ä¸²è¡Œï¼‰ - å…¨å±€æ„Ÿå—é‡ï¼ˆæ¯ä¸ªä½ç½®éƒ½èƒ½çœ‹åˆ°æ‰€æœ‰ä½ç½®ï¼‰\n\n\n9.4.2 ğŸ“ è®¡ç®—æœºåˆ¶\nQuery, Key, Value (Q, K, V)\nç»™å®šè¾“å…¥åºåˆ— X = [xâ‚, xâ‚‚, ..., xâ‚™]\n\né€šè¿‡çº¿æ€§å˜æ¢å¾—åˆ°ï¼š\nQ = X Â· W_Q  (queries)\nK = X Â· W_K  (keys)\nV = X Â· W_V  (values)\n\næ¯ä¸ªéƒ½æ˜¯ (seq_len, d_k) çŸ©é˜µ\nScaled Dot-Product Attention\nAttention(Q, K, V) = softmax(QÂ·K^T / âˆšd_k) Â· V\n\næ­¥éª¤ï¼š\n1. è®¡ç®— Q å’Œ K çš„ç‚¹ç§¯ï¼šQÂ·K^T\n   ç»“æœï¼š(seq_len, seq_len) æ³¨æ„åŠ›çŸ©é˜µ\n\n2. ç¼©æ”¾ï¼šé™¤ä»¥ âˆšd_k\n   é˜²æ­¢ç‚¹ç§¯è¿‡å¤§å¯¼è‡´ softmax é¥±å’Œ\n\n3. Softmaxï¼šå¯¹æ¯ä¸€è¡Œå½’ä¸€åŒ–\n   å¾—åˆ°æ³¨æ„åŠ›æƒé‡\n\n4. åŠ æƒæ±‚å’Œï¼šä¹˜ä»¥ V\n   å¾—åˆ°è¾“å‡º\n\n\n9.4.3 ğŸ’» å®ç°\nclass ScaledDotProductAttention(nn.Module):\n    def __init__(self, dropout=0.1):\n        super().__init__()\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, Q, K, V, mask=None):\n        \"\"\"\n        å‚æ•°ï¼š\n            Q: (batch, n_heads, seq_len, d_k)\n            K: (batch, n_heads, seq_len, d_k)\n            V: (batch, n_heads, seq_len, d_v)\n            mask: (batch, 1, seq_len, seq_len) å¯é€‰\n        \"\"\"\n        d_k = Q.size(-1)\n\n        # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°\n        scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(d_k)\n        # (batch, n_heads, seq_len, seq_len)\n\n        # åº”ç”¨ maskï¼ˆå¯é€‰ï¼Œç”¨äº padding æˆ–æœªæ¥ä¿¡æ¯ï¼‰\n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, -1e9)\n\n        # Softmax\n        attention_weights = torch.softmax(scores, dim=-1)\n        attention_weights = self.dropout(attention_weights)\n\n        # åŠ æƒæ±‚å’Œ\n        output = torch.matmul(attention_weights, V)\n        # (batch, n_heads, seq_len, d_v)\n\n        return output, attention_weights",
    "crumbs": [
      "ç¬¬äºŒéƒ¨åˆ†ï¼šè¿›é˜¶ç¯‡",
      "<span class='chapter-number'>9</span>Â  <span class='chapter-title'>ç¬¬å…«ç« ï¼šAttention ä¸ Transformer</span>"
    ]
  },
  {
    "objectID": "Chapter8.html#multi-head-attention",
    "href": "Chapter8.html#multi-head-attention",
    "title": "9Â  ç¬¬å…«ç« ï¼šAttention ä¸ Transformer",
    "section": "9.5 8.4 Multi-Head Attention",
    "text": "9.5 8.4 Multi-Head Attention\n\n9.5.1 ğŸ¯ ä¸ºä»€ä¹ˆéœ€è¦å¤šå¤´ï¼Ÿ\nå•å¤´æ³¨æ„åŠ›ï¼šåªæœ‰ä¸€ç»„ Q, K, V - å¯èƒ½åªå…³æ³¨æŸä¸€ç§æ¨¡å¼\nå¤šå¤´æ³¨æ„åŠ›ï¼šå¤šç»„ Q, K, V å¹¶è¡Œ - ä¸åŒçš„å¤´å¯ä»¥å…³æ³¨ä¸åŒçš„æ¨¡å¼ - ç±»ä¼¼ CNN çš„å¤šä¸ªå·ç§¯æ ¸\n\n\n9.5.2 ğŸ“ è®¡ç®—\nMulti-Head Attention(Q, K, V) = Concat(headâ‚, ..., head_h) Â· W_O\n\nå…¶ä¸­ï¼š\nhead_i = Attention(QÂ·W_Q^i, KÂ·W_K^i, VÂ·W_V^i)\n\nå‚æ•°ï¼š\n  W_Q^i âˆˆ â„^(d_model Ã— d_k)\n  W_K^i âˆˆ â„^(d_model Ã— d_k)\n  W_V^i âˆˆ â„^(d_model Ã— d_v)\n  W_O âˆˆ â„^(hÂ·d_v Ã— d_model)\n\né€šå¸¸ï¼šd_k = d_v = d_model / h\n\n\n9.5.3 ğŸ’» å®ç°\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, d_model, n_heads, dropout=0.1):\n        super().__init__()\n        assert d_model % n_heads == 0\n\n        self.d_model = d_model\n        self.n_heads = n_heads\n        self.d_k = d_model // n_heads\n\n        # çº¿æ€§å˜æ¢\n        self.W_Q = nn.Linear(d_model, d_model)\n        self.W_K = nn.Linear(d_model, d_model)\n        self.W_V = nn.Linear(d_model, d_model)\n        self.W_O = nn.Linear(d_model, d_model)\n\n        self.attention = ScaledDotProductAttention(dropout)\n        self.dropout = nn.Dropout(dropout)\n\n    def split_heads(self, x):\n        \"\"\"\n        (batch, seq_len, d_model)\n        â†’ (batch, n_heads, seq_len, d_k)\n        \"\"\"\n        batch_size, seq_len, d_model = x.size()\n        return x.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n\n    def combine_heads(self, x):\n        \"\"\"\n        (batch, n_heads, seq_len, d_k)\n        â†’ (batch, seq_len, d_model)\n        \"\"\"\n        batch_size, n_heads, seq_len, d_k = x.size()\n        return x.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n\n    def forward(self, Q, K, V, mask=None):\n        \"\"\"\n        å‚æ•°ï¼š\n            Q, K, V: (batch, seq_len, d_model)\n            mask: å¯é€‰\n        \"\"\"\n        # çº¿æ€§å˜æ¢\n        Q = self.W_Q(Q)  # (batch, seq_len, d_model)\n        K = self.W_K(K)\n        V = self.W_V(V)\n\n        # åˆ†æˆå¤šä¸ªå¤´\n        Q = self.split_heads(Q)  # (batch, n_heads, seq_len, d_k)\n        K = self.split_heads(K)\n        V = self.split_heads(V)\n\n        # è®¡ç®—æ³¨æ„åŠ›\n        attn_output, attn_weights = self.attention(Q, K, V, mask)\n        # (batch, n_heads, seq_len, d_k)\n\n        # åˆå¹¶å¤šä¸ªå¤´\n        output = self.combine_heads(attn_output)\n        # (batch, seq_len, d_model)\n\n        # æœ€åçš„çº¿æ€§å±‚\n        output = self.W_O(output)\n        output = self.dropout(output)\n\n        return output, attn_weights",
    "crumbs": [
      "ç¬¬äºŒéƒ¨åˆ†ï¼šè¿›é˜¶ç¯‡",
      "<span class='chapter-number'>9</span>Â  <span class='chapter-title'>ç¬¬å…«ç« ï¼šAttention ä¸ Transformer</span>"
    ]
  },
  {
    "objectID": "Chapter8.html#transformer-æ¶æ„",
    "href": "Chapter8.html#transformer-æ¶æ„",
    "title": "9Â  ç¬¬å…«ç« ï¼šAttention ä¸ Transformer",
    "section": "9.6 8.5 Transformer æ¶æ„ ğŸŒŸ",
    "text": "9.6 8.5 Transformer æ¶æ„ ğŸŒŸ\n\n9.6.1 ğŸ—ï¸ æ•´ä½“ç»“æ„\nè¾“å…¥åºåˆ—\n    â†“\n[Positional Encoding]\n    â†“\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  Encoder (Ã—Nå±‚)    â”‚\nâ”‚  - Multi-Head Attn  â”‚\nâ”‚  - Feed Forward     â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n    â†“\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  Decoder (Ã—Nå±‚)    â”‚\nâ”‚  - Masked Attn      â”‚\nâ”‚  - Cross Attn       â”‚\nâ”‚  - Feed Forward     â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n    â†“\n  è¾“å‡ºæ¦‚ç‡\n\n\n9.6.2 ğŸ”¹ ä½ç½®ç¼–ç  (Positional Encoding)\né—®é¢˜ï¼šSelf-Attention æ²¡æœ‰é¡ºåºä¿¡æ¯\nè§£å†³ï¼šæ·»åŠ ä½ç½®ç¼–ç \nPE(pos, 2i)   = sin(pos / 10000^(2i/d_model))\nPE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n\nå…¶ä¸­ï¼š\n  pos: ä½ç½®ï¼ˆ0, 1, 2, ...ï¼‰\n  i: ç»´åº¦ç´¢å¼•ï¼ˆ0, 1, ..., d_model/2ï¼‰\nå®ç°ï¼š\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=5000):\n        super().__init__()\n\n        # åˆ›å»ºä½ç½®ç¼–ç çŸ©é˜µ\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len).unsqueeze(1).float()\n        div_term = torch.exp(\n            torch.arange(0, d_model, 2).float() *\n            (-np.log(10000.0) / d_model)\n        )\n\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n\n        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        \"\"\"\n        å‚æ•°ï¼š\n            x: (batch, seq_len, d_model)\n        \"\"\"\n        seq_len = x.size(1)\n        x = x + self.pe[:, :seq_len, :]\n        return x\n\n\n\n9.6.3 ğŸ”¹ Encoder å±‚\nclass TransformerEncoderLayer(nn.Module):\n    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n        super().__init__()\n\n        # Multi-Head Self-Attention\n        self.self_attn = MultiHeadAttention(d_model, n_heads, dropout)\n\n        # Feed Forward\n        self.ff = nn.Sequential(\n            nn.Linear(d_model, d_ff),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(d_ff, d_model)\n        )\n\n        # Layer Normalization\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, mask=None):\n        \"\"\"\n        å‚æ•°ï¼š\n            x: (batch, seq_len, d_model)\n        \"\"\"\n        # Self-Attention + Residual + Norm\n        attn_output, _ = self.self_attn(x, x, x, mask)\n        x = self.norm1(x + self.dropout(attn_output))\n\n        # Feed Forward + Residual + Norm\n        ff_output = self.ff(x)\n        x = self.norm2(x + self.dropout(ff_output))\n\n        return x\n\n\n\n9.6.4 ğŸ”¹ Decoder å±‚\nclass TransformerDecoderLayer(nn.Module):\n    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n        super().__init__()\n\n        # Masked Self-Attention\n        self.self_attn = MultiHeadAttention(d_model, n_heads, dropout)\n\n        # Cross-Attention (Encoder-Decoder Attention)\n        self.cross_attn = MultiHeadAttention(d_model, n_heads, dropout)\n\n        # Feed Forward\n        self.ff = nn.Sequential(\n            nn.Linear(d_model, d_ff),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(d_ff, d_model)\n        )\n\n        # Layer Normalization\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.norm3 = nn.LayerNorm(d_model)\n\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, encoder_output, src_mask=None, tgt_mask=None):\n        \"\"\"\n        å‚æ•°ï¼š\n            x: (batch, tgt_len, d_model) ç›®æ ‡åºåˆ—\n            encoder_output: (batch, src_len, d_model) ç¼–ç å™¨è¾“å‡º\n            src_mask: source mask\n            tgt_mask: target maskï¼ˆé˜²æ­¢çœ‹åˆ°æœªæ¥ä¿¡æ¯ï¼‰\n        \"\"\"\n        # Masked Self-Attention\n        attn_output, _ = self.self_attn(x, x, x, tgt_mask)\n        x = self.norm1(x + self.dropout(attn_output))\n\n        # Cross-Attention\n        attn_output, _ = self.cross_attn(x, encoder_output, encoder_output, src_mask)\n        x = self.norm2(x + self.dropout(attn_output))\n\n        # Feed Forward\n        ff_output = self.ff(x)\n        x = self.norm3(x + self.dropout(ff_output))\n\n        return x\n\n\n\n9.6.5 ğŸ”¹ å®Œæ•´ Transformer\nclass Transformer(nn.Module):\n    def __init__(self, src_vocab_size, tgt_vocab_size,\n                 d_model=512, n_heads=8, n_layers=6,\n                 d_ff=2048, dropout=0.1, max_len=5000):\n        super().__init__()\n\n        self.d_model = d_model\n\n        # Embedding\n        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n\n        # Positional Encoding\n        self.pos_encoding = PositionalEncoding(d_model, max_len)\n\n        # Encoder\n        self.encoder_layers = nn.ModuleList([\n            TransformerEncoderLayer(d_model, n_heads, d_ff, dropout)\n            for _ in range(n_layers)\n        ])\n\n        # Decoder\n        self.decoder_layers = nn.ModuleList([\n            TransformerDecoderLayer(d_model, n_heads, d_ff, dropout)\n            for _ in range(n_layers)\n        ])\n\n        # Output\n        self.fc_out = nn.Linear(d_model, tgt_vocab_size)\n        self.dropout = nn.Dropout(dropout)\n\n    def make_src_mask(self, src):\n        \"\"\"åˆ›å»º source mask (padding)\"\"\"\n        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n        # (batch, 1, 1, src_len)\n        return src_mask\n\n    def make_tgt_mask(self, tgt):\n        \"\"\"åˆ›å»º target mask (padding + future)\"\"\"\n        tgt_len = tgt.size(1)\n\n        # Padding mask\n        tgt_pad_mask = (tgt != 0).unsqueeze(1).unsqueeze(2)\n        # (batch, 1, 1, tgt_len)\n\n        # Future mask\n        tgt_sub_mask = torch.tril(\n            torch.ones((tgt_len, tgt_len), device=tgt.device)\n        ).bool()\n        # (tgt_len, tgt_len)\n\n        tgt_mask = tgt_pad_mask & tgt_sub_mask\n        return tgt_mask\n\n    def encode(self, src, src_mask):\n        \"\"\"Encoder\"\"\"\n        x = self.src_embedding(src) * np.sqrt(self.d_model)\n        x = self.pos_encoding(x)\n        x = self.dropout(x)\n\n        for layer in self.encoder_layers:\n            x = layer(x, src_mask)\n\n        return x\n\n    def decode(self, tgt, encoder_output, src_mask, tgt_mask):\n        \"\"\"Decoder\"\"\"\n        x = self.tgt_embedding(tgt) * np.sqrt(self.d_model)\n        x = self.pos_encoding(x)\n        x = self.dropout(x)\n\n        for layer in self.decoder_layers:\n            x = layer(x, encoder_output, src_mask, tgt_mask)\n\n        return x\n\n    def forward(self, src, tgt):\n        \"\"\"\n        å‚æ•°ï¼š\n            src: (batch, src_len)\n            tgt: (batch, tgt_len)\n        \"\"\"\n        src_mask = self.make_src_mask(src)\n        tgt_mask = self.make_tgt_mask(tgt)\n\n        encoder_output = self.encode(src, src_mask)\n        decoder_output = self.decode(tgt, encoder_output, src_mask, tgt_mask)\n\n        output = self.fc_out(decoder_output)\n        return output",
    "crumbs": [
      "ç¬¬äºŒéƒ¨åˆ†ï¼šè¿›é˜¶ç¯‡",
      "<span class='chapter-number'>9</span>Â  <span class='chapter-title'>ç¬¬å…«ç« ï¼šAttention ä¸ Transformer</span>"
    ]
  },
  {
    "objectID": "Chapter8.html#bert-bidirectional-encoder-representations-from-transformers",
    "href": "Chapter8.html#bert-bidirectional-encoder-representations-from-transformers",
    "title": "9Â  ç¬¬å…«ç« ï¼šAttention ä¸ Transformer",
    "section": "9.7 8.6 BERT (Bidirectional Encoder Representations from Transformers)",
    "text": "9.7 8.6 BERT (Bidirectional Encoder Representations from Transformers)\n\n9.7.1 ğŸ¯ æ ¸å¿ƒæ€æƒ³\nåªä½¿ç”¨ Transformer çš„ Encoder\nè®­ç»ƒä»»åŠ¡ï¼š 1. Masked Language Model (MLM) - éšæœº mask 15% çš„è¯ - è®©æ¨¡å‹é¢„æµ‹è¢« mask çš„è¯\n\nNext Sentence Prediction (NSP)\n\nåˆ¤æ–­ä¸¤ä¸ªå¥å­æ˜¯å¦è¿ç»­\n\n\n\n\n9.7.2 ğŸ“ æ¶æ„\nè¾“å…¥ï¼š[CLS] å¥å­1 [SEP] å¥å­2 [SEP]\n\nEmbedding = Token Emb + Segment Emb + Position Emb\n    â†“\nTransformer Encoder (Ã—12 or 24å±‚)\n    â†“\nè¾“å‡ºï¼šæ¯ä¸ª token çš„è¡¨ç¤º\n\n[CLS] çš„è¾“å‡ºç”¨äºåˆ†ç±»ä»»åŠ¡\n\n\n9.7.3 ğŸ’» ä½¿ç”¨ BERTï¼ˆHugging Faceï¼‰\nfrom transformers import BertTokenizer, BertModel, BertForSequenceClassification\nimport torch\n\n# ==================== åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ ====================\n\n# Tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n# æ¨¡å‹\nmodel = BertModel.from_pretrained('bert-base-uncased')\nmodel.eval()\n\n# ==================== æ–‡æœ¬ç¼–ç  ====================\n\ntext = \"Hello, my name is BERT.\"\ninputs = tokenizer(text, return_tensors='pt')\n\n# inputs['input_ids']: token IDs\n# inputs['attention_mask']: mask (1=real, 0=padding)\n\n# ==================== è·å–è¡¨ç¤º ====================\n\nwith torch.no_grad():\n    outputs = model(**inputs)\n\n# outputs.last_hidden_state: (1, seq_len, 768)\n# outputs.pooler_output: (1, 768) [CLS] token\n\n# ==================== å¾®è°ƒç”¨äºåˆ†ç±» ====================\n\nclass BERTClassifier(nn.Module):\n    def __init__(self, n_classes):\n        super().__init__()\n        self.bert = BertModel.from_pretrained('bert-base-uncased')\n        self.dropout = nn.Dropout(0.3)\n        self.fc = nn.Linear(768, n_classes)\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.bert(\n            input_ids=input_ids,\n            attention_mask=attention_mask\n        )\n\n        # ä½¿ç”¨ [CLS] token\n        pooled_output = outputs.pooler_output\n        output = self.dropout(pooled_output)\n        output = self.fc(output)\n\n        return output\n\n# ä½¿ç”¨\nmodel = BERTClassifier(n_classes=2)\n\n# è®­ç»ƒ\noptimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)",
    "crumbs": [
      "ç¬¬äºŒéƒ¨åˆ†ï¼šè¿›é˜¶ç¯‡",
      "<span class='chapter-number'>9</span>Â  <span class='chapter-title'>ç¬¬å…«ç« ï¼šAttention ä¸ Transformer</span>"
    ]
  },
  {
    "objectID": "Chapter8.html#gpt-generative-pre-trained-transformer",
    "href": "Chapter8.html#gpt-generative-pre-trained-transformer",
    "title": "9Â  ç¬¬å…«ç« ï¼šAttention ä¸ Transformer",
    "section": "9.8 8.7 GPT (Generative Pre-trained Transformer)",
    "text": "9.8 8.7 GPT (Generative Pre-trained Transformer)\n\n9.8.1 ğŸ¯ æ ¸å¿ƒæ€æƒ³\nåªä½¿ç”¨ Transformer çš„ Decoder\nè®­ç»ƒä»»åŠ¡ï¼šè‡ªå›å½’è¯­è¨€æ¨¡å‹ - ç»™å®šå‰é¢çš„è¯ï¼Œé¢„æµ‹ä¸‹ä¸€ä¸ªè¯ - P(w_t | w_1, â€¦, w_{t-1})\n\n\n9.8.2 ğŸ“ æ¶æ„\nè¾“å…¥ï¼šw_1, w_2, ..., w_{t-1}\n    â†“\nToken Embedding + Position Embedding\n    â†“\nTransformer Decoder (ä»…è‡ªæ³¨æ„åŠ›ï¼ŒÃ—12/24/48å±‚)\n    â†“\né¢„æµ‹ï¼šw_t\nä¸ BERT çš„åŒºåˆ«ï¼š\n\n\n\nç‰¹æ€§\nBERT\nGPT\n\n\n\n\næ¶æ„\nEncoder only\nDecoder only\n\n\næ³¨æ„åŠ›\nåŒå‘\nå•å‘ï¼ˆcausalï¼‰\n\n\nè®­ç»ƒ\nMLM + NSP\nè¯­è¨€å»ºæ¨¡\n\n\nåº”ç”¨\nç†è§£ä»»åŠ¡\nç”Ÿæˆä»»åŠ¡\n\n\n\n\n\n9.8.3 ğŸ’» ä½¿ç”¨ GPT-2\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel\n\n# åŠ è½½æ¨¡å‹\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\nmodel = GPT2LMHeadModel.from_pretrained('gpt2')\n\n# æ–‡æœ¬ç”Ÿæˆ\ndef generate_text(prompt, max_length=50):\n    inputs = tokenizer(prompt, return_tensors='pt')\n\n    with torch.no_grad():\n        outputs = model.generate(\n            inputs['input_ids'],\n            max_length=max_length,\n            num_return_sequences=1,\n            no_repeat_ngram_size=2,\n            temperature=0.7,\n            top_k=50,\n            top_p=0.95\n        )\n\n    text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    return text\n\n# ä½¿ç”¨\nprompt = \"Once upon a time,\"\ngenerated_text = generate_text(prompt)\nprint(generated_text)",
    "crumbs": [
      "ç¬¬äºŒéƒ¨åˆ†ï¼šè¿›é˜¶ç¯‡",
      "<span class='chapter-number'>9</span>Â  <span class='chapter-title'>ç¬¬å…«ç« ï¼šAttention ä¸ Transformer</span>"
    ]
  },
  {
    "objectID": "Chapter8.html#æœ¬ç« ä½œä¸š",
    "href": "Chapter8.html#æœ¬ç« ä½œä¸š",
    "title": "9Â  ç¬¬å…«ç« ï¼šAttention ä¸ Transformer",
    "section": "9.9 ğŸ“ æœ¬ç« ä½œä¸š",
    "text": "9.9 ğŸ“ æœ¬ç« ä½œä¸š\n\n9.9.1 ä½œä¸š 1ï¼šä»é›¶å®ç° Transformer\n# TODO:\n# 1. å®ç°å®Œæ•´çš„ Transformer\n# 2. åœ¨æœºå™¨ç¿»è¯‘ä»»åŠ¡ä¸Šè®­ç»ƒ\n# 3. å¯è§†åŒ–æ³¨æ„åŠ›æƒé‡\n# 4. å¯¹æ¯”ä¸åŒå±‚æ•°å’Œå¤´æ•°çš„æ•ˆæœ\n\n\n9.9.2 ä½œä¸š 2ï¼šBERT å¾®è°ƒ\n# ä½¿ç”¨ Hugging Face Transformers\n# ä»»åŠ¡ï¼šæ–‡æœ¬åˆ†ç±»ï¼ˆIMDB æˆ– AG Newsï¼‰\n# 1. åŠ è½½é¢„è®­ç»ƒ BERT\n# 2. æ·»åŠ åˆ†ç±»å¤´\n# 3. å¾®è°ƒ\n# 4. è¯„ä¼°æ€§èƒ½\n# 5. å¯¹æ¯”ä»å¤´è®­ç»ƒ vs å¾®è°ƒ\n\n\n9.9.3 ä½œä¸š 3ï¼šæ–‡æœ¬ç”Ÿæˆ\n# ä½¿ç”¨ GPT-2 æˆ–è®­ç»ƒè‡ªå·±çš„æ¨¡å‹\n# 1. å®ç°ä¸åŒçš„é‡‡æ ·ç­–ç•¥\n#    - Greedy\n#    - Beam Search\n#    - Top-K\n#    - Nucleus (Top-P)\n# 2. å¯¹æ¯”ç”Ÿæˆè´¨é‡\n# 3. å®ç°æ¡ä»¶ç”Ÿæˆ",
    "crumbs": [
      "ç¬¬äºŒéƒ¨åˆ†ï¼šè¿›é˜¶ç¯‡",
      "<span class='chapter-number'>9</span>Â  <span class='chapter-title'>ç¬¬å…«ç« ï¼šAttention ä¸ Transformer</span>"
    ]
  },
  {
    "objectID": "Chapter8.html#æœ¬ç« å…³é”®æ¦‚å¿µ",
    "href": "Chapter8.html#æœ¬ç« å…³é”®æ¦‚å¿µ",
    "title": "9Â  ç¬¬å…«ç« ï¼šAttention ä¸ Transformer",
    "section": "9.10 ğŸ”‘ æœ¬ç« å…³é”®æ¦‚å¿µ",
    "text": "9.10 ğŸ”‘ æœ¬ç« å…³é”®æ¦‚å¿µ\n\n\n\næ¦‚å¿µ\nè¯´æ˜\n\n\n\n\nAttention\nåŠ¨æ€å…³æ³¨ç›¸å…³ä¿¡æ¯\n\n\nSelf-Attention\nåºåˆ—å†…éƒ¨çš„æ³¨æ„åŠ›\n\n\nMulti-Head\nå¤šç»„æ³¨æ„åŠ›å¹¶è¡Œ\n\n\nPositional Encoding\nä½ç½®ä¿¡æ¯ç¼–ç \n\n\nTransformer\nå®Œå…¨åŸºäºæ³¨æ„åŠ›çš„æ¶æ„\n\n\nEncoder-Decoder\nåºåˆ—åˆ°åºåˆ—è½¬æ¢\n\n\nBERT\né¢„è®­ç»ƒåŒå‘ç¼–ç å™¨\n\n\nGPT\né¢„è®­ç»ƒè‡ªå›å½’è§£ç å™¨\n\n\nMLM\næ©ç è¯­è¨€æ¨¡å‹\n\n\nFine-tuning\nå¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹",
    "crumbs": [
      "ç¬¬äºŒéƒ¨åˆ†ï¼šè¿›é˜¶ç¯‡",
      "<span class='chapter-number'>9</span>Â  <span class='chapter-title'>ç¬¬å…«ç« ï¼šAttention ä¸ Transformer</span>"
    ]
  },
  {
    "objectID": "Chapter9.html",
    "href": "Chapter9.html",
    "title": "10Â  ç¬¬ä¹ç« ï¼šè¿ç§»å­¦ä¹ ä¸å¾®è°ƒ (Transfer Learning & Fine-tuning)",
    "section": "",
    "text": "10.1 ğŸ“Œ ç« èŠ‚ç›®æ ‡",
    "crumbs": [
      "ç¬¬ä¸‰éƒ¨åˆ†ï¼šå®è·µä¸åº”ç”¨",
      "<span class='chapter-number'>10</span>Â  <span class='chapter-title'>ç¬¬ä¹ç« ï¼šè¿ç§»å­¦ä¹ ä¸å¾®è°ƒ (Transfer Learning & Fine-tuning)</span>"
    ]
  },
  {
    "objectID": "Chapter9.html#ç« èŠ‚ç›®æ ‡",
    "href": "Chapter9.html#ç« èŠ‚ç›®æ ‡",
    "title": "10Â  ç¬¬ä¹ç« ï¼šè¿ç§»å­¦ä¹ ä¸å¾®è°ƒ (Transfer Learning & Fine-tuning)",
    "section": "",
    "text": "ç†è§£è¿ç§»å­¦ä¹ çš„åŠ¨æœºå’ŒåŸç†\næŒæ¡é¢„è®­ç»ƒæ¨¡å‹çš„ä½¿ç”¨æ–¹æ³•\nå­¦ä¹ ä¸åŒçš„å¾®è°ƒç­–ç•¥\näº†è§£é¢†åŸŸè‡ªé€‚åº”æŠ€æœ¯\nå®æˆ˜ï¼šå›¾åƒåˆ†ç±»ã€æ–‡æœ¬åˆ†ç±»çš„è¿ç§»å­¦ä¹ ",
    "crumbs": [
      "ç¬¬ä¸‰éƒ¨åˆ†ï¼šå®è·µä¸åº”ç”¨",
      "<span class='chapter-number'>10</span>Â  <span class='chapter-title'>ç¬¬ä¹ç« ï¼šè¿ç§»å­¦ä¹ ä¸å¾®è°ƒ (Transfer Learning & Fine-tuning)</span>"
    ]
  },
  {
    "objectID": "Chapter9.html#ä¸ºä»€ä¹ˆéœ€è¦è¿ç§»å­¦ä¹ ",
    "href": "Chapter9.html#ä¸ºä»€ä¹ˆéœ€è¦è¿ç§»å­¦ä¹ ",
    "title": "10Â  ç¬¬ä¹ç« ï¼šè¿ç§»å­¦ä¹ ä¸å¾®è°ƒ (Transfer Learning & Fine-tuning)",
    "section": "10.2 9.1 ä¸ºä»€ä¹ˆéœ€è¦è¿ç§»å­¦ä¹ ï¼Ÿ",
    "text": "10.2 9.1 ä¸ºä»€ä¹ˆéœ€è¦è¿ç§»å­¦ä¹ ï¼Ÿ\n\n10.2.1 ğŸš¨ ä»é›¶è®­ç»ƒçš„é—®é¢˜\nä¼ ç»Ÿæ·±åº¦å­¦ä¹ ï¼š\næ”¶é›†å¤§é‡æ•°æ® â†’ è®¾è®¡ç½‘ç»œ â†’ ä»é›¶è®­ç»ƒ â†’ éƒ¨ç½²\n\né—®é¢˜ï¼š\n  âŒ éœ€è¦æµ·é‡æ ‡æ³¨æ•°æ®\n  âŒ è®­ç»ƒæ—¶é—´é•¿ï¼ˆå‡ å¤©åˆ°å‡ å‘¨ï¼‰\n  âŒ è®¡ç®—èµ„æºæ˜‚è´µ\n  âŒ å®¹æ˜“è¿‡æ‹Ÿåˆï¼ˆå°æ•°æ®é›†ï¼‰\nä¾‹å­ï¼šè®­ç»ƒ ResNet-50 on ImageNet\næ•°æ®ï¼š120ä¸‡å¼ æ ‡æ³¨å›¾ç‰‡\næ—¶é—´ï¼š8ä¸ª GPUï¼Œå‡ å¤©åˆ°ä¸€å‘¨\næˆæœ¬ï¼šæ•°åƒç¾å…ƒ\n\n\n10.2.2 âœ… è¿ç§»å­¦ä¹ çš„ä¼˜åŠ¿\næ ¸å¿ƒæ€æƒ³ï¼šåˆ©ç”¨å·²æœ‰çŸ¥è¯†åŠ é€Ÿæ–°ä»»åŠ¡å­¦ä¹ \né¢„è®­ç»ƒï¼ˆå¤§æ•°æ®é›†ï¼‰â†’ å¾®è°ƒï¼ˆå°æ•°æ®é›†ï¼‰â†’ éƒ¨ç½²\n\nä¼˜åŠ¿ï¼š\n  âœ“ éœ€è¦æ›´å°‘çš„æ•°æ®\n  âœ“ è®­ç»ƒæ›´å¿«ï¼ˆå°æ—¶çº§åˆ«ï¼‰\n  âœ“ æ€§èƒ½æ›´å¥½ï¼ˆç‰¹åˆ«æ˜¯å°æ•°æ®é›†ï¼‰\n  âœ“ é™ä½æˆæœ¬\n\n\n10.2.3 ğŸ§  ç›´è§‰ç†è§£\näººç±»å­¦ä¹ çš„ç±»æ¯”ï¼š\nå­¦ä¹ è¯†åˆ«çŒ«ï¼š\n  ä¸éœ€è¦ä»é›¶å­¦ä¹ \"ä»€ä¹ˆæ˜¯è¾¹ç¼˜\"ã€\"ä»€ä¹ˆæ˜¯çº¹ç†\"\n  å·²ç»æœ‰è§†è§‰ç³»ç»Ÿçš„åŸºç¡€çŸ¥è¯†\n  åªéœ€è¦å­¦ä¹ \"çŒ«çš„ç‰¹å¾\"\n\nè¿ç§»å­¦ä¹ ï¼š\n  é¢„è®­ç»ƒæ¨¡å‹ = å·²æœ‰çš„è§†è§‰/è¯­è¨€çŸ¥è¯†\n  å¾®è°ƒ = é’ˆå¯¹ç‰¹å®šä»»åŠ¡è°ƒæ•´",
    "crumbs": [
      "ç¬¬ä¸‰éƒ¨åˆ†ï¼šå®è·µä¸åº”ç”¨",
      "<span class='chapter-number'>10</span>Â  <span class='chapter-title'>ç¬¬ä¹ç« ï¼šè¿ç§»å­¦ä¹ ä¸å¾®è°ƒ (Transfer Learning & Fine-tuning)</span>"
    ]
  },
  {
    "objectID": "Chapter9.html#è¿ç§»å­¦ä¹ çš„åˆ†ç±»",
    "href": "Chapter9.html#è¿ç§»å­¦ä¹ çš„åˆ†ç±»",
    "title": "10Â  ç¬¬ä¹ç« ï¼šè¿ç§»å­¦ä¹ ä¸å¾®è°ƒ (Transfer Learning & Fine-tuning)",
    "section": "10.3 9.2 è¿ç§»å­¦ä¹ çš„åˆ†ç±»",
    "text": "10.3 9.2 è¿ç§»å­¦ä¹ çš„åˆ†ç±»\n\n10.3.1 ğŸ“Š æŒ‰ä»»åŠ¡å…³ç³»åˆ†ç±»\n\n10.3.1.1 1. å½’çº³è¿ç§» (Inductive Transfer)\næºä»»åŠ¡ â‰  ç›®æ ‡ä»»åŠ¡ï¼Œä½†ç›¸å…³\n\nä¾‹ï¼š\n  æºï¼šImageNet åˆ†ç±» (1000ç±»)\n  ç›®æ ‡ï¼šåŒ»å­¦å›¾åƒåˆ†ç±» (5ç±»)\n\n\n10.3.1.2 2. è½¬å¯¼è¿ç§» (Transductive Transfer)\næºä»»åŠ¡ = ç›®æ ‡ä»»åŠ¡ï¼Œä½†æ•°æ®åˆ†å¸ƒä¸åŒ\n\nä¾‹ï¼š\n  æºï¼šç”µå½±è¯„è®ºæƒ…æ„Ÿåˆ†æ\n  ç›®æ ‡ï¼šäº§å“è¯„è®ºæƒ…æ„Ÿåˆ†æ\n\n\n10.3.1.3 3. æ— ç›‘ç£è¿ç§» (Unsupervised Transfer)\næºä»»åŠ¡å’Œç›®æ ‡ä»»åŠ¡éƒ½æ— æ ‡ç­¾\n\nä¾‹ï¼š\n  èšç±»ã€é™ç»´ä»»åŠ¡\n\n\n\n\n10.3.2 ğŸ“Š æŒ‰è¿ç§»å†…å®¹åˆ†ç±»\n\n10.3.2.1 1. ç‰¹å¾è¿ç§» (Feature Transfer)\nè¿ç§»å­¦åˆ°çš„ç‰¹å¾è¡¨ç¤º\n\næ–¹æ³•ï¼šå›ºå®šé¢„è®­ç»ƒæ¨¡å‹ï¼Œåªè®­ç»ƒåˆ†ç±»å™¨\n\n\n10.3.2.2 2. å‚æ•°è¿ç§» (Parameter Transfer)\nè¿ç§»æ¨¡å‹å‚æ•°ä½œä¸ºåˆå§‹åŒ–\n\næ–¹æ³•ï¼šç”¨é¢„è®­ç»ƒæƒé‡åˆå§‹åŒ–ï¼Œç„¶åå¾®è°ƒ\n\n\n10.3.2.3 3. å…³ç³»è¿ç§» (Relation Transfer)\nè¿ç§»æ ·æœ¬é—´çš„å…³ç³»\n\nä¾‹ï¼šçŸ¥è¯†å›¾è°±ã€ç»“æ„åŒ–é¢„æµ‹",
    "crumbs": [
      "ç¬¬ä¸‰éƒ¨åˆ†ï¼šå®è·µä¸åº”ç”¨",
      "<span class='chapter-number'>10</span>Â  <span class='chapter-title'>ç¬¬ä¹ç« ï¼šè¿ç§»å­¦ä¹ ä¸å¾®è°ƒ (Transfer Learning & Fine-tuning)</span>"
    ]
  },
  {
    "objectID": "Chapter9.html#è®¡ç®—æœºè§†è§‰ä¸­çš„è¿ç§»å­¦ä¹ ",
    "href": "Chapter9.html#è®¡ç®—æœºè§†è§‰ä¸­çš„è¿ç§»å­¦ä¹ ",
    "title": "10Â  ç¬¬ä¹ç« ï¼šè¿ç§»å­¦ä¹ ä¸å¾®è°ƒ (Transfer Learning & Fine-tuning)",
    "section": "10.4 9.3 è®¡ç®—æœºè§†è§‰ä¸­çš„è¿ç§»å­¦ä¹ ",
    "text": "10.4 9.3 è®¡ç®—æœºè§†è§‰ä¸­çš„è¿ç§»å­¦ä¹ \n\n10.4.1 ğŸ–¼ï¸ é¢„è®­ç»ƒæ¨¡å‹\nå¸¸ç”¨é¢„è®­ç»ƒæ¨¡å‹ï¼ˆåœ¨ ImageNet ä¸Šè®­ç»ƒï¼‰ï¼š\nè½»é‡çº§ï¼š\n  - MobileNet (4M å‚æ•°)\n  - EfficientNet-B0 (5M)\n\nä¸­ç­‰ï¼š\n  - ResNet-50 (25M)\n  - VGG-16 (138M)\n\nå¤§å‹ï¼š\n  - ResNet-152 (60M)\n  - EfficientNet-B7 (66M)\n  - Vision Transformer (ViT) (86M)\n\n\n10.4.2 ğŸ“ ç‰¹å¾æå– vs å¾®è°ƒ\n\n10.4.2.1 ç‰¹å¾æå– (Feature Extraction)\nå†»ç»“é¢„è®­ç»ƒæ¨¡å‹ â†’ åªè®­ç»ƒæ–°çš„åˆ†ç±»å™¨\n\né€‚ç”¨åœºæ™¯ï¼š\n  âœ“ æ•°æ®é›†å¾ˆå° (&lt; 1000 æ ·æœ¬)\n  âœ“ ç›®æ ‡ä»»åŠ¡ä¸æºä»»åŠ¡ç›¸ä¼¼\n  âœ“ è®¡ç®—èµ„æºæœ‰é™\nå®ç°ï¼š\nimport torch\nimport torch.nn as nn\nfrom torchvision import models\n\n# åŠ è½½é¢„è®­ç»ƒæ¨¡å‹\nmodel = models.resnet50(pretrained=True)\n\n# å†»ç»“æ‰€æœ‰å‚æ•°\nfor param in model.parameters():\n    param.requires_grad = False\n\n# æ›¿æ¢æœ€åçš„å…¨è¿æ¥å±‚\nnum_features = model.fc.in_features\nmodel.fc = nn.Linear(num_features, num_classes)  # åªæœ‰è¿™å±‚ä¼šè®­ç»ƒ\n\n# åªä¼˜åŒ–æ–°æ·»åŠ çš„å±‚\noptimizer = torch.optim.Adam(model.fc.parameters(), lr=0.001)\n\n\n\n10.4.2.2 å¾®è°ƒ (Fine-tuning)\nè§£å†»éƒ¨åˆ†æˆ–å…¨éƒ¨å±‚ â†’ åœ¨æ–°æ•°æ®ä¸Šè®­ç»ƒ\n\né€‚ç”¨åœºæ™¯ï¼š\n  âœ“ æ•°æ®é›†ä¸­ç­‰å¤§å° (1k - 100k)\n  âœ“ ç›®æ ‡ä»»åŠ¡ä¸æºä»»åŠ¡æœ‰å·®å¼‚\n  âœ“ è¿½æ±‚æ›´å¥½æ€§èƒ½\nç­–ç•¥ï¼š\n1. å…¨å±€å¾®è°ƒï¼š\n   è§£å†»æ‰€æœ‰å±‚ï¼Œç”¨å°å­¦ä¹ ç‡è®­ç»ƒ\n\n2. é€å±‚å¾®è°ƒï¼š\n   å…ˆè®­ç»ƒé¡¶å±‚ï¼Œé€æ¸è§£å†»åº•å±‚\n\n3. åˆ¤åˆ«å¼å¾®è°ƒï¼š\n   ä¸åŒå±‚ç”¨ä¸åŒå­¦ä¹ ç‡\nå®ç°ï¼š\n# æ–¹æ³•1ï¼šå…¨å±€å¾®è°ƒï¼ˆå°å­¦ä¹ ç‡ï¼‰\nmodel = models.resnet50(pretrained=True)\n\n# æ›¿æ¢åˆ†ç±»å™¨\nmodel.fc = nn.Linear(model.fc.in_features, num_classes)\n\n# æ‰€æœ‰å‚æ•°éƒ½è®­ç»ƒï¼Œä½†ç”¨å°å­¦ä¹ ç‡\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n# æ–¹æ³•2ï¼šé€å±‚å¾®è°ƒ\nmodel = models.resnet50(pretrained=True)\nmodel.fc = nn.Linear(model.fc.in_features, num_classes)\n\n# ç¬¬ä¸€é˜¶æ®µï¼šåªè®­ç»ƒåˆ†ç±»å™¨\nfor param in model.parameters():\n    param.requires_grad = False\nfor param in model.fc.parameters():\n    param.requires_grad = True\n\noptimizer = torch.optim.Adam(model.fc.parameters(), lr=0.001)\n# è®­ç»ƒå‡ ä¸ª epoch...\n\n# ç¬¬äºŒé˜¶æ®µï¼šè§£å†» layer4\nfor param in model.layer4.parameters():\n    param.requires_grad = True\n\noptimizer = torch.optim.Adam([\n    {'params': model.fc.parameters(), 'lr': 0.001},\n    {'params': model.layer4.parameters(), 'lr': 0.0001}\n])\n# ç»§ç»­è®­ç»ƒ...\n# æ–¹æ³•3ï¼šåˆ¤åˆ«å¼å­¦ä¹ ç‡\nmodel = models.resnet50(pretrained=True)\nmodel.fc = nn.Linear(model.fc.in_features, num_classes)\n\n# ä¸åŒå±‚ç»„ä½¿ç”¨ä¸åŒå­¦ä¹ ç‡\noptimizer = torch.optim.Adam([\n    {'params': model.conv1.parameters(), 'lr': 1e-5},\n    {'params': model.layer1.parameters(), 'lr': 1e-5},\n    {'params': model.layer2.parameters(), 'lr': 1e-4},\n    {'params': model.layer3.parameters(), 'lr': 1e-4},\n    {'params': model.layer4.parameters(), 'lr': 1e-3},\n    {'params': model.fc.parameters(), 'lr': 1e-2}\n])\n\n\n\n\n10.4.3 ğŸ¯ å®æˆ˜ï¼šçŒ«ç‹—åˆ†ç±»ï¼ˆè¿ç§»å­¦ä¹ ï¼‰\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms, models\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport os\n\n# ==================== è¶…å‚æ•° ====================\nDATA_DIR = './data/cats_and_dogs'\nBATCH_SIZE = 32\nEPOCHS = 10\nLEARNING_RATE = 0.001\nNUM_CLASSES = 2\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# ==================== æ•°æ®å¢å¼º ====================\ndata_transforms = {\n    'train': transforms.Compose([\n        transforms.RandomResizedCrop(224),\n        transforms.RandomHorizontalFlip(),\n        transforms.RandomRotation(15),\n        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n    'val': transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ])\n}\n\n# ==================== æ•°æ®åŠ è½½ ====================\nimage_datasets = {\n    x: datasets.ImageFolder(os.path.join(DATA_DIR, x), data_transforms[x])\n    for x in ['train', 'val']\n}\n\ndataloaders = {\n    x: DataLoader(image_datasets[x], batch_size=BATCH_SIZE,\n                  shuffle=(x=='train'), num_workers=4)\n    for x in ['train', 'val']\n}\n\ndataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\nclass_names = image_datasets['train'].classes\n\nprint(f\"è®­ç»ƒé›†: {dataset_sizes['train']} å¼ \")\nprint(f\"éªŒè¯é›†: {dataset_sizes['val']} å¼ \")\nprint(f\"ç±»åˆ«: {class_names}\")\n\n# ==================== æ¨¡å‹å®šä¹‰ ====================\n\ndef create_model(model_name='resnet50', num_classes=2, feature_extract=False):\n    \"\"\"\n    åˆ›å»ºè¿ç§»å­¦ä¹ æ¨¡å‹\n\n    å‚æ•°:\n        model_name: é¢„è®­ç»ƒæ¨¡å‹åç§°\n        num_classes: è¾“å‡ºç±»åˆ«æ•°\n        feature_extract: True=ç‰¹å¾æå–, False=å¾®è°ƒ\n    \"\"\"\n    model = None\n\n    if model_name == 'resnet50':\n        model = models.resnet50(pretrained=True)\n\n        # ç‰¹å¾æå–æ¨¡å¼ï¼šå†»ç»“å‚æ•°\n        if feature_extract:\n            for param in model.parameters():\n                param.requires_grad = False\n\n        # æ›¿æ¢åˆ†ç±»å™¨\n        num_features = model.fc.in_features\n        model.fc = nn.Sequential(\n            nn.Dropout(0.5),\n            nn.Linear(num_features, num_classes)\n        )\n\n    elif model_name == 'efficientnet_b0':\n        model = models.efficientnet_b0(pretrained=True)\n\n        if feature_extract:\n            for param in model.parameters():\n                param.requires_grad = False\n\n        num_features = model.classifier[1].in_features\n        model.classifier = nn.Sequential(\n            nn.Dropout(0.3),\n            nn.Linear(num_features, num_classes)\n        )\n\n    elif model_name == 'vgg16':\n        model = models.vgg16(pretrained=True)\n\n        if feature_extract:\n            for param in model.features.parameters():\n                param.requires_grad = False\n\n        num_features = model.classifier[6].in_features\n        model.classifier[6] = nn.Linear(num_features, num_classes)\n\n    return model\n\n# ==================== è®­ç»ƒå‡½æ•° ====================\n\ndef train_model(model, criterion, optimizer, scheduler, num_epochs):\n    \"\"\"è®­ç»ƒæ¨¡å‹\"\"\"\n    best_acc = 0.0\n    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n\n    for epoch in range(num_epochs):\n        print(f'\\nEpoch {epoch+1}/{num_epochs}')\n        print('-' * 60)\n\n        # è®­ç»ƒå’ŒéªŒè¯\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                model.train()\n            else:\n                model.eval()\n\n            running_loss = 0.0\n            running_corrects = 0\n\n            # è¿›åº¦æ¡\n            pbar = tqdm(dataloaders[phase], desc=f'{phase.capitalize()}')\n\n            for inputs, labels in pbar:\n                inputs = inputs.to(DEVICE)\n                labels = labels.to(DEVICE)\n\n                optimizer.zero_grad()\n\n                # å‰å‘ä¼ æ’­\n                with torch.set_grad_enabled(phase == 'train'):\n                    outputs = model(inputs)\n                    _, preds = torch.max(outputs, 1)\n                    loss = criterion(outputs, labels)\n\n                    # åå‘ä¼ æ’­\n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n\n                # ç»Ÿè®¡\n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n\n                # æ›´æ–°è¿›åº¦æ¡\n                pbar.set_postfix({\n                    'loss': f'{loss.item():.4f}',\n                    'acc': f'{torch.sum(preds == labels.data).item() / len(labels):.4f}'\n                })\n\n            # Epoch ç»Ÿè®¡\n            epoch_loss = running_loss / dataset_sizes[phase]\n            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n\n            print(f'{phase.capitalize()} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n\n            # è®°å½•å†å²\n            history[f'{phase}_loss'].append(epoch_loss)\n            history[f'{phase}_acc'].append(epoch_acc.item())\n\n            # ä¿å­˜æœ€ä½³æ¨¡å‹\n            if phase == 'val' and epoch_acc &gt; best_acc:\n                best_acc = epoch_acc\n                torch.save(model.state_dict(), 'best_model.pth')\n                print(f'âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Acc: {best_acc:.4f})')\n\n        # å­¦ä¹ ç‡è°ƒåº¦\n        scheduler.step()\n\n    print(f'\\næœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_acc:.4f}')\n\n    # åŠ è½½æœ€ä½³æ¨¡å‹\n    model.load_state_dict(torch.load('best_model.pth'))\n\n    return model, history\n\n# ==================== å¯¹æ¯”å®éªŒ ====================\n\ndef compare_strategies():\n    \"\"\"å¯¹æ¯”ä¸åŒè¿ç§»å­¦ä¹ ç­–ç•¥\"\"\"\n    strategies = {\n        'Feature Extraction': {\n            'model': create_model('resnet50', NUM_CLASSES, feature_extract=True),\n            'lr': 0.001,\n            'color': 'blue'\n        },\n        'Fine-tuning (å…¨å±€)': {\n            'model':\n\n-----\n\n&gt; contonue\n\ncreate_model('resnet50', NUM_CLASSES, feature_extract=False),\n            'lr': 0.0001,\n            'color': 'red'\n        }\n    }\n\n    results = {}\n\n    for strategy_name, config in strategies.items():\n        print(f'\\n{\"=\"*70}')\n        print(f'ç­–ç•¥: {strategy_name}')\n        print(f'{\"=\"*70}')\n\n        model = config['model'].to(DEVICE)\n\n        # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨\n        criterion = nn.CrossEntropyLoss()\n        optimizer = optim.Adam(\n            filter(lambda p: p.requires_grad, model.parameters()),\n            lr=config['lr']\n        )\n        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n\n        # è®­ç»ƒ\n        model, history = train_model(model, criterion, optimizer, scheduler, EPOCHS)\n\n        results[strategy_name] = {\n            'model': model,\n            'history': history,\n            'color': config['color']\n        }\n\n    # ==================== å¯è§†åŒ–å¯¹æ¯” ====================\n\n    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n\n    # Loss å¯¹æ¯”\n    for strategy_name, data in results.items():\n        epochs_range = range(1, len(data['history']['train_loss']) + 1)\n        axes[0].plot(epochs_range, data['history']['train_loss'],\n                    label=f'{strategy_name} (Train)',\n                    linestyle='--', color=data['color'])\n        axes[0].plot(epochs_range, data['history']['val_loss'],\n                    label=f'{strategy_name} (Val)',\n                    linestyle='-', color=data['color'])\n\n    axes[0].set_xlabel('Epoch')\n    axes[0].set_ylabel('Loss')\n    axes[0].set_title('Loss Comparison')\n    axes[0].legend()\n    axes[0].grid(True, alpha=0.3)\n\n    # Accuracy å¯¹æ¯”\n    for strategy_name, data in results.items():\n        epochs_range = range(1, len(data['history']['train_acc']) + 1)\n        axes[1].plot(epochs_range, data['history']['train_acc'],\n                    label=f'{strategy_name} (Train)',\n                    linestyle='--', color=data['color'])\n        axes[1].plot(epochs_range, data['history']['val_acc'],\n                    label=f'{strategy_name} (Val)',\n                    linestyle='-', color=data['color'])\n\n    axes[1].set_xlabel('Epoch')\n    axes[1].set_ylabel('Accuracy')\n    axes[1].set_title('Accuracy Comparison')\n    axes[1].legend()\n    axes[1].grid(True, alpha=0.3)\n\n    plt.tight_layout()\n    plt.savefig('transfer_learning_comparison.png', dpi=300)\n    plt.show()\n\n    return results\n\n# ==================== å¯è§†åŒ–é¢„æµ‹ ====================\n\ndef visualize_predictions(model, num_images=16):\n    \"\"\"å¯è§†åŒ–æ¨¡å‹é¢„æµ‹\"\"\"\n    model.eval()\n\n    images_so_far = 0\n    fig = plt.figure(figsize=(16, 12))\n\n    with torch.no_grad():\n        for inputs, labels in dataloaders['val']:\n            inputs = inputs.to(DEVICE)\n            labels = labels.to(DEVICE)\n\n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n\n            for j in range(inputs.size()[0]):\n                images_so_far += 1\n                ax = plt.subplot(4, 4, images_so_far)\n                ax.axis('off')\n\n                # åå½’ä¸€åŒ–æ˜¾ç¤º\n                img = inputs.cpu().data[j]\n                img = img.numpy().transpose((1, 2, 0))\n                mean = np.array([0.485, 0.456, 0.406])\n                std = np.array([0.229, 0.224, 0.225])\n                img = std * img + mean\n                img = np.clip(img, 0, 1)\n\n                ax.imshow(img)\n\n                # æ ‡é¢˜ï¼šé¢„æµ‹ vs çœŸå®\n                color = 'green' if preds[j] == labels[j] else 'red'\n                ax.set_title(f'Pred: {class_names[preds[j]]}\\nTrue: {class_names[labels[j]]}',\n                           color=color, fontsize=10)\n\n                if images_so_far == num_images:\n                    plt.tight_layout()\n                    plt.savefig('predictions.png', dpi=300)\n                    plt.show()\n                    return\n\n# ==================== ä¸»ç¨‹åº ====================\n\nif __name__ == '__main__':\n    # å¯¹æ¯”ä¸åŒç­–ç•¥\n    results = compare_strategies()\n\n    # å¯è§†åŒ–æœ€ä½³æ¨¡å‹çš„é¢„æµ‹\n    best_strategy = max(results.items(),\n                       key=lambda x: max(x[1]['history']['val_acc']))\n    print(f'\\næœ€ä½³ç­–ç•¥: {best_strategy[0]}')\n\n    visualize_predictions(best_strategy[1]['model'])",
    "crumbs": [
      "ç¬¬ä¸‰éƒ¨åˆ†ï¼šå®è·µä¸åº”ç”¨",
      "<span class='chapter-number'>10</span>Â  <span class='chapter-title'>ç¬¬ä¹ç« ï¼šè¿ç§»å­¦ä¹ ä¸å¾®è°ƒ (Transfer Learning & Fine-tuning)</span>"
    ]
  },
  {
    "objectID": "Chapter9.html#è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„è¿ç§»å­¦ä¹ ",
    "href": "Chapter9.html#è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„è¿ç§»å­¦ä¹ ",
    "title": "10Â  ç¬¬ä¹ç« ï¼šè¿ç§»å­¦ä¹ ä¸å¾®è°ƒ (Transfer Learning & Fine-tuning)",
    "section": "10.5 9.4 è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„è¿ç§»å­¦ä¹ ",
    "text": "10.5 9.4 è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„è¿ç§»å­¦ä¹ \n\n10.5.1 ğŸ“ é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹\nå‘å±•å†ç¨‹ï¼š\n2013: Word2Vec, GloVe\n      â†“\n2018: ELMo (åŠ¨æ€è¯å‘é‡)\n      â†“\n2018: BERT (åŒå‘é¢„è®­ç»ƒ)\n      â†“\n2019: GPT-2 (å¤§è§„æ¨¡ç”Ÿæˆ)\n      â†“\n2020: GPT-3 (è¶…å¤§è§„æ¨¡)\n      â†“\n2023: ChatGPT, GPT-4\n\n\n10.5.2 ğŸ”¹ ä½¿ç”¨ Hugging Face Transformers\nfrom transformers import (\n    BertTokenizer, BertForSequenceClassification,\n    GPT2Tokenizer, GPT2LMHeadModel,\n    AutoTokenizer, AutoModelForSequenceClassification,\n    Trainer, TrainingArguments\n)\nimport torch\nfrom torch.utils.data import Dataset\nimport numpy as np\n\n# ==================== æ•°æ®é›†å®šä¹‰ ====================\n\nclass TextClassificationDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_length=128):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        label = self.labels[idx]\n\n        encoding = self.tokenizer(\n            text,\n            max_length=self.max_length,\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt'\n        )\n\n        return {\n            'input_ids': encoding['input_ids'].squeeze(),\n            'attention_mask': encoding['attention_mask'].squeeze(),\n            'labels': torch.tensor(label, dtype=torch.long)\n        }\n\n# ==================== BERT å¾®è°ƒ ====================\n\ndef fine_tune_bert(train_texts, train_labels, val_texts, val_labels,\n                   num_labels=2, epochs=3):\n    \"\"\"\n    BERT å¾®è°ƒç”¨äºæ–‡æœ¬åˆ†ç±»\n    \"\"\"\n    # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å’Œåˆ†è¯å™¨\n    model_name = 'bert-base-uncased'\n    tokenizer = BertTokenizer.from_pretrained(model_name)\n    model = BertForSequenceClassification.from_pretrained(\n        model_name,\n        num_labels=num_labels\n    )\n\n    # åˆ›å»ºæ•°æ®é›†\n    train_dataset = TextClassificationDataset(\n        train_texts, train_labels, tokenizer\n    )\n    val_dataset = TextClassificationDataset(\n        val_texts, val_labels, tokenizer\n    )\n\n    # è®­ç»ƒå‚æ•°\n    training_args = TrainingArguments(\n        output_dir='./results',\n        num_train_epochs=epochs,\n        per_device_train_batch_size=16,\n        per_device_eval_batch_size=64,\n        warmup_steps=500,\n        weight_decay=0.01,\n        logging_dir='./logs',\n        logging_steps=100,\n        evaluation_strategy='epoch',\n        save_strategy='epoch',\n        load_best_model_at_end=True,\n        learning_rate=2e-5,\n    )\n\n    # å®šä¹‰è¯„ä¼°æŒ‡æ ‡\n    def compute_metrics(eval_pred):\n        predictions, labels = eval_pred\n        predictions = np.argmax(predictions, axis=1)\n        accuracy = (predictions == labels).mean()\n        return {'accuracy': accuracy}\n\n    # è®­ç»ƒå™¨\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=val_dataset,\n        compute_metrics=compute_metrics\n    )\n\n    # è®­ç»ƒ\n    trainer.train()\n\n    # è¯„ä¼°\n    eval_results = trainer.evaluate()\n    print(f\"\\nè¯„ä¼°ç»“æœ: {eval_results}\")\n\n    return model, tokenizer\n\n# ==================== å®æˆ˜ç¤ºä¾‹ï¼šæƒ…æ„Ÿåˆ†æ ====================\n\ndef sentiment_analysis_example():\n    \"\"\"æƒ…æ„Ÿåˆ†æå®Œæ•´ç¤ºä¾‹\"\"\"\n\n    # ç¤ºä¾‹æ•°æ®ï¼ˆå®é™…åº”ä½¿ç”¨ IMDB ç­‰æ•°æ®é›†ï¼‰\n    train_texts = [\n        \"This movie is fantastic! I loved it.\",\n        \"Great film, highly recommended.\",\n        \"Amazing performance by the actors.\",\n        \"Terrible waste of time.\",\n        \"Boring and predictable plot.\",\n        \"I hated every minute of it.\"\n    ] * 100\n\n    train_labels = [1, 1, 1, 0, 0, 0] * 100  # 1=æ­£é¢, 0=è´Ÿé¢\n\n    val_texts = [\n        \"Excellent movie!\",\n        \"Not worth watching.\",\n        \"Pretty good film.\",\n        \"Absolutely awful.\"\n    ]\n    val_labels = [1, 0, 1, 0]\n\n    # å¾®è°ƒ BERT\n    model, tokenizer = fine_tune_bert(\n        train_texts, train_labels,\n        val_texts, val_labels,\n        num_labels=2,\n        epochs=3\n    )\n\n    # é¢„æµ‹å‡½æ•°\n    def predict(text):\n        encoding = tokenizer(\n            text,\n            max_length=128,\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt'\n        )\n\n        model.eval()\n        with torch.no_grad():\n            outputs = model(**encoding)\n            predictions = torch.softmax(outputs.logits, dim=1)\n            label = torch.argmax(predictions, dim=1).item()\n            confidence = predictions[0][label].item()\n\n        sentiment = \"æ­£é¢\" if label == 1 else \"è´Ÿé¢\"\n        return sentiment, confidence\n\n    # æµ‹è¯•\n    test_texts = [\n        \"This is an amazing movie!\",\n        \"Worst film I've ever seen.\",\n        \"It was okay, nothing special.\"\n    ]\n\n    print(\"\\né¢„æµ‹ç»“æœ:\")\n    for text in test_texts:\n        sentiment, confidence = predict(text)\n        print(f\"\\næ–‡æœ¬: {text}\")\n        print(f\"æƒ…æ„Ÿ: {sentiment} (ç½®ä¿¡åº¦: {confidence:.4f})\")\n\n    return model, tokenizer\n\n# è¿è¡Œç¤ºä¾‹\nif __name__ == '__main__':\n    model, tokenizer = sentiment_analysis_example()",
    "crumbs": [
      "ç¬¬ä¸‰éƒ¨åˆ†ï¼šå®è·µä¸åº”ç”¨",
      "<span class='chapter-number'>10</span>Â  <span class='chapter-title'>ç¬¬ä¹ç« ï¼šè¿ç§»å­¦ä¹ ä¸å¾®è°ƒ (Transfer Learning & Fine-tuning)</span>"
    ]
  },
  {
    "objectID": "Chapter9.html#é¢†åŸŸè‡ªé€‚åº”-domain-adaptation",
    "href": "Chapter9.html#é¢†åŸŸè‡ªé€‚åº”-domain-adaptation",
    "title": "10Â  ç¬¬ä¹ç« ï¼šè¿ç§»å­¦ä¹ ä¸å¾®è°ƒ (Transfer Learning & Fine-tuning)",
    "section": "10.6 9.5 é¢†åŸŸè‡ªé€‚åº” (Domain Adaptation)",
    "text": "10.6 9.5 é¢†åŸŸè‡ªé€‚åº” (Domain Adaptation)\n\n10.6.1 ğŸ¯ é—®é¢˜è®¾å®š\næºé¢†åŸŸï¼šæœ‰å¤§é‡æ ‡æ³¨æ•°æ®\nç›®æ ‡é¢†åŸŸï¼šæ•°æ®åˆ†å¸ƒä¸åŒï¼Œæ ‡æ³¨å°‘æˆ–æ— \n\nä¾‹å­ï¼š\n  æºï¼šæ–°é—»æ–‡æœ¬åˆ†ç±»\n  ç›®æ ‡ï¼šç¤¾äº¤åª’ä½“æ–‡æœ¬åˆ†ç±»\n\n\n10.6.2 ğŸ“ æ–¹æ³•åˆ†ç±»\n\n10.6.2.1 1. åŸºäºå®ä¾‹çš„æ–¹æ³•\né‡è¦æ€§åŠ æƒï¼š\nclass ImportanceWeightedLoss(nn.Module):\n    \"\"\"æ ¹æ®æ ·æœ¬ç›¸ä¼¼åº¦åŠ æƒæŸå¤±\"\"\"\n\n    def __init__(self, base_criterion):\n        super().__init__()\n        self.base_criterion = base_criterion\n\n    def compute_weights(self, source_features, target_features):\n        \"\"\"\n        è®¡ç®—æºåŸŸæ ·æœ¬çš„é‡è¦æ€§æƒé‡\n        ä½¿å¾—æºåŸŸåˆ†å¸ƒæ¥è¿‘ç›®æ ‡åŸŸ\n        \"\"\"\n        # ç®€åŒ–ç‰ˆï¼šåŸºäºç‰¹å¾è·ç¦»\n        distances = torch.cdist(source_features, target_features)\n        min_distances = distances.min(dim=1)[0]\n        weights = torch.exp(-min_distances)\n        weights = weights / weights.sum()\n        return weights\n\n    def forward(self, outputs, targets, weights):\n        loss = self.base_criterion(outputs, targets)\n        weighted_loss = (loss * weights).mean()\n        return weighted_loss\n\n\n\n10.6.2.2 2. åŸºäºç‰¹å¾çš„æ–¹æ³•\né¢†åŸŸå¯¹æŠ—è®­ç»ƒ (Domain Adversarial Training)ï¼š\nclass GradientReversalLayer(torch.autograd.Function):\n    \"\"\"æ¢¯åº¦åè½¬å±‚\"\"\"\n\n    @staticmethod\n    def forward(ctx, x, alpha):\n        ctx.alpha = alpha\n        return x.view_as(x)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        output = grad_output.neg() * ctx.alpha\n        return output, None\n\nclass DomainAdversarialNetwork(nn.Module):\n    \"\"\"é¢†åŸŸå¯¹æŠ—ç½‘ç»œ\"\"\"\n\n    def __init__(self, input_size, hidden_size, num_classes):\n        super().__init__()\n\n        # ç‰¹å¾æå–å™¨ï¼ˆå…±äº«ï¼‰\n        self.feature_extractor = nn.Sequential(\n            nn.Linear(input_size, hidden_size),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(hidden_size, hidden_size),\n            nn.ReLU()\n        )\n\n        # æ ‡ç­¾é¢„æµ‹å™¨\n        self.label_predictor = nn.Sequential(\n            nn.Dropout(0.5),\n            nn.Linear(hidden_size, num_classes)\n        )\n\n        # åŸŸåˆ†ç±»å™¨\n        self.domain_classifier = nn.Sequential(\n            nn.Linear(hidden_size, hidden_size),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(hidden_size, 2)  # 2ä¸ªåŸŸ\n        )\n\n    def forward(self, x, alpha=1.0):\n        # ç‰¹å¾æå–\n        features = self.feature_extractor(x)\n\n        # æ ‡ç­¾é¢„æµ‹\n        class_output = self.label_predictor(features)\n\n        # åŸŸåˆ†ç±»ï¼ˆæ¢¯åº¦åè½¬ï¼‰\n        reverse_features = GradientReversalLayer.apply(features, alpha)\n        domain_output = self.domain_classifier(reverse_features)\n\n        return class_output, domain_output\n\n# è®­ç»ƒ\ndef train_domain_adaptation(model, source_loader, target_loader,\n                            num_epochs=50):\n    \"\"\"é¢†åŸŸè‡ªé€‚åº”è®­ç»ƒ\"\"\"\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n    class_criterion = nn.CrossEntropyLoss()\n    domain_criterion = nn.CrossEntropyLoss()\n\n    for epoch in range(num_epochs):\n        model.train()\n\n        for (source_data, source_labels), (target_data, _) in zip(source_loader, target_loader):\n\n            # Alpha éšç€è®­ç»ƒå¢åŠ ï¼ˆä» 0 åˆ° 1ï¼‰\n            p = float(epoch) / num_epochs\n            alpha = 2. / (1. + np.exp(-10 * p)) - 1\n\n            # æºåŸŸæ•°æ®\n            class_output_s, domain_output_s = model(source_data, alpha)\n\n            # ç›®æ ‡åŸŸæ•°æ®\n            _, domain_output_t = model(target_data, alpha)\n\n            # åŸŸæ ‡ç­¾ï¼šæº=0ï¼Œç›®æ ‡=1\n            domain_label_s = torch.zeros(len(source_data)).long()\n            domain_label_t = torch.ones(len(target_data)).long()\n\n            # è®¡ç®—æŸå¤±\n            class_loss = class_criterion(class_output_s, source_labels)\n            domain_loss_s = domain_criterion(domain_output_s, domain_label_s)\n            domain_loss_t = domain_criterion(domain_output_t, domain_label_t)\n            domain_loss = domain_loss_s + domain_loss_t\n\n            total_loss = class_loss + domain_loss\n\n            # åå‘ä¼ æ’­\n            optimizer.zero_grad()\n            total_loss.backward()\n            optimizer.step()\n\n            if epoch % 10 == 0:\n                print(f'Epoch {epoch}: Class Loss={class_loss.item():.4f}, '\n                      f'Domain Loss={domain_loss.item():.4f}')\n\n    return model\n\n\n\n10.6.2.3 3. è‡ªè®­ç»ƒ (Self-Training)\nclass SelfTraining:\n    \"\"\"è‡ªè®­ç»ƒ/ä¼ªæ ‡ç­¾æ–¹æ³•\"\"\"\n\n    def __init__(self, model, confidence_threshold=0.9):\n        self.model = model\n        self.threshold = confidence_threshold\n\n    def generate_pseudo_labels(self, unlabeled_loader):\n        \"\"\"ç”Ÿæˆä¼ªæ ‡ç­¾\"\"\"\n        self.model.eval()\n\n        pseudo_data = []\n        pseudo_labels = []\n\n        with torch.no_grad():\n            for data in unlabeled_loader:\n                outputs = self.model(data)\n                probs = torch.softmax(outputs, dim=1)\n\n                # åªé€‰æ‹©é«˜ç½®ä¿¡åº¦æ ·æœ¬\n                max_probs, predictions = torch.max(probs, dim=1)\n\n                mask = max_probs &gt; self.threshold\n\n                pseudo_data.append(data[mask])\n                pseudo_labels.append(predictions[mask])\n\n        return torch.cat(pseudo_data), torch.cat(pseudo_labels)\n\n    def train(self, labeled_loader, unlabeled_loader, num_iterations=5):\n        \"\"\"è¿­ä»£è®­ç»ƒ\"\"\"\n\n        optimizer = torch.optim.Adam(self.model.parameters(), lr=0.001)\n        criterion = nn.CrossEntropyLoss()\n\n        for iteration in range(num_iterations):\n            print(f'\\n=== Iteration {iteration+1}/{num_iterations} ===')\n\n            # åœ¨æ ‡æ³¨æ•°æ®ä¸Šè®­ç»ƒ\n            self.model.train()\n            for data, labels in labeled_loader:\n                outputs = self.model(data)\n                loss = criterion(outputs, labels)\n\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n\n            # ç”Ÿæˆä¼ªæ ‡ç­¾\n            pseudo_data, pseudo_labels = self.generate_pseudo_labels(unlabeled_loader)\n\n            if len(pseudo_data) &gt; 0:\n                print(f'ç”Ÿæˆäº† {len(pseudo_data)} ä¸ªä¼ªæ ‡ç­¾')\n\n                # åœ¨ä¼ªæ ‡ç­¾æ•°æ®ä¸Šè®­ç»ƒ\n                pseudo_dataset = torch.utils.data.TensorDataset(\n                    pseudo_data, pseudo_labels\n                )\n                pseudo_loader = torch.utils.data.DataLoader(\n                    pseudo_dataset, batch_size=32, shuffle=True\n                )\n\n                self.model.train()\n                for data, labels in pseudo_loader:\n                    outputs = self.model(data)\n                    loss = criterion(outputs, labels)\n\n                    optimizer.zero_grad()\n                    loss.backward()\n                    optimizer.step()\n\n        return self.model",
    "crumbs": [
      "ç¬¬ä¸‰éƒ¨åˆ†ï¼šå®è·µä¸åº”ç”¨",
      "<span class='chapter-number'>10</span>Â  <span class='chapter-title'>ç¬¬ä¹ç« ï¼šè¿ç§»å­¦ä¹ ä¸å¾®è°ƒ (Transfer Learning & Fine-tuning)</span>"
    ]
  },
  {
    "objectID": "Chapter9.html#å°‘æ ·æœ¬å­¦ä¹ -few-shot-learning",
    "href": "Chapter9.html#å°‘æ ·æœ¬å­¦ä¹ -few-shot-learning",
    "title": "10Â  ç¬¬ä¹ç« ï¼šè¿ç§»å­¦ä¹ ä¸å¾®è°ƒ (Transfer Learning & Fine-tuning)",
    "section": "10.7 9.6 å°‘æ ·æœ¬å­¦ä¹  (Few-Shot Learning)",
    "text": "10.7 9.6 å°‘æ ·æœ¬å­¦ä¹  (Few-Shot Learning)\n\n10.7.1 ğŸ¯ é—®é¢˜å®šä¹‰\nN-way K-shot åˆ†ç±»ï¼š\n  N: ç±»åˆ«æ•°\n  K: æ¯ç±»çš„æ ·æœ¬æ•°\n\nä¾‹ï¼š5-way 1-shot\n  5ä¸ªç±»åˆ«ï¼Œæ¯ç±»åªæœ‰1ä¸ªæ ·æœ¬\n\n\n10.7.2 ğŸ“ å…ƒå­¦ä¹  (Meta-Learning)\nMAML (Model-Agnostic Meta-Learning)ï¼š\nclass MAML:\n    \"\"\"æ¨¡å‹æ— å…³çš„å…ƒå­¦ä¹ \"\"\"\n\n    def __init__(self, model, inner_lr=0.01, outer_lr=0.001):\n        self.model = model\n        self.inner_lr = inner_lr  # ä»»åŠ¡å†…å­¦ä¹ ç‡\n        self.outer_lr = outer_lr  # è·¨ä»»åŠ¡å­¦ä¹ ç‡\n        self.meta_optimizer = torch.optim.Adam(\n            model.parameters(), lr=outer_lr\n        )\n\n    def inner_loop(self, support_x, support_y, num_steps=5):\n        \"\"\"\n        ä»»åŠ¡å†…é€‚åº”ï¼ˆå¿«é€Ÿå­¦ä¹ ï¼‰\n\n        å‚æ•°ï¼š\n            support_x: æ”¯æŒé›†è¾“å…¥\n            support_y: æ”¯æŒé›†æ ‡ç­¾\n            num_steps: å†…å¾ªç¯æ­¥æ•°\n        \"\"\"\n        # å¤åˆ¶æ¨¡å‹å‚æ•°\n        fast_weights = [p.clone() for p in self.model.parameters()]\n\n        for step in range(num_steps):\n            # å‰å‘ä¼ æ’­\n            outputs = self.model(support_x)\n            loss = nn.CrossEntropyLoss()(outputs, support_y)\n\n            # è®¡ç®—æ¢¯åº¦\n            grads = torch.autograd.grad(\n                loss, fast_weights, create_graph=True\n            )\n\n            # æ›´æ–°å‚æ•°ï¼ˆä¸€æ­¥æ¢¯åº¦ä¸‹é™ï¼‰\n            fast_weights = [\n                w - self.inner_lr * g\n                for w, g in zip(fast_weights, grads)\n            ]\n\n        return fast_weights\n\n    def outer_loop(self, tasks, num_epochs=1000):\n        \"\"\"\n        è·¨ä»»åŠ¡å­¦ä¹ ï¼ˆå…ƒå­¦ä¹ ï¼‰\n\n        å‚æ•°ï¼š\n            tasks: ä»»åŠ¡åˆ—è¡¨ï¼Œæ¯ä¸ªä»»åŠ¡åŒ…å« (support_set, query_set)\n        \"\"\"\n        for epoch in range(num_epochs):\n            meta_loss = 0\n\n            for task in tasks:\n                support_x, support_y = task['support']\n                query_x, query_y = task['query']\n\n                # å†…å¾ªç¯ï¼šåœ¨æ”¯æŒé›†ä¸Šå¿«é€Ÿé€‚åº”\n                fast_weights = self.inner_loop(support_x, support_y)\n\n                # åœ¨æŸ¥è¯¢é›†ä¸Šè¯„ä¼°\n                # ä½¿ç”¨æ›´æ–°åçš„å‚æ•°\n                outputs = self.model(query_x, weights=fast_weights)\n                loss = nn.CrossEntropyLoss()(outputs, query_y)\n\n                meta_loss += loss\n\n            # å¤–å¾ªç¯ï¼šæ›´æ–°å…ƒå‚æ•°\n            meta_loss = meta_loss / len(tasks)\n\n            self.meta_optimizer.zero_grad()\n            meta_loss.backward()\n            self.meta_optimizer.step()\n\n            if epoch % 100 == 0:\n                print(f'Epoch {epoch}: Meta Loss = {meta_loss.item():.4f}')\n\n\n\n10.7.3 ğŸ“ åŸå‹ç½‘ç»œ (Prototypical Networks)\nclass PrototypicalNetwork(nn.Module):\n    \"\"\"åŸå‹ç½‘ç»œ\"\"\"\n\n    def __init__(self, encoder):\n        super().__init__()\n        self.encoder = encoder\n\n    def compute_prototypes(self, support_x, support_y, n_way):\n        \"\"\"\n        è®¡ç®—æ¯ä¸ªç±»çš„åŸå‹ï¼ˆå‡å€¼ï¼‰\n\n        å‚æ•°ï¼š\n            support_x: (n_way * n_support, *)\n            support_y: (n_way * n_support,)\n            n_way: ç±»åˆ«æ•°\n        \"\"\"\n        # ç¼–ç \n        embeddings = self.encoder(support_x)  # (n_way*n_support, d)\n\n        # è®¡ç®—åŸå‹\n        prototypes = []\n        for c in range(n_way):\n            mask = (support_y == c)\n            class_embeddings = embeddings[mask]\n            prototype = class_embeddings.mean(dim=0)\n            prototypes.append(prototype)\n\n        prototypes = torch.stack(prototypes)  # (n_way, d)\n        return prototypes\n\n    def forward(self, support_x, support_y, query_x, n_way):\n        \"\"\"\n        å‚æ•°ï¼š\n            support_x: æ”¯æŒé›†è¾“å…¥\n            support_y: æ”¯æŒé›†æ ‡ç­¾\n            query_x: æŸ¥è¯¢é›†è¾“å…¥\n            n_way: ç±»åˆ«æ•°\n        \"\"\"\n        # è®¡ç®—åŸå‹\n        prototypes = self.compute_prototypes(support_x, support_y, n_way)\n\n        # ç¼–ç æŸ¥è¯¢é›†\n        query_embeddings = self.encoder(query_x)  # (n_query, d)\n\n        # è®¡ç®—è·ç¦»ï¼ˆæ¬§æ°è·ç¦»ï¼‰\n        distances = torch.cdist(query_embeddings, prototypes)  # (n_query, n_way)\n\n        # è½¬æ¢ä¸ºæ¦‚ç‡ï¼ˆè´Ÿè·ç¦»çš„ softmaxï¼‰\n        logits = -distances\n\n        return logits\n\n# è®­ç»ƒ\ndef train_prototypical_network(model, tasks, num_epochs=1000):\n    \"\"\"è®­ç»ƒåŸå‹ç½‘ç»œ\"\"\"\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n    criterion = nn.CrossEntropyLoss()\n\n    for epoch in range(num_epochs):\n        total_loss = 0\n        total_acc = 0\n\n        for task in tasks:\n            support_x, support_y = task['support']\n            query_x, query_y = task['query']\n            n_way = len(torch.unique(support_y))\n\n            # å‰å‘ä¼ æ’­\n            logits = model(support_x, support_y, query_x, n_way)\n\n            # è®¡ç®—æŸå¤±\n            loss = criterion(logits, query_y)\n\n            # åå‘ä¼ æ’­\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            # ç»Ÿè®¡\n            total_loss += loss.item()\n            preds = torch.argmax(logits, dim=1)\n            total_acc += (preds == query_y).float().mean().item()\n\n        if epoch % 100 == 0:\n            avg_loss = total_loss / len(tasks)\n            avg_acc = total_acc / len(tasks)\n            print(f'Epoch {epoch}: Loss={avg_loss:.4f}, Acc={avg_acc:.4f}')\n\n    return model",
    "crumbs": [
      "ç¬¬ä¸‰éƒ¨åˆ†ï¼šå®è·µä¸åº”ç”¨",
      "<span class='chapter-number'>10</span>Â  <span class='chapter-title'>ç¬¬ä¹ç« ï¼šè¿ç§»å­¦ä¹ ä¸å¾®è°ƒ (Transfer Learning & Fine-tuning)</span>"
    ]
  },
  {
    "objectID": "Chapter9.html#çŸ¥è¯†è’¸é¦-knowledge-distillation",
    "href": "Chapter9.html#çŸ¥è¯†è’¸é¦-knowledge-distillation",
    "title": "10Â  ç¬¬ä¹ç« ï¼šè¿ç§»å­¦ä¹ ä¸å¾®è°ƒ (Transfer Learning & Fine-tuning)",
    "section": "10.8 9.7 çŸ¥è¯†è’¸é¦ (Knowledge Distillation)",
    "text": "10.8 9.7 çŸ¥è¯†è’¸é¦ (Knowledge Distillation)\n\n10.8.1 ğŸ¯ æ ¸å¿ƒæ€æƒ³\næ•™å¸ˆæ¨¡å‹ï¼ˆå¤§ï¼‰â†’ çŸ¥è¯† â†’ å­¦ç”Ÿæ¨¡å‹ï¼ˆå°ï¼‰\n\nç›®æ ‡ï¼š\n  ç”¨å¤§æ¨¡å‹çš„\"è½¯æ ‡ç­¾\"è®­ç»ƒå°æ¨¡å‹\n  å°æ¨¡å‹è·å¾—å¤§æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›\n\n\n10.8.2 ğŸ“ æ¸©åº¦ Softmax\næ ‡å‡† Softmaxï¼š\n  p_i = exp(z_i) / Î£_j exp(z_j)\n\næ¸©åº¦ Softmaxï¼š\n  p_i = exp(z_i/T) / Î£_j exp(z_j/T)\n\nT &gt; 1: è¾“å‡ºæ›´å¹³æ»‘ï¼ˆ\"è½¯\"æ ‡ç­¾ï¼‰\nT = 1: æ ‡å‡† softmax\nT â†’ âˆ: å‡åŒ€åˆ†å¸ƒ\n\n\n10.8.3 ğŸ’» å®ç°\nclass DistillationLoss(nn.Module):\n    \"\"\"çŸ¥è¯†è’¸é¦æŸå¤±\"\"\"\n\n    def __init__(self, temperature=3.0, alpha=0.5):\n        super().__init__()\n        self.temperature = temperature\n        self.alpha = alpha\n        self.criterion = nn.CrossEntropyLoss()\n        self.kl_div = nn.KLDivLoss(reduction='batchmean')\n\n    def forward(self, student_logits, teacher_logits, targets):\n        \"\"\"\n        å‚æ•°ï¼š\n            student_logits: å­¦ç”Ÿæ¨¡å‹è¾“å‡º\n            teacher_logits: æ•™å¸ˆæ¨¡å‹è¾“å‡º\n            targets: çœŸå®æ ‡ç­¾\n        \"\"\"\n        # ç¡¬æ ‡ç­¾æŸå¤±\n        hard_loss = self.criterion(student_logits, targets)\n\n        # è½¯æ ‡ç­¾æŸå¤±ï¼ˆKL æ•£åº¦ï¼‰\n        student_soft = F.log_softmax(student_logits / self.temperature, dim=1)\n        teacher_soft = F.softmax(teacher_logits / self.temperature, dim=1)\n        soft_loss = self.kl_div(student_soft, teacher_soft) * (self.temperature ** 2)\n\n        # ç»„åˆæŸå¤±\n        total_loss = self.alpha * hard_loss + (1 - self.alpha) * soft_loss\n\n        return total_loss\n\ndef knowledge_distillation(teacher_model, student_model, train_loader,\n                           num_epochs=50, temperature=3.0):\n    \"\"\"\n    çŸ¥è¯†è’¸é¦è®­ç»ƒ\n\n    å‚æ•°ï¼š\n        teacher_model: é¢„è®­ç»ƒçš„å¤§æ¨¡å‹\n        student_model: å¾…è®­ç»ƒçš„å°æ¨¡å‹\n        train_loader: æ•°æ®åŠ è½½å™¨\n        num_epochs: è®­ç»ƒè½®æ•°\n        temperature: æ¸©åº¦å‚æ•°\n    \"\"\"\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n    teacher_model.to(device)\n    student_model.to(device)\n\n    teacher_model.eval()  # æ•™å¸ˆæ¨¡å‹ä¸è®­ç»ƒ\n\n    optimizer = torch.optim.Adam(student_model.parameters(), lr=0.001)\n    distillation_loss = DistillationLoss(temperature=temperature, alpha=0.7)\n\n    for epoch in range(num_epochs):\n        student_model.train()\n\n        total_loss = 0\n        correct = 0\n        total = 0\n\n        for inputs, targets in train_loader:\n            inputs, targets = inputs.to(device), targets.to(device)\n\n            # æ•™å¸ˆæ¨¡å‹è¾“å‡ºï¼ˆä¸éœ€è¦æ¢¯åº¦ï¼‰\n            with torch.no_grad():\n                teacher_logits = teacher_model(inputs)\n\n            # å­¦ç”Ÿæ¨¡å‹è¾“å‡º\n            student_logits = student_model(inputs)\n\n            # è®¡ç®—è’¸é¦æŸå¤±\n            loss = distillation_loss(student_logits, teacher_logits, targets)\n\n            # åå‘ä¼ æ’­\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            # ç»Ÿè®¡\n            total_loss += loss.item()\n            _, predicted = student_logits.max(1)\n            total += targets.size(0)\n            correct += predicted.eq(targets).sum().item()\n\n        accuracy = 100. * correct / total\n        print(f'Epoch {epoch+1}: Loss={total_loss/len(train_loader):.4f}, '\n              f'Acc={accuracy:.2f}%')\n\n    return student_model\n\n# ä½¿ç”¨ç¤ºä¾‹\nif __name__ == '__main__':\n    # æ•™å¸ˆæ¨¡å‹ï¼ˆå¤§ï¼‰\n    teacher = models.resnet50(pretrained=True)\n    teacher.fc = nn.Linear(teacher.fc.in_features, 10)\n\n    # å­¦ç”Ÿæ¨¡å‹ï¼ˆå°ï¼‰\n    student = models.resnet18(pretraine\n\n-----\n\n&gt; Continue\n\nd=False)\n    student.fc = nn.Linear(student.fc.in_features, 10)\n\n    # çŸ¥è¯†è’¸é¦\n    student = knowledge_distillation(\n        teacher, student, train_loader,\n        num_epochs=50, temperature=3.0\n    )",
    "crumbs": [
      "ç¬¬ä¸‰éƒ¨åˆ†ï¼šå®è·µä¸åº”ç”¨",
      "<span class='chapter-number'>10</span>Â  <span class='chapter-title'>ç¬¬ä¹ç« ï¼šè¿ç§»å­¦ä¹ ä¸å¾®è°ƒ (Transfer Learning & Fine-tuning)</span>"
    ]
  },
  {
    "objectID": "Chapter9.html#å®æˆ˜å®Œæ•´è¿ç§»å­¦ä¹ é¡¹ç›®",
    "href": "Chapter9.html#å®æˆ˜å®Œæ•´è¿ç§»å­¦ä¹ é¡¹ç›®",
    "title": "10Â  ç¬¬ä¹ç« ï¼šè¿ç§»å­¦ä¹ ä¸å¾®è°ƒ (Transfer Learning & Fine-tuning)",
    "section": "10.9 9.8 å®æˆ˜ï¼šå®Œæ•´è¿ç§»å­¦ä¹ é¡¹ç›®",
    "text": "10.9 9.8 å®æˆ˜ï¼šå®Œæ•´è¿ç§»å­¦ä¹ é¡¹ç›®\n\n10.9.1 ğŸ“‹ ä»»åŠ¡ï¼šåŒ»å­¦å›¾åƒåˆ†ç±»\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms, models\nfrom PIL import Image\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport seaborn as sns\n\n# ==================== è‡ªå®šä¹‰æ•°æ®é›† ====================\n\nclass MedicalImageDataset(Dataset):\n    \"\"\"åŒ»å­¦å›¾åƒæ•°æ®é›†\"\"\"\n\n    def __init__(self, root_dir, transform=None):\n        self.root_dir = root_dir\n        self.transform = transform\n        self.classes = os.listdir(root_dir)\n        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n\n        # æ”¶é›†æ‰€æœ‰å›¾åƒè·¯å¾„\n        self.images = []\n        self.labels = []\n\n        for class_name in self.classes:\n            class_dir = os.path.join(root_dir, class_name)\n            for img_name in os.listdir(class_dir):\n                img_path = os.path.join(class_dir, img_name)\n                self.images.append(img_path)\n                self.labels.append(self.class_to_idx[class_name])\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        img_path = self.images[idx]\n        image = Image.open(img_path).convert('RGB')\n        label = self.labels[idx]\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, label\n\n# ==================== æ•°æ®å¢å¼ºç­–ç•¥ ====================\n\ndef get_transforms(phase='train'):\n    \"\"\"è·å–æ•°æ®å˜æ¢\"\"\"\n\n    if phase == 'train':\n        return transforms.Compose([\n            transforms.Resize((256, 256)),\n            transforms.RandomCrop(224),\n            transforms.RandomHorizontalFlip(),\n            transforms.RandomVerticalFlip(),\n            transforms.RandomRotation(20),\n            transforms.ColorJitter(brightness=0.2, contrast=0.2,\n                                 saturation=0.2, hue=0.1),\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406],\n                               [0.229, 0.224, 0.225])\n        ])\n    else:\n        return transforms.Compose([\n            transforms.Resize((256, 256)),\n            transforms.CenterCrop(224),\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406],\n                               [0.229, 0.224, 0.225])\n        ])\n\n# ==================== æ¨¡å‹æ„å»º ====================\n\nclass TransferLearningModel(nn.Module):\n    \"\"\"è¿ç§»å­¦ä¹ æ¨¡å‹\"\"\"\n\n    def __init__(self, model_name='resnet50', num_classes=5,\n                 pretrained=True, freeze_backbone=False):\n        super().__init__()\n\n        # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹\n        if model_name == 'resnet50':\n            self.backbone = models.resnet50(pretrained=pretrained)\n            num_features = self.backbone.fc.in_features\n\n            # å†»ç»“éª¨å¹²ç½‘ç»œ\n            if freeze_backbone:\n                for param in self.backbone.parameters():\n                    param.requires_grad = False\n\n            # æ›¿æ¢åˆ†ç±»å™¨\n            self.backbone.fc = nn.Sequential(\n                nn.Dropout(0.5),\n                nn.Linear(num_features, 512),\n                nn.ReLU(),\n                nn.BatchNorm1d(512),\n                nn.Dropout(0.3),\n                nn.Linear(512, num_classes)\n            )\n\n        elif model_name == 'efficientnet_b3':\n            self.backbone = models.efficientnet_b3(pretrained=pretrained)\n\n            if freeze_backbone:\n                for param in self.backbone.parameters():\n                    param.requires_grad = False\n\n            num_features = self.backbone.classifier[1].in_features\n            self.backbone.classifier = nn.Sequential(\n                nn.Dropout(0.3),\n                nn.Linear(num_features, num_classes)\n            )\n\n        elif model_name == 'vit_b_16':\n            self.backbone = models.vit_b_16(pretrained=pretrained)\n\n            if freeze_backbone:\n                for param in self.backbone.parameters():\n                    param.requires_grad = False\n\n            num_features = self.backbone.heads.head.in_features\n            self.backbone.heads.head = nn.Linear(num_features, num_classes)\n\n    def forward(self, x):\n        return self.backbone(x)\n\n# ==================== è®­ç»ƒå™¨ç±» ====================\n\nclass Trainer:\n    \"\"\"è®­ç»ƒå™¨\"\"\"\n\n    def __init__(self, model, train_loader, val_loader,\n                 criterion, optimizer, scheduler, device,\n                 num_epochs=50, early_stopping_patience=10):\n        self.model = model\n        self.train_loader = train_loader\n        self.val_loader = val_loader\n        self.criterion = criterion\n        self.optimizer = optimizer\n        self.scheduler = scheduler\n        self.device = device\n        self.num_epochs = num_epochs\n        self.early_stopping_patience = early_stopping_patience\n\n        self.best_val_acc = 0.0\n        self.patience_counter = 0\n        self.history = {\n            'train_loss': [], 'train_acc': [],\n            'val_loss': [], 'val_acc': []\n        }\n\n    def train_epoch(self):\n        \"\"\"è®­ç»ƒä¸€ä¸ª epoch\"\"\"\n        self.model.train()\n        running_loss = 0.0\n        correct = 0\n        total = 0\n\n        for inputs, labels in self.train_loader:\n            inputs, labels = inputs.to(self.device), labels.to(self.device)\n\n            self.optimizer.zero_grad()\n            outputs = self.model(inputs)\n            loss = self.criterion(outputs, labels)\n            loss.backward()\n\n            # æ¢¯åº¦è£å‰ª\n            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n\n            self.optimizer.step()\n\n            running_loss += loss.item() * inputs.size(0)\n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n\n        epoch_loss = running_loss / total\n        epoch_acc = 100. * correct / total\n\n        return epoch_loss, epoch_acc\n\n    def validate(self):\n        \"\"\"éªŒè¯\"\"\"\n        self.model.eval()\n        running_loss = 0.0\n        correct = 0\n        total = 0\n\n        all_preds = []\n        all_labels = []\n\n        with torch.no_grad():\n            for inputs, labels in self.val_loader:\n                inputs, labels = inputs.to(self.device), labels.to(self.device)\n\n                outputs = self.model(inputs)\n                loss = self.criterion(outputs, labels)\n\n                running_loss += loss.item() * inputs.size(0)\n                _, predicted = outputs.max(1)\n                total += labels.size(0)\n                correct += predicted.eq(labels).sum().item()\n\n                all_preds.extend(predicted.cpu().numpy())\n                all_labels.extend(labels.cpu().numpy())\n\n        epoch_loss = running_loss / total\n        epoch_acc = 100. * correct / total\n\n        return epoch_loss, epoch_acc, all_preds, all_labels\n\n    def train(self):\n        \"\"\"å®Œæ•´è®­ç»ƒæµç¨‹\"\"\"\n        print(f\"å¼€å§‹è®­ç»ƒï¼Œè®¾å¤‡: {self.device}\")\n        print(\"=\"*70)\n\n        for epoch in range(self.num_epochs):\n            print(f'\\nEpoch {epoch+1}/{self.num_epochs}')\n            print('-' * 70)\n\n            # è®­ç»ƒ\n            train_loss, train_acc = self.train_epoch()\n\n            # éªŒè¯\n            val_loss, val_acc, _, _ = self.validate()\n\n            # è®°å½•å†å²\n            self.history['train_loss'].append(train_loss)\n            self.history['train_acc'].append(train_acc)\n            self.history['val_loss'].append(val_loss)\n            self.history['val_acc'].append(val_acc)\n\n            # æ‰“å°ç»“æœ\n            print(f'è®­ç»ƒ - Loss: {train_loss:.4f}, Acc: {train_acc:.2f}%')\n            print(f'éªŒè¯ - Loss: {val_loss:.4f}, Acc: {val_acc:.2f}%')\n\n            # å­¦ä¹ ç‡è°ƒåº¦\n            if self.scheduler:\n                self.scheduler.step(val_loss)\n                print(f'å­¦ä¹ ç‡: {self.optimizer.param_groups[0][\"lr\"]:.6f}')\n\n            # ä¿å­˜æœ€ä½³æ¨¡å‹\n            if val_acc &gt; self.best_val_acc:\n                self.best_val_acc = val_acc\n                torch.save(self.model.state_dict(), 'best_model.pth')\n                print(f'âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Val Acc: {val_acc:.2f}%)')\n                self.patience_counter = 0\n            else:\n                self.patience_counter += 1\n\n            # Early Stopping\n            if self.patience_counter &gt;= self.early_stopping_patience:\n                print(f'\\nEarly stopping triggered at epoch {epoch+1}')\n                break\n\n        print(f'\\nè®­ç»ƒå®Œæˆï¼æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {self.best_val_acc:.2f}%')\n\n        # åŠ è½½æœ€ä½³æ¨¡å‹\n        self.model.load_state_dict(torch.load('best_model.pth'))\n\n        return self.history\n\n# ==================== è¯„ä¼°å’Œå¯è§†åŒ– ====================\n\nclass Evaluator:\n    \"\"\"è¯„ä¼°å™¨\"\"\"\n\n    def __init__(self, model, test_loader, class_names, device):\n        self.model = model\n        self.test_loader = test_loader\n        self.class_names = class_names\n        self.device = device\n\n    def evaluate(self):\n        \"\"\"å®Œæ•´è¯„ä¼°\"\"\"\n        self.model.eval()\n\n        all_preds = []\n        all_labels = []\n        all_probs = []\n\n        with torch.no_grad():\n            for inputs, labels in self.test_loader:\n                inputs = inputs.to(self.device)\n\n                outputs = self.model(inputs)\n                probs = torch.softmax(outputs, dim=1)\n                _, predicted = outputs.max(1)\n\n                all_preds.extend(predicted.cpu().numpy())\n                all_labels.extend(labels.numpy())\n                all_probs.extend(probs.cpu().numpy())\n\n        all_preds = np.array(all_preds)\n        all_labels = np.array(all_labels)\n        all_probs = np.array(all_probs)\n\n        # å‡†ç¡®ç‡\n        accuracy = (all_preds == all_labels).mean()\n        print(f'\\næµ‹è¯•é›†å‡†ç¡®ç‡: {accuracy*100:.2f}%\\n')\n\n        # åˆ†ç±»æŠ¥å‘Š\n        print(\"åˆ†ç±»æŠ¥å‘Š:\")\n        print(classification_report(all_labels, all_preds,\n                                   target_names=self.class_names))\n\n        # æ··æ·†çŸ©é˜µ\n        self.plot_confusion_matrix(all_labels, all_preds)\n\n        # ROC æ›²çº¿ï¼ˆå¤šåˆ†ç±»ï¼‰\n        if len(self.class_names) &lt;= 10:\n            self.plot_roc_curves(all_labels, all_probs)\n\n        return all_preds, all_labels, all_probs\n\n    def plot_confusion_matrix(self, labels, preds):\n        \"\"\"ç»˜åˆ¶æ··æ·†çŸ©é˜µ\"\"\"\n        cm = confusion_matrix(labels, preds)\n\n        plt.figure(figsize=(10, 8))\n        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n                   xticklabels=self.class_names,\n                   yticklabels=self.class_names)\n        plt.xlabel('é¢„æµ‹æ ‡ç­¾')\n        plt.ylabel('çœŸå®æ ‡ç­¾')\n        plt.title('æ··æ·†çŸ©é˜µ')\n        plt.tight_layout()\n        plt.savefig('confusion_matrix.png', dpi=300)\n        plt.show()\n\n    def plot_roc_curves(self, labels, probs):\n        \"\"\"ç»˜åˆ¶ ROC æ›²çº¿\"\"\"\n        from sklearn.metrics import roc_curve, auc\n        from sklearn.preprocessing import label_binarize\n\n        # äºŒå€¼åŒ–æ ‡ç­¾\n        labels_bin = label_binarize(labels, classes=range(len(self.class_names)))\n\n        plt.figure(figsize=(10, 8))\n\n        for i, class_name in enumerate(self.class_names):\n            fpr, tpr, _ = roc_curve(labels_bin[:, i], probs[:, i])\n            roc_auc = auc(fpr, tpr)\n\n            plt.plot(fpr, tpr, label=f'{class_name} (AUC = {roc_auc:.2f})')\n\n        plt.plot([0, 1], [0, 1], 'k--', label='éšæœºçŒœæµ‹')\n        plt.xlabel('False Positive Rate')\n        plt.ylabel('True Positive Rate')\n        plt.title('ROC æ›²çº¿')\n        plt.legend()\n        plt.grid(True, alpha=0.3)\n        plt.tight_layout()\n        plt.savefig('roc_curves.png', dpi=300)\n        plt.show()\n\n# ==================== ä¸»ç¨‹åº ====================\n\ndef main():\n    \"\"\"ä¸»å‡½æ•°\"\"\"\n\n    # è¶…å‚æ•°\n    DATA_DIR = './data/medical_images'\n    BATCH_SIZE = 32\n    NUM_EPOCHS = 50\n    LEARNING_RATE = 0.001\n    NUM_CLASSES = 5\n    MODEL_NAME = 'resnet50'  # 'resnet50', 'efficientnet_b3', 'vit_b_16'\n\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n    # æ•°æ®åŠ è½½\n    train_dataset = MedicalImageDataset(\n        os.path.join(DATA_DIR, 'train'),\n        transform=get_transforms('train')\n    )\n    val_dataset = MedicalImageDataset(\n        os.path.join(DATA_DIR, 'val'),\n        transform=get_transforms('val')\n    )\n    test_dataset = MedicalImageDataset(\n        os.path.join(DATA_DIR, 'test'),\n        transform=get_transforms('test')\n    )\n\n    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n                             shuffle=True, num_workers=4)\n    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE,\n                           shuffle=False, num_workers=4)\n    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n                            shuffle=False, num_workers=4)\n\n    print(f\"è®­ç»ƒé›†: {len(train_dataset)} å¼ \")\n    print(f\"éªŒè¯é›†: {len(val_dataset)} å¼ \")\n    print(f\"æµ‹è¯•é›†: {len(test_dataset)} å¼ \")\n    print(f\"ç±»åˆ«: {train_dataset.classes}\")\n\n    # åˆ›å»ºæ¨¡å‹\n    model = TransferLearningModel(\n        model_name=MODEL_NAME,\n        num_classes=NUM_CLASSES,\n        pretrained=True,\n        freeze_backbone=False  # å…ˆå…¨å±€å¾®è°ƒ\n    ).to(device)\n\n    # æŸå¤±å’Œä¼˜åŒ–å™¨\n    criterion = nn.CrossEntropyLoss()\n\n    # åˆ†å±‚å­¦ä¹ ç‡\n    backbone_params = []\n    classifier_params = []\n\n    for name, param in model.named_parameters():\n        if 'fc' in name or 'classifier' in name or 'head' in name:\n            classifier_params.append(param)\n        else:\n            backbone_params.append(param)\n\n    optimizer = optim.Adam([\n        {'params': backbone_params, 'lr': LEARNING_RATE * 0.1},\n        {'params': classifier_params, 'lr': LEARNING_RATE}\n    ], weight_decay=1e-4)\n\n    # å­¦ä¹ ç‡è°ƒåº¦\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, mode='min', patience=5, factor=0.5, verbose=True\n    )\n\n    # è®­ç»ƒ\n    trainer = Trainer(\n        model=model,\n        train_loader=train_loader,\n        val_loader=val_loader,\n        criterion=criterion,\n        optimizer=optimizer,\n        scheduler=scheduler,\n        device=device,\n        num_epochs=NUM_EPOCHS,\n        early_stopping_patience=10\n    )\n\n    history = trainer.train()\n\n    # å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹\n    plot_training_history(history)\n\n    # è¯„ä¼°\n    evaluator = Evaluator(\n        model=model,\n        test_loader=test_loader,\n        class_names=train_dataset.classes,\n        device=device\n    )\n\n    preds, labels, probs = evaluator.evaluate()\n\n    # Grad-CAM å¯è§†åŒ–\n    visualize_gradcam(model, test_loader, device, num_images=8)\n\n    return model, history\n\n# ==================== å¯è§†åŒ–å‡½æ•° ====================\n\ndef plot_training_history(history):\n    \"\"\"ç»˜åˆ¶è®­ç»ƒå†å²\"\"\"\n    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n\n    epochs = range(1, len(history['train_loss']) + 1)\n\n    # Loss\n    axes[0].plot(epochs, history['train_loss'], 'b-', label='è®­ç»ƒ Loss', linewidth=2)\n    axes[0].plot(epochs, history['val_loss'], 'r-', label='éªŒè¯ Loss', linewidth=2)\n    axes[0].set_xlabel('Epoch', fontsize=12)\n    axes[0].set_ylabel('Loss', fontsize=12)\n    axes[0].set_title('è®­ç»ƒå’ŒéªŒè¯ Loss', fontsize=14)\n    axes[0].legend(fontsize=11)\n    axes[0].grid(True, alpha=0.3)\n\n    # Accuracy\n    axes[1].plot(epochs, history['train_acc'], 'b-', label='è®­ç»ƒå‡†ç¡®ç‡', linewidth=2)\n    axes[1].plot(epochs, history['val_acc'], 'r-', label='éªŒè¯å‡†ç¡®ç‡', linewidth=2)\n    axes[1].set_xlabel('Epoch', fontsize=12)\n    axes[1].set_ylabel('å‡†ç¡®ç‡ (%)', fontsize=12)\n    axes[1].set_title('è®­ç»ƒå’ŒéªŒè¯å‡†ç¡®ç‡', fontsize=14)\n    axes[1].legend(fontsize=11)\n    axes[1].grid(True, alpha=0.3)\n\n    plt.tight_layout()\n    plt.savefig('training_history.png', dpi=300)\n    plt.show()\n\n# ==================== Grad-CAM å¯è§†åŒ– ====================\n\nclass GradCAM:\n    \"\"\"Grad-CAM ç±»æ¿€æ´»æ˜ å°„\"\"\"\n\n    def __init__(self, model, target_layer):\n        self.model = model\n        self.target_layer = target_layer\n        self.gradients = None\n        self.activations = None\n\n        # æ³¨å†Œé’©å­\n        target_layer.register_forward_hook(self.save_activation)\n        target_layer.register_backward_hook(self.save_gradient)\n\n    def save_activation(self, module, input, output):\n        self.activations = output.detach()\n\n    def save_gradient(self, module, grad_input, grad_output):\n        self.gradients = grad_output[0].detach()\n\n    def generate_cam(self, input_image, target_class=None):\n        \"\"\"ç”Ÿæˆ CAM\"\"\"\n        # å‰å‘ä¼ æ’­\n        output = self.model(input_image)\n\n        if target_class is None:\n            target_class = output.argmax(dim=1)\n\n        # åå‘ä¼ æ’­\n        self.model.zero_grad()\n        class_score = output[:, target_class]\n        class_score.backward()\n\n        # è®¡ç®—æƒé‡\n        pooled_gradients = torch.mean(self.gradients, dim=[2, 3])\n\n        # åŠ æƒæ±‚å’Œ\n        for i in range(self.activations.size(1)):\n            self.activations[:, i, :, :] *= pooled_gradients[:, i].view(-1, 1, 1)\n\n        # CAM\n        cam = torch.mean(self.activations, dim=1).squeeze()\n        cam = F.relu(cam)\n        cam = cam - cam.min()\n        cam = cam / cam.max()\n\n        return cam.cpu().numpy()\n\ndef visualize_gradcam(model, test_loader, device, num_images=8):\n    \"\"\"å¯è§†åŒ– Grad-CAM\"\"\"\n\n    # è·å–ç›®æ ‡å±‚\n    if hasattr(model.backbone, 'layer4'):\n        target_layer = model.backbone.layer4[-1]\n    elif hasattr(model.backbone, 'features'):\n        target_layer = model.backbone.features[-1]\n    else:\n        print(\"æ— æ³•æ‰¾åˆ°ç›®æ ‡å±‚ï¼Œè·³è¿‡ Grad-CAM\")\n        return\n\n    gradcam = GradCAM(model, target_layer)\n\n    model.eval()\n\n    fig, axes = plt.subplots(4, 4, figsize=(16, 16))\n    axes = axes.flatten()\n\n    images_shown = 0\n\n    for images, labels in test_loader:\n        if images_shown &gt;= num_images:\n            break\n\n        for i in range(min(len(images), num_images - images_shown)):\n            image = images[i:i+1].to(device)\n            label = labels[i].item()\n\n            # ç”Ÿæˆ CAM\n            cam = gradcam.generate_cam(image)\n\n            # åå½’ä¸€åŒ–å›¾åƒ\n            img = images[i].cpu().numpy().transpose(1, 2, 0)\n            mean = np.array([0.485, 0.456, 0.406])\n            std = np.array([0.229, 0.224, 0.225])\n            img = std * img + mean\n            img = np.clip(img, 0, 1)\n\n            # è°ƒæ•´ CAM å¤§å°\n            import cv2\n            cam_resized = cv2.resize(cam, (224, 224))\n\n            # å åŠ æ˜¾ç¤º\n            ax = axes[images_shown * 2]\n            ax.imshow(img)\n            ax.set_title(f'åŸå›¾ (æ ‡ç­¾: {label})')\n            ax.axis('off')\n\n            ax = axes[images_shown * 2 + 1]\n            ax.imshow(img)\n            ax.imshow(cam_resized, cmap='jet', alpha=0.5)\n            ax.set_title('Grad-CAM')\n            ax.axis('off')\n\n            images_shown += 1\n\n            if images_shown &gt;= num_images:\n                break\n\n    plt.tight_layout()\n    plt.savefig('gradcam_visualization.png', dpi=300)\n    plt.show()\n\n# ==================== è¿è¡Œ ====================\n\nif __name__ == '__main__':\n    model, history = main()",
    "crumbs": [
      "ç¬¬ä¸‰éƒ¨åˆ†ï¼šå®è·µä¸åº”ç”¨",
      "<span class='chapter-number'>10</span>Â  <span class='chapter-title'>ç¬¬ä¹ç« ï¼šè¿ç§»å­¦ä¹ ä¸å¾®è°ƒ (Transfer Learning & Fine-tuning)</span>"
    ]
  },
  {
    "objectID": "Chapter9.html#è¿ç§»å­¦ä¹ æœ€ä½³å®è·µ",
    "href": "Chapter9.html#è¿ç§»å­¦ä¹ æœ€ä½³å®è·µ",
    "title": "10Â  ç¬¬ä¹ç« ï¼šè¿ç§»å­¦ä¹ ä¸å¾®è°ƒ (Transfer Learning & Fine-tuning)",
    "section": "10.10 9.9 è¿ç§»å­¦ä¹ æœ€ä½³å®è·µ",
    "text": "10.10 9.9 è¿ç§»å­¦ä¹ æœ€ä½³å®è·µ\n\n10.10.1 âœ… æ•°æ®é›†å¤§å°ç­–ç•¥\ndef choose_strategy(dataset_size, similarity_to_pretrain):\n    \"\"\"\n    æ ¹æ®æ•°æ®é›†å¤§å°å’Œç›¸ä¼¼åº¦é€‰æ‹©ç­–ç•¥\n\n    å‚æ•°ï¼š\n        dataset_size: æ•°æ®é›†å¤§å°\n        similarity_to_pretrain: ä¸é¢„è®­ç»ƒæ•°æ®çš„ç›¸ä¼¼åº¦\n    \"\"\"\n\n    if dataset_size &lt; 1000:\n        if similarity_to_pretrain == 'high':\n            return \"ç‰¹å¾æå–ï¼ˆå†»ç»“éª¨å¹²ç½‘ç»œï¼‰\"\n        else:\n            return \"æ•°æ®å¢å¼º + è½»å¾®å¾®è°ƒï¼ˆå°å­¦ä¹ ç‡ï¼‰\"\n\n    elif 1000 &lt;= dataset_size &lt; 10000:\n        if similarity_to_pretrain == 'high':\n            return \"å¾®è°ƒé¡¶å±‚ï¼ˆè§£å†»åå‡ å±‚ï¼‰\"\n        else:\n            return \"å…¨å±€å¾®è°ƒï¼ˆå°å­¦ä¹ ç‡ + æ•°æ®å¢å¼ºï¼‰\"\n\n    else:  # &gt; 10000\n        if similarity_to_pretrain == 'high':\n            return \"å…¨å±€å¾®è°ƒ\"\n        else:\n            return \"å…¨å±€å¾®è°ƒ æˆ– ä»å¤´è®­ç»ƒ\"\n\n\n10.10.2 ğŸ“Š å­¦ä¹ ç‡ç­–ç•¥\n# ç­–ç•¥1ï¼šåˆ¤åˆ«å¼å­¦ä¹ ç‡\noptimizer = optim.Adam([\n    {'params': model.layer1.parameters(), 'lr': 1e-5},\n    {'params': model.layer2.parameters(), 'lr': 1e-4},\n    {'params': model.layer3.parameters(), 'lr': 1e-3},\n    {'params': model.fc.parameters(), 'lr': 1e-2}\n])\n\n# ç­–ç•¥2ï¼šæ¸è¿›å¼è§£å†»\ndef progressive_unfreezing(model, epoch, unfreeze_schedule):\n    \"\"\"\n    æ¸è¿›å¼è§£å†»\n\n    unfreeze_schedule: {epoch: [layer_names]}\n    \"\"\"\n    if epoch in unfreeze_schedule:\n        for layer_name in unfreeze_schedule[epoch]:\n            layer = getattr(model, layer_name)\n            for param in layer.parameters():\n                param.requires_grad = True\n        print(f\"Epoch {epoch}: è§£å†» {unfreeze_schedule[epoch]}\")\n\n# ä½¿ç”¨\nunfreeze_schedule = {\n    0: ['fc'],           # epoch 0: åªè®­ç»ƒåˆ†ç±»å™¨\n    5: ['layer4'],       # epoch 5: è§£å†» layer4\n    10: ['layer3'],      # epoch 10: è§£å†» layer3\n    15: ['layer2'],      # epoch 15: è§£å†» layer2\n}\n\n\n10.10.3 ğŸ¯ æ•°æ®å¢å¼ºæŠ€å·§\n# é«˜çº§æ•°æ®å¢å¼º\nfrom torchvision.transforms import autoaugment, v2\n\nadvanced_transforms = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.RandomCrop(224),\n\n    # AutoAugment\n    autoaugment.AutoAugment(\n        autoaugment.AutoAugmentPolicy.IMAGENET\n    ),\n\n    # RandAugment\n    # autoaugment.RandAugment(),\n\n    # Mixup / CutMix (éœ€è¦ç‰¹æ®Šå¤„ç†)\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406],\n                        [0.229, 0.224, 0.225])\n])\n\n# Mixup\nclass MixupDataset(Dataset):\n    def __init__(self, dataset, alpha=0.2):\n        self.dataset = dataset\n        self.alpha = alpha\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, idx):\n        img1, label1 = self.dataset[idx]\n\n        # éšæœºé€‰æ‹©å¦ä¸€ä¸ªæ ·æœ¬\n        idx2 = np.random.randint(0, len(self.dataset))\n        img2, label2 = self.dataset[idx2]\n\n        # Mixup\n        lam = np.random.beta(self.alpha, self.alpha)\n        mixed_img = lam * img1 + (1 - lam) * img2\n\n        return mixed_img, (label1, label2, lam)",
    "crumbs": [
      "ç¬¬ä¸‰éƒ¨åˆ†ï¼šå®è·µä¸åº”ç”¨",
      "<span class='chapter-number'>10</span>Â  <span class='chapter-title'>ç¬¬ä¹ç« ï¼šè¿ç§»å­¦ä¹ ä¸å¾®è°ƒ (Transfer Learning & Fine-tuning)</span>"
    ]
  },
  {
    "objectID": "Chapter9.html#æœ¬ç« ä½œä¸š",
    "href": "Chapter9.html#æœ¬ç« ä½œä¸š",
    "title": "10Â  ç¬¬ä¹ç« ï¼šè¿ç§»å­¦ä¹ ä¸å¾®è°ƒ (Transfer Learning & Fine-tuning)",
    "section": "10.11 ğŸ“ æœ¬ç« ä½œä¸š",
    "text": "10.11 ğŸ“ æœ¬ç« ä½œä¸š\n\n10.11.1 ä½œä¸š 1ï¼šå¯¹æ¯”å®éªŒ\n# åœ¨åŒä¸€ä¸ªæ•°æ®é›†ä¸Šå¯¹æ¯”ï¼š\n# 1. ä»é›¶è®­ç»ƒ\n# 2. ç‰¹å¾æå–\n# 3. å¾®è°ƒï¼ˆå†»ç»“éƒ¨åˆ†å±‚ï¼‰\n# 4. å¾®è°ƒï¼ˆå…¨å±€ï¼‰\n# 5. çŸ¥è¯†è’¸é¦\n\n# è®°å½•ï¼š\n#   - è®­ç»ƒæ—¶é—´\n#   - æœ€ç»ˆå‡†ç¡®ç‡\n#   - å‚æ•°é‡\n#   - æ”¶æ•›æ›²çº¿\n\n# åˆ†æï¼šå“ªç§ç­–ç•¥æœ€é€‚åˆä½ çš„æ•°æ®é›†ï¼Ÿ\n\n\n10.11.2 ä½œä¸š 2ï¼šåŒ»å­¦å›¾åƒåˆ†ç±»\n# ä½¿ç”¨çœŸå®åŒ»å­¦å›¾åƒæ•°æ®é›†ï¼ˆå¦‚ Chest X-Rayï¼‰\n# è¦æ±‚ï¼š\n# 1. EDA å’Œæ•°æ®é¢„å¤„ç†\n# 2. å°è¯•è‡³å°‘ 3 ç§é¢„è®­ç»ƒæ¨¡å‹\n# 3. å®ç°æ•°æ®å¢å¼º\n# 4. ä½¿ç”¨ Grad-CAM å¯è§†åŒ–\n# 5. è¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼ˆå‡†ç¡®ç‡ã€F1ã€AUCï¼‰\n# 6. åˆ†æé”™è¯¯æ¡ˆä¾‹\n# 7. ç¼–å†™å®Œæ•´æŠ¥å‘Š\n\n\n10.11.3 ä½œä¸š 3ï¼šå°‘æ ·æœ¬å­¦ä¹ \n# å®ç°å°‘æ ·æœ¬å­¦ä¹ \n# ä»»åŠ¡ï¼š5-way 1-shot / 5-shot åˆ†ç±»\n# æ–¹æ³•ï¼š\n# 1. åŸå‹ç½‘ç»œ\n# 2. MAML\n# 3. å¯¹æ¯”å­¦ä¹ ï¼ˆSimCLRï¼‰\n\n# åœ¨ Omniglot æˆ– Mini-ImageNet ä¸Šæµ‹è¯•\n\n\n10.11.4 ä½œä¸š 4ï¼šé¢†åŸŸè‡ªé€‚åº”\n# å®ç°é¢†åŸŸè‡ªé€‚åº”\n# æºåŸŸï¼šMNIST\n# ç›®æ ‡åŸŸï¼šSVHN æˆ– USPS\n\n# æ–¹æ³•ï¼š\n# 1. é¢†åŸŸå¯¹æŠ—è®­ç»ƒ\n# 2. è‡ªè®­ç»ƒ\n# 3. å¯¹æ¯”ä¸¤ç§æ–¹æ³•çš„æ•ˆæœ",
    "crumbs": [
      "ç¬¬ä¸‰éƒ¨åˆ†ï¼šå®è·µä¸åº”ç”¨",
      "<span class='chapter-number'>10</span>Â  <span class='chapter-title'>ç¬¬ä¹ç« ï¼šè¿ç§»å­¦ä¹ ä¸å¾®è°ƒ (Transfer Learning & Fine-tuning)</span>"
    ]
  },
  {
    "objectID": "Chapter9.html#æœ¬ç« å…³é”®æ¦‚å¿µ",
    "href": "Chapter9.html#æœ¬ç« å…³é”®æ¦‚å¿µ",
    "title": "10Â  ç¬¬ä¹ç« ï¼šè¿ç§»å­¦ä¹ ä¸å¾®è°ƒ (Transfer Learning & Fine-tuning)",
    "section": "10.12 ğŸ”‘ æœ¬ç« å…³é”®æ¦‚å¿µ",
    "text": "10.12 ğŸ”‘ æœ¬ç« å…³é”®æ¦‚å¿µ\n\n\n\næ¦‚å¿µ\nè¯´æ˜\n\n\n\n\nè¿ç§»å­¦ä¹ \nåˆ©ç”¨å·²æœ‰çŸ¥è¯†åŠ é€Ÿæ–°ä»»åŠ¡å­¦ä¹ \n\n\né¢„è®­ç»ƒæ¨¡å‹\nåœ¨å¤§æ•°æ®é›†ä¸Šè®­ç»ƒçš„æ¨¡å‹\n\n\nç‰¹å¾æå–\nå†»ç»“éª¨å¹²ç½‘ç»œï¼Œåªè®­ç»ƒåˆ†ç±»å™¨\n\n\nå¾®è°ƒ\nè§£å†»éƒ¨åˆ†æˆ–å…¨éƒ¨å±‚è¿›è¡Œè®­ç»ƒ\n\n\nåˆ¤åˆ«å¼å­¦ä¹ ç‡\nä¸åŒå±‚ä½¿ç”¨ä¸åŒå­¦ä¹ ç‡\n\n\né¢†åŸŸè‡ªé€‚åº”\nå¤„ç†æºåŸŸå’Œç›®æ ‡åŸŸåˆ†å¸ƒä¸åŒ\n\n\nå°‘æ ·æœ¬å­¦ä¹ \nç”¨æå°‘æ ·æœ¬å­¦ä¹ æ–°ä»»åŠ¡\n\n\nå…ƒå­¦ä¹ \nå­¦ä¹ å¦‚ä½•å­¦ä¹ \n\n\nçŸ¥è¯†è’¸é¦\nç”¨å¤§æ¨¡å‹è®­ç»ƒå°æ¨¡å‹\n\n\nMAML\næ¨¡å‹æ— å…³çš„å…ƒå­¦ä¹ \n\n\nåŸå‹ç½‘ç»œ\nåŸºäºè·ç¦»çš„å°‘æ ·æœ¬å­¦ä¹ \n\n\nGrad-CAM\nç±»æ¿€æ´»æ˜ å°„å¯è§†åŒ–",
    "crumbs": [
      "ç¬¬ä¸‰éƒ¨åˆ†ï¼šå®è·µä¸åº”ç”¨",
      "<span class='chapter-number'>10</span>Â  <span class='chapter-title'>ç¬¬ä¹ç« ï¼šè¿ç§»å­¦ä¹ ä¸å¾®è°ƒ (Transfer Learning & Fine-tuning)</span>"
    ]
  },
  {
    "objectID": "Chapter9.html#è¿›é˜¶è¯é¢˜",
    "href": "Chapter9.html#è¿›é˜¶è¯é¢˜",
    "title": "10Â  ç¬¬ä¹ç« ï¼šè¿ç§»å­¦ä¹ ä¸å¾®è°ƒ (Transfer Learning & Fine-tuning)",
    "section": "10.13 9.10 è¿›é˜¶è¯é¢˜",
    "text": "10.13 9.10 è¿›é˜¶è¯é¢˜\n\n10.13.1 ğŸ”¹ å¤šä»»åŠ¡å­¦ä¹  (Multi-Task Learning)\nclass MultiTaskModel(nn.Module):\n    \"\"\"å¤šä»»åŠ¡å­¦ä¹ æ¨¡å‹\"\"\"\n\n    def __init__(self, backbone, num_classes_list):\n        \"\"\"\n        å‚æ•°ï¼š\n            backbone: å…±äº«çš„ç‰¹å¾æå–å™¨\n            num_classes_list: æ¯ä¸ªä»»åŠ¡çš„ç±»åˆ«æ•°åˆ—è¡¨\n        \"\"\"\n        super().__init__()\n\n        self.backbone = backbone\n\n        # ä¸ºæ¯ä¸ªä»»åŠ¡åˆ›å»ºç‹¬ç«‹çš„åˆ†ç±»å™¨\n        self.task_heads = nn.ModuleList([\n            nn.Linear(backbone.output_dim, num_classes)\n            for num_classes in num_classes_list\n        ])\n\n    def forward(self, x):\n        # å…±äº«ç‰¹å¾æå–\n        features = self.backbone(x)\n\n        # å¤šä¸ªä»»åŠ¡çš„è¾“å‡º\n        outputs = [head(features) for head in self.task_heads]\n\n        return outputs\n\n# å¤šä»»åŠ¡æŸå¤±\nclass MultiTaskLoss(nn.Module):\n    \"\"\"å¤šä»»åŠ¡æŸå¤±ï¼ˆä¸ç¡®å®šæ€§åŠ æƒï¼‰\"\"\"\n\n    def __init__(self, num_tasks):\n        super().__init__()\n        # å¯å­¦ä¹ çš„ä»»åŠ¡æƒé‡ï¼ˆlog æ–¹å·®ï¼‰\n        self.log_vars = nn.Parameter(torch.zeros(num_tasks))\n\n    def forward(self, losses):\n        \"\"\"\n        å‚æ•°ï¼š\n            losses: æ¯ä¸ªä»»åŠ¡çš„æŸå¤±åˆ—è¡¨\n        \"\"\"\n        weighted_losses = []\n\n        for i, loss in enumerate(losses):\n            precision = torch.exp(-self.log_vars[i])\n            weighted_loss = precision * loss + self.log_vars[i]\n            weighted_losses.append(weighted_loss)\n\n        return sum(weighted_losses)\n\n# ä½¿ç”¨ç¤ºä¾‹\ndef train_multitask():\n    # åˆ›å»ºæ¨¡å‹\n    backbone = models.resnet50(pretrained=True)\n    backbone.fc = nn.Identity()  # ç§»é™¤æœ€åçš„ FC å±‚\n    backbone.output_dim = 2048\n\n    model = MultiTaskModel(\n        backbone=backbone,\n        num_classes_list=[10, 5, 2]  # 3ä¸ªä»»åŠ¡\n    )\n\n    # å¤šä»»åŠ¡æŸå¤±\n    criterion = MultiTaskLoss(num_tasks=3)\n\n    # è®­ç»ƒå¾ªç¯\n    for images, (labels1, labels2, labels3) in dataloader:\n        outputs = model(images)\n\n        # è®¡ç®—æ¯ä¸ªä»»åŠ¡çš„æŸå¤±\n        losses = [\n            nn.CrossEntropyLoss()(outputs[0], labels1),\n            nn.CrossEntropyLoss()(outputs[1], labels2),\n            nn.CrossEntropyLoss()(outputs[2], labels3)\n        ]\n\n        # ç»„åˆæŸå¤±\n        total_loss = criterion(losses)\n\n        # åå‘ä¼ æ’­\n        optimizer.zero_grad()\n        total_loss.backward()\n        optimizer.step()\n\n\n\n10.13.2 ğŸ”¹ æŒç»­å­¦ä¹  (Continual Learning)\nclass ElasticWeightConsolidation:\n    \"\"\"å¼¹æ€§æƒé‡å·©å›º (EWC)\"\"\"\n\n    def __init__(self, model, dataloader, lambda_ewc=1000):\n        self.model = model\n        self.lambda_ewc = lambda_ewc\n\n        # è®¡ç®— Fisher ä¿¡æ¯çŸ©é˜µ\n        self.fisher_matrix = self._compute_fisher(dataloader)\n\n        # ä¿å­˜å½“å‰å‚æ•°\n        self.optimal_params = {\n            name: param.clone().detach()\n            for name, param in model.named_parameters()\n        }\n\n    def _compute_fisher(self, dataloader):\n        \"\"\"è®¡ç®— Fisher ä¿¡æ¯çŸ©é˜µ\"\"\"\n        fisher = {}\n\n        self.model.eval()\n\n        for name, param in self.model.named_parameters():\n            fisher[name] = torch.zeros_like(param)\n\n        for inputs, labels in dataloader:\n            self.model.zero_grad()\n\n            outputs = self.model(inputs)\n            loss = nn.CrossEntropyLoss()(outputs, labels)\n            loss.backward()\n\n            for name, param in self.model.named_parameters():\n                if param.grad is not None:\n                    fisher[name] += param.grad.pow(2)\n\n        # å½’ä¸€åŒ–\n        num_samples = len(dataloader.dataset)\n        for name in fisher:\n            fisher[name] /= num_samples\n\n        return fisher\n\n    def penalty(self):\n        \"\"\"EWC æƒ©ç½šé¡¹\"\"\"\n        loss = 0\n\n        for name, param in self.model.named_parameters():\n            if name in self.fisher_matrix:\n                loss += (self.fisher_matrix[name] *\n                        (param - self.optimal_params[name]).pow(2)).sum()\n\n        return self.lambda_ewc * loss\n\n# ä½¿ç”¨\ndef train_with_ewc(model, old_task_loader, new_task_loader):\n    \"\"\"ä½¿ç”¨ EWC è®­ç»ƒæ–°ä»»åŠ¡\"\"\"\n\n    # åœ¨æ—§ä»»åŠ¡ä¸Šè®¡ç®— Fisher çŸ©é˜µ\n    ewc = ElasticWeightConsolidation(model, old_task_loader)\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n    criterion = nn.CrossEntropyLoss()\n\n    # åœ¨æ–°ä»»åŠ¡ä¸Šè®­ç»ƒ\n    for epoch in range(num_epochs):\n        for inputs, labels in new_task_loader:\n            outputs = model(inputs)\n\n            # æ–°ä»»åŠ¡æŸå¤± + EWC æƒ©ç½š\n            loss = criterion(outputs, labels) + ewc.penalty()\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n\n\n10.13.3 ğŸ”¹ å¯¹æ¯”å­¦ä¹  (Contrastive Learning)\nclass SimCLR(nn.Module):\n    \"\"\"SimCLR å¯¹æ¯”å­¦ä¹ \"\"\"\n\n    def __init__(self, base_encoder, projection_dim=128):\n        super().__init__()\n\n        # ç¼–ç å™¨\n        self.encoder = base_encoder\n\n        # æŠ•å½±å¤´\n        self.projector = nn.Sequential(\n            nn.Linear(base_encoder.output_dim, 2048),\n            nn.ReLU(),\n            nn.Linear(2048, projection_dim)\n        )\n\n    def forward(self, x):\n        # ç¼–ç \n        features = self.encoder(x)\n\n        # æŠ•å½±\n        z = self.projector(features)\n\n        # L2 å½’ä¸€åŒ–\n        z = F.normalize(z, dim=1)\n\n        return z\n\nclass NTXentLoss(nn.Module):\n    \"\"\"å½’ä¸€åŒ–æ¸©åº¦äº¤å‰ç†µæŸå¤± (NT-Xent)\"\"\"\n\n    def __init__(self, temperature=0.5):\n        super().__init__()\n        self.temperature = temperature\n\n    def forward(self, z_i, z_j):\n        \"\"\"\n        å‚æ•°ï¼š\n            z_i, z_j: ä¸¤ä¸ªå¢å¼ºè§†å›¾çš„è¡¨ç¤º (batch_size, projection_dim)\n        \"\"\"\n        batch_size = z_i.size(0)\n\n        # æ‹¼æ¥\n        z = torch.cat([z_i, z_j], dim=0)  # (2*batch_size, dim)\n\n        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ\n        sim_matrix = torch.mm(z, z.T) / self.temperature\n\n        # åˆ›å»ºæ ‡ç­¾ï¼šå¯¹è§’çº¿å¤–çš„å¯¹åº”ä½ç½®ä¸ºæ­£æ ·æœ¬\n        labels = torch.arange(batch_size).to(z.device)\n        labels = torch.cat([labels + batch_size, labels])\n\n        # æ©ç ï¼šå»æ‰è‡ªå·±å’Œè‡ªå·±çš„ç›¸ä¼¼åº¦\n        mask = torch.eye(2 * batch_size, dtype=torch.bool).to(z.device)\n        sim_matrix = sim_matrix.masked_fill(mask, -1e9)\n\n        # è®¡ç®—æŸå¤±\n        loss = nn.CrossEntropyLoss()(sim_matrix, labels)\n\n        return loss\n\n# è®­ç»ƒ SimCLR\ndef train_simclr(model, dataloader, num_epochs=100):\n    \"\"\"è®­ç»ƒ SimCLR\"\"\"\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n    criterion = NTXentLoss(temperature=0.5)\n\n    for epoch in range(num_epochs):\n        for (x_i, x_j), _ in dataloader:  # x_i, x_j æ˜¯ä¸¤ä¸ªå¢å¼ºè§†å›¾\n            # å‰å‘ä¼ æ’­\n            z_i = model(x_i)\n            z_j = model(x_j)\n\n            # è®¡ç®—æŸå¤±\n            loss = criterion(z_i, z_j)\n\n            # åå‘ä¼ æ’­\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n        if epoch % 10 == 0:\n            print(f'Epoch {epoch}: Loss = {loss.item():.4f}')\n\n    return model\n\n# ä½¿ç”¨é¢„è®­ç»ƒçš„ SimCLR è¿›è¡Œä¸‹æ¸¸ä»»åŠ¡\ndef finetune_simclr(pretrained_encoder, train_loader, num_classes):\n    \"\"\"å¾®è°ƒ SimCLR ç¼–ç å™¨\"\"\"\n\n    # å†»ç»“ç¼–ç å™¨\n    for param in pretrained_encoder.parameters():\n        param.requires_grad = False\n\n    # æ·»åŠ çº¿æ€§åˆ†ç±»å™¨\n    classifier = nn.Linear(pretrained_encoder.output_dim, num_classes)\n\n    # è®­ç»ƒåˆ†ç±»å™¨\n    optimizer = torch.optim.Adam(classifier.parameters(), lr=0.001)\n    criterion = nn.CrossEntropyLoss()\n\n    for epoch in range(num_epochs):\n        for inputs, labels in train_loader:\n            # æå–ç‰¹å¾ï¼ˆå†»ç»“ï¼‰\n            with torch.no_grad():\n                features = pretrained_encoder(inputs)\n\n            # åˆ†ç±»\n            outputs = classifier(features)\n            loss = criterion(outputs, labels)\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()",
    "crumbs": [
      "ç¬¬ä¸‰éƒ¨åˆ†ï¼šå®è·µä¸åº”ç”¨",
      "<span class='chapter-number'>10</span>Â  <span class='chapter-title'>ç¬¬ä¹ç« ï¼šè¿ç§»å­¦ä¹ ä¸å¾®è°ƒ (Transfer Learning & Fine-tuning)</span>"
    ]
  },
  {
    "objectID": "Chapter9.html#å®ç”¨å·¥å…·å’ŒæŠ€å·§",
    "href": "Chapter9.html#å®ç”¨å·¥å…·å’ŒæŠ€å·§",
    "title": "10Â  ç¬¬ä¹ç« ï¼šè¿ç§»å­¦ä¹ ä¸å¾®è°ƒ (Transfer Learning & Fine-tuning)",
    "section": "10.14 9.11 å®ç”¨å·¥å…·å’ŒæŠ€å·§",
    "text": "10.14 9.11 å®ç”¨å·¥å…·å’ŒæŠ€å·§\n\n10.14.1 ğŸ› ï¸ æ¨¡å‹è½¬æ¢å’Œéƒ¨ç½²\n# 1. ONNX å¯¼å‡º\ndef export_to_onnx(model, dummy_input, output_path):\n    \"\"\"å¯¼å‡ºä¸º ONNX æ ¼å¼\"\"\"\n    model.eval()\n\n    torch.onnx.export(\n        model,\n        dummy_input,\n        output_path,\n        export_params=True,\n        opset_version=11,\n        do_constant_folding=True,\n        input_names=['input'],\n        output_names=['output'],\n        dynamic_axes={\n            'input': {0: 'batch_size'},\n            'output': {0: 'batch_size'}\n        }\n    )\n\n    print(f\"æ¨¡å‹å·²å¯¼å‡ºåˆ° {output_path}\")\n\n# ä½¿ç”¨\ndummy_input = torch.randn(1, 3, 224, 224)\nexport_to_onnx(model, dummy_input, 'model.onnx')\n\n# 2. TorchScript è½¬æ¢\nscripted_model = torch.jit.script(model)\nscripted_model.save('model_scripted.pt')\n\n# 3. é‡åŒ–ï¼ˆå‡å°æ¨¡å‹å¤§å°ï¼‰\ndef quantize_model(model):\n    \"\"\"åŠ¨æ€é‡åŒ–\"\"\"\n    quantized_model = torch.quantization.quantize_dynamic(\n        model,\n        {nn.Linear, nn.Conv2d},\n        dtype=torch.qint8\n    )\n    return quantized_model\n\nquantized = quantize_model(model)\n\n\n\n10.14.2 ğŸ“Š æ¨¡å‹åˆ†æå·¥å…·\nfrom torchinfo import summary\nfrom fvcore.nn import FlopCountAnalysis, parameter_count\n\ndef analyze_model(model, input_size=(1, 3, 224, 224)):\n    \"\"\"åˆ†ææ¨¡å‹\"\"\"\n\n    # æ¨¡å‹æ‘˜è¦\n    print(\"=\"*70)\n    print(\"æ¨¡å‹ç»“æ„:\")\n    print(\"=\"*70)\n    summary(model, input_size=input_size)\n\n    # å‚æ•°é‡\n    total_params = sum(p.numel() for p in model.parameters())\n    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n    print(f\"\\næ€»å‚æ•°: {total_params:,}\")\n    print(f\"å¯è®­ç»ƒå‚æ•°: {trainable_params:,}\")\n    print(f\"ä¸å¯è®­ç»ƒå‚æ•°: {total_params - trainable_params:,}\")\n\n    # FLOPs\n    dummy_input = torch.randn(input_size)\n    flops = FlopCountAnalysis(model, dummy_input)\n    print(f\"\\nFLOPs: {flops.total():,}\")\n\n    # å†…å­˜å ç”¨\n    print(f\"æ¨¡å‹å¤§å°: {total_params * 4 / 1024 / 1024:.2f} MB (FP32)\")\n\n    # æ¨ç†é€Ÿåº¦æµ‹è¯•\n    model.eval()\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model.to(device)\n    dummy_input = dummy_input.to(device)\n\n    import time\n\n    # é¢„çƒ­\n    for _ in range(10):\n        _ = model(dummy_input)\n\n    # æµ‹è¯•\n    num_iterations = 100\n    torch.cuda.synchronize() if torch.cuda.is_available() else None\n    start = time.time()\n\n    with torch.no_grad():\n        for _ in range(num_iterations):\n            _ = model(dummy_input)\n\n    torch.cuda.synchronize() if torch.cuda.is_available() else None\n    end = time.time()\n\n    avg_time = (end - start) / num_iterations\n    fps = 1 / avg_time\n\n    print(f\"\\næ¨ç†æ—¶é—´: {avg_time*1000:.2f} ms\")\n    print(f\"FPS: {fps:.2f}\")\n\n# ä½¿ç”¨\nanalyze_model(model)\n\n\n\n10.14.3 ğŸ” é”™è¯¯åˆ†æå·¥å…·\nclass ErrorAnalyzer:\n    \"\"\"é”™è¯¯åˆ†æå·¥å…·\"\"\"\n\n    def __init__(self, model, dataloader, class_names, device):\n        self.model = model\n        self.dataloader = dataloader\n        self.class_names = class_names\n        self.device = device\n\n    def analyze(self):\n        \"\"\"å®Œæ•´é”™è¯¯åˆ†æ\"\"\"\n        self.model.eval()\n\n        errors = []\n\n        with torch.no_grad():\n            for inputs, labels in self.dataloader:\n                inputs, labels = inputs.to(self.device), labels.to(self.device)\n\n                outputs = self.model(inputs)\n                probs = torch.softmax(outputs, dim=1)\n                preds = outputs.argmax(dim=1)\n\n                # æ‰¾å‡ºé”™è¯¯æ ·æœ¬\n                wrong_mask = preds != labels\n\n                if wrong_mask.any():\n                    for i in torch.where(wrong_mask)[0]:\n                        errors.append({\n                            'image': inputs[i].cpu(),\n                            'true_label': labels[i].item(),\n                            'pred_label': preds[i].item(),\n                            'confidence': probs[i, preds[i]].item(),\n                            'true_prob': probs[i, labels[i]].item()\n                        })\n\n        print(f\"æ€»é”™è¯¯æ•°: {len(errors)}\")\n\n        # æŒ‰ç±»åˆ«ç»Ÿè®¡é”™è¯¯\n        self._error_by_class(errors)\n\n        # æŒ‰ç½®ä¿¡åº¦åˆ†æ\n        self._error_by_confidence(errors)\n\n        # å¯è§†åŒ–æœ€éš¾æ ·æœ¬\n        self._visualize_hard_examples(errors)\n\n        return errors\n\n    def _error_by_class(self, errors):\n        \"\"\"æŒ‰ç±»åˆ«ç»Ÿè®¡é”™è¯¯\"\"\"\n        from collections import defaultdict\n\n        error_count = defaultdict(int)\n        confusion = defaultdict(lambda: defaultdict(int))\n\n        for error in errors:\n            true_label = error['true_label']\n            pred_label = error['pred_label']\n\n            error_count[true_label] += 1\n            confusion[true_label][pred_label] += 1\n\n        print(\"\\næ¯ç±»é”™è¯¯æ•°:\")\n        for class_id, count in sorted(error_count.items()):\n            print(f\"  {self.class_names[class_id]}: {count}\")\n\n        print(\"\\næœ€å¸¸è§çš„æ··æ·†:\")\n        for true_id, pred_dict in confusion.items():\n            for pred_id, count in sorted(pred_dict.items(),\n                                        key=lambda x: x[1],\n                                        reverse=True)[:3]:\n                print(f\"  {self.class_names[true_id]} â†’ \"\n                      f\"{self.class_names[pred_id]}: {count}\")\n\n    def _error_by_confidence(self, errors):\n        \"\"\"æŒ‰ç½®ä¿¡åº¦åˆ†æ\"\"\"\n        confidences = [e['confidence'] for e in errors]\n\n        print(f\"\\né”™è¯¯é¢„æµ‹çš„ç½®ä¿¡åº¦:\")\n        print(f\"  å¹³å‡: {np.mean(confidences):.4f}\")\n        print(f\"  ä¸­ä½æ•°: {np.median(confidences):.4f}\")\n        print(f\"  æœ€å¤§: {np.max(confidences):.4f}\")\n        print(f\"  æœ€å°: {np.min(confidences):.4f}\")\n\n    def _visualize_hard_examples(self, errors, num_examples=16):\n        \"\"\"å¯è§†åŒ–æœ€éš¾çš„æ ·æœ¬\"\"\"\n        # æŒ‰ç½®ä¿¡åº¦æ’åºï¼ˆé«˜ç½®ä¿¡åº¦ä½†é”™è¯¯ï¼‰\n        sorted_errors = sorted(errors,\n                              key=lambda x: x['confidence'],\n                              reverse=True)[:num_examples]\n\n        fig, axes = plt.subplots(4, 4, figsize=(16, 16))\n        axes = axes.flatten()\n\n        for i, error in enumerate(sorted_errors):\n            if i &gt;= num_examples:\n                break\n\n            # åå½’ä¸€åŒ–\n            img = error['image'].numpy().transpose(1, 2, 0)\n            mean = np.array([0.485, 0.456, 0.406])\n            std = np.array([0.229, 0.224, 0.225])\n            img = std * img + mean\n            img = np.clip(img, 0, 1)\n\n            ax = axes[i]\n            ax.imshow(img)\n            ax.set_title(\n                f\"çœŸå®: {self.class_names[error['true_label']]}\\n\"\n                f\"é¢„æµ‹: {self.class_names[error['pred_label']]}\\n\"\n                f\"ç½®ä¿¡åº¦: {error['confidence']:.3f}\",\n                fontsize=10,\n                color='red'\n            )\n            ax.axis('off')\n\n        plt.tight_layout()\n        plt.savefig('hard_examples.png', dpi=300)\n        plt.show()\n\n# ä½¿ç”¨\nanalyzer = ErrorAnalyzer(model, test_loader, class_names, device)\nerrors = analyzer.analyze()",
    "crumbs": [
      "ç¬¬ä¸‰éƒ¨åˆ†ï¼šå®è·µä¸åº”ç”¨",
      "<span class='chapter-number'>10</span>Â  <span class='chapter-title'>ç¬¬ä¹ç« ï¼šè¿ç§»å­¦ä¹ ä¸å¾®è°ƒ (Transfer Learning & Fine-tuning)</span>"
    ]
  },
  {
    "objectID": "Chapter9.html#æ¨èèµ„æº",
    "href": "Chapter9.html#æ¨èèµ„æº",
    "title": "10Â  ç¬¬ä¹ç« ï¼šè¿ç§»å­¦ä¹ ä¸å¾®è°ƒ (Transfer Learning & Fine-tuning)",
    "section": "10.15 ğŸ“š æ¨èèµ„æº",
    "text": "10.15 ğŸ“š æ¨èèµ„æº\n\n10.15.1 ğŸ“– è®ºæ–‡\nè¿ç§»å­¦ä¹ åŸºç¡€ï¼š - â€œA Survey on Transfer Learningâ€ (Pan & Yang, 2010) - â€œHow transferable are features in deep neural networks?â€ (Yosinski et al., 2014)\né¢†åŸŸè‡ªé€‚åº”ï¼š - â€œDomain-Adversarial Training of Neural Networksâ€ (Ganin et al., 2016) - â€œUnsupervised Domain Adaptation by Backpropagationâ€ (Ganin & Lempitsky, 2015)\nå°‘æ ·æœ¬å­¦ä¹ ï¼š - â€œModel-Agnostic Meta-Learning (MAML)â€ (Finn et al., 2017) - â€œPrototypical Networks for Few-shot Learningâ€ (Snell et al., 2017) - â€œMatching Networks for One Shot Learningâ€ (Vinyals et al., 2016)\nçŸ¥è¯†è’¸é¦ï¼š - â€œDistilling the Knowledge in a Neural Networkâ€ (Hinton et al., 2015)\nå¯¹æ¯”å­¦ä¹ ï¼š - â€œA Simple Framework for Contrastive Learning (SimCLR)â€ (Chen et al., 2020) - â€œMomentum Contrast (MoCo)â€ (He et al., 2020)\n\n\n10.15.2 ğŸ”§ å·¥å…·å’Œåº“\n# Hugging Face Transformers\nfrom transformers import AutoModel, AutoTokenizer\n\n# Timm (PyTorch Image Models)\nimport timm\nmodel = timm.create_model('resnet50', pretrained=True)\n\n# PyTorch Lightning (ç®€åŒ–è®­ç»ƒ)\nimport pytorch_lightning as pl\n\n# Weights & Biases (å®éªŒè·Ÿè¸ª)\nimport wandb\n\n# TensorBoard\nfrom torch.utils.tensorboard import SummaryWriter",
    "crumbs": [
      "ç¬¬ä¸‰éƒ¨åˆ†ï¼šå®è·µä¸åº”ç”¨",
      "<span class='chapter-number'>10</span>Â  <span class='chapter-title'>ç¬¬ä¹ç« ï¼šè¿ç§»å­¦ä¹ ä¸å¾®è°ƒ (Transfer Learning & Fine-tuning)</span>"
    ]
  },
  {
    "objectID": "Chapter9.html#æ€»ç»“",
    "href": "Chapter9.html#æ€»ç»“",
    "title": "10Â  ç¬¬ä¹ç« ï¼šè¿ç§»å­¦ä¹ ä¸å¾®è°ƒ (Transfer Learning & Fine-tuning)",
    "section": "10.16 ğŸ“ æ€»ç»“",
    "text": "10.16 ğŸ“ æ€»ç»“\n\n10.16.1 âœ… è¿ç§»å­¦ä¹ ä½•æ—¶æœ‰æ•ˆï¼Ÿ\nâœ“ æºä»»åŠ¡å’Œç›®æ ‡ä»»åŠ¡ç›¸å…³\nâœ“ ç›®æ ‡ä»»åŠ¡æ•°æ®è¾ƒå°‘\nâœ“ æºä»»åŠ¡æ•°æ®é‡å¤§ä¸”è´¨é‡é«˜\nâœ“ æœ‰åˆé€‚çš„é¢„è®­ç»ƒæ¨¡å‹å¯ç”¨\n\n\n10.16.2 âŒ è¿ç§»å­¦ä¹ ä½•æ—¶æ— æ•ˆï¼Ÿ\nâœ— æºä»»åŠ¡å’Œç›®æ ‡ä»»åŠ¡å®Œå…¨ä¸ç›¸å…³\nâœ— ç›®æ ‡ä»»åŠ¡æ•°æ®å……è¶³\nâœ— é¢„è®­ç»ƒæ¨¡å‹ä¸åŒ¹é…ç›®æ ‡ä»»åŠ¡\nâœ— è®¡ç®—èµ„æºå……è¶³ï¼Œå¯ä»å¤´è®­ç»ƒ\n\n\n10.16.3 ğŸ¯ å®è·µå»ºè®®\n\nä¼˜å…ˆå°è¯•é¢„è®­ç»ƒæ¨¡å‹\næ ¹æ®æ•°æ®é‡é€‰æ‹©ç­–ç•¥ï¼ˆç‰¹å¾æå– vs å¾®è°ƒï¼‰\nä½¿ç”¨åˆ¤åˆ«å¼å­¦ä¹ ç‡\nå……åˆ†åˆ©ç”¨æ•°æ®å¢å¼º\nç›‘æ§éªŒè¯é›†é¿å…è¿‡æ‹Ÿåˆ\nå¯è§†åŒ–ç†è§£æ¨¡å‹è¡Œä¸ºï¼ˆGrad-CAMç­‰ï¼‰\nè®°å½•å®éªŒç»“æœï¼ˆWeights & Biasesï¼‰\né”™è¯¯åˆ†ææŒ‡å¯¼æ”¹è¿›",
    "crumbs": [
      "ç¬¬ä¸‰éƒ¨åˆ†ï¼šå®è·µä¸åº”ç”¨",
      "<span class='chapter-number'>10</span>Â  <span class='chapter-title'>ç¬¬ä¹ç« ï¼šè¿ç§»å­¦ä¹ ä¸å¾®è°ƒ (Transfer Learning & Fine-tuning)</span>"
    ]
  },
  {
    "objectID": "Chapter10.html",
    "href": "Chapter10.html",
    "title": "11Â  ç¬¬åç« ï¼šå¼ºåŒ–å­¦ä¹  (Reinforcement Learning)",
    "section": "",
    "text": "11.1 ğŸ“Œ ç« èŠ‚ç›®æ ‡",
    "crumbs": [
      "ç¬¬ä¸‰éƒ¨åˆ†ï¼šå®è·µä¸åº”ç”¨",
      "<span class='chapter-number'>11</span>Â  <span class='chapter-title'>ç¬¬åç« ï¼šå¼ºåŒ–å­¦ä¹  (Reinforcement Learning)</span>"
    ]
  },
  {
    "objectID": "Chapter10.html#ç« èŠ‚ç›®æ ‡",
    "href": "Chapter10.html#ç« èŠ‚ç›®æ ‡",
    "title": "11Â  ç¬¬åç« ï¼šå¼ºåŒ–å­¦ä¹  (Reinforcement Learning)",
    "section": "",
    "text": "ç†è§£å¼ºåŒ–å­¦ä¹ çš„åŸºæœ¬æ¦‚å¿µå’Œæ¡†æ¶\næŒæ¡ Q-Learning å’Œ DQN ç®—æ³•\nå­¦ä¹ ç­–ç•¥æ¢¯åº¦æ–¹æ³•\näº†è§£ Actor-Critic æ¶æ„\nå®æˆ˜ï¼šè®­ç»ƒ Agent ç©æ¸¸æˆ",
    "crumbs": [
      "ç¬¬ä¸‰éƒ¨åˆ†ï¼šå®è·µä¸åº”ç”¨",
      "<span class='chapter-number'>11</span>Â  <span class='chapter-title'>ç¬¬åç« ï¼šå¼ºåŒ–å­¦ä¹  (Reinforcement Learning)</span>"
    ]
  },
  {
    "objectID": "Chapter10.html#å¼ºåŒ–å­¦ä¹ åŸºç¡€",
    "href": "Chapter10.html#å¼ºåŒ–å­¦ä¹ åŸºç¡€",
    "title": "11Â  ç¬¬åç« ï¼šå¼ºåŒ–å­¦ä¹  (Reinforcement Learning)",
    "section": "11.2 10.1 å¼ºåŒ–å­¦ä¹ åŸºç¡€",
    "text": "11.2 10.1 å¼ºåŒ–å­¦ä¹ åŸºç¡€\n\n11.2.1 ğŸ¯ ä»€ä¹ˆæ˜¯å¼ºåŒ–å­¦ä¹ ï¼Ÿ\nå®šä¹‰ï¼šæ™ºèƒ½ä½“ï¼ˆAgentï¼‰é€šè¿‡ä¸ç¯å¢ƒï¼ˆEnvironmentï¼‰äº¤äº’ï¼Œå­¦ä¹ æœ€ä¼˜ç­–ç•¥ä»¥æœ€å¤§åŒ–ç´¯ç§¯å¥–åŠ±ã€‚\nä¸ç›‘ç£å­¦ä¹ çš„åŒºåˆ«ï¼š\nç›‘ç£å­¦ä¹ ï¼š\n  è¾“å…¥ â†’ æ¨¡å‹ â†’ è¾“å‡º\n  æœ‰æ˜ç¡®çš„æ ‡ç­¾æŒ‡å¯¼\n\nå¼ºåŒ–å­¦ä¹ ï¼š\n  çŠ¶æ€ â†’ åŠ¨ä½œ â†’ å¥–åŠ± + æ–°çŠ¶æ€\n  æ²¡æœ‰æ˜ç¡®æ ‡ç­¾ï¼Œåªæœ‰å¥–åŠ±ä¿¡å·\n  éœ€è¦æ¢ç´¢å’Œè¯•é”™\n\n\n11.2.2 ğŸ“ æ ¸å¿ƒæ¦‚å¿µ\n\n11.2.2.1 é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ (MDP)\nMDP = (S, A, P, R, Î³)\n\nS: çŠ¶æ€ç©ºé—´ (States)\nA: åŠ¨ä½œç©ºé—´ (Actions)\nP: çŠ¶æ€è½¬ç§»æ¦‚ç‡ P(s'|s,a)\nR: å¥–åŠ±å‡½æ•° R(s,a,s')\nÎ³: æŠ˜æ‰£å› å­ (0 â‰¤ Î³ &lt; 1)\nç¤ºä¾‹ï¼šè¿·å®«æ¸¸æˆ\nâ”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”\nâ”‚ S â”‚   â”‚   â”‚   â”‚  S = èµ·ç‚¹\nâ”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤  G = ç»ˆç‚¹\nâ”‚   â”‚ â–“ â”‚   â”‚   â”‚  â–“ = éšœç¢\nâ”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤\nâ”‚   â”‚   â”‚ â–“ â”‚   â”‚  åŠ¨ä½œ: â†‘â†“â†â†’\nâ”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤\nâ”‚   â”‚   â”‚   â”‚ G â”‚  å¥–åŠ±: åˆ°è¾¾G=+10, ç¢°å£=-1\nâ””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜\n\n\n\n11.2.2.2 å…³é”®æœ¯è¯­\n1. ç­–ç•¥ (Policy) Ï€\nÏ€(a|s): åœ¨çŠ¶æ€ s ä¸‹é€‰æ‹©åŠ¨ä½œ a çš„æ¦‚ç‡\n\nç¡®å®šæ€§ç­–ç•¥: a = Ï€(s)\néšæœºç­–ç•¥: a ~ Ï€(Â·|s)\n2. ä»·å€¼å‡½æ•° (Value Function)\nçŠ¶æ€ä»·å€¼å‡½æ•° V^Ï€(s):\n  V^Ï€(s) = E[âˆ‘(t=0 to âˆ) Î³^tÂ·r_t | s_0=s, Ï€]\n\n  è¡¨ç¤ºï¼šä»çŠ¶æ€ s å¼€å§‹ï¼Œéµå¾ªç­–ç•¥ Ï€ çš„æœŸæœ›ç´¯ç§¯å¥–åŠ±\n\nåŠ¨ä½œä»·å€¼å‡½æ•° Q^Ï€(s,a):\n  Q^Ï€(s,a) = E[âˆ‘(t=0 to âˆ) Î³^tÂ·r_t | s_0=s, a_0=a, Ï€]\n\n  è¡¨ç¤ºï¼šåœ¨çŠ¶æ€ s æ‰§è¡ŒåŠ¨ä½œ a åï¼Œéµå¾ªç­–ç•¥ Ï€ çš„æœŸæœ›ç´¯ç§¯å¥–åŠ±\n3. æœ€ä¼˜ç­–ç•¥\nÏ€* = argmax_Ï€ V^Ï€(s)  å¯¹æ‰€æœ‰ s\n\næœ€ä¼˜ä»·å€¼å‡½æ•°:\n  V*(s) = max_Ï€ V^Ï€(s)\n  Q*(s,a) = max_Ï€ Q^Ï€(s,a)\n\nè´å°”æ›¼æœ€ä¼˜æ–¹ç¨‹:\n  V*(s) = max_a [R(s,a) + Î³Â·âˆ‘_s' P(s'|s,a)Â·V*(s')]\n  Q*(s,a) = R(s,a) + Î³Â·âˆ‘_s' P(s'|s,a)Â·max_a' Q*(s',a')\n\n\n\n11.2.2.3 æ¢ç´¢ vs åˆ©ç”¨ (Exploration vs Exploitation)\nåˆ©ç”¨ (Exploitation):\n  é€‰æ‹©å½“å‰å·²çŸ¥æœ€å¥½çš„åŠ¨ä½œ\n\næ¢ç´¢ (Exploration):\n  å°è¯•æ–°åŠ¨ä½œï¼Œå‘ç°æ›´å¥½çš„ç­–ç•¥\n\nå¹³è¡¡ç­–ç•¥:\n  Îµ-greedy: ä»¥æ¦‚ç‡ Îµ éšæœºæ¢ç´¢ï¼Œå¦åˆ™åˆ©ç”¨\n  Softmax: æ ¹æ® Q å€¼åˆ†å¸ƒé‡‡æ ·\n  UCB (Upper Confidence Bound)",
    "crumbs": [
      "ç¬¬ä¸‰éƒ¨åˆ†ï¼šå®è·µä¸åº”ç”¨",
      "<span class='chapter-number'>11</span>Â  <span class='chapter-title'>ç¬¬åç« ï¼šå¼ºåŒ–å­¦ä¹  (Reinforcement Learning)</span>"
    ]
  },
  {
    "objectID": "Chapter10.html#q-learning",
    "href": "Chapter10.html#q-learning",
    "title": "11Â  ç¬¬åç« ï¼šå¼ºåŒ–å­¦ä¹  (Reinforcement Learning)",
    "section": "11.3 10.2 Q-Learning",
    "text": "11.3 10.2 Q-Learning\n\n11.3.1 ğŸ“ ç®—æ³•åŸç†\næ ¸å¿ƒæ€æƒ³ï¼šå­¦ä¹ åŠ¨ä½œä»·å€¼å‡½æ•° Q(s,a)\næ›´æ–°è§„åˆ™ï¼š\nQ(s,a) â† Q(s,a) + Î±Â·[r + Î³Â·max_a' Q(s',a') - Q(s,a)]\n\nå…¶ä¸­ï¼š\n  Î±: å­¦ä¹ ç‡\n  r: å³æ—¶å¥–åŠ±\n  Î³: æŠ˜æ‰£å› å­\n  s': ä¸‹ä¸€çŠ¶æ€\n\næ—¶åºå·®åˆ†è¯¯å·® (TD Error):\n  Î´ = r + Î³Â·max_a' Q(s',a') - Q(s,a)\n\n\n11.3.2 ğŸ’» ä»é›¶å®ç° Q-Learning\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom collections import defaultdict\n\nclass QLearningAgent:\n    \"\"\"Q-Learning æ™ºèƒ½ä½“\"\"\"\n\n    def __init__(self, n_states, n_actions,\n                 learning_rate=0.1, discount_factor=0.95,\n                 epsilon=0.1, epsilon_decay=0.995, epsilon_min=0.01):\n        self.n_states = n_states\n        self.n_actions = n_actions\n        self.lr = learning_rate\n        self.gamma = discount_factor\n        self.epsilon = epsilon\n        self.epsilon_decay = epsilon_decay\n        self.epsilon_min = epsilon_min\n\n        # Q è¡¨ï¼šå­—å…¸å½¢å¼\n        self.q_table = defaultdict(lambda: np.zeros(n_actions))\n\n    def get_action(self, state, training=True):\n        \"\"\"\n        é€‰æ‹©åŠ¨ä½œï¼ˆÎµ-greedy ç­–ç•¥ï¼‰\n\n        å‚æ•°:\n            state: å½“å‰çŠ¶æ€\n            training: æ˜¯å¦è®­ç»ƒæ¨¡å¼\n        \"\"\"\n        if training and np.random.random() &lt; self.epsilon:\n            # æ¢ç´¢ï¼šéšæœºåŠ¨ä½œ\n            return np.random.randint(self.n_actions)\n        else:\n            # åˆ©ç”¨ï¼šé€‰æ‹©æœ€ä¼˜åŠ¨ä½œ\n            return np.argmax(self.q_table[state])\n\n    def update(self, state, action, reward, next_state, done):\n        \"\"\"\n        æ›´æ–° Q å€¼\n\n        å‚æ•°:\n            state: å½“å‰çŠ¶æ€\n            action: æ‰§è¡Œçš„åŠ¨ä½œ\n            reward: è·å¾—çš„å¥–åŠ±\n            next_state: ä¸‹ä¸€çŠ¶æ€\n            done: æ˜¯å¦ç»ˆæ­¢\n        \"\"\"\n        # å½“å‰ Q å€¼\n        current_q = self.q_table[state][action]\n\n        # ç›®æ ‡ Q å€¼\n        if done:\n            target_q = reward\n        else:\n            target_q = reward + self.gamma * np.max(self.q_table[next_state])\n\n        # æ›´æ–° Q å€¼\n        self.q_table[state][action] += self.lr * (target_q - current_q)\n\n        # è¡°å‡ epsilon\n        if self.epsilon &gt; self.epsilon_min:\n            self.epsilon *= self.epsilon_decay\n\n    def get_q_values(self, state):\n        \"\"\"è·å–çŠ¶æ€çš„æ‰€æœ‰ Q å€¼\"\"\"\n        return self.q_table[state]\n\n# ==================== ç¯å¢ƒï¼šç½‘æ ¼ä¸–ç•Œ ====================\n\nclass GridWorld:\n    \"\"\"ç®€å•çš„ç½‘æ ¼ä¸–ç•Œç¯å¢ƒ\"\"\"\n\n    def __init__(self, size=5):\n        self.size = size\n        self.n_states = size * size\n        self.n_actions = 4  # ä¸Šä¸‹å·¦å³\n\n        # èµ·ç‚¹å’Œç»ˆç‚¹\n        self.start_pos = (0, 0)\n        self.goal_pos = (size-1, size-1)\n\n        # éšœç¢ç‰©\n        self.obstacles = [(1, 1), (2, 2), (3, 1)]\n\n        # å½“å‰ä½ç½®\n        self.current_pos = self.start_pos\n\n    def reset(self):\n        \"\"\"é‡ç½®ç¯å¢ƒ\"\"\"\n        self.current_pos = self.start_pos\n        return self._pos_to_state(self.current_pos)\n\n    def _pos_to_state(self, pos):\n        \"\"\"ä½ç½®è½¬çŠ¶æ€ç¼–å·\"\"\"\n        return pos[0] * self.size + pos[1]\n\n    def _state_to_pos(self, state):\n        \"\"\"çŠ¶æ€ç¼–å·è½¬ä½ç½®\"\"\"\n        return (state // self.size, state % self.size)\n\n    def step(self, action):\n        \"\"\"\n        æ‰§è¡ŒåŠ¨ä½œ\n\n        åŠ¨ä½œç¼–ç : 0=ä¸Š, 1=ä¸‹, 2=å·¦, 3=å³\n\n        è¿”å›: (next_state, reward, done, info)\n        \"\"\"\n        row, col = self.current_pos\n\n        # ç§»åŠ¨\n        if action == 0:  # ä¸Š\n            row = max(0, row - 1)\n        elif action == 1:  # ä¸‹\n            row = min(self.size - 1, row + 1)\n        elif action == 2:  # å·¦\n            col = max(0, col - 1)\n        elif action == 3:  # å³\n            col = min(self.size - 1, col + 1)\n\n        next_pos = (row, col)\n\n        # æ£€æŸ¥éšœç¢ç‰©\n        if next_pos in self.obstacles:\n            next_pos = self.current_pos  # ç¢°å£ä¸ç§»åŠ¨\n            reward = -1\n        elif next_pos == self.goal_pos:\n            reward = 10  # åˆ°è¾¾ç»ˆç‚¹\n        else:\n            reward = -0.1  # æ¯æ­¥å°æƒ©ç½š\n\n        self.current_pos = next_pos\n        next_state = self._pos_to_state(next_pos)\n        done = (next_pos == self.goal_pos)\n\n        return next_state, reward, done, {}\n\n    def render(self):\n        \"\"\"å¯è§†åŒ–ç¯å¢ƒ\"\"\"\n        grid = np.zeros((self.size, self.size))\n\n        # æ ‡è®°éšœç¢ç‰©\n        for obs in self.obstacles:\n            grid[obs] = -1\n\n        # æ ‡è®°ç»ˆç‚¹\n        grid[self.goal_pos] = 2\n\n        # æ ‡è®°å½“å‰ä½ç½®\n        grid[self.current_pos] = 1\n\n        return grid\n\n# ==================== è®­ç»ƒ Q-Learning ====================\n\ndef train_qlearning(env, agent, num_episodes=1000):\n    \"\"\"è®­ç»ƒ Q-Learning Agent\"\"\"\n\n    rewards_history = []\n    epsilon_history = []\n\n    for episode in range(num_episodes):\n        state = env.reset()\n        total_reward = 0\n        done = False\n        steps = 0\n\n        while not done and steps &lt; 100:\n            # é€‰æ‹©åŠ¨ä½œ\n            action = agent.get_action(state, training=True)\n\n            # æ‰§è¡ŒåŠ¨ä½œ\n            next_state, reward, done, _ = env.step(action)\n\n            # æ›´æ–° Q å€¼\n            agent.update(state, action, reward, next_state, done)\n\n            state = next_state\n            total_reward += reward\n            steps += 1\n\n        rewards_history.append(total_reward)\n        epsilon_history.append(agent.epsilon)\n\n        if (episode + 1) % 100 == 0:\n            avg_reward = np.mean(rewards_history[-100:])\n            print(f'Episode {episode+1}: Avg Reward = {avg_reward:.2f}, '\n                  f'Epsilon = {agent.epsilon:.3f}')\n\n    return rewards_history, epsilon_history\n\n# ==================== å¯è§†åŒ– ====================\n\ndef visualize_policy(agent, env):\n    \"\"\"å¯è§†åŒ–å­¦åˆ°çš„ç­–ç•¥\"\"\"\n\n    action_symbols = ['â†‘', 'â†“', 'â†', 'â†’']\n\n    fig, ax = plt.subplots(figsize=(8, 8))\n\n    # ç»˜åˆ¶ç½‘æ ¼\n    for i in range(env.size):\n        for j in range(env.size):\n            pos = (i, j)\n            state = env._pos_to_state(pos)\n\n            # èƒŒæ™¯è‰²\n            if pos == env.goal_pos:\n                color = 'green'\n            elif pos in env.obstacles:\n                color = 'gray'\n            else:\n                color = 'white'\n\n            rect = plt.Rectangle((j, env.size-1-i), 1, 1,\n                                 facecolor=color, edgecolor='black')\n            ax.add_patch(rect)\n\n            # æœ€ä¼˜åŠ¨ä½œ\n            if pos not in env.obstacles and pos != env.goal_pos:\n                action = agent.get_action(state, training=False)\n                ax.text(j+0.5, env.size-1-i+0.5, action_symbols[action],\n                       ha='center', va='center', fontsize=20)\n\n    ax.set_xlim(0, env.size)\n    ax.set_ylim(0, env.size)\n    ax.set_aspect('equal')\n    ax.set_title('å­¦åˆ°çš„ç­–ç•¥', fontsize=16)\n    ax.axis('off')\n\n    plt.tight_layout()\n    plt.savefig('policy.png', dpi=300)\n    plt.show()\n\ndef plot_training_results(rewards_history, epsilon_history):\n    \"\"\"ç»˜åˆ¶è®­ç»ƒç»“æœ\"\"\"\n\n    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n\n    # å¥–åŠ±æ›²çº¿\n    episodes = np.arange(len(rewards_history))\n    axes[0].plot(episodes, rewards_history, alpha=0.3, label='åŸå§‹')\n\n    # ç§»åŠ¨å¹³å‡\n    window = 100\n    moving_avg = np.convolve(rewards_history,\n                            np.ones(window)/window,\n                            mode='valid')\n    axes[0].plot(episodes[window-1:], moving_avg,\n                label=f'{window} Episode ç§»åŠ¨å¹³å‡', linewidth=2)\n\n    axes[0].set_xlabel('Episode')\n    axes[0].set_ylabel('Total Reward')\n    axes[0].set_title('è®­ç»ƒå¥–åŠ±')\n    axes[0].legend()\n    axes[0].grid(True, alpha=0.3)\n\n    # Epsilon è¡°å‡\n    axes[1].plot(epsilon_history)\n    axes[1].set_xlabel('Episode')\n    axes[1].set_ylabel('Epsilon')\n    axes[1].set_title('æ¢ç´¢ç‡è¡°å‡')\n    axes[1].grid(True, alpha=0.3)\n\n    plt.tight_layout()\n    plt.savefig('training_results.png', dpi=300)\n    plt.show()\n\n# ==================== ä¸»ç¨‹åº ====================\n\nif __name__ == '__main__':\n    # åˆ›å»ºç¯å¢ƒ\n    env = GridWorld(size=5)\n\n    # åˆ›å»º Agent\n    agent = QLearningAgent(\n        n_states=env.n_states,\n        n_actions=env.n_actions,\n        learning_rate=0.1,\n        discount_factor=0.95,\n        epsilon=1.0,\n        epsilon_decay=0.995,\n        epsilon_min=0.01\n    )\n\n    # è®­ç»ƒ\n    print(\"å¼€å§‹è®­ç»ƒ...\")\n    rewards_history, epsilon_history = train_qlearning(env, agent, num_episodes=1000)\n\n    # å¯è§†åŒ–ç»“æœ\n    plot_training_results(rewards_history, epsilon_history)\n    visualize_policy(agent, env)\n\n    # æµ‹è¯•å­¦åˆ°çš„ç­–ç•¥\n    print(\"\\næµ‹è¯•ç­–ç•¥:\")\n    state = env.reset()\n    done = False\n    steps = 0\n\n    print(\"åˆå§‹çŠ¶æ€:\")\n    print(env.render())\n\n    while not done and steps &lt; 20:\n        action = agent.get_action(state, training=False)\n        state, reward, done, _ = env.step(action)\n        steps += 1\n\n        print(f\"\\næ­¥éª¤ {steps}:\")\n        print(env.render())\n\n        if done:\n            print(f\"\\næˆåŠŸåˆ°è¾¾ç»ˆç‚¹ï¼æ€»æ­¥æ•°: {steps}\")",
    "crumbs": [
      "ç¬¬ä¸‰éƒ¨åˆ†ï¼šå®è·µä¸åº”ç”¨",
      "<span class='chapter-number'>11</span>Â  <span class='chapter-title'>ç¬¬åç« ï¼šå¼ºåŒ–å­¦ä¹  (Reinforcement Learning)</span>"
    ]
  },
  {
    "objectID": "Chapter10.html#deep-q-network-dqn",
    "href": "Chapter10.html#deep-q-network-dqn",
    "title": "11Â  ç¬¬åç« ï¼šå¼ºåŒ–å­¦ä¹  (Reinforcement Learning)",
    "section": "11.4 10.3 Deep Q-Network (DQN)",
    "text": "11.4 10.3 Deep Q-Network (DQN)\n\n11.4.1 ğŸ¯ ä» Q-Learning åˆ° DQN\nQ-Learning çš„å±€é™ï¼š\né—®é¢˜ï¼š\n  1. çŠ¶æ€ç©ºé—´å¤§æ—¶ï¼ŒQ è¡¨æ— æ³•å­˜å‚¨\n     (å¦‚ Atari æ¸¸æˆï¼šåƒç´ çŠ¶æ€ç©ºé—´å·¨å¤§)\n\n  2. æ— æ³•æ³›åŒ–åˆ°æœªè§è¿‡çš„çŠ¶æ€\n\nè§£å†³ï¼š\n  ç”¨ç¥ç»ç½‘ç»œé€¼è¿‘ Q å‡½æ•°\n  Q(s,a) â‰ˆ Q(s,a;Î¸)\n\n\n11.4.2 ğŸ“ DQN æ ¸å¿ƒåˆ›æ–°\n1. ç»éªŒå›æ”¾ (Experience Replay)\né—®é¢˜ï¼š\n  - è¿ç»­æ ·æœ¬é«˜åº¦ç›¸å…³\n  - ç ´å IID å‡è®¾\n\nè§£å†³ï¼š\n  - å­˜å‚¨ç»éªŒåˆ° Replay Buffer\n  - éšæœºé‡‡æ · mini-batch è®­ç»ƒ\n\nBuffer: (s, a, r, s', done)\n2. ç›®æ ‡ç½‘ç»œ (Target Network)\né—®é¢˜ï¼š\n  - ç›®æ ‡ Q å€¼ä¹Ÿåœ¨å˜åŒ–\n  - è®­ç»ƒä¸ç¨³å®š\n\nè§£å†³ï¼š\n  - ä½¿ç”¨å•ç‹¬çš„ç›®æ ‡ç½‘ç»œ Q'(s,a;Î¸â»)\n  - å®šæœŸä»ä¸»ç½‘ç»œå¤åˆ¶å‚æ•°\n\næŸå¤±å‡½æ•°:\n  L(Î¸) = E[(r + Î³Â·max_a' Q'(s',a';Î¸â») - Q(s,a;Î¸))Â²]\n\n\n11.4.3 ğŸ’» å®ç° DQN\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom collections import deque\nimport random\nimport gym\nimport numpy as np\n\n# ==================== ç¥ç»ç½‘ç»œ ====================\n\nclass DQN(nn.Module):\n    \"\"\"Deep Q-Network\"\"\"\n\n    def __init__(self, state_dim, action_dim, hidden_dim=128):\n        super(DQN, self).__init__()\n\n        self.fc1 = nn.Linear(state_dim, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n        self.fc3 = nn.Linear(hidden_dim, action_dim)\n\n    def forward(self, state):\n        \"\"\"\n        å‰å‘ä¼ æ’­\n\n        å‚æ•°:\n            state: (batch_size, state_dim)\n\n        è¿”å›:\n            Qå€¼: (batch_size, action_dim)\n        \"\"\"\n        x = F.relu(self.fc1(state))\n        x = F.relu(self.fc2(x))\n        q_values = self.fc3(x)\n        return q_values\n\n# ==================== ç»éªŒå›æ”¾ ====================\n\nclass ReplayBuffer:\n    \"\"\"ç»éªŒå›æ”¾ç¼“å†²åŒº\"\"\"\n\n    def __init__(self, capacity=10000):\n        self.buffer = deque(maxlen=capacity)\n\n    def push(self, state, action, reward, next_state, done):\n        \"\"\"æ·»åŠ ç»éªŒ\"\"\"\n        self.buffer.append((state, action, reward, next_state, done))\n\n    def sample(self, batch_size):\n        \"\"\"éšæœºé‡‡æ ·\"\"\"\n        batch = random.sample(self.buffer, batch_size)\n\n        states, actions, rewards, next_states, dones = zip(*batch)\n\n        return (\n            np.array(states),\n            np.array(actions),\n            np.array(rewards),\n            np.array(next_states),\n            np.array(dones)\n        )\n\n    def __len__(self):\n        return len(self.buffer)\n\n# ==================== DQN Agent ====================\n\nclass DQNAgent:\n    \"\"\"DQN æ™ºèƒ½ä½“\"\"\"\n\n    def __init__(self, state_dim, action_dim,\n                 learning_rate=1e-3, gamma=0.99,\n                 epsilon_start=1.0, epsilon_end=0.01, epsilon_decay=0.995,\n                 buffer_size=10000, batch_size=64,\n                 target_update_freq=10):\n\n        self.state_dim = state_dim\n        self.action_dim = action_dim\n        self.gamma = gamma\n        self.epsilon = epsilon_start\n        self.epsilon_end = epsilon_end\n        self.epsilon_decay = epsilon_decay\n        self.batch_size = batch_size\n        self.target_update_freq = target_update_freq\n\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n        # ä¸»ç½‘ç»œå’Œç›®æ ‡ç½‘ç»œ\n        self.policy_net = DQN(state_dim, action_dim).to(self.device)\n        self.target_net = DQN(state_dim, action_dim).to(self.device)\n        self.target_net.load_state_dict(self.policy_net.state_dict())\n        self.target_net.eval()\n\n        # ä¼˜åŒ–å™¨\n        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=learning_rate)\n\n        # ç»éªŒå›æ”¾\n        self.memory = ReplayBuffer(buffer_size)\n\n        # è®­ç»ƒæ­¥æ•°\n        self.steps = 0\n\n    def select_action(self, state, training=True):\n        \"\"\"é€‰æ‹©åŠ¨ä½œï¼ˆÎµ-greedyï¼‰\"\"\"\n        if training and random.random() &lt; self.epsilon:\n            return random.randrange(self.action_dim)\n        else:\n            with torch.no_grad():\n                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n                q_values = self.policy_net(state_tensor)\n                return q_values.argmax(dim=1).item()\n\n    def train_step(self):\n        \"\"\"è®­ç»ƒä¸€æ­¥\"\"\"\n        if len(self.memory) &lt; self.batch_size:\n            return None\n\n        # é‡‡æ ·ç»éªŒ\n        states, actions, rewards, next_states, dones = self.memory.sample(self.batch_size)\n\n        # è½¬ä¸º tensor\n        states = torch.FloatTensor(states).to(self.device)\n        actions = torch.LongTensor(actions).unsqueeze(1).to(self.device)\n        rewards = torch.FloatTensor(rewards).unsqueeze(1).to(self.device)\n        next_states = torch.FloatTensor(next_states).to(self.device)\n        dones = torch.FloatTensor(dones).unsqueeze(1).to(self.device)\n\n        # å½“å‰ Q å€¼\n        current_q_values = self.policy_net(states).gather(1, actions)\n\n        # ç›®æ ‡ Q å€¼\n        with torch.no_grad():\n            next_q_values = self.target_net(next_states).max(1)[0].unsqueeze(1)\n            target_q_values = rewards + self.gamma * next_q_values * (1 - dones)\n\n        # è®¡ç®—æŸå¤±\n        loss = F.mse_loss(current_q_values, target_q_values)\n\n        # åå‘ä¼ æ’­\n        self.optimizer.zero_grad()\n        loss.backward()\n\n        # æ¢¯åº¦è£å‰ª\n        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), max_norm=10)\n\n        self.optimizer.step()\n\n        # æ›´æ–°ç›®æ ‡ç½‘ç»œ\n        self.steps += 1\n        if self.steps % self.target_update_freq == 0:\n            self.target_net.load_state_dict(self.policy_net.state_dict())\n\n        # è¡°å‡ epsilon\n        if self.epsilon &gt; self.epsilon_end:\n            self.epsilon *= self.epsilon_decay\n\n        return loss.item()\n\n    def save(self, path):\n        \"\"\"ä¿å­˜æ¨¡å‹\"\"\"\n        torch.save({\n            'policy_net': self.policy_net.state_dict(),\n            'target_net': self.target_net.state_dict(),\n            'optimizer': self.optimizer.state_dict(),\n            'steps': self.steps,\n            'epsilon': self.epsilon\n        }, path)\n\n    def load(self, path):\n        \"\"\"åŠ è½½æ¨¡å‹\"\"\"\n        checkpoint = torch.load(path)\n        self.policy_net.load_state_dict(checkpoint['policy_net'])\n        self.target_net.load_state_dict(checkpoint['target_net'])\n        self.optimizer.load_state_dict(checkpoint['optimizer'])\n        self.steps = checkpoint['steps']\n        self.epsilon = checkpoint['epsilon']\n\n# ==================== è®­ç»ƒ DQN ====================\n\ndef train_dqn(env_name='CartPole-v1', num_episodes=500):\n    \"\"\"è®­ç»ƒ DQN\"\"\"\n\n    # åˆ›å»ºç¯å¢ƒ\n    env = gym.make(env_name)\n    state_dim = env.observation_space.shape[0]\n    action_dim = env.action_space.n\n\n    # åˆ›å»º Agent\n    agent = DQNAgent(\n        state_dim=state_dim,\n        action_dim=action_dim,\n        learning_rate=1e-3,\n        gamma=0.99,\n        epsilon_start=1.0,\n        epsilon_end=0.01,\n        epsilon_decay=0.995,\n        buffer_size=10000,\n        batch_size=64,\n        target_update_freq=10\n    )\n\n    # è®­ç»ƒå†å²\n    rewards_history = []\n    losses_history = []\n\n    for episode in range(num_episodes):\n        state = env.reset()\n        if isinstance(state, tuple):\n            state = state[0]  # gym æ–°ç‰ˆæœ¬è¿”å› (state, info)\n\n        total_reward = 0\n        episode_losses = []\n        done = False\n\n        while not done:\n            # é€‰æ‹©åŠ¨ä½œ\n            action = agent.select_action(state, training=True)\n\n            # æ‰§è¡ŒåŠ¨ä½œ\n            result = env.step(action)\n            if len(result) == 5:  # æ–°ç‰ˆæœ¬ gym\n                next_state, reward, terminated, truncated, _ = result\n                done = terminated or truncated\n            else:  # æ—§ç‰ˆæœ¬\n                next_state, reward, done, _ = result\n\n            # å­˜å‚¨ç»éªŒ\n            agent.memory.push(state, action, reward, next_state, done)\n\n            # è®­ç»ƒ\n            loss = agent.train_step()\n            if loss is not None:\n                episode_losses.append(loss)\n\n            state = next_state\n            total_reward += reward\n\n        rewards_history.append(total_reward)\n        if episode_losses:\n            losses_history.append(np.mean(episode_losses))\n\n        if (episode + 1) % 10 == 0:\n            avg_reward = np.mean(rewards_history[-10:])\n            print(f'Episode {episode+1}: Avg Reward = {avg_reward:.2f}, '\n                  f'Epsilon = {agent.epsilon:.3f}, '\n                  f'Buffer Size = {len(agent.memory)}')\n\n    env.close()\n\n    return agent, rewards_history, losses_history\n\n# ==================== å¯è§†åŒ– ====================\n\ndef plot_dqn_results(rewards_history, losses_history):\n    \"\"\"ç»˜åˆ¶ DQN è®­ç»ƒç»“æœ\"\"\"\n\n    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n\n    # å¥–åŠ±æ›²çº¿\n    episodes = np.arange(len(rewards_history))\n    axes[0].plot(episodes, rewards_history, alpha=0.3)\n\n    window = 10\n    moving_avg = np.convolve(rewards_history,\n                            np.ones(window)/window,\n                            mode='valid')\n    axes[0].plot(episodes[window-1:], moving_avg,\n                linewidth=2, label=f'{window} Episode MA')\n\n    axes[0].set_xlabel('Episode')\n    axes[0].set_ylabel('Total Reward')\n    axes[0].set_title('DQN è®­ç»ƒå¥–åŠ±')\n    axes[0].legend()\n    axes[0].grid(True, alpha=0.3)\n\n    # æŸå¤±æ›²çº¿\n    if losses_history:\n        axes[1].plot(losses_history)\n        axes[1].set_xlabel('Episode')\n        axes[1].set_ylabel('Loss')\n        axes[1].set_title('DQN è®­ç»ƒæŸå¤±')\n        axes[1].grid(True, alpha=0.3)\n\n    plt.tight_layout()\n    plt.savefig('dqn_results.png', dpi=300)\n    plt.show()\n\n# ==================== ä¸»ç¨‹åº ====================\n\nif __name__ == '__main__':\n    print(\"å¼€å§‹è®­ç»ƒ DQN...\")\n    agent, rewards, losses = train_dqn('CartPole-v1', num_episodes=500)\n\n    # å¯è§†åŒ–\n    plot_dqn_results(rewards, losses)\n\n    # ä¿å­˜æ¨¡å‹\n    agent.save('dqn_model.pth')\n\n    # æµ‹è¯•\n    print(\"\\næµ‹è¯•è®­ç»ƒå¥½çš„ Agent:\")\n    env = gym.make('CartPole-v1', render_mode='human')\n    state = env.reset()[0]\n\n    total_reward = 0\n    done = False\n\n    while not done:\n        action = agent.select_action(state, training=False)\n        result = env.step(action)\n        state, reward, terminated, truncated, _ = result\n        done = terminated or truncated\n        total_reward += reward\n\n    print(f\"æµ‹è¯•æ€»å¥–åŠ±: {total_reward}\")\n    env.close()",
    "crumbs": [
      "ç¬¬ä¸‰éƒ¨åˆ†ï¼šå®è·µä¸åº”ç”¨",
      "<span class='chapter-number'>11</span>Â  <span class='chapter-title'>ç¬¬åç« ï¼šå¼ºåŒ–å­¦ä¹  (Reinforcement Learning)</span>"
    ]
  },
  {
    "objectID": "Chapter10.html#policy-gradient-ç­–ç•¥æ¢¯åº¦",
    "href": "Chapter10.html#policy-gradient-ç­–ç•¥æ¢¯åº¦",
    "title": "11Â  ç¬¬åç« ï¼šå¼ºåŒ–å­¦ä¹  (Reinforcement Learning)",
    "section": "11.5 10.4 Policy Gradient ç­–ç•¥æ¢¯åº¦",
    "text": "11.5 10.4 Policy Gradient ç­–ç•¥æ¢¯åº¦\n\n11.5.1 ğŸ¯ æ ¸å¿ƒæ€æƒ³\nä¸ Q-Learning çš„åŒºåˆ«ï¼š\nQ-Learning (Value-based):\n  å­¦ä¹ ä»·å€¼å‡½æ•° Q(s,a)\n  é—´æ¥å¾—åˆ°ç­–ç•¥ï¼šÏ€(s) = argmax_a Q(s,a)\n\nPolicy Gradient (Policy-based):\n  ç›´æ¥å­¦ä¹ ç­–ç•¥ Ï€(a|s;Î¸)\n  ä¼˜åŒ–ç›®æ ‡ï¼šæœ€å¤§åŒ–æœŸæœ›å›æŠ¥\n\n\n11.5.2 ğŸ“ REINFORCE ç®—æ³•\nç›®æ ‡å‡½æ•°ï¼š\nJ(Î¸) = E_Ï„~Ï€_Î¸ [âˆ‘_t r_t]\n\næ¢¯åº¦ï¼ˆç­–ç•¥æ¢¯åº¦å®šç†ï¼‰ï¼š\nâˆ‡_Î¸ J(Î¸) = E_Ï„~Ï€_Î¸ [âˆ‘_t âˆ‡_Î¸ log Ï€_Î¸(a_t|s_t) Â· G_t]\n\nå…¶ä¸­ï¼š\n  G_t = âˆ‘_{t'=t}^T Î³^{t'-t} Â· r_{t'}  (ç´¯ç§¯å›æŠ¥)\nç›´è§‰ç†è§£ï¼š\nå¦‚æœåŠ¨ä½œ a å¸¦æ¥äº†æ­£å›æŠ¥ï¼š\n  â†’ å¢åŠ  log Ï€(a|s)\n  â†’ æé«˜è¯¥åŠ¨ä½œçš„æ¦‚ç‡\n\nå¦‚æœåŠ¨ä½œ a å¸¦æ¥äº†è´Ÿå›æŠ¥ï¼š\n  â†’ å‡å°‘ log Ï€(a|s)\n  â†’ é™ä½è¯¥åŠ¨ä½œçš„æ¦‚ç‡\n\n\n11.5.3 ğŸ’» å®ç° REINFORCE\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.distributions import Categorical\nimport gym\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# ==================== ç­–ç•¥ç½‘ç»œ ====================\n\nclass PolicyNetwork(nn.Module):\n    \"\"\"ç­–ç•¥ç½‘ç»œ\"\"\"\n\n    def __init__(self, state_dim, action_dim, hidden_dim=128):\n        super(PolicyNetwork, self).__init__()\n\n        self.fc1 = nn.Linear(state_dim, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n        self.fc3 = nn.Linear(hidden_dim, action_dim)\n\n    def forward(self, state):\n        \"\"\"\n        å‰å‘ä¼ æ’­\n\n        è¿”å›åŠ¨ä½œæ¦‚ç‡åˆ†å¸ƒ\n        \"\"\"\n        x = F.relu(self.fc1(state))\n        x = F.relu(self.fc2(x))\n        action_probs = F.softmax(self.fc3(x), dim=-1)\n        return action_probs\n\n# ==================== REINFORCE Agent ====================\n\nclass REINFORCEAgent:\n    \"\"\"REINFORCE æ™ºèƒ½ä½“\"\"\"\n\n    def __init__(self, state_dim, action_dim,\n                 learning_rate=1e-3, gamma=0.99):\n\n        self.gamma = gamma\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n        # ç­–ç•¥ç½‘ç»œ\n        self.policy = PolicyNetwork(state_dim, action_dim).to(self.device)\n        self.optimizer = optim.Adam(self.policy.parameters(), lr=learning_rate)\n\n        # å­˜å‚¨è½¨è¿¹\n        self.saved_log_probs = []\n        self.rewards = []\n\n    def select_action(self, state):\n        \"\"\"\n        æ ¹æ®ç­–ç•¥é‡‡æ ·åŠ¨ä½œ\n\n        è¿”å›: åŠ¨ä½œ + log æ¦‚ç‡\n        \"\"\"\n        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n\n        # è·å–åŠ¨ä½œæ¦‚ç‡åˆ†å¸ƒ\n        action_probs = self.policy(state_tensor)\n\n        # é‡‡æ ·åŠ¨ä½œ\n        dist = Categorical(action_probs)\n        action = dist.sample()\n\n        # ä¿å­˜ log æ¦‚ç‡\n        self.saved_log_probs.append(dist.log_prob(action))\n\n        return action.item()\n\n    def compute_returns(self, rewards):\n        \"\"\"\n        è®¡ç®—ç´¯ç§¯å›æŠ¥ (discounted returns)\n\n        G_t = r_t + Î³Â·r_{t+1} + Î³Â²Â·r_{t+2} + ...\n        \"\"\"\n        returns = []\n        G = 0\n\n        for r in reversed(rewards):\n            G = r + self.gamma * G\n            returns.insert(0, G)\n\n        # æ ‡å‡†åŒ–ï¼ˆå‡å°æ–¹å·®ï¼‰\n        returns = torch.tensor(returns).to(self.device)\n        returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n\n        return returns\n\n    def update(self):\n        \"\"\"æ›´æ–°ç­–ç•¥\"\"\"\n        # è®¡ç®—ç´¯ç§¯å›æŠ¥\n        returns = self.compute_returns(self.rewards)\n\n        # è®¡ç®—ç­–ç•¥æ¢¯åº¦\n        policy_loss = []\n        for log_prob, G in zip(self.saved_log_probs, returns):\n            policy_loss.append(-log_prob * G)\n\n        policy_loss = torch.cat(policy_loss).sum()\n\n        # åå‘ä¼ æ’­\n        self.optimizer.zero_grad()\n        policy_loss.backward()\n        self.optimizer.step()\n\n        # æ¸…ç©ºè½¨è¿¹\n        self.saved_log_probs = []\n        self.rewards = []\n\n        return policy_loss.item()\n\n# ==================== è®­ç»ƒ REINFORCE ====================\n\ndef train_reinforce(env_name='CartPole-v1', num_episodes=1000):\n    \"\"\"è®­ç»ƒ REINFORCE\"\"\"\n\n    # åˆ›å»ºç¯å¢ƒ\n    env = gym.make(env_name)\n    state_dim = env.observation_space.shape[0]\n    action_dim = env.action_space.n\n\n    # åˆ›å»º Agent\n    agent = REINFORCEAgent(\n        state_dim=state_dim,\n        action_dim=action_dim,\n        learning_rate=1e-3,\n        gamma=0.99\n    )\n\n    rewards_history = []\n\n    for episode in range(num_episodes):\n        state = env.reset()\n        if isinstance(state, tuple):\n            state = state[0]\n\n        episode_reward = 0\n        done = False\n\n        # æ”¶é›†ä¸€æ¡è½¨è¿¹\n        while not done:\n            action = agent.select_action(state)\n\n            result = env.step(action)\n            if len(result) == 5:\n                next_state, reward, terminated, truncated, _ = result\n                done = terminated or truncated\n            else:\n                next_state, reward, done, _ = result\n\n            agent.rewards.append(reward)\n            episode_reward += reward\n            state = next_state\n\n        # æ›´æ–°ç­–ç•¥\n        loss = agent.update()\n\n        rewards_history.append(episode_reward)\n\n        if (episode + 1) % 10 == 0:\n            avg_reward = np.mean(rewards_history[-10:])\n            print(f'Episode {episode+1}: Avg Reward = {avg_reward:.2f}')\n\n    env.close()\n\n    return agent, rewards_history\n\n# ==================== ä¸»ç¨‹åº ====================\n\nif __name__ == '__main__':\n    print(\"å¼€å§‹è®­ç»ƒ REINFORCE...\")\n    agent, rewards = train_reinforce('CartPole-v1', num_episodes=1000)\n\n    # å¯è§†åŒ–\n    plt.figure(figsize=(12, 5))\n\n    episodes = np.arange(len(rewards))\n    plt.plot(episodes, rewards, alpha=0.3)\n\n    window = 10\n    moving_avg = np.convolve(rewards, np.ones(window)/window, mode='valid')\n    plt.plot(episodes[window-1:], moving_avg, linewidth=2,\n            label=f'{window} Episode MA')\n\n    plt.xlabel('Episode')\n    plt.ylabel('Total Reward')\n    plt.title('REINFORCE è®­ç»ƒå¥–åŠ±')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    plt.tight_layout()\n    plt.savefig('reinforce_results.png', dpi=300)\n    plt.show()",
    "crumbs": [
      "ç¬¬ä¸‰éƒ¨åˆ†ï¼šå®è·µä¸åº”ç”¨",
      "<span class='chapter-number'>11</span>Â  <span class='chapter-title'>ç¬¬åç« ï¼šå¼ºåŒ–å­¦ä¹  (Reinforcement Learning)</span>"
    ]
  },
  {
    "objectID": "Chapter10.html#actor-critic-ç®—æ³•",
    "href": "Chapter10.html#actor-critic-ç®—æ³•",
    "title": "11Â  ç¬¬åç« ï¼šå¼ºåŒ–å­¦ä¹  (Reinforcement Learning)",
    "section": "11.6 10.5 Actor-Critic ç®—æ³•",
    "text": "11.6 10.5 Actor-Critic ç®—æ³•\n\n11.6.1 ğŸ¯ æ ¸å¿ƒæ€æƒ³\nç»“åˆ Value-based å’Œ Policy-basedï¼š\nActor (ç­–ç•¥ç½‘ç»œ):\n  Ï€(a|s;Î¸)\n  è´Ÿè´£é€‰æ‹©åŠ¨ä½œ\n\nCritic (ä»·å€¼ç½‘ç»œ):\n  V(s;w) æˆ– Q(s,a;w)\n  è´Ÿè´£è¯„ä¼°åŠ¨ä½œ\n\nä¼˜åŠ¿ï¼š\n  - Actor æä¾›ç­–ç•¥\n  - Critic å‡å°æ–¹å·®ï¼ˆä¸éœ€è¦ç­‰åˆ° episode ç»“æŸï¼‰\n\n\n11.6.2 ğŸ“ ä¼˜åŠ¿å‡½æ•° (Advantage Function)\nA(s,a) = Q(s,a) - V(s)\n\nå«ä¹‰ï¼š\n  åŠ¨ä½œ a æ¯”å¹³å‡å¥½å¤šå°‘\n\nç­–ç•¥æ¢¯åº¦ï¼š\n  âˆ‡_Î¸ J(Î¸) = E[âˆ‡_Î¸ log Ï€_Î¸(a|s) Â· A(s,a)]\n\n\n11.6.3 ğŸ’» å®ç° A2C (Advantage Actor-Critic)\nclass ActorCritic(nn.Module):\n    \"\"\"Actor-Critic ç½‘ç»œ\"\"\"\n\n    def __init__(self, state_dim, action_dim, hidden_dim=128):\n        super(ActorCritic, self).__init__()\n\n        # å…±äº«ç‰¹å¾æå–å±‚\n        self.fc1 = nn.Linear(state_dim, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n\n        # Actor å¤´ï¼ˆç­–ç•¥ï¼‰\n        self.actor = nn.Linear(hidden_dim, action_dim)\n\n        # Critic å¤´ï¼ˆä»·å€¼ï¼‰\n        self.critic = nn.Linear(hidden_dim, 1)\n\n    def forward(self, state):\n        \"\"\"\n        å‰å‘ä¼ æ’­\n\n        è¿”å›: action_probs, state_value\n        \"\"\"\n        x = F.relu(self.fc1(state))\n        x = F.relu(self.fc2(x))\n\n        # Actor è¾“å‡ºåŠ¨ä½œæ¦‚ç‡\n        action_probs = F.softmax(self.actor(x), dim=-1)\n\n        # Critic è¾“å‡ºçŠ¶æ€ä»·å€¼\n        state_value = self.critic(x)\n\n        return action_probs, state_value\n\nclass A2CAgent:\n    \"\"\"A2C æ™ºèƒ½ä½“\"\"\"\n\n    def __init__(self, state_dim, action_dim,\n                 learning_rate=1e-3, gamma=0.99, entropy_coef=0.01):\n\n        self.gamma = gamma\n        self.entropy_coef = entropy_coef\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n        # Actor-Critic ç½‘ç»œ\n        self.ac_net = ActorCritic(state_dim, action_dim).to(self.device)\n        self.optimizer = optim.Adam(self.ac_net.parameters(), lr=learning_rate)\n\n    def select_action(self, state):\n        \"\"\"é€‰æ‹©åŠ¨ä½œ\"\"\"\n        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n\n        action_probs, state_value = self.ac_net(state_tensor)\n\n        # é‡‡æ ·åŠ¨ä½œ\n        dist = Categorical(action_probs)\n        action = dist.sample()\n\n        return action.item(), dist.log_prob(action), dist.entropy(), state_value\n\n    def update(self, states, actions, rewards, next_states, dones):\n        \"\"\"\n        æ›´æ–°ç½‘ç»œ\n\n        å‚æ•°ä¸ºä¸€ä¸ª batch çš„ç»éªŒ\n        \"\"\"\n        states = torch.FloatTensor(states).to(self.device)\n        actions = torch.LongTensor(actions).to(self.device)\n        rewards = torch.FloatTensor(rewards).to(self.device)\n        next_states = torch.FloatTensor(next_states).to(self.device)\n        dones = torch.FloatTensor(dones).to(self.device)\n\n        # è®¡ç®—å½“å‰çŠ¶æ€çš„ä»·å€¼å’ŒåŠ¨ä½œæ¦‚ç‡\n        action_probs, state_values = self.ac_net(states)\n\n        # è®¡ç®—ä¸‹ä¸€çŠ¶æ€çš„ä»·å€¼\n        with torch.no_grad():\n            _, next_state_values = self.ac_net(next_states)\n            # TD ç›®æ ‡\n            td_targets = rewards + self.gamma * next_state_values.squeeze() * (1 - dones)\n\n        # ä¼˜åŠ¿å‡½æ•°\n        advantages = td_targets - state_values.squeeze()\n\n        # Actor æŸå¤±ï¼ˆç­–ç•¥æ¢¯åº¦ï¼‰\n        dist = Categorical(action_probs)\n        log_probs = dist.log_prob(actions)\n        actor_loss = -(log_probs * advantages.detach()).mean()\n\n        # Critic æŸå¤±ï¼ˆTD errorï¼‰\n        critic_loss = F.mse_loss(state_values.squeeze(), td_targets)\n\n        # ç†µæ­£åˆ™åŒ–ï¼ˆé¼“åŠ±æ¢ç´¢ï¼‰\n        entropy = dist.entropy().mean()\n\n        # æ€»æŸå¤±\n        total_loss = actor_loss + 0.5 * critic_loss - self.entropy_coef * entropy\n\n        # åå‘ä¼ æ’­\n        self.optimizer.zero_grad()\n        total_loss.backward()\n        torch.nn.utils.clip_grad_norm_(self.ac_net.parameters(), max_norm=0.5)\n        self.optimizer.step()\n\n        return total_loss.item(), actor_loss.item(), critic_loss.item()\n\n# ==================== è®­ç»ƒ A2C ====================\n\ndef train_a2c(env_name='CartPole-v1', num_episodes=500, batch_size=5):\n    \"\"\"è®­ç»ƒ A2C\"\"\"\n\n    env = gym.make(env_name)\n    state_dim = env.observation_space.shape[0]\n    action_dim = env.action_space.n\n\n    agent = A2CAgent(\n        state_dim=state_dim,\n        action_dim=action_dim,\n        learning_rate=1e-3,\n        gamma=0.99,\n        entropy_coef=0.01\n    )\n\n    rewards_history = []\n\n    for episode in range(num_episodes):\n        states, actions, rewards, next_states, dones = [], [], [], [], []\n\n        state = env.reset()\n        if isinstance(state, tuple):\n            state = state[0]\n\n        episode_reward = 0\n        done = False\n\n        # æ”¶é›† batch ä¸ªæ ·æœ¬\n        while not done:\n            action, log_prob, entropy, state_value = agent.select_action(state)\n\n            result = env.step(action)\n            if len(result) == 5:\n                next_state, reward, terminated, truncated, _ = result\n                done = terminated or truncated\n            else:\n                next_state, reward, done, _ = result\n\n            states.append(state)\n            actions.append(action)\n            rewards.append(reward)\n            next_states.append(next_state)\n            dones.append(done)\n\n            state = next_state\n            episode_reward += reward\n\n            # è¾¾åˆ° batch_size æˆ– episode ç»“æŸï¼Œæ›´æ–°\n            if len(states) &gt;= batch_size or done:\n                loss, actor_loss, critic_loss = agent.update(\n                    states, actions, rewards, next_states, dones\n                )\n                states, actions, rewards, next_states, dones = [], [], [], [], []\n\n        rewards_history.append(episode_reward)\n\n        if (episode + 1) % 10 == 0:\n            avg_reward = np.mean(rewards_history[-10:])\n            print(f'Episode {episode+1}: Avg Reward = {avg_reward:.2f}')\n\n    env.close()\n\n    return agent, rewards_history\n\n# è¿è¡Œ\nif __name__ == '__main__':\n    print(\"å¼€å§‹è®­ç»ƒ A2C...\")\n    agent, rewards = train_a2c('CartPole-v1', num_episodes=500)",
    "crumbs": [
      "ç¬¬ä¸‰éƒ¨åˆ†ï¼šå®è·µä¸åº”ç”¨",
      "<span class='chapter-number'>11</span>Â  <span class='chapter-title'>ç¬¬åç« ï¼šå¼ºåŒ–å­¦ä¹  (Reinforcement Learning)</span>"
    ]
  },
  {
    "objectID": "Chapter10.html#å®æˆ˜atari-æ¸¸æˆ",
    "href": "Chapter10.html#å®æˆ˜atari-æ¸¸æˆ",
    "title": "11Â  ç¬¬åç« ï¼šå¼ºåŒ–å­¦ä¹  (Reinforcement Learning)",
    "section": "11.7 10.6 å®æˆ˜ï¼šAtari æ¸¸æˆ",
    "text": "11.7 10.6 å®æˆ˜ï¼šAtari æ¸¸æˆ\n\n11.7.1 ğŸ® ç¯å¢ƒè®¾ç½®\nimport gym\nfrom gym.wrappers import AtariPreprocessing, FrameStack\nimport ale_py\n\ndef make_atari_env(env_name='BreakoutNoFrameskip-v4', frame_stack=4):\n    \"\"\"\n    åˆ›å»º Atari ç¯å¢ƒ\n\n    é¢„å¤„ç†ï¼š\n      - ç°åº¦åŒ–\n      - é™é‡‡æ ·åˆ° 84x84\n      - Frame stacking (å †å å¤šå¸§)\n    \"\"\"\n    env = gym.make(env_name)\n\n    # Atari é¢„å¤„ç†\n    env = AtariPreprocessing(\n        env,\n        noop_max=30,\n        frame_skip=4,\n        screen_size=84,\n        terminal_on_life_loss=False,\n        grayscale_obs=True,\n        grayscale_newaxis=False,\n        scale_obs=True\n    )\n\n    # å †å å¸§\n    env = FrameStack(env, num_stack=frame_stack)\n\n    return env\n\n\n11.7.2 ğŸ§  CNN-based DQN\nclass AtariDQN(nn.Module):\n    \"\"\"ç”¨äº Atari æ¸¸æˆçš„ DQN\"\"\"\n\n    def __init__(self, num_actions, frame_stack=4):\n        super(AtariDQN, self).__init__()\n\n        # å·ç§¯å±‚\n        self.conv1 = nn.Conv2d(frame_stack, 32, kernel_size=8, stride=4)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n\n        # å…¨è¿æ¥å±‚\n        self.fc1 = nn.Linear(64 * 7 * 7, 512)\n        self.fc2 = nn.Linear(512, num_actions)\n\n    def forward(self, x):\n        \"\"\"\n        å‚æ•°:\n            x: (batch, frame_stack, 84, 84)\n        \"\"\"\n        x = F.relu(self.conv1(x))\n        x = F.relu(self.conv2(x))\n        x = F.relu(self.conv3(x))\n\n        x = x.view(x.size(0), -1)  # Flatten\n\n        x = F.relu(self.fc1(x))\n        q_values = self.fc2(x)\n\n        return q_values\n\n# è®­ç»ƒï¼ˆä¸ä¹‹å‰ç±»ä¼¼ï¼Œä½†è¾“å…¥æ˜¯å›¾åƒï¼‰\n# æ³¨æ„ï¼šAtari æ¸¸æˆè®­ç»ƒéœ€è¦å¤§é‡è®¡ç®—èµ„æºå’Œæ—¶é—´",
    "crumbs": [
      "ç¬¬ä¸‰éƒ¨åˆ†ï¼šå®è·µä¸åº”ç”¨",
      "<span class='chapter-number'>11</span>Â  <span class='chapter-title'>ç¬¬åç« ï¼šå¼ºåŒ–å­¦ä¹  (Reinforcement Learning)</span>"
    ]
  },
  {
    "objectID": "Chapter10.html#é«˜çº§è¯é¢˜",
    "href": "Chapter10.html#é«˜çº§è¯é¢˜",
    "title": "11Â  ç¬¬åç« ï¼šå¼ºåŒ–å­¦ä¹  (Reinforcement Learning)",
    "section": "11.8 10.7 é«˜çº§è¯é¢˜",
    "text": "11.8 10.7 é«˜çº§è¯é¢˜\n\n11.8.1 ğŸ”¹ ä¼˜å…ˆç»éªŒå›æ”¾ (Prioritized Experience Replay)\næ ¸å¿ƒæ€æƒ³ï¼šé‡è¦çš„ç»éªŒæ›´é¢‘ç¹åœ°è¢«é‡‡æ ·\nclass PrioritizedReplayBuffer:\n    \"\"\"ä¼˜å…ˆç»éªŒå›æ”¾\"\"\"\n\n    def __init__(self, capacity=10000, alpha=0.6, beta=0.4):\n        self.capacity = capacity\n        self.alpha = alpha  # ä¼˜å…ˆçº§æŒ‡æ•°\n        self.beta = beta    # é‡è¦æ€§é‡‡æ ·æŒ‡æ•°\n        self.buffer = []\n        self.priorities = np.zeros(capacity)\n        self.pos = 0\n\n    def push(self, transition):\n        \"\"\"æ·»åŠ ç»éªŒ\"\"\"\n        max_priority = self.priorities.max() if self.buffer else 1.0\n\n        if len(self.buffer) &lt; self.capacity:\n            self.buffer.append(transition)\n        else:\n            self.buffer[self.pos] = transition\n\n        self.priorities[self.pos] = max_priority\n        self.pos = (self.pos + 1) % self.capacity\n\n    def sample(self, batch_size):\n        \"\"\"æŒ‰ä¼˜å…ˆçº§é‡‡æ ·\"\"\"\n        if len(self.buffer) == self.capacity:\n            priorities = self.priorities\n        else:\n            priorities = self.priorities[:self.pos]\n\n        # è®¡ç®—é‡‡æ ·æ¦‚ç‡\n        probs = priorities ** self.alpha\n        probs /= probs.sum()\n\n        # é‡‡æ ·ç´¢å¼•\n        indices = np.random.choice(len(self.buffer), batch_size, p=probs)\n\n        # è®¡ç®—é‡è¦æ€§é‡‡æ ·æƒé‡\n        total = len(self.buffer)\n        weights = (total * probs[indices]) ** (-self.beta)\n        weights /= weights.max()\n\n        samples = [self.buffer[idx] for idx in indices]\n\n        return samples, indices, weights\n\n    def update_priorities(self, indices, priorities):\n        \"\"\"æ›´æ–°ä¼˜å…ˆçº§\"\"\"\n        for idx, priority in zip(indices, priorities):\n            self.priorities[idx] = priority\n\n\n\n11.8.2 ğŸ”¹ Double DQN\né—®é¢˜ï¼šDQN å€¾å‘äºé«˜ä¼° Q å€¼\nè§£å†³ï¼šç”¨ä¸»ç½‘ç»œé€‰æ‹©åŠ¨ä½œï¼Œç›®æ ‡ç½‘ç»œè¯„ä¼° Q å€¼\n# æ ‡å‡† DQN\ntarget_q = rewards + gamma * target_net(next_states).max(1)[0]\n\n# Double DQN\nbest_actions = policy_net(next_states).argmax(1)\ntarget_q = rewards + gamma * target_net(next_states).gather(1, best_actions)\n\n\n\n11.8.3 ğŸ”¹ Dueling DQN\næ¶æ„æ”¹è¿›ï¼šåˆ†ç¦»ä»·å€¼å’Œä¼˜åŠ¿\nclass DuelingDQN(nn.Module):\n    def __init__(self, state_dim, action_dim, hidden_dim=128):\n        super().__init__()\n\n        self.feature = nn.Sequential(\n            nn.Linear(state_dim, hidden_dim),\n            nn.ReLU()\n        )\n\n        # çŠ¶æ€ä»·å€¼æµ\n        self.value_stream = nn.Sequential(\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, 1)\n        )\n\n        # ä¼˜åŠ¿æµ\n        self.advantage_stream = nn.Sequential(\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, action_dim)\n        )\n\n    def forward(self, state):\n        features = self.feature(state)\n\n        value = self.value_stream(features)\n        advantages = self.advantage_stream(features)\n\n        # Q(s,a) = V(s) + (A(s,a) - mean(A(s,Â·)))\n        q_values = value + (advantages - advantages.mean(dim=1, keepdim=True))\n\n        return q_values\n\n\n\n11.8.4 ğŸ”¹ PPO (Proximal Policy Optimization)\nç›®å‰æœ€æµè¡Œçš„ç­–ç•¥æ¢¯åº¦ç®—æ³•\nclass PPOAgent:\n    \"\"\"PPO æ™ºèƒ½ä½“\"\"\"\n\n    def __init__(self, state_dim, action_dim, clip_epsilon=0.2):\n        self.clip_epsilon = clip_epsilon\n\n        self.actor_critic = ActorCritic(state_dim, action_dim)\n        self.optimizer = optim.Adam(self.actor_critic.parameters(), lr=3e-4)\n\n    def compute_ppo_loss(self, states, actions, old_log_probs,\n                         advantages, returns):\n        \"\"\"\n        è®¡ç®— PPO æŸå¤±\n\n        PPO-Clip ç›®æ ‡:\n          L = min(r_t(Î¸)Â·A_t, clip(r_t(Î¸), 1-Îµ, 1+Îµ)Â·A_t)\n\n        å…¶ä¸­ r_t(Î¸) = Ï€_Î¸(a|s) / Ï€_Î¸_old(a|s)\n        \"\"\"\n        # æ–°çš„åŠ¨ä½œæ¦‚ç‡\n        action_probs, state_values = self.actor_critic(states)\n        dist = Categorical(action_probs)\n        new_log_probs = dist.log_prob(actions)\n\n        # æ¦‚ç‡æ¯”\n        ratio = torch.exp(new_log_probs - old_log_probs)\n\n        # Clipped objective\n        surr1 = ratio * advantages\n        surr2 = torch.clamp(ratio, 1 - self.clip_epsilon,\n                          1 + self.clip_epsilon) * advantages\n        actor_loss = -torch.min(surr1, surr2).mean()\n\n        # Critic æŸå¤±\n        critic_loss = F.mse_loss(state_values.squeeze(), returns)\n\n        # ç†µ bonus\n        entropy = dist.entropy().mean()\n\n        total_loss = actor_loss + 0.5 * critic_loss - 0.01 * entropy\n\n        return total_loss",
    "crumbs": [
      "ç¬¬ä¸‰éƒ¨åˆ†ï¼šå®è·µä¸åº”ç”¨",
      "<span class='chapter-number'>11</span>Â  <span class='chapter-title'>ç¬¬åç« ï¼šå¼ºåŒ–å­¦ä¹  (Reinforcement Learning)</span>"
    ]
  },
  {
    "objectID": "Chapter10.html#æœ¬ç« ä½œä¸š",
    "href": "Chapter10.html#æœ¬ç« ä½œä¸š",
    "title": "11Â  ç¬¬åç« ï¼šå¼ºåŒ–å­¦ä¹  (Reinforcement Learning)",
    "section": "11.9 ğŸ“ æœ¬ç« ä½œä¸š",
    "text": "11.9 ğŸ“ æœ¬ç« ä½œä¸š\n\n11.9.1 ä½œä¸š 1ï¼šå®ç° Q-Learning\n# åœ¨ç½‘æ ¼ä¸–ç•Œæˆ– FrozenLake ç¯å¢ƒä¸­ï¼š\n# 1. å®ç° Q-Learning\n# 2. å¯è§†åŒ– Q è¡¨çš„æ¼”åŒ–è¿‡ç¨‹\n# 3. å¯¹æ¯”ä¸åŒè¶…å‚æ•°ï¼ˆlr, Î³, Îµï¼‰çš„å½±å“\n# 4. åˆ†ææ”¶æ•›é€Ÿåº¦\n\n\n11.9.2 ä½œä¸š 2ï¼šDQN ç© CartPole\n# 1. å®ç°å®Œæ•´çš„ DQN\n# 2. æ·»åŠ ç»éªŒå›æ”¾å’Œç›®æ ‡ç½‘ç»œ\n# 3. è®°å½•è®­ç»ƒæ›²çº¿\n# 4. å¯¹æ¯”ï¼š\n#    - DQN vs Q-Learning\n#    - ä¸åŒç½‘ç»œæ¶æ„\n#    - ä¸åŒè¶…å‚æ•°\n\n\n11.9.3 ä½œä¸š 3ï¼šPolicy Gradient\n# 1. å®ç° REINFORCE å’Œ A2C\n# 2. åœ¨ CartPole æˆ– LunarLander ä¸Šè®­ç»ƒ\n# 3. å¯¹æ¯”ä¸¤ç§ç®—æ³•çš„ï¼š\n#    - æ”¶æ•›é€Ÿåº¦\n#    - æ ·æœ¬æ•ˆç‡\n#    - æœ€ç»ˆæ€§èƒ½\n# 4. å¯è§†åŒ–ç­–ç•¥çš„æ¼”åŒ–\n\n\n11.9.4 ä½œä¸š 4ï¼šæŒ‘æˆ˜é¡¹ç›®\n# é€‰æ‹©ä»¥ä¸‹ä¹‹ä¸€ï¼š\n#\n# 1. å®ç° Double DQN æˆ– Dueling DQN\n#    åœ¨ Atari æ¸¸æˆä¸Šæµ‹è¯•\n#\n# 2. å®ç° PPO\n#    è®­ç»ƒè¿ç»­æ§åˆ¶ä»»åŠ¡ï¼ˆå¦‚ MuJoCoï¼‰\n#\n# 3. å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ \n#    å®ç°ç®€å•çš„åˆä½œ/ç«äº‰ç¯å¢ƒ",
    "crumbs": [
      "ç¬¬ä¸‰éƒ¨åˆ†ï¼šå®è·µä¸åº”ç”¨",
      "<span class='chapter-number'>11</span>Â  <span class='chapter-title'>ç¬¬åç« ï¼šå¼ºåŒ–å­¦ä¹  (Reinforcement Learning)</span>"
    ]
  },
  {
    "objectID": "Chapter10.html#æœ¬ç« å…³é”®æ¦‚å¿µ",
    "href": "Chapter10.html#æœ¬ç« å…³é”®æ¦‚å¿µ",
    "title": "11Â  ç¬¬åç« ï¼šå¼ºåŒ–å­¦ä¹  (Reinforcement Learning)",
    "section": "11.10 ğŸ”‘ æœ¬ç« å…³é”®æ¦‚å¿µ",
    "text": "11.10 ğŸ”‘ æœ¬ç« å…³é”®æ¦‚å¿µ\n\n\n\næ¦‚å¿µ\nè¯´æ˜\n\n\n\n\nMDP\né©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹\n\n\nç­–ç•¥\nçŠ¶æ€åˆ°åŠ¨ä½œçš„æ˜ å°„\n\n\nä»·å€¼å‡½æ•°\næœŸæœ›ç´¯ç§¯å¥–åŠ±\n\n\nQ-Learning\nåŸºäºå€¼çš„ RL\n\n\nDQN\næ·±åº¦ Q ç½‘ç»œ\n\n\nç»éªŒå›æ”¾\næ‰“ç ´æ ·æœ¬ç›¸å…³æ€§\n\n\nç›®æ ‡ç½‘ç»œ\nç¨³å®šè®­ç»ƒ\n\n\nPolicy Gradient\nåŸºäºç­–ç•¥çš„ RL\n\n\nREINFORCE\nè’™ç‰¹å¡æ´›ç­–ç•¥æ¢¯åº¦\n\n\nActor-Critic\nç»“åˆä»·å€¼å’Œç­–ç•¥\n\n\nPPO\nè¿‘ç«¯ç­–ç•¥ä¼˜åŒ–\n\n\n\néœ€è¦æˆ‘ç»§ç»­å†™ç¬¬åä¸€ç« ï¼šæ— ç›‘ç£å­¦ä¹ å—ï¼Ÿ",
    "crumbs": [
      "ç¬¬ä¸‰éƒ¨åˆ†ï¼šå®è·µä¸åº”ç”¨",
      "<span class='chapter-number'>11</span>Â  <span class='chapter-title'>ç¬¬åç« ï¼šå¼ºåŒ–å­¦ä¹  (Reinforcement Learning)</span>"
    ]
  },
  {
    "objectID": "Chapter11.html",
    "href": "Chapter11.html",
    "title": "12Â  ç¬¬åä¸€ç« ï¼šæ— ç›‘ç£å­¦ä¹  (Unsupervised Learning)",
    "section": "",
    "text": "12.1 ğŸ“Œ ç« èŠ‚ç›®æ ‡",
    "crumbs": [
      "ç¬¬ä¸‰éƒ¨åˆ†ï¼šå®è·µä¸åº”ç”¨",
      "<span class='chapter-number'>12</span>Â  <span class='chapter-title'>ç¬¬åä¸€ç« ï¼šæ— ç›‘ç£å­¦ä¹  (Unsupervised Learning)</span>"
    ]
  },
  {
    "objectID": "Chapter11.html#ç« èŠ‚ç›®æ ‡",
    "href": "Chapter11.html#ç« èŠ‚ç›®æ ‡",
    "title": "12Â  ç¬¬åä¸€ç« ï¼šæ— ç›‘ç£å­¦ä¹  (Unsupervised Learning)",
    "section": "",
    "text": "ç†è§£æ— ç›‘ç£å­¦ä¹ çš„æ ¸å¿ƒæ€æƒ³\næŒæ¡èšç±»ç®—æ³•ï¼ˆK-means, DBSCAN, å±‚æ¬¡èšç±»ï¼‰\nå­¦ä¹ é™ç»´æŠ€æœ¯ï¼ˆPCA, t-SNE, UMAPï¼‰\näº†è§£è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•\nå®æˆ˜ï¼šæ•°æ®æ¢ç´¢ã€ç‰¹å¾å­¦ä¹ ã€å¼‚å¸¸æ£€æµ‹",
    "crumbs": [
      "ç¬¬ä¸‰éƒ¨åˆ†ï¼šå®è·µä¸åº”ç”¨",
      "<span class='chapter-number'>12</span>Â  <span class='chapter-title'>ç¬¬åä¸€ç« ï¼šæ— ç›‘ç£å­¦ä¹  (Unsupervised Learning)</span>"
    ]
  },
  {
    "objectID": "Chapter11.html#æ— ç›‘ç£å­¦ä¹ æ¦‚è¿°",
    "href": "Chapter11.html#æ— ç›‘ç£å­¦ä¹ æ¦‚è¿°",
    "title": "12Â  ç¬¬åä¸€ç« ï¼šæ— ç›‘ç£å­¦ä¹  (Unsupervised Learning)",
    "section": "12.2 11.1 æ— ç›‘ç£å­¦ä¹ æ¦‚è¿°",
    "text": "12.2 11.1 æ— ç›‘ç£å­¦ä¹ æ¦‚è¿°\n\n12.2.1 ğŸ¯ ä»€ä¹ˆæ˜¯æ— ç›‘ç£å­¦ä¹ ï¼Ÿ\nå®šä¹‰ï¼šä»æ— æ ‡ç­¾æ•°æ®ä¸­å­¦ä¹ æ•°æ®çš„å†…åœ¨ç»“æ„å’Œæ¨¡å¼\nä¸ç›‘ç£å­¦ä¹ çš„åŒºåˆ«ï¼š\nç›‘ç£å­¦ä¹ ï¼š\n  è¾“å…¥: (X, y)  æœ‰æ ‡ç­¾\n  ç›®æ ‡: å­¦ä¹  f: X â†’ y\n\næ— ç›‘ç£å­¦ä¹ ï¼š\n  è¾“å…¥: X only  æ— æ ‡ç­¾\n  ç›®æ ‡: å‘ç°æ•°æ®çš„éšè—ç»“æ„\n\n\n12.2.2 ğŸ“Š ä¸»è¦ä»»åŠ¡\n\n12.2.2.1 1. èšç±» (Clustering)\nç›®æ ‡ï¼šå°†ç›¸ä¼¼æ ·æœ¬åˆ†ç»„\n\nåº”ç”¨ï¼š\n  - å®¢æˆ·åˆ†ç¾¤\n  - å›¾åƒåˆ†å‰²\n  - æ–‡æ¡£ç»„ç»‡\n  - åŸºå› åˆ†æ\n\n\n12.2.2.2 2. é™ç»´ (Dimensionality Reduction)\nç›®æ ‡ï¼šåœ¨ä½ç»´ç©ºé—´ä¿ç•™æ•°æ®ç‰¹æ€§\n\nåº”ç”¨ï¼š\n  - å¯è§†åŒ–\n  - æ•°æ®å‹ç¼©\n  - å™ªå£°æ¶ˆé™¤\n  - ç‰¹å¾æå–\n\n\n12.2.2.3 3. å¯†åº¦ä¼°è®¡ (Density Estimation)\nç›®æ ‡ï¼šä¼°è®¡æ•°æ®çš„æ¦‚ç‡åˆ†å¸ƒ\n\nåº”ç”¨ï¼š\n  - å¼‚å¸¸æ£€æµ‹\n  - ç”Ÿæˆæ¨¡å‹\n\n\n12.2.2.4 4. è¡¨ç¤ºå­¦ä¹  (Representation Learning)\nç›®æ ‡ï¼šå­¦ä¹ æœ‰ç”¨çš„æ•°æ®è¡¨ç¤º\n\nåº”ç”¨ï¼š\n  - è‡ªç›‘ç£å­¦ä¹ \n  - é¢„è®­ç»ƒæ¨¡å‹",
    "crumbs": [
      "ç¬¬ä¸‰éƒ¨åˆ†ï¼šå®è·µä¸åº”ç”¨",
      "<span class='chapter-number'>12</span>Â  <span class='chapter-title'>ç¬¬åä¸€ç« ï¼šæ— ç›‘ç£å­¦ä¹  (Unsupervised Learning)</span>"
    ]
  },
  {
    "objectID": "Chapter11.html#èšç±»ç®—æ³•",
    "href": "Chapter11.html#èšç±»ç®—æ³•",
    "title": "12Â  ç¬¬åä¸€ç« ï¼šæ— ç›‘ç£å­¦ä¹  (Unsupervised Learning)",
    "section": "12.3 11.2 èšç±»ç®—æ³•",
    "text": "12.3 11.2 èšç±»ç®—æ³•\n\n12.3.1 ğŸ”¹ K-Means èšç±»\n\n12.3.1.1 ç®—æ³•åŸç†\nç›®æ ‡ï¼šæœ€å°åŒ–ç±»å†…è·ç¦»å¹³æ–¹å’Œ\n\nJ = âˆ‘_{k=1}^K âˆ‘_{xâˆˆC_k} ||x - Î¼_k||Â²\n\nç®—æ³•æ­¥éª¤ï¼š\n1. éšæœºåˆå§‹åŒ– K ä¸ªä¸­å¿ƒ Î¼_k\n2. åˆ†é…ï¼šæ¯ä¸ªç‚¹åˆ†é…åˆ°æœ€è¿‘çš„ä¸­å¿ƒ\n3. æ›´æ–°ï¼šé‡æ–°è®¡ç®—æ¯ä¸ªç°‡çš„ä¸­å¿ƒ\n4. é‡å¤ 2-3 ç›´åˆ°æ”¶æ•›\n\n\n12.3.1.2 å®ç°\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_blobs\n\nclass KMeans:\n    \"\"\"K-Means èšç±»\"\"\"\n\n    def __init__(self, n_clusters=3, max_iters=100, random_state=None):\n        self.n_clusters = n_clusters\n        self.max_iters = max_iters\n        self.random_state = random_state\n        self.centers = None\n        self.labels = None\n\n    def fit(self, X):\n        \"\"\"\n        è®­ç»ƒ K-Means\n\n        å‚æ•°:\n            X: (n_samples, n_features)\n        \"\"\"\n        np.random.seed(self.random_state)\n        n_samples = X.shape[0]\n\n        # éšæœºåˆå§‹åŒ–ä¸­å¿ƒ\n        random_indices = np.random.choice(n_samples, self.n_clusters, replace=False)\n        self.centers = X[random_indices]\n\n        for iteration in range(self.max_iters):\n            # åˆ†é…æ ·æœ¬åˆ°æœ€è¿‘çš„ä¸­å¿ƒ\n            labels = self._assign_clusters(X)\n\n            # æ›´æ–°ä¸­å¿ƒ\n            new_centers = self._update_centers(X, labels)\n\n            # æ£€æŸ¥æ”¶æ•›\n            if np.allclose(self.centers, new_centers):\n                print(f\"æ”¶æ•›äºç¬¬ {iteration+1} æ¬¡è¿­ä»£\")\n                break\n\n            self.centers = new_centers\n\n        self.labels = labels\n        return self\n\n    def _assign_clusters(self, X):\n        \"\"\"åˆ†é…æ ·æœ¬åˆ°æœ€è¿‘çš„ç°‡\"\"\"\n        distances = np.sqrt(((X[:, np.newaxis] - self.centers) ** 2).sum(axis=2))\n        return np.argmin(distances, axis=1)\n\n    def _update_centers(self, X, labels):\n        \"\"\"æ›´æ–°ç°‡ä¸­å¿ƒ\"\"\"\n        new_centers = np.zeros((self.n_clusters, X.shape[1]))\n\n        for k in range(self.n_clusters):\n            cluster_points = X[labels == k]\n            if len(cluster_points) &gt; 0:\n                new_centers[k] = cluster_points.mean(axis=0)\n            else:\n                # å¦‚æœç°‡ä¸ºç©ºï¼Œé‡æ–°éšæœºåˆå§‹åŒ–\n                new_centers[k] = X[np.random.randint(X.shape[0])]\n\n        return new_centers\n\n    def predict(self, X):\n        \"\"\"é¢„æµ‹æ–°æ ·æœ¬çš„ç°‡\"\"\"\n        return self._assign_clusters(X)\n\n    def fit_predict(self, X):\n        \"\"\"è®­ç»ƒå¹¶é¢„æµ‹\"\"\"\n        self.fit(X)\n        return self.labels\n\n# ==================== å¯è§†åŒ– K-Means ====================\n\ndef visualize_kmeans(X, kmeans, title='K-Means Clustering'):\n    \"\"\"å¯è§†åŒ– K-Means ç»“æœ\"\"\"\n\n    fig, ax = plt.subplots(figsize=(10, 8))\n\n    # ç»˜åˆ¶æ•°æ®ç‚¹\n    scatter = ax.scatter(X[:, 0], X[:, 1], c=kmeans.labels,\n                        cmap='viridis', alpha=0.6, s=50)\n\n    # ç»˜åˆ¶ä¸­å¿ƒ\n    ax.scatter(kmeans.centers[:, 0], kmeans.centers[:, 1],\n              c='red', marker='X', s=200, edgecolor='black',\n              linewidth=2, label='Centroids')\n\n    ax.set_xlabel('Feature 1')\n    ax.set_ylabel('Feature 2')\n    ax.set_title(title)\n    ax.legend()\n    plt.colorbar(scatter, label='Cluster')\n    plt.grid(True, alpha=0.3)\n    plt.tight_layout()\n    plt.show()\n\n# ==================== è‚˜éƒ¨æ³•åˆ™ (Elbow Method) ====================\n\ndef elbow_method(X, max_k=10):\n    \"\"\"ä½¿ç”¨è‚˜éƒ¨æ³•åˆ™ç¡®å®šæœ€ä½³ K å€¼\"\"\"\n\n    inertias = []\n    K_range = range(1, max_k + 1)\n\n    for k in K_range:\n        kmeans = KMeans(n_clusters=k, random_state=42)\n        kmeans.fit(X)\n\n        # è®¡ç®—ç°‡å†…å¹³æ–¹å’Œ (inertia)\n        labels = kmeans.labels\n        inertia = 0\n        for i in range(k):\n            cluster_points = X[labels == i]\n            if len(cluster_points) &gt; 0:\n                inertia += ((cluster_points - kmeans.centers[i]) ** 2).sum()\n\n        inertias.append(inertia)\n\n    # ç»˜åˆ¶è‚˜éƒ¨æ›²çº¿\n    plt.figure(figsize=(10, 6))\n    plt.plot(K_range, inertias, 'bo-', linewidth=2, markersize=8)\n    plt.xlabel('Number of Clusters (K)')\n    plt.ylabel('Inertia (Within-cluster Sum of Squares)')\n    plt.title('Elbow Method for Optimal K')\n    plt.grid(True, alpha=0.3)\n    plt.tight_layout()\n    plt.show()\n\n    return inertias\n\n# ==================== ç¤ºä¾‹ ====================\n\nif __name__ == '__main__':\n    # ç”Ÿæˆåˆæˆæ•°æ®\n    X, y_true = make_blobs(n_samples=300, centers=4,\n                           n_features=2, random_state=42)\n\n    # è‚˜éƒ¨æ³•åˆ™\n    print(\"ä½¿ç”¨è‚˜éƒ¨æ³•åˆ™ç¡®å®šæœ€ä½³ K...\")\n    elbow_method(X, max_k=10)\n\n    # K-Means èšç±»\n    kmeans = KMeans(n_clusters=4, random_state=42)\n    kmeans.fit(X)\n\n    print(f\"\\nç°‡ä¸­å¿ƒ:\\n{kmeans.centers}\")\n    print(f\"ç°‡åˆ†é…: {kmeans.labels}\")\n\n    # å¯è§†åŒ–\n    visualize_kmeans(X, kmeans)\n\n\n\n\n12.3.2 ğŸ”¹ DBSCAN (Density-Based Spatial Clustering)\n\n12.3.2.1 ç®—æ³•åŸç†\nåŸºäºå¯†åº¦çš„èšç±»ï¼š\n  - æ ¸å¿ƒç‚¹ï¼šÎµ é‚»åŸŸå†…è‡³å°‘æœ‰ MinPts ä¸ªç‚¹\n  - è¾¹ç•Œç‚¹ï¼šåœ¨æ ¸å¿ƒç‚¹çš„ Îµ é‚»åŸŸå†…ï¼Œä½†è‡ªå·±ä¸æ˜¯æ ¸å¿ƒç‚¹\n  - å™ªå£°ç‚¹ï¼šæ—¢ä¸æ˜¯æ ¸å¿ƒç‚¹ä¹Ÿä¸æ˜¯è¾¹ç•Œç‚¹\n\nä¼˜åŠ¿ï¼š\n  âœ“ å¯ä»¥å‘ç°ä»»æ„å½¢çŠ¶çš„ç°‡\n  âœ“ ä¸éœ€è¦é¢„å…ˆæŒ‡å®šç°‡æ•°\n  âœ“ èƒ½è¯†åˆ«å™ªå£°ç‚¹\n\nå‚æ•°ï¼š\n  - Îµ (epsilon): é‚»åŸŸåŠå¾„\n  - MinPts: æœ€å°ç‚¹æ•°\n\n\n12.3.2.2 å®ç°\nfrom sklearn.neighbors import NearestNeighbors\n\nclass DBSCAN:\n    \"\"\"DBSCAN èšç±»\"\"\"\n\n    def __init__(self, eps=0.5, min_samples=5):\n        self.eps = eps\n        self.min_samples = min_samples\n        self.labels = None\n\n    def fit(self, X):\n        \"\"\"è®­ç»ƒ DBSCAN\"\"\"\n        n_samples = X.shape[0]\n\n        # è®¡ç®—æ‰€æœ‰ç‚¹çš„é‚»åŸŸ\n        neighbors_model = NearestNeighbors(radius=self.eps)\n        neighbors_model.fit(X)\n        neighborhoods = neighbors_model.radius_neighbors(X, return_distance=False)\n\n        # åˆå§‹åŒ–æ ‡ç­¾ï¼ˆ-1 è¡¨ç¤ºæœªåˆ†ç±»ï¼‰\n        labels = np.full(n_samples, -1)\n\n        # å½“å‰ç°‡ ID\n        cluster_id = 0\n\n        for i in range(n_samples):\n            # å¦‚æœå·²åˆ†ç±»ï¼Œè·³è¿‡\n            if labels[i] != -1:\n                continue\n\n            # è·å–é‚»åŸŸ\n            neighbors = neighborhoods[i]\n\n            # å¦‚æœä¸æ˜¯æ ¸å¿ƒç‚¹ï¼Œæ ‡è®°ä¸ºå™ªå£°ï¼ˆæš‚æ—¶ï¼‰\n            if len(neighbors) &lt; self.min_samples:\n                labels[i] = -1\n                continue\n\n            # å¼€å§‹æ–°ç°‡\n            labels[i] = cluster_id\n\n            # ç§å­é›†åˆï¼ˆå¾…æ‰©å±•çš„ç‚¹ï¼‰\n            seeds = set(neighbors) - {i}\n\n            while seeds:\n                q = seeds.pop()\n\n                # å¦‚æœæ˜¯å™ªå£°ç‚¹ï¼Œæ”¹ä¸ºè¾¹ç•Œç‚¹\n                if labels[q] == -1:\n                    labels[q] = cluster_id\n\n                # å¦‚æœå·²åˆ†ç±»åˆ°å…¶ä»–ç°‡ï¼Œè·³è¿‡\n                if labels[q] != -1:\n                    continue\n\n                labels[q] = cluster_id\n\n                # å¦‚æœ q ä¹Ÿæ˜¯æ ¸å¿ƒç‚¹ï¼Œæ‰©å±•ç§å­é›†\n                q_neighbors = neighborhoods[q]\n                if len(q_neighbors) &gt;= self.min_samples:\n                    seeds.update(q_neighbors)\n\n            cluster_id += 1\n\n        self.labels = labels\n        return self\n\n    def fit_predict(self, X):\n        \"\"\"è®­ç»ƒå¹¶è¿”å›æ ‡ç­¾\"\"\"\n        self.fit(X)\n        return self.labels\n\n# ==================== å¯è§†åŒ– DBSCAN ====================\n\ndef visualize_dbscan(X, dbscan):\n    \"\"\"å¯è§†åŒ– DBSCAN ç»“æœ\"\"\"\n\n    fig, ax = plt.subplots(figsize=(10, 8))\n\n    # æ ¸å¿ƒæ ·æœ¬ mask\n    unique_labels = set(dbscan.labels)\n    colors = plt.cm.Spectral(np.linspace(0, 1, len(unique_labels)))\n\n    for k, col in zip(unique_labels, colors):\n        if k == -1:\n            # å™ªå£°ç‚¹ç”¨é»‘è‰²è¡¨ç¤º\n            col = [0, 0, 0, 1]\n\n        class_member_mask = (dbscan.labels == k)\n\n        xy = X[class_member_mask]\n        ax.scatter(xy[:, 0], xy[:, 1], c=[col],\n                  s=50, alpha=0.6,\n                  label=f'Cluster {k}' if k != -1 else 'Noise')\n\n    ax.set_xlabel('Feature 1')\n    ax.set_ylabel('Feature 2')\n    ax.set_title('DBSCAN Clustering')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    plt.tight_layout()\n    plt.show()\n\n# ==================== ç¤ºä¾‹ ====================\n\nif __name__ == '__main__':\n    # ç”Ÿæˆæœˆç‰™å½¢æ•°æ®\n    from sklearn.datasets import make_moons\n    X, _ = make_moons(n_samples=300, noise=0.05, random_state=42)\n\n    # DBSCAN èšç±»\n    dbscan = DBSCAN(eps=0.2, min_samples=5)\n    dbscan.fit(X)\n\n    print(f\"å‘ç° {len(set(dbscan.labels)) - (1 if -1 in dbscan.labels else 0)} ä¸ªç°‡\")\n    print(f\"å™ªå£°ç‚¹æ•°: {list(dbscan.labels).count(-1)}\")\n\n    # å¯è§†åŒ–\n    visualize_dbscan(X, dbscan)\n\n\n\n\n12.3.3 ğŸ”¹ å±‚æ¬¡èšç±» (Hierarchical Clustering)\n\n12.3.3.1 ç®—æ³•åŸç†\nä¸¤ç§ç­–ç•¥ï¼š\n\n1. å‡èš (Agglomerative)ï¼šè‡ªåº•å‘ä¸Š\n   - æ¯ä¸ªç‚¹åˆå§‹åŒ–ä¸ºä¸€ä¸ªç°‡\n   - é€æ­¥åˆå¹¶æœ€ç›¸ä¼¼çš„ç°‡\n   - ç›´åˆ°å½¢æˆä¸€ä¸ªå¤§ç°‡æˆ–è¾¾åˆ°åœæ­¢æ¡ä»¶\n\n2. åˆ†è£‚ (Divisive)ï¼šè‡ªé¡¶å‘ä¸‹\n   - æ‰€æœ‰ç‚¹åˆå§‹åŒ–ä¸ºä¸€ä¸ªç°‡\n   - é€æ­¥åˆ†è£‚ç°‡\n\né“¾æ¥æ–¹æ³• (Linkage):\n  - Single: æœ€è¿‘ç‚¹è·ç¦»\n  - Complete: æœ€è¿œç‚¹è·ç¦»\n  - Average: å¹³å‡è·ç¦»\n  - Ward: æœ€å°åŒ–æ–¹å·®\n\n\n12.3.3.2 å®ç°\nfrom scipy.cluster.hierarchy import dendrogram, linkage\nfrom scipy.spatial.distance import pdist, squareform\n\nclass AgglomerativeClustering:\n    \"\"\"å‡èšå±‚æ¬¡èšç±»\"\"\"\n\n    def __init__(self, n_clusters=2, linkage='ward'):\n        self.n_clusters = n_clusters\n        self.linkage_method = linkage\n        self.labels = None\n        self.linkage_matrix = None\n\n    def fit(self, X):\n        \"\"\"è®­ç»ƒå±‚æ¬¡èšç±»\"\"\"\n        # è®¡ç®—é“¾æ¥çŸ©é˜µ\n        self.linkage_matrix = linkage(X, method=self.linkage_method)\n\n        # æ ¹æ® n_clusters åˆ‡å‰²æ ‘çŠ¶å›¾\n        from scipy.cluster.hierarchy import fcluster\n        self\n\n-----\n\n&gt; continue\n\n.labels = fcluster(self.linkage_matrix,\n                                     self.n_clusters,\n                                     criterion='maxclust') - 1  # è½¬ä¸º 0-based\n\n        return self\n\n    def fit_predict(self, X):\n        \"\"\"è®­ç»ƒå¹¶è¿”å›æ ‡ç­¾\"\"\"\n        self.fit(X)\n        return self.labels\n\n# ==================== æ ‘çŠ¶å›¾å¯è§†åŒ– ====================\n\ndef plot_dendrogram(X, method='ward', title='Hierarchical Clustering Dendrogram'):\n    \"\"\"ç»˜åˆ¶æ ‘çŠ¶å›¾\"\"\"\n\n    plt.figure(figsize=(12, 6))\n\n    # è®¡ç®—é“¾æ¥çŸ©é˜µ\n    Z = linkage(X, method=method)\n\n    # ç»˜åˆ¶æ ‘çŠ¶å›¾\n    dendrogram(Z)\n\n    plt.xlabel('Sample Index')\n    plt.ylabel('Distance')\n    plt.title(title)\n    plt.grid(True, alpha=0.3)\n    plt.tight_layout()\n    plt.show()\n\n# ==================== ç¤ºä¾‹ ====================\n\nif __name__ == '__main__':\n    # ç”Ÿæˆæ•°æ®\n    X, _ = make_blobs(n_samples=100, centers=3,\n                      n_features=2, random_state=42)\n\n    # ç»˜åˆ¶æ ‘çŠ¶å›¾\n    plot_dendrogram(X, method='ward')\n\n    # å±‚æ¬¡èšç±»\n    hc = AgglomerativeClustering(n_clusters=3, linkage='ward')\n    labels = hc.fit_predict(X)\n\n    # å¯è§†åŒ–ç»“æœ\n    plt.figure(figsize=(10, 8))\n    plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=50)\n    plt.xlabel('Feature 1')\n    plt.ylabel('Feature 2')\n    plt.title('Hierarchical Clustering Result')\n    plt.colorbar(label='Cluster')\n    plt.grid(True, alpha=0.3)\n    plt.tight_layout()\n    plt.show()",
    "crumbs": [
      "ç¬¬ä¸‰éƒ¨åˆ†ï¼šå®è·µä¸åº”ç”¨",
      "<span class='chapter-number'>12</span>Â  <span class='chapter-title'>ç¬¬åä¸€ç« ï¼šæ— ç›‘ç£å­¦ä¹  (Unsupervised Learning)</span>"
    ]
  },
  {
    "objectID": "Chapter11.html#é™ç»´æŠ€æœ¯",
    "href": "Chapter11.html#é™ç»´æŠ€æœ¯",
    "title": "12Â  ç¬¬åä¸€ç« ï¼šæ— ç›‘ç£å­¦ä¹  (Unsupervised Learning)",
    "section": "12.4 11.3 é™ç»´æŠ€æœ¯",
    "text": "12.4 11.3 é™ç»´æŠ€æœ¯\n\n12.4.1 ğŸ”¹ ä¸»æˆåˆ†åˆ†æ (PCA)\n\n12.4.1.1 ç®—æ³•åŸç†\nç›®æ ‡ï¼šæ‰¾åˆ°æ–¹å·®æœ€å¤§çš„æ–¹å‘\n\næ•°å­¦è¡¨è¾¾ï¼š\n1. ä¸­å¿ƒåŒ–æ•°æ®ï¼šX_centered = X - mean(X)\n2. è®¡ç®—åæ–¹å·®çŸ©é˜µï¼šC = (1/n)Â·X^TÂ·X\n3. ç‰¹å¾å€¼åˆ†è§£ï¼šC = VÂ·Î›Â·V^T\n4. é€‰æ‹©å‰ k ä¸ªä¸»æˆåˆ†\n\næŠ•å½±ï¼š\n  Z = XÂ·V_k  (é™ç»´åçš„æ•°æ®)\n\né‡æ„ï¼š\n  X_reconstructed = ZÂ·V_k^T\n\n\n12.4.1.2 å®ç°\nclass PCA:\n    \"\"\"ä¸»æˆåˆ†åˆ†æ\"\"\"\n\n    def __init__(self, n_components=2):\n        self.n_components = n_components\n        self.components = None\n        self.mean = None\n        self.explained_variance = None\n        self.explained_variance_ratio = None\n\n    def fit(self, X):\n        \"\"\"è®­ç»ƒ PCA\"\"\"\n        # ä¸­å¿ƒåŒ–\n        self.mean = np.mean(X, axis=0)\n        X_centered = X - self.mean\n\n        # è®¡ç®—åæ–¹å·®çŸ©é˜µ\n        cov_matrix = np.cov(X_centered.T)\n\n        # ç‰¹å¾å€¼åˆ†è§£\n        eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n\n        # æŒ‰ç‰¹å¾å€¼é™åºæ’åº\n        idx = eigenvalues.argsort()[::-1]\n        eigenvalues = eigenvalues[idx]\n        eigenvectors = eigenvectors[:, idx]\n\n        # é€‰æ‹©å‰ n_components ä¸ªä¸»æˆåˆ†\n        self.components = eigenvectors[:, :self.n_components]\n        self.explained_variance = eigenvalues[:self.n_components]\n        self.explained_variance_ratio = (\n            self.explained_variance / eigenvalues.sum()\n        )\n\n        return self\n\n    def transform(self, X):\n        \"\"\"é™ç»´\"\"\"\n        X_centered = X - self.mean\n        return np.dot(X_centered, self.components)\n\n    def fit_transform(self, X):\n        \"\"\"è®­ç»ƒå¹¶é™ç»´\"\"\"\n        self.fit(X)\n        return self.transform(X)\n\n    def inverse_transform(self, Z):\n        \"\"\"é‡æ„\"\"\"\n        return np.dot(Z, self.components.T) + self.mean\n\n# ==================== å¯è§†åŒ– PCA ====================\n\ndef visualize_pca(X, y=None, title='PCA Visualization'):\n    \"\"\"å¯è§†åŒ– PCA é™ç»´ç»“æœ\"\"\"\n\n    # PCA é™åˆ° 2D\n    pca = PCA(n_components=2)\n    X_pca = pca.fit_transform(X)\n\n    plt.figure(figsize=(12, 5))\n\n    # é™ç»´åçš„æ•°æ®\n    plt.subplot(1, 2, 1)\n    if y is not None:\n        scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1],\n                            c=y, cmap='viridis', alpha=0.6)\n        plt.colorbar(scatter, label='Class')\n    else:\n        plt.scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.6)\n\n    plt.xlabel(f'PC1 ({pca.explained_variance_ratio[0]:.2%})')\n    plt.ylabel(f'PC2 ({pca.explained_variance_ratio[1]:.2%})')\n    plt.title('PCA Projection')\n    plt.grid(True, alpha=0.3)\n\n    # æ–¹å·®è§£é‡Šæ¯”ä¾‹\n    plt.subplot(1, 2, 2)\n    n_components = min(10, X.shape[1])\n    pca_full = PCA(n_components=n_components)\n    pca_full.fit(X)\n\n    cumsum = np.cumsum(pca_full.explained_variance_ratio)\n\n    plt.plot(range(1, n_components+1),\n            pca_full.explained_variance_ratio,\n            'bo-', label='Individual')\n    plt.plot(range(1, n_components+1),\n            cumsum,\n            'rs-', label='Cumulative')\n\n    plt.xlabel('Principal Component')\n    plt.ylabel('Explained Variance Ratio')\n    plt.title('Variance Explained')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n\n    plt.tight_layout()\n    plt.show()\n\n# ==================== ç¤ºä¾‹ï¼šæ‰‹å†™æ•°å­— ====================\n\nif __name__ == '__main__':\n    from sklearn.datasets import load_digits\n\n    # åŠ è½½ MNIST æ•°å­—æ•°æ®é›†\n    digits = load_digits()\n    X, y = digits.data, digits.target\n\n    print(f\"åŸå§‹ç»´åº¦: {X.shape}\")\n\n    # PCA é™ç»´\n    pca = PCA(n_components=2)\n    X_pca = pca.fit_transform(X)\n\n    print(f\"é™ç»´å: {X_pca.shape}\")\n    print(f\"æ–¹å·®è§£é‡Šæ¯”ä¾‹: {pca.explained_variance_ratio}\")\n\n    # å¯è§†åŒ–\n    visualize_pca(X, y, title='PCA on MNIST Digits')\n\n\n\n\n12.4.2 ğŸ”¹ t-SNE (t-Distributed Stochastic Neighbor Embedding)\n\n12.4.2.1 ç®—æ³•åŸç†\nç›®æ ‡ï¼šä¿æŒå±€éƒ¨ç»“æ„\n\næ­¥éª¤ï¼š\n1. è®¡ç®—é«˜ç»´ç©ºé—´ä¸­ç‚¹å¯¹çš„ç›¸ä¼¼åº¦ p_ij\n2. åœ¨ä½ç»´ç©ºé—´éšæœºåˆå§‹åŒ–\n3. è®¡ç®—ä½ç»´ç©ºé—´ä¸­çš„ç›¸ä¼¼åº¦ q_ij\n4. æœ€å°åŒ– KL æ•£åº¦ï¼šKL(P||Q)\n\nç‰¹ç‚¹ï¼š\n  âœ“ æ“…é•¿å¯è§†åŒ–èšç±»ç»“æ„\n  âœ“ ä¿ç•™å±€éƒ¨é‚»åŸŸ\n  âœ— è®¡ç®—å¤æ‚åº¦é«˜ O(nÂ²)\n  âœ— å…¨å±€ç»“æ„ä¸ä¿è¯\n  âœ— æ¯æ¬¡è¿è¡Œç»“æœä¸åŒ\n\n\n12.4.2.2 ä½¿ç”¨ sklearn\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\n\ndef visualize_tsne(X, y, perplexity=30, title='t-SNE Visualization'):\n    \"\"\"t-SNE å¯è§†åŒ–\"\"\"\n\n    print(\"è¿è¡Œ t-SNE...\")\n    tsne = TSNE(n_components=2, perplexity=perplexity,\n                random_state=42, n_iter=1000)\n    X_tsne = tsne.fit_transform(X)\n\n    plt.figure(figsize=(10, 8))\n\n    if y is not None:\n        scatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1],\n                            c=y, cmap='tab10', alpha=0.6, s=20)\n        plt.colorbar(scatter, label='Class')\n    else:\n        plt.scatter(X_tsne[:, 0], X_tsne[:, 1], alpha=0.6, s=20)\n\n    plt.xlabel('t-SNE Component 1')\n    plt.ylabel('t-SNE Component 2')\n    plt.title(title)\n    plt.grid(True, alpha=0.3)\n    plt.tight_layout()\n    plt.show()\n\n# ==================== å¯¹æ¯” PCA å’Œ t-SNE ====================\n\ndef compare_pca_tsne(X, y):\n    \"\"\"å¯¹æ¯” PCA å’Œ t-SNE\"\"\"\n\n    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n\n    # PCA\n    pca = PCA(n_components=2)\n    X_pca = pca.fit_transform(X)\n\n    scatter1 = axes[0].scatter(X_pca[:, 0], X_pca[:, 1],\n                              c=y, cmap='tab10', alpha=0.6, s=20)\n    axes[0].set_xlabel(f'PC1 ({pca.explained_variance_ratio[0]:.2%})')\n    axes[0].set_ylabel(f'PC2 ({pca.explained_variance_ratio[1]:.2%})')\n    axes[0].set_title('PCA')\n    axes[0].grid(True, alpha=0.3)\n    plt.colorbar(scatter1, ax=axes[0], label='Class')\n\n    # t-SNE\n    print(\"è¿è¡Œ t-SNE...\")\n    tsne = TSNE(n_components=2, random_state=42)\n    X_tsne = tsne.fit_transform(X)\n\n    scatter2 = axes[1].scatter(X_tsne[:, 0], X_tsne[:, 1],\n                              c=y, cmap='tab10', alpha=0.6, s=20)\n    axes[1].set_xlabel('t-SNE 1')\n    axes[1].set_ylabel('t-SNE 2')\n    axes[1].set_title('t-SNE')\n    axes[1].grid(True, alpha=0.3)\n    plt.colorbar(scatter2, ax=axes[1], label='Class')\n\n    plt.tight_layout()\n    plt.show()\n\n# ==================== ç¤ºä¾‹ ====================\n\nif __name__ == '__main__':\n    from sklearn.datasets import load_digits\n\n    digits = load_digits()\n    X, y = digits.data, digits.target\n\n    # é™é‡‡æ ·ï¼ˆt-SNE å¾ˆæ…¢ï¼‰\n    from sklearn.model_selection import train_test_split\n    X_sample, _, y_sample, _ = train_test_split(\n        X, y, train_size=500, stratify=y, random_state=42\n    )\n\n    # å¯¹æ¯”\n    compare_pca_tsne(X_sample, y_sample)\n\n\n\n\n12.4.3 ğŸ”¹ UMAP (Uniform Manifold Approximation and Projection)\nä¼˜åŠ¿ï¼š\n  âœ“ æ¯” t-SNE å¿«\n  âœ“ æ›´å¥½åœ°ä¿ç•™å…¨å±€ç»“æ„\n  âœ“ æ”¯æŒæ–°æ•°æ®çš„ transform\n\nä½¿ç”¨ï¼š\nimport umap\n\ndef visualize_umap(X, y, title='UMAP Visualization'):\n    \"\"\"UMAP å¯è§†åŒ–\"\"\"\n\n    print(\"è¿è¡Œ UMAP...\")\n    reducer = umap.UMAP(n_components=2, random_state=42)\n    X_umap = reducer.fit_transform(X)\n\n    plt.figure(figsize=(10, 8))\n    scatter = plt.scatter(X_umap[:, 0], X_umap[:, 1],\n                         c=y, cmap='tab10', alpha=0.6, s=20)\n    plt.colorbar(scatter, label='Class')\n    plt.xlabel('UMAP 1')\n    plt.ylabel('UMAP 2')\n    plt.title(title)\n    plt.grid(True, alpha=0.3)\n    plt.tight_layout()\n    plt.show()",
    "crumbs": [
      "ç¬¬ä¸‰éƒ¨åˆ†ï¼šå®è·µä¸åº”ç”¨",
      "<span class='chapter-number'>12</span>Â  <span class='chapter-title'>ç¬¬åä¸€ç« ï¼šæ— ç›‘ç£å­¦ä¹  (Unsupervised Learning)</span>"
    ]
  },
  {
    "objectID": "Chapter11.html#è‡ªç›‘ç£å­¦ä¹ -self-supervised-learning",
    "href": "Chapter11.html#è‡ªç›‘ç£å­¦ä¹ -self-supervised-learning",
    "title": "12Â  ç¬¬åä¸€ç« ï¼šæ— ç›‘ç£å­¦ä¹  (Unsupervised Learning)",
    "section": "12.5 11.4 è‡ªç›‘ç£å­¦ä¹  (Self-Supervised Learning)",
    "text": "12.5 11.4 è‡ªç›‘ç£å­¦ä¹  (Self-Supervised Learning)\n\n12.5.1 ğŸ¯ æ ¸å¿ƒæ€æƒ³\nä»æ— æ ‡ç­¾æ•°æ®ä¸­è‡ªåŠ¨ç”Ÿæˆç›‘ç£ä¿¡å·\n\nå¸¸è§é¢„è®­ç»ƒä»»åŠ¡ï¼š\n\nå›¾åƒï¼š\n  - æ—‹è½¬é¢„æµ‹\n  - æ‹¼å›¾æ±‚è§£\n  - å›¾åƒä¿®å¤\n  - å¯¹æ¯”å­¦ä¹ \n\næ–‡æœ¬ï¼š\n  - æ©ç è¯­è¨€æ¨¡å‹ (MLM)\n  - ä¸‹ä¸€å¥é¢„æµ‹ (NSP)\n  - è‡ªå›å½’è¯­è¨€æ¨¡å‹\n\n\n12.5.2 ğŸ”¹ å›¾åƒå¯¹æ¯”å­¦ä¹ ï¼šSimCLR\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass SimCLR(nn.Module):\n    \"\"\"SimCLR å¯¹æ¯”å­¦ä¹ æ¡†æ¶\"\"\"\n\n    def __init__(self, base_encoder, projection_dim=128):\n        super(SimCLR, self).__init__()\n\n        # ç¼–ç å™¨ï¼ˆå¦‚ ResNetï¼‰\n        self.encoder = base_encoder\n\n        # æŠ•å½±å¤´\n        self.projector = nn.Sequential(\n            nn.Linear(base_encoder.output_dim, 2048),\n            nn.ReLU(),\n            nn.Linear(2048, projection_dim)\n        )\n\n    def forward(self, x):\n        # æå–ç‰¹å¾\n        h = self.encoder(x)\n\n        # æŠ•å½±\n        z = self.projector(h)\n\n        # L2 å½’ä¸€åŒ–\n        z = F.normalize(z, dim=1)\n\n        return z\n\nclass NTXentLoss(nn.Module):\n    \"\"\"å½’ä¸€åŒ–æ¸©åº¦äº¤å‰ç†µæŸå¤±\"\"\"\n\n    def __init__(self, temperature=0.5):\n        super(NTXentLoss, self).__init__()\n        self.temperature = temperature\n\n    def forward(self, z_i, z_j):\n        \"\"\"\n        å‚æ•°:\n            z_i, z_j: ä¸¤ä¸ªå¢å¼ºè§†å›¾çš„è¡¨ç¤º\n        \"\"\"\n        batch_size = z_i.size(0)\n\n        # æ‹¼æ¥\n        z = torch.cat([z_i, z_j], dim=0)  # (2B, D)\n\n        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ\n        sim = torch.mm(z, z.T) / self.temperature  # (2B, 2B)\n\n        # æ©ç ï¼šå»æ‰å¯¹è§’çº¿\n        mask = torch.eye(2 * batch_size, dtype=torch.bool).to(z.device)\n        sim = sim.masked_fill(mask, -1e9)\n\n        # æ­£æ ·æœ¬ï¼šå¯¹è§’å—å¤–çš„å¯¹åº”ä½ç½®\n        positive_pairs = torch.arange(batch_size).to(z.device)\n        positive_pairs = torch.cat([\n            positive_pairs + batch_size,  # z_i çš„æ­£æ ·æœ¬æ˜¯ z_j\n            positive_pairs                 # z_j çš„æ­£æ ·æœ¬æ˜¯ z_i\n        ])\n\n        # äº¤å‰ç†µæŸå¤±\n        loss = F.cross_entropy(sim, positive_pairs)\n\n        return loss\n\n# ==================== æ•°æ®å¢å¼º ====================\n\nfrom torchvision import transforms\n\ndef get_simclr_augmentation():\n    \"\"\"SimCLR æ•°æ®å¢å¼º\"\"\"\n\n    color_jitter = transforms.ColorJitter(\n        brightness=0.8, contrast=0.8,\n        saturation=0.8, hue=0.2\n    )\n\n    return transforms.Compose([\n        transforms.RandomResizedCrop(224),\n        transforms.RandomHorizontalFlip(),\n        transforms.RandomApply([color_jitter], p=0.8),\n        transforms.RandomGrayscale(p=0.2),\n        transforms.ToTensor(),\n        transforms.Normalize(\n            mean=[0.485, 0.456, 0.406],\n            std=[0.229, 0.224, 0.225]\n        )\n    ])\n\n# ==================== è®­ç»ƒ SimCLR ====================\n\ndef train_simclr(model, dataloader, num_epochs=100):\n    \"\"\"è®­ç»ƒ SimCLR\"\"\"\n\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model.to(device)\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n    criterion = NTXentLoss(temperature=0.5)\n\n    for epoch in range(num_epochs):\n        model.train()\n        total_loss = 0\n\n        for (x_i, x_j), _ in dataloader:\n            x_i, x_j = x_i.to(device), x_j.to(device)\n\n            # å‰å‘ä¼ æ’­\n            z_i = model(x_i)\n            z_j = model(x_j)\n\n            # è®¡ç®—æŸå¤±\n            loss = criterion(z_i, z_j)\n\n            # åå‘ä¼ æ’­\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n\n        if (epoch + 1) % 10 == 0:\n            print(f'Epoch {epoch+1}: Loss = {total_loss/len(dataloader):.4f}')\n\n    return model\n\n\n\n12.5.3 ğŸ”¹ è‡ªç¼–ç å™¨ (Autoencoder)\nclass Autoencoder(nn.Module):\n    \"\"\"è‡ªç¼–ç å™¨\"\"\"\n\n    def __init__(self, input_dim, encoding_dim):\n        super(Autoencoder, self).__init__()\n\n        # ç¼–ç å™¨\n        self.encoder = nn.Sequential(\n            nn.Linear(input_dim, 512),\n            nn.ReLU(),\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Linear(256, encoding_dim)\n        )\n\n        # è§£ç å™¨\n        self.decoder = nn.Sequential(\n            nn.Linear(encoding_dim, 256),\n            nn.ReLU(),\n            nn.Linear(256, 512),\n            nn.ReLU(),\n            nn.Linear(512, input_dim),\n            nn.Sigmoid()  # è¾“å‡º [0, 1]\n        )\n\n    def forward(self, x):\n        # ç¼–ç \n        encoded = self.encoder(x)\n\n        # è§£ç \n        decoded = self.decoder(encoded)\n\n        return decoded\n\n    def encode(self, x):\n        \"\"\"ä»…ç¼–ç \"\"\"\n        return self.encoder(x)\n\n# ==================== è®­ç»ƒè‡ªç¼–ç å™¨ ====================\n\ndef train_autoencoder(model, dataloader, num_epochs=50):\n    \"\"\"è®­ç»ƒè‡ªç¼–ç å™¨\"\"\"\n\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model.to(device)\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n    criterion = nn.MSELoss()\n\n    for epoch in range(num_epochs):\n        model.train()\n        total_loss = 0\n\n        for data, _ in dataloader:\n            data = data.to(device)\n            data = data.view(data.size(0), -1)  # Flatten\n\n            # å‰å‘ä¼ æ’­\n            reconstructed = model(data)\n\n            # é‡æ„æŸå¤±\n            loss = criterion(reconstructed, data)\n\n            # åå‘ä¼ æ’­\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n\n        if (epoch + 1) % 10 == 0:\n            print(f'Epoch {epoch+1}: Loss = {total_loss/len(dataloader):.4f}')\n\n    return model\n\n# ==================== å¯è§†åŒ–é‡æ„ ====================\n\ndef visualize_reconstruction(model, dataloader, num_images=10):\n    \"\"\"å¯è§†åŒ–é‡æ„ç»“æœ\"\"\"\n\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model.eval()\n\n    # è·å–ä¸€æ‰¹æ•°æ®\n    data, _ = next(iter(dataloader))\n    data = data[:num_images].to(device)\n\n    # é‡æ„\n    with torch.no_grad():\n        data_flat = data.view(data.size(0), -1)\n        reconstructed = model(data_flat)\n        reconstructed = reconstructed.view_as(data)\n\n    # ç»˜å›¾\n    fig, axes = plt.subplots(2, num_images, figsize=(num_images*2, 4))\n\n    for i in range(num_images):\n        # åŸå›¾\n        axes[0, i].imshow(data[i].cpu().squeeze(), cmap='gray')\n        axes[0, i].axis('off')\n        if i == 0:\n            axes[0, i].set_title('Original', fontsize=12)\n\n        # é‡æ„\n        axes[1, i].imshow(reconstructed[i].cpu().squeeze(), cmap='gray')\n        axes[1, i].axis('off')\n        if i == 0:\n            axes[1, i].set_title('Reconstructed', fontsize=12)\n\n    plt.tight_layout()\n    plt.show()\n\n# ==================== ç¤ºä¾‹ ====================\n\nif __name__ == '__main__':\n    from torchvision.datasets import MNIST\n    from torch.utils.data import DataLoader\n\n    # åŠ è½½ MNIST\n    transform = transforms.Compose([\n        transforms.ToTensor()\n    ])\n\n    train_dataset = MNIST(root='./data', train=True,\n                         download=True, transform=transform)\n    train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n\n    # åˆ›å»ºè‡ªç¼–ç å™¨\n    autoencoder = Autoencoder(input_dim=784, encoding_dim=32)\n\n    # è®­ç»ƒ\n    print(\"è®­ç»ƒè‡ªç¼–ç å™¨...\")\n    train_autoencoder(autoencoder, train_loader, num_epochs=20)\n\n    # å¯è§†åŒ–\n    visualize_reconstruction(autoencoder, train_loader)\n\n\n\n12.5.4 ğŸ”¹ å˜åˆ†è‡ªç¼–ç å™¨ (VAE)\nclass VAE(nn.Module):\n    \"\"\"å˜åˆ†è‡ªç¼–ç å™¨\"\"\"\n\n    def __init__(self, input_dim, latent_dim):\n        super(VAE, self).__init__()\n\n        # ç¼–ç å™¨\n        self.fc1 = nn.Linear(input_dim, 512)\n        self.fc21 = nn.Linear(512, latent_dim)  # å‡å€¼\n        self.fc22 = nn.Linear(512, latent_dim)  # å¯¹æ•°æ–¹å·®\n\n        # è§£ç å™¨\n        self.fc3 = nn.Linear(latent_dim, 512)\n        self.fc4 = nn.Linear(512, input_dim)\n\n    def encode(self, x):\n        \"\"\"ç¼–ç ä¸ºå‡å€¼å’Œæ–¹å·®\"\"\"\n        h = F.relu(self.fc1(x))\n        mu = self.fc21(h)\n        logvar = self.fc22(h)\n        return mu, logvar\n\n    def reparameterize(self, mu, logvar):\n        \"\"\"é‡å‚æ•°åŒ–æŠ€å·§\"\"\"\n        std = torch.exp(0.5 * logvar)\n        eps = torch.randn_like(std)\n        return mu + eps * std\n\n    def decode(self, z):\n        \"\"\"è§£ç \"\"\"\n        h = F.relu(self.fc3(z))\n        return torch.sigmoid(self.fc4(h))\n\n    def forward(self, x):\n        mu, logvar = self.encode(x)\n        z = self.reparameterize(mu, logvar)\n        return self.decode(z), mu, logvar\n\ndef vae_loss(recon_x, x, mu, logvar):\n    \"\"\"VAE æŸå¤±å‡½æ•°\"\"\"\n    # é‡æ„æŸå¤±\n    recon_loss = F.binary_cross_entropy(recon_x, x, reduction='sum')\n\n    # KL æ•£åº¦\n    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n\n    return recon_loss + kl_loss\n\n# è®­ç»ƒç±»ä¼¼è‡ªç¼–ç å™¨ï¼Œä½†ä½¿ç”¨ vae_loss",
    "crumbs": [
      "ç¬¬ä¸‰éƒ¨åˆ†ï¼šå®è·µä¸åº”ç”¨",
      "<span class='chapter-number'>12</span>Â  <span class='chapter-title'>ç¬¬åä¸€ç« ï¼šæ— ç›‘ç£å­¦ä¹  (Unsupervised Learning)</span>"
    ]
  },
  {
    "objectID": "Chapter11.html#å®æˆ˜å®¢æˆ·åˆ†ç¾¤",
    "href": "Chapter11.html#å®æˆ˜å®¢æˆ·åˆ†ç¾¤",
    "title": "12Â  ç¬¬åä¸€ç« ï¼šæ— ç›‘ç£å­¦ä¹  (Unsupervised Learning)",
    "section": "12.6 11.5 å®æˆ˜ï¼šå®¢æˆ·åˆ†ç¾¤",
    "text": "12.6 11.5 å®æˆ˜ï¼šå®¢æˆ·åˆ†ç¾¤\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# ==================== ç”Ÿæˆæ¨¡æ‹Ÿå®¢æˆ·æ•°æ® ====================\n\ndef generate_customer_data(n_samples=1000):\n    \"\"\"ç”Ÿæˆæ¨¡æ‹Ÿå®¢æˆ·æ•°æ®\"\"\"\n\n    np.random.seed(42)\n\n    data = {\n        'customer_id': range(n_samples),\n        'age': np.random.randint(18, 70, n_samples),\n        'income': np.random.lognormal(10, 1, n_samples),\n        'spending_score': np.random.randint(1, 100, n_samples),\n        'num_purchases': np.random.poisson(5, n_samples),\n        'avg_purchase_value': np.random.gamma(50, 2, n_samples),\n        'days_since_last_purchase': np.random.exponential(30, n_samples)\n    }\n\n    df = pd.DataFrame(data)\n    return df\n\n# ==================== å®¢æˆ·åˆ†ç¾¤æµç¨‹ ====================\n\nclass CustomerSegmentation:\n    \"\"\"å®¢æˆ·åˆ†ç¾¤åˆ†æ\"\"\"\n\n    def __init__(self, n_clusters=4):\n        self.n_clusters = n_clusters\n        self.scaler = StandardScaler()\n        self.pca = PCA(n_components=2)\n        self.kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n\n    def preprocess(self, df):\n        \"\"\"æ•°æ®é¢„å¤„ç†\"\"\"\n        # é€‰æ‹©æ•°å€¼ç‰¹å¾\n        features = ['age', 'income', 'spending_score',\n                   'num_purchases', 'avg_purchase_value',\n                   'days_since_last_purchase']\n\n        X = df[features].values\n\n        # æ ‡å‡†åŒ–\n        X_scaled = self.scaler.fit_transform(X)\n\n        return X_scaled, features\n\n    def fit(self, df):\n        \"\"\"è®­ç»ƒåˆ†ç¾¤æ¨¡å‹\"\"\"\n        X_scaled, features = self.preprocess(df)\n\n        # K-Means èšç±»\n        labels = self.kmeans.fit_predict(X_scaled)\n\n        # PCA é™ç»´ç”¨äºå¯è§†åŒ–\n        X_pca = self.pca.fit_transform(X_scaled)\n\n        # æ·»åŠ åˆ° DataFrame\n        df['cluster'] = labels\n        df['pca1'] = X_pca[:, 0]\n        df['pca2'] = X_pca[:, 1]\n\n        return df\n\n    def analyze_clusters(self, df):\n        \"\"\"åˆ†æç°‡ç‰¹å¾\"\"\"\n        features = ['age', 'income', 'spending_score',\n                   'num_purchases', 'avg_purchase_value',\n                   'days_since_last_purchase']\n\n        print(\"\\nå„ç°‡ç»Ÿè®¡ä¿¡æ¯:\")\n        print(\"=\"*80)\n\n        for cluster_id in range(self.n_clusters):\n            cluster_data = df[df['cluster'] == cluster_id]\n\n            print(f\"\\nç°‡ {cluster_id} (n={len(cluster_data)}):\")\n            print(cluster_data[features].describe().T[['mean', 'std']])\n\n        return df.groupby('cluster')[features].mean()\n\n    def visualize(self, df):\n        \"\"\"å¯è§†åŒ–åˆ†ç¾¤ç»“æœ\"\"\"\n        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n\n        # 1. PCA ç©ºé—´ä¸­çš„ç°‡\n        axes[0, 0].scatter(df['pca1'], df['pca2'],\n                          c=df['cluster'], cmap='viridis',\n                          alpha=0.6, s=50)\n        axes[0, 0].set_xlabel('PC1')\n        axes[0, 0].set_ylabel('PC2')\n        axes[0, 0].set_title('Customer Segments in PCA Space')\n        axes[0, 0].grid(True, alpha=0.3)\n\n        # 2. å¹´é¾„ vs æ”¶å…¥\n        for cluster_id in range(self.n_clusters):\n            cluster_data = df[df['cluster'] == cluster_id]\n            axes[0, 1].scatter(cluster_data['age'],\n                             cluster_data['income'],\n                             label=f'Cluster {cluster_id}',\n                             alpha=0.6, s=30)\n\n        axes[0, 1].set_xlabel('Age')\n        axes[0, 1].set_ylabel('Income')\n        axes[0, 1].set_title('Age vs Income by Cluster')\n        axes[0, 1].legend()\n        axes[0, 1].grid(True, alpha=0.3)\n\n        # 3. æ¶ˆè´¹åˆ†æ•°åˆ†å¸ƒ\n        for cluster_id in range(self.n_clusters):\n            cluster_data = df[df['cluster'] == cluster_id]\n            axes[1, 0].hist(cluster_data['spending_score'],\n                          alpha=0.5, bins=20,\n                          label=f'Cluster {cluster_id}')\n\n        axes[1, 0].set_xlabel('Spending Score')\n        axes[1, 0].set_ylabel('Frequency')\n        axes[1, 0].set_title('Spending Score Distribution')\n        axes[1, 0].legend()\n        axes[1, 0].grid(True, alpha=0.3)\n\n        # 4. ç°‡å¤§å°\n        cluster_sizes = df['cluster'].value_counts().sort_index()\n        axes[1, 1].bar(cluster_sizes.index, cluster_sizes.values,\n                      color='steelblue', alpha=0.7)\n        axes[1, 1].set_xlabel('Cluster')\n        axes[1, 1].set_ylabel('Number of Customers')\n        axes[1, 1].set_title('Cluster Sizes')\n        axes[1, 1].grid(True, alpha=0.3, axis='y')\n\n        plt.tight_layout()\n        plt.show()\n\n    def recommend_actions(self, cluster_stats):\n        \"\"\"ä¸ºæ¯ä¸ªç°‡æ¨èè¥é”€ç­–ç•¥\"\"\"\n        print(\"\\nè¥é”€ç­–ç•¥æ¨è:\")\n        print(\"=\"*80)\n\n        for cluster_id in range(self.n_clusters):\n            stats = cluster_stats.loc[cluster_id]\n\n            print(f\"\\nç°‡ {cluster_id}:\")\n\n            if stats['income'] &gt; cluster_stats['income'].median():\n                if stats['spending_score'] &gt; cluster_stats['spending_score'].median():\n                    print(\"  ç±»å‹: é«˜ä»·å€¼å®¢æˆ· ğŸ’\")\n                    print(\"  ç­–ç•¥: VIP æœåŠ¡ã€é«˜ç«¯äº§å“æ¨è\")\n                else:\n                    print(\"  ç±»å‹: æ½œåŠ›å®¢æˆ· ğŸ“ˆ\")\n                    print(\"  ç­–ç•¥: ä¸ªæ€§åŒ–æ¨èã€ä¿ƒé”€æ´»åŠ¨\")\n            else:\n                if stats['spending_score'] &gt; cluster_stats['spending_score'].median():\n                    print(\"  ç±»å‹: æ´»è·ƒå®¢æˆ· â­\")\n                    print(\"  ç­–ç•¥: å¿ è¯šåº¦è®¡åˆ’ã€ä¼šå‘˜ä¼˜æƒ \")\n                else:\n                    print(\"  ç±»å‹: ä½æ´»è·ƒå®¢æˆ· ğŸ’¤\")\n                    print(\"  ç­–ç•¥: æ¿€æ´»campaignã€æŠ˜æ‰£ä¼˜æƒ \")\n\n# ==================== ä¸»ç¨‹åº ====================\n\nif __name__ == '__main__':\n    # ç”Ÿæˆæ•°æ®\n    df = generate_customer_data(n_samples=1000)\n\n    print(\"å®¢æˆ·æ•°æ®é¢„è§ˆ:\")\n    print(df.head())\n\n    # å®¢æˆ·åˆ†ç¾¤\n    segmentation = CustomerSegmentation(n_clusters=4)\n    df = segmentation.fit(df)\n\n    # åˆ†æç°‡\n    cluster_stats = segmentation.analyze_clusters(df)\n\n    # å¯è§†åŒ–\n    segmentation.visualize(df)\n\n    # æ¨èç­–ç•¥\n    segmentation.recommend_actions(cluster_stats)",
    "crumbs": [
      "ç¬¬ä¸‰éƒ¨åˆ†ï¼šå®è·µä¸åº”ç”¨",
      "<span class='chapter-number'>12</span>Â  <span class='chapter-title'>ç¬¬åä¸€ç« ï¼šæ— ç›‘ç£å­¦ä¹  (Unsupervised Learning)</span>"
    ]
  },
  {
    "objectID": "Chapter11.html#æœ¬ç« ä½œä¸š",
    "href": "Chapter11.html#æœ¬ç« ä½œä¸š",
    "title": "12Â  ç¬¬åä¸€ç« ï¼šæ— ç›‘ç£å­¦ä¹  (Unsupervised Learning)",
    "section": "12.7 ğŸ“ æœ¬ç« ä½œä¸š",
    "text": "12.7 ğŸ“ æœ¬ç« ä½œä¸š\n\n12.7.1 ä½œä¸š 1ï¼šèšç±»å¯¹æ¯”\n# åœ¨åŒä¸€æ•°æ®é›†ä¸Šå¯¹æ¯”ï¼š\n# 1. K-Means\n# 2. DBSCAN\n# 3. å±‚æ¬¡èšç±»\n\n# è¯„ä¼°æŒ‡æ ‡ï¼š\n#   - Silhouette Score\n#   - Davies-Bouldin Index\n#   - Calinski-Harabasz Index\n\n# åˆ†æï¼š\n#   - å“ªç§ç®—æ³•æœ€é€‚åˆä½ çš„æ•°æ®ï¼Ÿ\n#   - ä¸åŒå‚æ•°çš„å½±å“\n\n\n12.7.2 ä½œä¸š 2ï¼šé™ç»´æŠ€æœ¯å¯¹æ¯”\n# åœ¨ MNIST æˆ– Fashion-MNIST ä¸Šå¯¹æ¯”ï¼š\n# 1. PCA\n# 2. t-SNE\n# 3. UMAP\n\n# è¯„ä¼°ï¼š\n#   - å¯è§†åŒ–æ•ˆæœ\n#   - è¿è¡Œæ—¶é—´\n#   - ä¿ç•™çš„ä¿¡æ¯é‡\n#   - åœ¨é™ç»´åæ•°æ®ä¸Šè®­ç»ƒåˆ†ç±»å™¨çš„æ€§èƒ½\n\n\n12.7.3 ä½œä¸š 3ï¼šå¼‚å¸¸æ£€æµ‹\n# å®ç°å¼‚å¸¸æ£€æµ‹ç³»ç»Ÿï¼š\n# 1. ä½¿ç”¨ Autoencoder\n# 2. ä½¿ç”¨ Isolation Forest\n# 3. ä½¿ç”¨ One-Class SVM\n\n# æ•°æ®é›†ï¼šä¿¡ç”¨å¡æ¬ºè¯ˆæ£€æµ‹\n# è¯„ä¼°ï¼šROC-AUC, Precision-Recall\n\n\n12.7.4 ä½œä¸š 4ï¼šè‡ªç›‘ç£å­¦ä¹ \n# å®ç°ä¸€ä¸ªè‡ªç›‘ç£å­¦ä¹ pipelineï¼š\n# 1. é€‰æ‹©é¢„è®­ç»ƒä»»åŠ¡ï¼ˆå¦‚ SimCLRï¼‰\n# 2. åœ¨æ— æ ‡ç­¾æ•°æ®ä¸Šé¢„è®­ç»ƒ\n# 3. åœ¨å°é‡æ ‡æ³¨æ•°æ®ä¸Šå¾®è°ƒ\n# 4. å¯¹æ¯”ï¼šä»é›¶è®­ç»ƒ vs è‡ªç›‘ç£é¢„è®­ç»ƒ",
    "crumbs": [
      "ç¬¬ä¸‰éƒ¨åˆ†ï¼šå®è·µä¸åº”ç”¨",
      "<span class='chapter-number'>12</span>Â  <span class='chapter-title'>ç¬¬åä¸€ç« ï¼šæ— ç›‘ç£å­¦ä¹  (Unsupervised Learning)</span>"
    ]
  },
  {
    "objectID": "Chapter11.html#æœ¬ç« å…³é”®æ¦‚å¿µ",
    "href": "Chapter11.html#æœ¬ç« å…³é”®æ¦‚å¿µ",
    "title": "12Â  ç¬¬åä¸€ç« ï¼šæ— ç›‘ç£å­¦ä¹  (Unsupervised Learning)",
    "section": "12.8 ğŸ”‘ æœ¬ç« å…³é”®æ¦‚å¿µ",
    "text": "12.8 ğŸ”‘ æœ¬ç« å…³é”®æ¦‚å¿µ\n\n\n\næ¦‚å¿µ\nè¯´æ˜\n\n\n\n\næ— ç›‘ç£å­¦ä¹ \nä»æ— æ ‡ç­¾æ•°æ®å­¦ä¹ ç»“æ„\n\n\nèšç±»\nå°†ç›¸ä¼¼æ ·æœ¬åˆ†ç»„\n\n\nK-Means\nåŸºäºä¸­å¿ƒçš„èšç±»\n\n\nDBSCAN\nåŸºäºå¯†åº¦çš„èšç±»\n\n\nå±‚æ¬¡èšç±»\næ„å»ºèšç±»æ ‘\n\n\nPCA\nçº¿æ€§é™ç»´\n\n\nt-SNE\néçº¿æ€§é™ç»´ï¼ˆå¯è§†åŒ–ï¼‰\n\n\nUMAP\nå¿«é€Ÿéçº¿æ€§é™ç»´\n\n\nè‡ªç›‘ç£å­¦ä¹ \nè‡ªåŠ¨ç”Ÿæˆç›‘ç£ä¿¡å·\n\n\nå¯¹æ¯”å­¦ä¹ \nå­¦ä¹ ç›¸ä¼¼å’Œä¸ç›¸ä¼¼\n\n\nè‡ªç¼–ç å™¨\né‡æ„å­¦ä¹ è¡¨ç¤º",
    "crumbs": [
      "ç¬¬ä¸‰éƒ¨åˆ†ï¼šå®è·µä¸åº”ç”¨",
      "<span class='chapter-number'>12</span>Â  <span class='chapter-title'>ç¬¬åä¸€ç« ï¼šæ— ç›‘ç£å­¦ä¹  (Unsupervised Learning)</span>"
    ]
  },
  {
    "objectID": "Chapter12.html",
    "href": "Chapter12.html",
    "title": "13Â  ç¬¬åäºŒç« ï¼šå¯è§£é‡Šæ€§ä¸å¯¹æŠ—æ”»å‡»",
    "section": "",
    "text": "13.1 ğŸ“Œ ç« èŠ‚ç›®æ ‡",
    "crumbs": [
      "ç¬¬ä¸‰éƒ¨åˆ†ï¼šå®è·µä¸åº”ç”¨",
      "<span class='chapter-number'>13</span>Â  <span class='chapter-title'>ç¬¬åäºŒç« ï¼šå¯è§£é‡Šæ€§ä¸å¯¹æŠ—æ”»å‡»</span>"
    ]
  },
  {
    "objectID": "Chapter12.html#ç« èŠ‚ç›®æ ‡",
    "href": "Chapter12.html#ç« èŠ‚ç›®æ ‡",
    "title": "13Â  ç¬¬åäºŒç« ï¼šå¯è§£é‡Šæ€§ä¸å¯¹æŠ—æ”»å‡»",
    "section": "",
    "text": "ç†è§£æ·±åº¦å­¦ä¹ çš„é»‘ç›’é—®é¢˜\næŒæ¡æ¨¡å‹å¯è§£é‡Šæ€§æŠ€æœ¯\nå­¦ä¹ å¯¹æŠ—æ”»å‡»å’Œé˜²å¾¡æ–¹æ³•\näº†è§£é²æ£’æ€§å’Œå®‰å…¨æ€§\nå®æˆ˜ï¼šå¯è§†åŒ–æ¨¡å‹å†³ç­–ã€ç”Ÿæˆå¯¹æŠ—æ ·æœ¬",
    "crumbs": [
      "ç¬¬ä¸‰éƒ¨åˆ†ï¼šå®è·µä¸åº”ç”¨",
      "<span class='chapter-number'>13</span>Â  <span class='chapter-title'>ç¬¬åäºŒç« ï¼šå¯è§£é‡Šæ€§ä¸å¯¹æŠ—æ”»å‡»</span>"
    ]
  },
  {
    "objectID": "Chapter12.html#ä¸ºä»€ä¹ˆéœ€è¦å¯è§£é‡Šæ€§",
    "href": "Chapter12.html#ä¸ºä»€ä¹ˆéœ€è¦å¯è§£é‡Šæ€§",
    "title": "13Â  ç¬¬åäºŒç« ï¼šå¯è§£é‡Šæ€§ä¸å¯¹æŠ—æ”»å‡»",
    "section": "13.2 12.1 ä¸ºä»€ä¹ˆéœ€è¦å¯è§£é‡Šæ€§ï¼Ÿ",
    "text": "13.2 12.1 ä¸ºä»€ä¹ˆéœ€è¦å¯è§£é‡Šæ€§ï¼Ÿ\n\n13.2.1 ğŸ¯ é»‘ç›’é—®é¢˜\næ·±åº¦å­¦ä¹ æ¨¡å‹ = é»‘ç›’ï¼Ÿ\n\nè¾“å…¥ â†’ [ç¥ç»ç½‘ç»œ] â†’ è¾“å‡º\n        ï¼Ÿï¼Ÿï¼Ÿ\n\né—®é¢˜ï¼š\n  - ä¸ºä»€ä¹ˆåšå‡ºè¿™ä¸ªé¢„æµ‹ï¼Ÿ\n  - æ¨¡å‹å­¦åˆ°äº†ä»€ä¹ˆç‰¹å¾ï¼Ÿ\n  - å¦‚ä½•è°ƒè¯•é”™è¯¯ï¼Ÿ\n  - å¦‚ä½•å»ºç«‹ä¿¡ä»»ï¼Ÿ\n\n\n13.2.2 ğŸ“Š åº”ç”¨åœºæ™¯éœ€æ±‚\nåŒ»ç–—è¯Šæ–­ï¼š\n  \"ä¸ºä»€ä¹ˆè¯Šæ–­ä¸ºç™Œç—‡ï¼Ÿ\"\n  â†’ éœ€è¦æŒ‡å‡ºå…³é”®åŒºåŸŸ\n\né‡‘èé£æ§ï¼š\n  \"ä¸ºä»€ä¹ˆæ‹’ç»è´·æ¬¾ï¼Ÿ\"\n  â†’ æ³•å¾‹è¦æ±‚å¯è§£é‡Š\n\nè‡ªåŠ¨é©¾é©¶ï¼š\n  \"ä¸ºä»€ä¹ˆåšå‡ºè¿™ä¸ªå†³ç­–ï¼Ÿ\"\n  â†’ å®‰å…¨æ€§è¦æ±‚",
    "crumbs": [
      "ç¬¬ä¸‰éƒ¨åˆ†ï¼šå®è·µä¸åº”ç”¨",
      "<span class='chapter-number'>13</span>Â  <span class='chapter-title'>ç¬¬åäºŒç« ï¼šå¯è§£é‡Šæ€§ä¸å¯¹æŠ—æ”»å‡»</span>"
    ]
  },
  {
    "objectID": "Chapter12.html#å¯è§£é‡Šæ€§æ–¹æ³•åˆ†ç±»",
    "href": "Chapter12.html#å¯è§£é‡Šæ€§æ–¹æ³•åˆ†ç±»",
    "title": "13Â  ç¬¬åäºŒç« ï¼šå¯è§£é‡Šæ€§ä¸å¯¹æŠ—æ”»å‡»",
    "section": "13.3 12.2 å¯è§£é‡Šæ€§æ–¹æ³•åˆ†ç±»",
    "text": "13.3 12.2 å¯è§£é‡Šæ€§æ–¹æ³•åˆ†ç±»\n\n13.3.1 ğŸ“ åˆ†ç±»ç»´åº¦\n\n13.3.1.1 1. å…¨å±€ vs å±€éƒ¨\nå…¨å±€è§£é‡Š (Global Interpretation):\n  - æ¨¡å‹æ•´ä½“å¦‚ä½•å·¥ä½œ\n  - å“ªäº›ç‰¹å¾æœ€é‡è¦\n  - ä¾‹ï¼šç‰¹å¾é‡è¦æ€§\n\nå±€éƒ¨è§£é‡Š (Local Interpretation):\n  - å•ä¸ªé¢„æµ‹å¦‚ä½•äº§ç”Ÿ\n  - ä¸ºä»€ä¹ˆè¿™ä¸ªæ ·æœ¬è¢«åˆ†ç±»ä¸ºX\n  - ä¾‹ï¼šLIME, SHAP\n\n\n13.3.1.2 2. æ¨¡å‹ç‰¹å®š vs æ¨¡å‹æ— å…³\næ¨¡å‹ç‰¹å®š (Model-Specific):\n  - é’ˆå¯¹ç‰¹å®šæ¨¡å‹æ¶æ„\n  - ä¾‹ï¼šç¥ç»ç½‘ç»œçš„æ¢¯åº¦å¯è§†åŒ–\n\næ¨¡å‹æ— å…³ (Model-Agnostic):\n  - é€‚ç”¨äºä»»ä½•æ¨¡å‹\n  - ä¾‹ï¼šLIME, SHAP",
    "crumbs": [
      "ç¬¬ä¸‰éƒ¨åˆ†ï¼šå®è·µä¸åº”ç”¨",
      "<span class='chapter-number'>13</span>Â  <span class='chapter-title'>ç¬¬åäºŒç« ï¼šå¯è§£é‡Šæ€§ä¸å¯¹æŠ—æ”»å‡»</span>"
    ]
  },
  {
    "objectID": "Chapter12.html#ç‰¹å¾é‡è¦æ€§",
    "href": "Chapter12.html#ç‰¹å¾é‡è¦æ€§",
    "title": "13Â  ç¬¬åäºŒç« ï¼šå¯è§£é‡Šæ€§ä¸å¯¹æŠ—æ”»å‡»",
    "section": "13.4 12.3 ç‰¹å¾é‡è¦æ€§",
    "text": "13.4 12.3 ç‰¹å¾é‡è¦æ€§\n\n13.4.1 ğŸ”¹ æ’åˆ—é‡è¦æ€§ (Permutation Importance)\nåŸç†ï¼šæ‰“ä¹±æŸä¸ªç‰¹å¾ï¼Œçœ‹æ€§èƒ½ä¸‹é™å¤šå°‘\nimport numpy as np\nfrom sklearn.metrics import accuracy_score\nimport matplotlib.pyplot as plt\n\ndef permutation_importance(model, X, y, metric=accuracy_score, n_repeats=10):\n    \"\"\"\n    è®¡ç®—æ’åˆ—é‡è¦æ€§\n\n    å‚æ•°:\n        model: è®­ç»ƒå¥½çš„æ¨¡å‹\n        X: ç‰¹å¾çŸ©é˜µ\n        y: æ ‡ç­¾\n        metric: è¯„ä¼°æŒ‡æ ‡\n        n_repeats: é‡å¤æ¬¡æ•°\n    \"\"\"\n    # åŸºçº¿æ€§èƒ½\n    baseline_score = metric(y, model.predict(X))\n\n    importances = []\n    n_features = X.shape[1]\n\n    for feature_idx in range(n_features):\n        scores = []\n\n        for _ in range(n_repeats):\n            X_permuted = X.copy()\n\n            # æ‰“ä¹±è¯¥ç‰¹å¾\n            np.random.shuffle(X_permuted[:, feature_idx])\n\n            # è®¡ç®—æ€§èƒ½ä¸‹é™\n            permuted_score = metric(y, model.predict(X_permuted))\n            score_decrease = baseline_score - permuted_score\n            scores.append(score_decrease)\n\n        # å¹³å‡é‡è¦æ€§\n        importances.append({\n            'feature': feature_idx,\n            'importance': np.mean(scores),\n            'std': np.std(scores)\n        })\n\n    return sorted(importances, key=lambda x: x['importance'], reverse=True)\n\n# ==================== å¯è§†åŒ– ====================\n\ndef plot_feature_importance(importances, feature_names=None):\n    \"\"\"ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§\"\"\"\n\n    indices = [imp['feature'] for imp in importances]\n    values = [imp['importance'] for imp in importances]\n    stds = [imp['std'] for imp in importances]\n\n    if feature_names is None:\n        feature_names = [f'Feature {i}' for i in indices]\n    else:\n        feature_names = [feature_names[i] for i in indices]\n\n    plt.figure(figsize=(10, 6))\n    plt.barh(range(len(values)), values, xerr=stds,\n            color='steelblue', alpha=0.7)\n    plt.yticks(range(len(values)), feature_names)\n    plt.xlabel('Importance')\n    plt.title('Feature Importance (Permutation)')\n    plt.grid(True, alpha=0.3, axis='x')\n    plt.tight_layout()\n    plt.show()\n\n# ==================== ç¤ºä¾‹ ====================\n\nif __name__ == '__main__':\n    from sklearn.datasets import load_iris\n    from sklearn.ensemble import RandomForestClassifier\n    from sklearn.model_selection import train_test_split\n\n    # åŠ è½½æ•°æ®\n    iris = load_iris()\n    X, y = iris.data, iris.target\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.3, random_state=42\n    )\n\n    # è®­ç»ƒæ¨¡å‹\n    model = RandomForestClassifier(random_state=42)\n    model.fit(X_train, y_train)\n\n    # è®¡ç®—é‡è¦æ€§\n    importances = permutation_importance(model, X_test, y_test)\n\n    # å¯è§†åŒ–\n    plot_feature_importance(importances, iris.feature_names)",
    "crumbs": [
      "ç¬¬ä¸‰éƒ¨åˆ†ï¼šå®è·µä¸åº”ç”¨",
      "<span class='chapter-number'>13</span>Â  <span class='chapter-title'>ç¬¬åäºŒç« ï¼šå¯è§£é‡Šæ€§ä¸å¯¹æŠ—æ”»å‡»</span>"
    ]
  },
  {
    "objectID": "Chapter12.html#æ¢¯åº¦å¯è§†åŒ–æŠ€æœ¯",
    "href": "Chapter12.html#æ¢¯åº¦å¯è§†åŒ–æŠ€æœ¯",
    "title": "13Â  ç¬¬åäºŒç« ï¼šå¯è§£é‡Šæ€§ä¸å¯¹æŠ—æ”»å‡»",
    "section": "13.5 12.4 æ¢¯åº¦å¯è§†åŒ–æŠ€æœ¯",
    "text": "13.5 12.4 æ¢¯åº¦å¯è§†åŒ–æŠ€æœ¯\n\n13.5.1 ğŸ”¹ Saliency Mapsï¼ˆæ˜¾è‘—å›¾ï¼‰\nåŸç†ï¼šè®¡ç®—è¾“å‡ºå¯¹è¾“å…¥çš„æ¢¯åº¦\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nclass SaliencyMap:\n    \"\"\"æ˜¾è‘—å›¾ç”Ÿæˆå™¨\"\"\"\n\n    def __init__(self, model):\n        self.model = model\n        self.model.eval()\n\n    def generate(self, image, target_class=None):\n        \"\"\"\n        ç”Ÿæˆæ˜¾è‘—å›¾\n\n        å‚æ•°:\n            image: è¾“å…¥å›¾åƒ (C, H, W)\n            target_class: ç›®æ ‡ç±»åˆ«ï¼ˆNone åˆ™ä½¿ç”¨é¢„æµ‹ç±»åˆ«ï¼‰\n        \"\"\"\n        # ç¡®ä¿éœ€è¦æ¢¯åº¦\n        image = image.unsqueeze(0).requires_grad_(True)\n\n        # å‰å‘ä¼ æ’­\n        output = self.model(image)\n\n        # é€‰æ‹©ç›®æ ‡ç±»åˆ«\n        if target_class is None:\n            target_class = output.argmax(dim=1).item()\n\n        # åå‘ä¼ æ’­\n        self.model.zero_grad()\n        output[0, target_class].backward()\n\n        # è·å–æ¢¯åº¦\n        saliency = image.grad.data.abs()\n\n        # å–æœ€å¤§å€¼ä½œä¸ºæ˜¾è‘—æ€§\n        saliency = saliency.max(dim=1)[0]  # (1, H, W)\n\n        return saliency.squeeze().cpu().numpy()\n\n# ==================== å¯è§†åŒ– ====================\n\ndef visualize_saliency(image, saliency, title='Saliency Map'):\n    \"\"\"å¯è§†åŒ–æ˜¾è‘—å›¾\"\"\"\n\n    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n    # åŸå›¾\n    if image.shape[0] == 3:  # RGB\n        img_display = image.permute(1, 2, 0).cpu().numpy()\n    else:  # ç°åº¦\n        img_display = image.squeeze().cpu().numpy()\n\n    axes[0].imshow(img_display, cmap='gray' if len(img_display.shape)==2 else None)\n    axes[0].set_title('Original Image')\n    axes[0].axis('off')\n\n    # æ˜¾è‘—å›¾\n    axes[1].imshow(saliency, cmap='hot')\n    axes[1].set_title('Saliency Map')\n    axes[1].axis('off')\n\n    # å åŠ \n    axes[2].imshow(img_display, cmap='gray' if len(img_display.shape)==2 else None)\n    axes[2].imshow(saliency, cmap='hot', alpha=0.5)\n    axes[2].set_title('Overlay')\n    axes[2].axis('off')\n\n    plt.suptitle(title)\n    plt.tight_layout()\n    plt.show()\n\n\n\n13.5.2 ğŸ”¹ Grad-CAM (Gradient-weighted Class Activation Mapping)\nåŸç†ï¼šç»“åˆæ¢¯åº¦å’Œç‰¹å¾å›¾\nclass GradCAM:\n    \"\"\"Grad-CAM ç±»æ¿€æ´»æ˜ å°„\"\"\"\n\n    def __init__(self, model, target_layer):\n        self.model = model\n        self.target_layer = target_layer\n\n        self.gradients = None\n        self.activations = None\n\n        # æ³¨å†Œé’©å­\n        self.target_layer.register_forward_hook(self._save_activation)\n        self.target_layer.register_backward_hook(self._save_gradient)\n\n    def _save_activation(self, module, input, output):\n        \"\"\"ä¿å­˜æ¿€æ´»\"\"\"\n        self.activations = output.detach()\n\n    def _save_gradient(self, module, grad_input, grad_output):\n        \"\"\"ä¿å­˜æ¢¯åº¦\"\"\"\n        self.gradients = grad_output[0].detach()\n\n    def generate(self, image, target_class=None):\n        \"\"\"\n        ç”Ÿæˆ Grad-CAM\n\n        è¿”å›: CAM çƒ­åŠ›å›¾\n        \"\"\"\n        # å‰å‘ä¼ æ’­\n        output = self.model(image)\n\n        if target_class is None:\n            target_class = output.argmax(dim=1).item()\n\n        # åå‘ä¼ æ’­\n        self.model.zero_grad()\n        output[0, target_class].backward()\n\n        # è®¡ç®—æƒé‡ï¼ˆå…¨å±€å¹³å‡æ± åŒ–æ¢¯åº¦ï¼‰\n        weights = self.gradients.mean(dim=(2, 3), keepdim=True)\n\n        # åŠ æƒæ±‚å’Œ\n        cam = (weights * self.activations).sum(dim=1, keepdim=True)\n\n        # ReLU\n        cam = F.relu(cam)\n\n        # å½’ä¸€åŒ–\n        cam = cam - cam.min()\n        cam = cam / cam.max()\n\n        return cam.squeeze().cpu().numpy()\n\n# ==================== å¯è§†åŒ– Grad-CAM ====================\n\ndef visualize_gradcam(image, cam, title='Grad-CAM'):\n    \"\"\"å¯è§†åŒ– Grad-CAM\"\"\"\n\n    import cv2\n\n    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n    # åŸå›¾\n    if image.shape[0] == 3:\n        img_display = image.permute(1, 2, 0).cpu().numpy()\n    else:\n        img_display = image.squeeze().cpu().numpy()\n\n    # å½’ä¸€åŒ–åˆ° [0, 1]\n    img_display = (img_display - img_display.min()) / (img_display.max() - img_display.min())\n\n    axes[0].imshow(img_display)\n    axes[0].set_title('Original Image')\n    axes[0].axis('off')\n\n    # CAM çƒ­åŠ›å›¾\n    axes[1].imshow(cam, cmap='jet')\n    axes[1].set_title('Grad-CAM')\n    axes[1].axis('off')\n\n    # å åŠ \n    # è°ƒæ•´ CAM å¤§å°åˆ°å›¾åƒå¤§å°\n    cam_resized = cv2.resize(cam, (img_display.shape[1], img_display.shape[0]))\n\n    axes[2].imshow(img_display)\n    axes[2].imshow(cam_resized, cmap='jet', alpha=0.5)\n    axes[2].set_title('Overlay')\n    axes[2].axis('off')\n\n    plt.suptitle(title)\n    plt.tight_layout()\n    plt.show()\n\n# ==================== ä½¿ç”¨ç¤ºä¾‹ ====================\n\nif __name__ == '__main__':\n    from torchvision import models, transforms\n    from PIL import Image\n\n    # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹\n    model = models.resnet50(pretrained=True)\n    model.eval()\n\n    # ç›®æ ‡å±‚ï¼ˆResNet çš„æœ€åä¸€ä¸ªå·ç§¯å±‚ï¼‰\n    target_layer = model.layer4[-1]\n\n    # åˆ›å»º Grad-CAM\n    gradcam = GradCAM(model, target_layer)\n\n    # åŠ è½½å›¾åƒ\n    transform = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ])\n\n    image = Image.open('example.jpg')\n    image_tensor = transform(image).unsqueeze(0)\n\n    # ç”Ÿæˆ Grad-CAM\n    cam = gradcam.generate(image_tensor)\n\n    # å¯è§†åŒ–\n    visualize_gradcam(image_tensor[0], cam)",
    "crumbs": [
      "ç¬¬ä¸‰éƒ¨åˆ†ï¼šå®è·µä¸åº”ç”¨",
      "<span class='chapter-number'>13</span>Â  <span class='chapter-title'>ç¬¬åäºŒç« ï¼šå¯è§£é‡Šæ€§ä¸å¯¹æŠ—æ”»å‡»</span>"
    ]
  },
  {
    "objectID": "Chapter12.html#lime-local-interpretable-model-agnostic-explanations",
    "href": "Chapter12.html#lime-local-interpretable-model-agnostic-explanations",
    "title": "13Â  ç¬¬åäºŒç« ï¼šå¯è§£é‡Šæ€§ä¸å¯¹æŠ—æ”»å‡»",
    "section": "13.6 12.5 LIME (Local Interpretable Model-agnostic Explanations)",
    "text": "13.6 12.5 LIME (Local Interpretable Model-agnostic Explanations)\n\n13.6.1 ğŸ¯ æ ¸å¿ƒæ€æƒ³\nä¸ºå•ä¸ªé¢„æµ‹æä¾›å±€éƒ¨çº¿æ€§è§£é‡Šï¼š\n\n1. åœ¨é¢„æµ‹ç‚¹é™„è¿‘é‡‡æ ·\n2. ç”¨é»‘ç›’æ¨¡å‹é¢„æµ‹è¿™äº›æ ·æœ¬\n3. è®­ç»ƒç®€å•æ¨¡å‹ï¼ˆå¦‚çº¿æ€§æ¨¡å‹ï¼‰æ‹Ÿåˆ\n4. ç”¨ç®€å•æ¨¡å‹è§£é‡Š\n\n\n13.6.2 ğŸ’» å®ç°\nimport numpy as np\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics.pairwise import euclidean_distances\n\nclass LIME:\n    \"\"\"LIME è§£é‡Šå™¨\"\"\"\n\n    def __init__(self, kernel_width=0.25, n_samples=5000):\n        self.kernel_width = kernel_width\n        self.n_samples = n_samples\n\n    def explain_instance(self, instance, predict_fn, num_features=10):\n        \"\"\"\n        è§£é‡Šå•ä¸ªå®ä¾‹\n\n        å‚æ•°:\n            instance: è¦è§£é‡Šçš„å®ä¾‹\n            predict_fn: é¢„æµ‹å‡½æ•°\n            num_features: è¿”å›æœ€é‡è¦çš„ç‰¹å¾æ•°\n        \"\"\"\n        # åœ¨å®ä¾‹é™„è¿‘é‡‡æ ·\n        samples = self._generate_samples(instance)\n\n        # ç”¨é»‘ç›’æ¨¡å‹é¢„æµ‹\n        predictions = predict_fn(samples)\n\n        # è®¡ç®—æƒé‡ï¼ˆè·ç¦»è¶Šè¿‘æƒé‡è¶Šå¤§ï¼‰\n        distances = euclidean_distances(samples, instance.reshape(1, -1)).ravel()\n        weights = self._kernel(distances)\n\n        # è®­ç»ƒçº¿æ€§æ¨¡å‹\n        linear_model = Ridge(alpha=1.0)\n        linear_model.fit(samples, predictions, sample_weight=weights)\n\n        # è·å–ç‰¹å¾é‡è¦æ€§\n        feature_importance = linear_model.coef_\n\n        # è¿”å›æœ€é‡è¦çš„ç‰¹å¾\n        top_features = np.argsort(np.abs(feature_importance))[-num_features:][::-1]\n\n        explanation = [\n            (feature_idx, feature_importance[feature_idx])\n            for feature_idx in top_features\n        ]\n\n        return explanation\n\n    def _generate_samples(self, instance):\n        \"\"\"åœ¨å®ä¾‹é™„è¿‘ç”Ÿæˆæ ·æœ¬\"\"\"\n        n_features = len(instance)\n\n        # é«˜æ–¯æ‰°åŠ¨\n        samples = np.random.normal(\n            loc=instance,\n            scale=1.0,\n            size=(self.n_samples, n_features)\n        )\n\n        return samples\n\n    def _kernel(self, distances):\n        \"\"\"æ ¸å‡½æ•°ï¼ˆè·ç¦» â†’ æƒé‡ï¼‰\"\"\"\n        return np.exp(-(distances ** 2) / (self.kernel_width ** 2))\n\n# ==================== å›¾åƒ LIME ====================\n\nclass ImageLIME:\n    \"\"\"å›¾åƒ LIME è§£é‡Šå™¨\"\"\"\n\n    def __init__(self, n_samples=1000, n_segments=50):\n        self.n_samples = n_samples\n        self.n_segments = n_segments\n\n    def explain_instance(self, image, predict_fn, top_labels=1):\n        \"\"\"\n        è§£é‡Šå›¾åƒåˆ†ç±»\n\n        å‚æ•°:\n            image: è¾“å…¥å›¾åƒ (H, W, C)\n            predict_fn: é¢„æµ‹å‡½æ•°\n            top_labels: è§£é‡Šçš„ç±»åˆ«æ•°\n        \"\"\"\n        from skimage.segmentation import quickshift\n\n        # è¶…åƒç´ åˆ†å‰²\n        segments = quickshift(image, kernel_size=4, max_dist=200, ratio=0.2)\n        n_segments = len(np.unique(segments))\n\n        # ç”Ÿæˆæ‰°åŠ¨æ ·æœ¬\n        samples = np.zeros((self.n_samples, n_segments))\n        perturbed_images = []\n\n        for i in range(self.n_samples):\n            # éšæœºmaskä¸€äº›è¶…åƒç´ \n            active_segments = np.random.choice(\n                [0, 1], size=n_segments, p=[0.5, 0.5]\n            )\n            samples[i] = active_segments\n\n            # ç”Ÿæˆæ‰°åŠ¨å›¾åƒ\n            perturbed_image = image.copy()\n            for seg_id in range(n_segments):\n                if active_segments[seg_id] == 0:\n                    perturbed_image[segments == seg_id] = 0\n\n            perturbed_images.append(perturbed_image)\n\n        # é¢„æµ‹\n        perturbed_images = np.array(perturbed_images)\n        predictions = predict_fn(perturbed_images)\n\n        # è®­ç»ƒçº¿æ€§æ¨¡å‹\n        from sklearn.linear_model import Ridge\n\n        explanations = []\n        for label in range(top_labels):\n            linear_model = Ridge(alpha=1.0)\n            linear_model.fit(samples, predictions[:, label])\n\n            # è·å–è¶…åƒç´ é‡è¦æ€§\n            segment_importance = linear_model.coef_\n\n            explanations.append({\n                'label': label,\n                'segments': segments,\n                'importance': segment_importance\n            })\n\n        return explanations\n\n    def visualize_explanation(self, image, explanation, threshold=0.1):\n        \"\"\"å¯è§†åŒ–è§£é‡Š\"\"\"\n        segments = explanation['segments']\n        importance = explanation['importance']\n\n        # å½’ä¸€åŒ–é‡è¦æ€§\n        importance = (importance - importance.min()) / (importance.max() - importance.min())\n\n        # åˆ›å»ºæ©ç \n        mask = np.zeros(image.shape[:2])\n        for seg_id in range(len(importance)):\n            if importance[seg_id] &gt; threshold:\n                mask[segments == seg_id] = importance[seg_id]\n\n        # å¯è§†åŒ–\n        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n        axes[0].imshow(image)\n        axes[0].set_title('Original Image')\n        axes[0].axis('off')\n\n        axes[1].imshow(mask, cmap='hot')\n        axes[1].set_title('Importance Map')\n        axes[1].axis('off')\n\n        axes[2].imshow(image)\n        axes[2].imshow(mask, cmap='hot', alpha=0.5)\n        axes[2].set_title('Overlay')\n        axes[2].axis('off')\n\n        plt.tight_layout()\n        plt.show()",
    "crumbs": [
      "ç¬¬ä¸‰éƒ¨åˆ†ï¼šå®è·µä¸åº”ç”¨",
      "<span class='chapter-number'>13</span>Â  <span class='chapter-title'>ç¬¬åäºŒç« ï¼šå¯è§£é‡Šæ€§ä¸å¯¹æŠ—æ”»å‡»</span>"
    ]
  },
  {
    "objectID": "Chapter12.html#shap-shapley-additive-explanations",
    "href": "Chapter12.html#shap-shapley-additive-explanations",
    "title": "13Â  ç¬¬åäºŒç« ï¼šå¯è§£é‡Šæ€§ä¸å¯¹æŠ—æ”»å‡»",
    "section": "13.7 12.6 SHAP (SHapley Additive exPlanations)",
    "text": "13.7 12.6 SHAP (SHapley Additive exPlanations)\n\n13.7.1 ğŸ¯ æ ¸å¿ƒæ€æƒ³\nåŸºäºåšå¼ˆè®ºçš„ Shapley å€¼ï¼š\n\næ¯ä¸ªç‰¹å¾å¯¹é¢„æµ‹çš„è¾¹é™…è´¡çŒ®\n\nSHAP å€¼æ€§è´¨ï¼š\n  1. å±€éƒ¨å‡†ç¡®æ€§\n  2. ç¼ºå¤±æ€§\n  3. ä¸€è‡´æ€§\n\n\n13.7.2 ğŸ’» ä½¿ç”¨ SHAP åº“\nimport shap\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# ==================== æ ‘æ¨¡å‹ SHAP ====================\n\ndef explain_tree_model(model, X, feature_names=None):\n    \"\"\"è§£é‡Šæ ‘æ¨¡å‹\"\"\"\n\n    # åˆ›å»º SHAP è§£é‡Šå™¨\n    explainer = shap.TreeExplainer(model)\n\n    # è®¡ç®— SHAP å€¼\n    shap_values = explainer.shap_values(X)\n\n    # å¯è§†åŒ–\n    # 1. æ‘˜è¦å›¾\n    shap.summary_plot(shap_values, X, feature_names=feature_names)\n\n    # 2. å•ä¸ªæ ·æœ¬è§£é‡Š\n    shap.force_plot(\n        explainer.expected_value,\n        shap_values[0],\n        X[0],\n        feature_names=feature_names,\n        matplotlib=True\n    )\n    plt.show()\n\n    # 3. ä¾èµ–å›¾\n    if feature_names:\n        shap.dependence_plot(\n            feature_names[0],\n            shap_values,\n            X,\n            feature_names=feature_names\n        )\n\n    return shap_values\n\n# ==================== æ·±åº¦å­¦ä¹  SHAP ====================\n\ndef explain_deep_model(model, X, background_data):\n    \"\"\"è§£é‡Šæ·±åº¦å­¦ä¹ æ¨¡å‹\"\"\"\n\n    # åˆ›å»º DeepExplainer\n    explainer = shap.DeepExplainer(model, background_data)\n\n    # è®¡ç®— SHAP å€¼\n    shap_values = explainer.shap_values(X)\n\n    # å›¾åƒå¯è§†åŒ–\n    if len(X.shape) == 4:  # å›¾åƒæ•°æ®\n        shap.image_plot(shap_values, X)\n\n    return shap_values\n\n# ==================== ç¤ºä¾‹ ====================\n\nif __name__ == '__main__':\n    from sklearn.datasets import load_boston\n    from sklearn.ensemble import RandomForestRegressor\n\n    # åŠ è½½æ•°æ®\n    boston = load_boston()\n    X, y = boston.data, boston.target\n\n    # è®­ç»ƒæ¨¡å‹\n    model = RandomForestRegressor(random_state=42)\n    model.fit(X, y)\n\n    # SHAP è§£é‡Š\n    shap_values = explain_tree_model(\n        model, X[:100], feature_names=boston.feature_names\n    )",
    "crumbs": [
      "ç¬¬ä¸‰éƒ¨åˆ†ï¼šå®è·µä¸åº”ç”¨",
      "<span class='chapter-number'>13</span>Â  <span class='chapter-title'>ç¬¬åäºŒç« ï¼šå¯è§£é‡Šæ€§ä¸å¯¹æŠ—æ”»å‡»</span>"
    ]
  },
  {
    "objectID": "Chapter12.html#å¯¹æŠ—æ”»å‡»-adversarial-attacks",
    "href": "Chapter12.html#å¯¹æŠ—æ”»å‡»-adversarial-attacks",
    "title": "13Â  ç¬¬åäºŒç« ï¼šå¯è§£é‡Šæ€§ä¸å¯¹æŠ—æ”»å‡»",
    "section": "13.8 12.7 å¯¹æŠ—æ”»å‡» (Adversarial Attacks)",
    "text": "13.8 12.7 å¯¹æŠ—æ”»å‡» (Adversarial Attacks)\n\n13.8.1 ğŸ¯ ä»€ä¹ˆæ˜¯å¯¹æŠ—æ ·æœ¬ï¼Ÿ\nå¯¹æŠ—æ ·æœ¬ï¼šæ•…æ„è®¾è®¡çš„è¾“å…¥ï¼Œä½¿æ¨¡å‹äº§ç”Ÿé”™è¯¯é¢„æµ‹\n\nx_adv = x + Î´\n\nå…¶ä¸­ Î´ æ˜¯ç²¾å¿ƒè®¾è®¡çš„å¾®å°æ‰°åŠ¨ï¼Œäººçœ¼å‡ ä¹æ— æ³•å¯Ÿè§‰\nç¤ºä¾‹ï¼š\nåŸå›¾ï¼šç†ŠçŒ« â†’ é¢„æµ‹ï¼šç†ŠçŒ« (99% ç½®ä¿¡åº¦)\n     â†“ + å¾®å°å™ªå£°\nå¯¹æŠ—æ ·æœ¬ï¼šç†ŠçŒ«? â†’ é¢„æµ‹ï¼šé•¿è‡‚çŒ¿ (99% ç½®ä¿¡åº¦)\n\n\n\n13.8.2 ğŸ”¹ FGSM (Fast Gradient Sign Method)\nåŸç†ï¼šæ²¿ç€æ¢¯åº¦æ–¹å‘æ·»åŠ æ‰°åŠ¨\nx_adv = x + Îµ Â· sign(âˆ‡_x L(Î¸, x, y))\n\nÎµ: æ‰°åŠ¨å¹…åº¦\nL: æŸå¤±å‡½æ•°\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass FGSM:\n    \"\"\"Fast Gradient Sign Method æ”»å‡»\"\"\"\n\n    def __init__(self, model, epsilon=0.03):\n        self.model = model\n        self.epsilon = epsilon\n        self.model.eval()\n\n    def attack(self, images, labels):\n        \"\"\"\n        ç”Ÿæˆå¯¹æŠ—æ ·æœ¬\n\n        å‚æ•°:\n            images: åŸå§‹å›¾åƒ (batch, C, H, W)\n            labels: çœŸå®æ ‡ç­¾\n        \"\"\"\n        images = images.clone().detach().requires_grad_(True)\n\n        # å‰å‘ä¼ æ’­\n        outputs = self.model(images)\n        loss = F.cross_entropy(outputs, labels)\n\n        # åå‘ä¼ æ’­\n        self.model.zero_grad()\n        loss.backward()\n\n        # è·å–æ¢¯åº¦ç¬¦å·\n        data_grad = images.grad.data\n        sign_data_grad = data_grad.sign()\n\n        # ç”Ÿæˆå¯¹æŠ—æ ·æœ¬\n        perturbed_images = images + self.epsilon * sign_data_grad\n\n        # è£å‰ªåˆ°åˆæ³•èŒƒå›´ [0, 1]\n        perturbed_images = torch.clamp(perturbed_images, 0, 1)\n\n        return perturbed_images.detach()\n\n# ==================== å¯è§†åŒ–å¯¹æŠ—æ”»å‡» ====================\n\ndef visualize_adversarial_attack(model, image, label, epsilon=0.03):\n    \"\"\"å¯è§†åŒ–å¯¹æŠ—æ”»å‡»æ•ˆæœ\"\"\"\n\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model.to(device)\n    image = image.to(device)\n    label = label.to(device)\n\n    # åŸå§‹é¢„æµ‹\n    with torch.no_grad():\n        output_orig = model(image.unsqueeze(0))\n        pred_orig = output_orig.argmax(dim=1).item()\n        conf_orig = F.softmax(output_orig, dim=1)[0, pred_orig].item()\n\n    # ç”Ÿæˆå¯¹æŠ—æ ·æœ¬\n    fgsm = FGSM(model, epsilon=epsilon)\n    adv_image = fgsm.attack(image.unsqueeze(0), label.unsqueeze(0))\n\n    # å¯¹æŠ—æ ·æœ¬é¢„æµ‹\n    with torch.no_grad():\n        output_adv = model(adv_image)\n        pred_adv = output_adv.argmax(dim=1).item()\n        conf_adv = F.softmax(output_adv, dim=1)[0, pred_adv].item()\n\n    # æ‰°åŠ¨\n    perturbation = (adv_image - image.unsqueeze(0)).squeeze()\n\n    # å¯è§†åŒ–\n    fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n\n    # åŸå›¾\n    img_orig = image.permute(1, 2, 0).cpu().numpy()\n    axes[0].imshow(img_orig)\n    axes[0].set_title(f'Original\\nPred: {pred_orig} ({conf_orig:.2%})')\n    axes[0].axis('off')\n\n    # æ‰°åŠ¨ï¼ˆæ”¾å¤§æ˜¾ç¤ºï¼‰\n    pert_display = perturbation.permute(1, 2, 0).cpu().numpy()\n    pert_display = (pert_display - pert_display.min()) / (pert_display.max() - pert_display.min())\n    axes[1].imshow(pert_display)\n    axes[1].set_title(f'Perturbation (Ã—10)')\n    axes[1].axis('off')\n\n    # å¯¹æŠ—æ ·æœ¬\n    img_adv = adv_image.squeeze().permute(1, 2, 0).cpu().numpy()\n    axes[2].imshow(img_adv)\n    axes[2].set_title(f'Adversarial\\nPred: {pred_adv} ({conf_adv:.2%})')\n    axes[2].axis('off')\n\n    # å·®å¼‚\n    diff = np.abs(img_adv - img_orig)\n    axes[3].imshow(diff)\n    axes[3].set_title('Absolute Difference')\n    axes[3].axis('off')\n\n    plt.tight_layout()\n    plt.show()\n\n    return pred_orig == pred_adv  # æ”»å‡»æ˜¯å¦æˆåŠŸ\n\n\n\n13.8.3 ğŸ”¹ PGD (Projected Gradient Descent)\næ›´å¼ºçš„æ”»å‡»ï¼šè¿­ä»£ç‰ˆ FGSM\nclass PGD:\n    \"\"\"Projected Gradient Descent æ”»å‡»\"\"\"\n\n    def __init__(self, model, epsilon=0.03, alpha=0.01, num_iter=40):\n        self.model = model\n        self.epsilon = epsilon\n        self.alpha = alpha\n        self.num_iter = num_iter\n        self.model.eval()\n\n    def attack(self, images, labels):\n        \"\"\"ç”Ÿæˆå¯¹æŠ—æ ·æœ¬\"\"\"\n\n        # éšæœºåˆå§‹åŒ–\n        delta = torch.zeros_like(images).uniform_(-self.epsilon, self.epsilon)\n        delta.requires_grad = True\n\n        for _ in range(self.num_iter):\n            # å‰å‘ä¼ æ’­\n            outputs = self.model(images + delta)\n            loss = F.cross_entropy(outputs, labels)\n\n            # åå‘ä¼ æ’­\n            loss.backward()\n\n            # æ›´æ–°æ‰°åŠ¨\n            delta.data = delta.data + self.alpha * delta.grad.sign()\n\n            # æŠ•å½±åˆ° Îµ-çƒå†…\n            delta.data = torch.clamp(delta.data, -self.epsilon, self.epsilon)\n\n            # ç¡®ä¿åœ¨åˆæ³•èŒƒå›´å†…\n            delta.data = torch.clamp(images.data + delta.data, 0, 1) - images.data\n\n            # æ¸…ç©ºæ¢¯åº¦\n            delta.grad.zero_()\n\n        return (images + delta).detach()\n\n\n\n13.8.4 ğŸ”¹ C&W Attack (Carlini & Wagner)\næœ€ä¼˜åŒ–æ”»å‡»ï¼š\næœ€å°åŒ–: ||Î´||_2 + cÂ·loss(x+Î´, t)\n\nå…¶ä¸­ t æ˜¯ç›®æ ‡ç±»åˆ«ï¼ˆå®šå‘æ”»å‡»ï¼‰\nclass CWAttack:\n    \"\"\"Carlini & Wagner L2 æ”»å‡»\"\"\"\n\n    def __init__(self, model, c=1.0, kappa=0, learning_rate=0.01, num_iter=1000):\n        self.model = model\n        self.c = c\n        self.kappa = kappa\n        self.learning_rate = learning_rate\n        self.num_iter = num_iter\n        self.model.eval()\n\n    def attack(self, images, labels, targeted=False, target_labels=None):\n        \"\"\"\n        C&W æ”»å‡»\n\n        å‚æ•°:\n            targeted: æ˜¯å¦ä¸ºå®šå‘æ”»å‡»\n            target_labels: ç›®æ ‡ç±»åˆ«ï¼ˆå®šå‘æ”»å‡»æ—¶ä½¿ç”¨ï¼‰\n        \"\"\"\n        batch_size = images.size(0)\n\n        # ä½¿ç”¨ tanh ç©ºé—´\n        w = torch.zeros_like(images, requires_grad=True)\n        optimizer = torch.optim.Adam([w], lr=self.learning_rate)\n\n        best_adv = images.clone()\n        best_l2 = float('inf') * torch.ones(batch_size)\n\n        for iteration in range(self.num_iter):\n            # è½¬æ¢å›å›¾åƒç©ºé—´\n            adv_images = 0.5 * (torch.tanh(w) + 1)\n\n            # é¢„æµ‹\n            outputs = self.model(adv_images)\n\n            # C&W æŸå¤±\n            if targeted:\n                # å®šå‘æ”»å‡»ï¼šæœ€å¤§åŒ–ç›®æ ‡ç±»åˆ«\n                loss_adv = self._cw_loss(outputs, target_labels, targeted=True)\n            else:\n                # éå®šå‘æ”»å‡»ï¼šæœ€å°åŒ–çœŸå®ç±»åˆ«\n                loss_adv = self._cw_loss(outputs, labels, targeted=False)\n\n            # L2 è·ç¦»\n            l2_dist = torch.norm((adv_images - images).view(batch_size, -1), p=2, dim=1)\n\n            # æ€»æŸå¤±\n            loss = l2_dist.sum() + self.c * loss_adv.sum()\n\n            # ä¼˜åŒ–\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            # æ›´æ–°æœ€ä½³æ ·æœ¬\n            for i in range(batch_size):\n                if l2_dist[i] &lt; best_l2[i]:\n                    pred = outputs[i].argmax().item()\n                    if targeted:\n                        if pred == target_labels[i].item():\n                            best_l2[i] = l2_dist[i]\n                            best_adv[i] = adv_images[i]\n                    else:\n                        if pred != labels[i].item():\n                            best_l2[i] = l2_dist[i]\n                            best_adv[i] = adv_images[i]\n\n        return best_adv.detach()\n\n    def _cw_loss(self, outputs, labels, targeted):\n        \"\"\"C&W æŸå¤±å‡½æ•°\"\"\"\n        real = outputs.gather(1, labels.unsqueeze(1)).squeeze(1)\n\n        # è·å–é™¤çœŸå®ç±»åˆ«å¤–çš„æœ€å¤§ logit\n        other, _ = torch.max(outputs - 1e9 * F.one_hot(labels, outputs.size(1)), dim=1)\n\n        if targeted:\n            # å®šå‘ï¼šmax(other - real, -kappa)\n            loss = torch.clamp(other - real, min=-self.kappa)\n        else:\n            # éå®šå‘ï¼šmax(real - other, -kappa)\n            loss = torch.clamp(real - other, min=-self.kappa)\n\n        return loss",
    "crumbs": [
      "ç¬¬ä¸‰éƒ¨åˆ†ï¼šå®è·µä¸åº”ç”¨",
      "<span class='chapter-number'>13</span>Â  <span class='chapter-title'>ç¬¬åäºŒç« ï¼šå¯è§£é‡Šæ€§ä¸å¯¹æŠ—æ”»å‡»</span>"
    ]
  },
  {
    "objectID": "Chapter12.html#å¯¹æŠ—é˜²å¾¡",
    "href": "Chapter12.html#å¯¹æŠ—é˜²å¾¡",
    "title": "13Â  ç¬¬åäºŒç« ï¼šå¯è§£é‡Šæ€§ä¸å¯¹æŠ—æ”»å‡»",
    "section": "13.9 12.8 å¯¹æŠ—é˜²å¾¡",
    "text": "13.9 12.8 å¯¹æŠ—é˜²å¾¡\n\n13.9.1 ğŸ”¹ å¯¹æŠ—è®­ç»ƒ (Adversarial Training)\næœ€æœ‰æ•ˆçš„é˜²å¾¡æ–¹æ³•\ndef adversarial_training(model, train_loader, num_epochs=10, epsilon=0.03):\n    \"\"\"å¯¹æŠ—è®­ç»ƒ\"\"\"\n\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model.to(device)\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n    criterion = nn.CrossEntropyLoss()\n\n    # åˆ›å»ºæ”»å‡»å™¨\n    fgsm = FGSM(model, epsilon=epsilon)\n\n    for epoch in range(num_epochs):\n        model.train()\n        total_loss = 0\n        correct_clean = 0\n        correct_adv = 0\n        total = 0\n\n        for images, labels in train_loader:\n            images, labels = images.to(device), labels.to(device)\n\n            # ç”Ÿæˆå¯¹æŠ—æ ·æœ¬\n            adv_images = fgsm.attack(images, labels)\n\n            # åˆå¹¶å¹²å‡€æ ·æœ¬å’Œå¯¹æŠ—æ ·æœ¬\n            all_images = torch.cat([images, adv_images])\n            all_labels = torch.cat([labels, labels])\n\n            # å‰å‘ä¼ æ’­\n            outputs = model(all_images)\n            loss = criterion(outputs, all_labels)\n\n            # åå‘ä¼ æ’­\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            # ç»Ÿè®¡\n            total_loss += loss.item()\n\n            # å‡†ç¡®ç‡ï¼ˆåˆ†åˆ«ç»Ÿè®¡ï¼‰\n            outputs_clean = outputs[:len(images)]\n            outputs_adv = outputs[len(images):]\n\n            _, pred_clean = outputs_clean.max(1)\n            _, pred_adv = outputs_adv.max(1)\n\n            correct_clean += pred_clean.eq(labels).sum().item()\n            correct_adv += pred_adv.eq(labels).sum().item()\n            total += labels.size(0)\n\n        print(f'Epoch {epoch+1}: Loss={total_loss/len(train_loader):.4f}, '\n              f'Clean Acc={100.*correct_clean/total:.2f}%, '\n              f'Adv Acc={100.*correct_adv/total:.2f}%')\n\n    return model\n\n\n\n13.9.2 ğŸ”¹ è¾“å…¥å˜æ¢é˜²å¾¡\nclass InputTransformDefense:\n    \"\"\"è¾“å…¥å˜æ¢é˜²å¾¡\"\"\"\n\n    def __init__(self, model):\n        self.model = model\n        self.model.eval()\n\n    def predict_with_defense(self, images):\n        \"\"\"å¸¦é˜²å¾¡çš„é¢„æµ‹\"\"\"\n\n        # 1. JPEG å‹ç¼©\n        images_jpeg = self._jpeg_compression(images)\n\n        # 2. éšæœºè°ƒæ•´å¤§å°å’Œå¡«å……\n        images_resized = self._random_resize_pad(images_jpeg)\n\n        # 3. ä½æ·±åº¦é™ä½\n        images_quantized = self._bit_depth_reduction(images_resized)\n\n        # é¢„æµ‹\n        with torch.no_grad():\n            outputs = self.model(images_quantized)\n\n        return outputs\n\n    def _jpeg_compression(self, images, quality=75):\n        \"\"\"JPEG å‹ç¼©\"\"\"\n        from PIL import Image\n        from io import BytesIO\n\n        compressed = []\n        for img in images:\n            img_pil = transforms.ToPILImage()(img.cpu())\n            buffer = BytesIO()\n            img_pil.save(buffer, format='JPEG', quality=quality)\n            buffer.seek(0)\n            img_compressed = Image.open(buffer)\n            compressed.append(transforms.ToTensor()(img_compressed))\n\n        return torch.stack(compressed).to(images.device)\n\n    def _random_resize_pad(self, images, resize_factor=0.9):\n        \"\"\"éšæœºè°ƒæ•´å¤§å°å’Œå¡«å……\"\"\"\n        B, C, H, W = images.shape\n        new_size = int(H * resize_factor)\n\n        resized = F.interpolate(images, size=new_size, mode='bilinear')\n\n        # éšæœºä½ç½®å¡«å……\n        pad_size = H - new_size\n        pad_top = torch.randint(0, pad_size + 1, (1,)).item()\n        pad_left = torch.randint(0, pad_size + 1, (1,)).item()\n\n        padded = F.pad(resized,\n                      (pad_left, pad_size - pad_left,\n                       pad_top, pad_size - pad_top))\n\n        return padded\n\n    def _bit_depth_reduction(self, images, bits=4):\n        \"\"\"ä½æ·±åº¦é™ä½\"\"\"\n        levels = 2 ** bits\n        images_quantized = torch.round(images * (levels - 1)) / (levels - 1)\n        return images_quantized\n\n\n\n13.9.3 ğŸ”¹ é›†æˆé˜²å¾¡\nclass EnsembleDefense:\n    \"\"\"é›†æˆé˜²å¾¡\"\"\"\n\n    def __init__(self, models):\n        self.models = models\n        for model in self.models:\n            model.eval()\n\n    def predict(self, images):\n        \"\"\"é›†æˆé¢„æµ‹\"\"\"\n\n        all_outputs = []\n\n        with torch.no_grad():\n            for model in self.models:\n                outputs = model(images)\n                all_outputs.append(F.softmax(outputs, dim=1))\n\n        # å¹³å‡æ¦‚ç‡\n        ensemble_output = torch.stack(all_outputs).mean(dim=0)\n\n        return ensemble_output",
    "crumbs": [
      "ç¬¬ä¸‰éƒ¨åˆ†ï¼šå®è·µä¸åº”ç”¨",
      "<span class='chapter-number'>13</span>Â  <span class='chapter-title'>ç¬¬åäºŒç« ï¼šå¯è§£é‡Šæ€§ä¸å¯¹æŠ—æ”»å‡»</span>"
    ]
  },
  {
    "objectID": "Chapter12.html#é²æ£’æ€§è¯„ä¼°",
    "href": "Chapter12.html#é²æ£’æ€§è¯„ä¼°",
    "title": "13Â  ç¬¬åäºŒç« ï¼šå¯è§£é‡Šæ€§ä¸å¯¹æŠ—æ”»å‡»",
    "section": "13.10 12.9 é²æ£’æ€§è¯„ä¼°",
    "text": "13.10 12.9 é²æ£’æ€§è¯„ä¼°\nclass RobustnessEvaluator:\n    \"\"\"é²æ£’æ€§è¯„ä¼°å™¨\"\"\"\n\n    def __init__(self, model, test_loader, device):\n        self.model = model\n        self.test_loader = test_loader\n        self.device = device\n        self.model.eval()\n\n    def evaluate_clean_accuracy(self):\n        \"\"\"è¯„ä¼°å¹²å‡€æ ·æœ¬å‡†ç¡®ç‡\"\"\"\n        correct = 0\n        total = 0\n\n        with torch.no_grad():\n            for images, labels in self.test_loader:\n                images, labels = images.to(self.device), labels.to(self.device)\n\n                outputs = self.model(images)\n                _, predicted = outputs.max(1)\n\n                total += labels.size(0)\n                correct += predicted.eq(labels).sum().item()\n\n        accuracy = 100. * correct / total\n        print(f\"Clean Accuracy: {accuracy:.2f}%\")\n        return accuracy\n\n    def evaluate_adversarial_robustness(self, attack, epsilons=[0.01, 0.03, 0.1]):\n        \"\"\"è¯„ä¼°å¯¹æŠ—é²æ£’æ€§\"\"\"\n\n        results = {}\n\n        for epsilon in epsilons:\n            attack.epsilon = epsilon\n\n            correct = 0\n            total = 0\n\n            for images, labels in self.test_loader:\n                images, labels = images.to(self.device), labels.to(self.device)\n\n                # ç”Ÿæˆå¯¹æŠ—æ ·æœ¬\n                adv_images = attack.attack(images, labels)\n\n                # é¢„æµ‹\n                with torch.no_grad():\n                    outputs = self.model(adv_images)\n                    _, predicted = outputs.max(1)\n\n                total += labels.size(0)\n                correct += predicted.eq(labels).sum().item()\n\n            accuracy = 100. * correct / total\n            results[epsilon] = accuracy\n            print(f\"Adversarial Accuracy (Îµ={epsilon}): {accuracy:.2f}%\")\n\n        return results\n\n    def plot_robustness_curve(self, results):\n        \"\"\"ç»˜åˆ¶é²æ£’æ€§æ›²çº¿\"\"\"\n        epsilons = sorted(results.keys())\n        accuracies = [results[eps] for eps in epsilons]\n\n        plt.figure(figsize=(10, 6))\n        plt.plot(epsilons, accuracies, 'bo-', linewidth=2, markersize=8)\n        plt.xlabel('Perturbation Magnitude (Îµ)')\n        plt.ylabel('Accuracy (%)')\n        plt.title('Model Robustness to Adversarial Attacks')\n        plt.grid(True, alpha=0.3)\n        plt.tight_layout()\n        plt.show()\n\n# ==================== ä½¿ç”¨ç¤ºä¾‹ ====================\n\nif __name__ == '__main__':\n    # è¯„ä¼°æ¨¡å‹é²æ£’æ€§\n    evaluator = RobustnessEvaluator(model, test_loader, device)\n\n    # å¹²å‡€å‡†ç¡®ç‡\n    evaluator.evaluate_clean_accuracy()\n\n    # å¯¹æŠ—é²æ£’æ€§\n    fgsm = FGSM(model)\n    results = evaluator.evaluate_adversarial_robustness(\n        fgsm, epsilons=[0.0, 0.01, 0.03, 0.05, 0.1, 0.2]\n    )\n\n    # ç»˜åˆ¶æ›²çº¿\n    evaluator.plot_robustness_curve(results)",
    "crumbs": [
      "ç¬¬ä¸‰éƒ¨åˆ†ï¼šå®è·µä¸åº”ç”¨",
      "<span class='chapter-number'>13</span>Â  <span class='chapter-title'>ç¬¬åäºŒç« ï¼šå¯è§£é‡Šæ€§ä¸å¯¹æŠ—æ”»å‡»</span>"
    ]
  },
  {
    "objectID": "Chapter12.html#æœ¬ç« ä½œä¸š",
    "href": "Chapter12.html#æœ¬ç« ä½œä¸š",
    "title": "13Â  ç¬¬åäºŒç« ï¼šå¯è§£é‡Šæ€§ä¸å¯¹æŠ—æ”»å‡»",
    "section": "13.11 ğŸ“ æœ¬ç« ä½œä¸š",
    "text": "13.11 ğŸ“ æœ¬ç« ä½œä¸š\n\n13.11.1 ä½œä¸š 1ï¼šæ¨¡å‹å¯è§£é‡Šæ€§\n# åœ¨å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸Šï¼š\n# 1. å®ç° Saliency Map\n# 2. å®ç° Grad-CAM\n# 3. ä½¿ç”¨ LIME è§£é‡Šé¢„æµ‹\n# 4. å¯¹æ¯”ä¸‰ç§æ–¹æ³•çš„ç»“æœ\n# 5. åˆ†ææ¨¡å‹å…³æ³¨çš„åŒºåŸŸæ˜¯å¦åˆç†\n\n\n13.11.2 ä½œä¸š 2ï¼šå¯¹æŠ—æ”»å‡»å®éªŒ\n# å®ç°å¹¶å¯¹æ¯”ï¼š\n# 1. FGSM\n# 2. PGD\n# 3. C&W\n#\n# è¯„ä¼°ï¼š\n#   - æ”»å‡»æˆåŠŸç‡\n#   - æ‰°åŠ¨å¤§å°ï¼ˆL2, Lâˆï¼‰\n#   - å¯æ„ŸçŸ¥æ€§\n#   - è®¡ç®—æ—¶é—´\n\n\n13.11.3 ä½œä¸š 3ï¼šå¯¹æŠ—é˜²å¾¡\n# å®ç°å¹¶è¯„ä¼°é˜²å¾¡æ–¹æ³•ï¼š\n# 1. å¯¹æŠ—è®­ç»ƒ\n# 2. è¾“å…¥å˜æ¢\n# 3. é›†æˆé˜²å¾¡\n#\n# å¯¹æ¯”ï¼š\n#   - å¹²å‡€æ ·æœ¬å‡†ç¡®ç‡\n#   - å¯¹æŠ—æ ·æœ¬å‡†ç¡®ç‡\n#   - é²æ£’æ€§ vs å‡†ç¡®æ€§ trade-off\n\n\n13.11.4 ä½œä¸š 4ï¼šå¯ä¿¡ AI ç³»ç»Ÿ\n# è®¾è®¡ä¸€ä¸ªå¯ä¿¡ AI ç³»ç»Ÿï¼š\n# 1. æä¾›é¢„æµ‹è§£é‡Š\n# 2. è¯„ä¼°é¢„æµ‹ç½®ä¿¡åº¦\n# 3. æ£€æµ‹å¯¹æŠ—æ ·æœ¬\n# 4. æä¾›ä¸ç¡®å®šæ€§ä¼°è®¡\n#\n# åœ¨åŒ»ç–—æˆ–é‡‘èåœºæ™¯ä¸­æµ‹è¯•",
    "crumbs": [
      "ç¬¬ä¸‰éƒ¨åˆ†ï¼šå®è·µä¸åº”ç”¨",
      "<span class='chapter-number'>13</span>Â  <span class='chapter-title'>ç¬¬åäºŒç« ï¼šå¯è§£é‡Šæ€§ä¸å¯¹æŠ—æ”»å‡»</span>"
    ]
  },
  {
    "objectID": "Chapter12.html#æœ¬ç« å…³é”®æ¦‚å¿µ",
    "href": "Chapter12.html#æœ¬ç« å…³é”®æ¦‚å¿µ",
    "title": "13Â  ç¬¬åäºŒç« ï¼šå¯è§£é‡Šæ€§ä¸å¯¹æŠ—æ”»å‡»",
    "section": "13.12 ğŸ”‘ æœ¬ç« å…³é”®æ¦‚å¿µ",
    "text": "13.12 ğŸ”‘ æœ¬ç« å…³é”®æ¦‚å¿µ\n\n\n\næ¦‚å¿µ\nè¯´æ˜\n\n\n\n\nå¯è§£é‡Šæ€§\nç†è§£æ¨¡å‹å†³ç­–è¿‡ç¨‹\n\n\nSaliency Map\nåŸºäºæ¢¯åº¦çš„æ˜¾è‘—æ€§\n\n\nGrad-CAM\nç±»æ¿€æ´»æ˜ å°„\n\n\nLIME\nå±€éƒ¨çº¿æ€§è§£é‡Š\n\n\nSHAP\nShapley å€¼è§£é‡Š\n\n\nå¯¹æŠ—æ ·æœ¬\nç²¾å¿ƒè®¾è®¡çš„æ‰°åŠ¨è¾“å…¥\n\n\nFGSM\nå¿«é€Ÿæ¢¯åº¦ç¬¦å·æ”»å‡»\n\n\nPGD\næŠ•å½±æ¢¯åº¦ä¸‹é™æ”»å‡»\n\n\nC&W\nä¼˜åŒ–æ”»å‡»\n\n\nå¯¹æŠ—è®­ç»ƒ\næœ€æœ‰æ•ˆçš„é˜²å¾¡\n\n\né²æ£’æ€§\nå¯¹æ‰°åŠ¨çš„æŠµæŠ—èƒ½åŠ›\n\n\n\n\néœ€è¦æˆ‘ç»§ç»­å†™ç¬¬åä¸‰ç« ï¼šå¤§è¯­è¨€æ¨¡å‹æ—¶ä»£å—ï¼Ÿè¿™å°†æ˜¯æœ€åä¸€ç« ï¼Œæ¶µç›– LLMã€Prompt Engineeringã€In-Context Learning ç­‰å‰æ²¿è¯é¢˜ã€‚",
    "crumbs": [
      "ç¬¬ä¸‰éƒ¨åˆ†ï¼šå®è·µä¸åº”ç”¨",
      "<span class='chapter-number'>13</span>Â  <span class='chapter-title'>ç¬¬åäºŒç« ï¼šå¯è§£é‡Šæ€§ä¸å¯¹æŠ—æ”»å‡»</span>"
    ]
  },
  {
    "objectID": "Chapter13.html",
    "href": "Chapter13.html",
    "title": "14Â  ç¬¬åä¸‰ç« ï¼šå¤§è¯­è¨€æ¨¡å‹æ—¶ä»£",
    "section": "",
    "text": "14.1 ğŸ“Œ ç« èŠ‚ç›®æ ‡",
    "crumbs": [
      "ç¬¬ä¸‰éƒ¨åˆ†ï¼šå®è·µä¸åº”ç”¨",
      "<span class='chapter-number'>14</span>Â  <span class='chapter-title'>ç¬¬åä¸‰ç« ï¼šå¤§è¯­è¨€æ¨¡å‹æ—¶ä»£</span>"
    ]
  },
  {
    "objectID": "Chapter13.html#ç« èŠ‚ç›®æ ‡",
    "href": "Chapter13.html#ç« èŠ‚ç›®æ ‡",
    "title": "14Â  ç¬¬åä¸‰ç« ï¼šå¤§è¯­è¨€æ¨¡å‹æ—¶ä»£",
    "section": "",
    "text": "ç†è§£å¤§è¯­è¨€æ¨¡å‹ (LLM) çš„æ ¸å¿ƒåŸç†\næŒæ¡ Prompt Engineering æŠ€å·§\nå­¦ä¹  In-Context Learning å’Œ Few-Shot Learning\näº†è§£ LLM çš„å¾®è°ƒæ–¹æ³•ï¼ˆLoRA, PEFTï¼‰\næ¢ç´¢ LLM çš„åº”ç”¨å’Œæœªæ¥æ–¹å‘",
    "crumbs": [
      "ç¬¬ä¸‰éƒ¨åˆ†ï¼šå®è·µä¸åº”ç”¨",
      "<span class='chapter-number'>14</span>Â  <span class='chapter-title'>ç¬¬åä¸‰ç« ï¼šå¤§è¯­è¨€æ¨¡å‹æ—¶ä»£</span>"
    ]
  },
  {
    "objectID": "Chapter13.html#ä»-gpt-åˆ°-chatgptå¤§è¯­è¨€æ¨¡å‹çš„æ¼”è¿›",
    "href": "Chapter13.html#ä»-gpt-åˆ°-chatgptå¤§è¯­è¨€æ¨¡å‹çš„æ¼”è¿›",
    "title": "14Â  ç¬¬åä¸‰ç« ï¼šå¤§è¯­è¨€æ¨¡å‹æ—¶ä»£",
    "section": "14.2 13.1 ä» GPT åˆ° ChatGPTï¼šå¤§è¯­è¨€æ¨¡å‹çš„æ¼”è¿›",
    "text": "14.2 13.1 ä» GPT åˆ° ChatGPTï¼šå¤§è¯­è¨€æ¨¡å‹çš„æ¼”è¿›\n\n14.2.1 ğŸŒŸ å…³é”®é‡Œç¨‹ç¢‘\n2017: Transformer (Attention Is All You Need)\n      â†“\n2018: GPT-1 (117M å‚æ•°)\n      BERT (340M å‚æ•°)\n      â†“\n2019: GPT-2 (1.5B å‚æ•°)\n      T5, BART, XLNet\n      â†“\n2020: GPT-3 (175B å‚æ•°) ğŸ‘‘\n      - Few-shot learning\n      - In-context learning\n      â†“\n2022: ChatGPT (GPT-3.5 + RLHF)\n      InstructGPT\n      â†“\n2023: GPT-4 (å¤šæ¨¡æ€)\n      Claude, LLaMA, PaLM\n      â†“\n2024: Gemini, Claude 3\n      å¼€æºæ¨¡å‹çˆ†å‘\n\n\n\n14.2.2 ğŸ“ æ ¸å¿ƒèƒ½åŠ›çš„æ¶Œç°\nè§„æ¨¡å®šå¾‹ (Scaling Laws)ï¼š\næ€§èƒ½ âˆ log(æ¨¡å‹å¤§å° Ã— æ•°æ®é‡ Ã— è®¡ç®—é‡)\n\næ¶Œç°èƒ½åŠ› (Emergent Abilities):\n  - å°‘æ ·æœ¬å­¦ä¹ \n  - æŒ‡ä»¤éµå¾ª\n  - æ€ç»´é“¾æ¨ç†\n  - ä»£ç ç”Ÿæˆ\n  - å¤šæ­¥æ¨ç†",
    "crumbs": [
      "ç¬¬ä¸‰éƒ¨åˆ†ï¼šå®è·µä¸åº”ç”¨",
      "<span class='chapter-number'>14</span>Â  <span class='chapter-title'>ç¬¬åä¸‰ç« ï¼šå¤§è¯­è¨€æ¨¡å‹æ—¶ä»£</span>"
    ]
  },
  {
    "objectID": "Chapter13.html#llm-çš„æ¶æ„åŸç†",
    "href": "Chapter13.html#llm-çš„æ¶æ„åŸç†",
    "title": "14Â  ç¬¬åä¸‰ç« ï¼šå¤§è¯­è¨€æ¨¡å‹æ—¶ä»£",
    "section": "14.3 13.2 LLM çš„æ¶æ„åŸç†",
    "text": "14.3 13.2 LLM çš„æ¶æ„åŸç†\n\n14.3.1 ğŸ—ï¸ Transformer å›é¡¾\nè¾“å…¥ Token\n    â†“\n[Embedding + Positional Encoding]\n    â†“\n[Transformer Block] Ã—N\n  - Multi-Head Attention\n  - Feed-Forward Network\n  - Layer Normalization\n  - Residual Connection\n    â†“\n[Language Model Head]\n    â†“\nä¸‹ä¸€ä¸ª Token çš„æ¦‚ç‡åˆ†å¸ƒ\n\n\n14.3.2 ğŸ”¹ GPT æ¶æ„ç‰¹ç‚¹\nåªç”¨ Decoderï¼ˆè‡ªå›å½’ï¼‰ï¼š\nclass GPTBlock(nn.Module):\n    \"\"\"GPT Transformer å—\"\"\"\n\n    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n        super().__init__()\n\n        # Causal Self-Attentionï¼ˆåªçœ‹å‰æ–‡ï¼‰\n        self.attn = MultiHeadAttention(d_model, n_heads, dropout, causal=True)\n\n        # Feed-Forward\n        self.ff = nn.Sequential(\n            nn.Linear(d_model, d_ff),\n            nn.GELU(),\n            nn.Linear(d_ff, d_model),\n            nn.Dropout(dropout)\n        )\n\n        # Layer Norm\n        self.ln1 = nn.LayerNorm(d_model)\n        self.ln2 = nn.LayerNorm(d_model)\n\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, mask=None):\n        # Pre-LN æ¶æ„\n        x = x + self.attn(self.ln1(x), mask)\n        x = x + self.ff(self.ln2(x))\n        return x\n\nclass GPTModel(nn.Module):\n    \"\"\"ç®€åŒ–çš„ GPT æ¨¡å‹\"\"\"\n\n    def __init__(self, vocab_size, d_model=768, n_heads=12,\n                 n_layers=12, d_ff=3072, max_len=1024, dropout=0.1):\n        super().__init__()\n\n        # Token Embedding\n        self.token_emb = nn.Embedding(vocab_size, d_model)\n\n        # Positional Embeddingï¼ˆå¯å­¦ä¹ ï¼‰\n        self.pos_emb = nn.Embedding(max_len, d_model)\n\n        # Transformer Blocks\n        self.blocks = nn.ModuleList([\n            GPTBlock(d_model, n_heads, d_ff, dropout)\n            for _ in range(n_layers)\n        ])\n\n        # Final Layer Norm\n        self.ln_f = nn.LayerNorm(d_model)\n\n        # Language Model Head\n        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)\n\n        # æƒé‡å…±äº«ï¼ˆembedding å’Œ lm_headï¼‰\n        self.lm_head.weight = self.token_emb.weight\n\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, input_ids, labels=None):\n        \"\"\"\n        å‚æ•°:\n            input_ids: (batch, seq_len)\n            labels: (batch, seq_len) å¯é€‰ï¼Œç”¨äºè®­ç»ƒ\n        \"\"\"\n        batch_size, seq_len = input_ids.shape\n\n        # Embedding\n        token_embeddings = self.token_emb(input_ids)  # (B, T, D)\n\n        # Positional Embedding\n        positions = torch.arange(0, seq_len, device=input_ids.device)\n        position_embeddings = self.pos_emb(positions)  # (T, D)\n\n        x = self.dropout(token_embeddings + position_embeddings)\n\n        # Causal Maskï¼ˆä¸‹ä¸‰è§’çŸ©é˜µï¼‰\n        mask = torch.tril(torch.ones(seq_len, seq_len, device=input_ids.device))\n        mask = mask.view(1, 1, seq_len, seq_len)\n\n        # Transformer Blocks\n        for block in self.blocks:\n            x = block(x, mask)\n\n        x = self.ln_f(x)\n\n        # Logits\n        logits = self.lm_head(x)  # (B, T, vocab_size)\n\n        # è®¡ç®—æŸå¤±ï¼ˆå¦‚æœæä¾›äº†æ ‡ç­¾ï¼‰\n        loss = None\n        if labels is not None:\n            # ç§»ä½ï¼šé¢„æµ‹ä¸‹ä¸€ä¸ª token\n            shift_logits = logits[:, :-1, :].contiguous()\n            shift_labels = labels[:, 1:].contiguous()\n\n            loss = F.cross_entropy(\n                shift_logits.view(-1, shift_logits.size(-1)),\n                shift_labels.view(-1),\n                ignore_index=-100\n            )\n\n        return logits, loss\n\n    def generate(self, input_ids, max_new_tokens=100,\n                temperature=1.0, top_k=None, top_p=None):\n        \"\"\"\n        è‡ªå›å½’ç”Ÿæˆ\n\n        å‚æ•°:\n            input_ids: (batch, seq_len) è¾“å…¥åºåˆ—\n            max_new_tokens: ç”Ÿæˆçš„æœ€å¤§ token æ•°\n            temperature: æ¸©åº¦å‚æ•°ï¼ˆæ§åˆ¶éšæœºæ€§ï¼‰\n            top_k: Top-K é‡‡æ ·\n            top_p: Nucleus (Top-P) é‡‡æ ·\n        \"\"\"\n        for _ in range(max_new_tokens):\n            # æˆªæ–­åˆ°æœ€å¤§é•¿åº¦ï¼ˆé¿å…è¶…è¿‡ä½ç½®ç¼–ç ï¼‰\n            input_ids_cond = input_ids if input_ids.size(1) &lt;= 1024 else input_ids[:, -1024:]\n\n            # å‰å‘ä¼ æ’­\n            logits, _ = self.forward(input_ids_cond)\n\n            # è·å–æœ€åä¸€ä¸ªä½ç½®çš„ logits\n            logits = logits[:, -1, :] / temperature\n\n            # Top-K é‡‡æ ·\n            if top_k is not None:\n                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n                logits[logits &lt; v[:, [-1]]] = -float('Inf')\n\n            # Top-P (Nucleus) é‡‡æ ·\n            if top_p is not None:\n                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n                cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n\n                # ç§»é™¤ç´¯ç§¯æ¦‚ç‡è¶…è¿‡ top_p çš„ tokens\n                sorted_indices_to_remove = cumulative_probs &gt; top_p\n                sorted_indices_to_remove[:, 1:] = sorted_indices_to_remove[:, :-1].clone()\n                sorted_indices_to_remove[:, 0] = 0\n\n                for i in range(logits.size(0)):\n                    indices_to_remove = sorted_indices[i, sorted_indices_to_remove[i]]\n                    logits[i, indices_to_remove] = -float('Inf')\n\n            # é‡‡æ ·\n            probs = F.softmax(logits, dim=-1)\n            next_token = torch.multinomial(probs, num_samples=1)\n\n            # æ‹¼æ¥\n            input_ids = torch.cat([input_ids, next_token], dim=1)\n\n        return input_ids",
    "crumbs": [
      "ç¬¬ä¸‰éƒ¨åˆ†ï¼šå®è·µä¸åº”ç”¨",
      "<span class='chapter-number'>14</span>Â  <span class='chapter-title'>ç¬¬åä¸‰ç« ï¼šå¤§è¯­è¨€æ¨¡å‹æ—¶ä»£</span>"
    ]
  },
  {
    "objectID": "Chapter13.html#é¢„è®­ç»ƒè‡ªç›‘ç£å­¦ä¹ ",
    "href": "Chapter13.html#é¢„è®­ç»ƒè‡ªç›‘ç£å­¦ä¹ ",
    "title": "14Â  ç¬¬åä¸‰ç« ï¼šå¤§è¯­è¨€æ¨¡å‹æ—¶ä»£",
    "section": "14.4 13.3 é¢„è®­ç»ƒï¼šè‡ªç›‘ç£å­¦ä¹ ",
    "text": "14.4 13.3 é¢„è®­ç»ƒï¼šè‡ªç›‘ç£å­¦ä¹ \n\n14.4.1 ğŸ¯ é¢„è®­ç»ƒä»»åŠ¡\nè¯­è¨€æ¨¡å‹ç›®æ ‡ï¼š\n  ç»™å®šä¸Šæ–‡ xâ‚, xâ‚‚, ..., x_{t-1}ï¼Œé¢„æµ‹ x_t\n\n  P(x_t | xâ‚, ..., x_{t-1})\n\næŸå¤±å‡½æ•°ï¼š\n  L = -âˆ‘_t log P(x_t | xâ‚, ..., x_{t-1})\n\n\n14.4.2 ğŸ’» é¢„è®­ç»ƒæµç¨‹\ndef train_gpt(model, dataloader, num_epochs=10):\n    \"\"\"é¢„è®­ç»ƒ GPT\"\"\"\n\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model.to(device)\n\n    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4,\n                                  betas=(0.9, 0.95), weight_decay=0.1)\n\n    # å­¦ä¹ ç‡è°ƒåº¦ï¼ˆwarmup + cosine decayï¼‰\n    def get_lr(step, warmup_steps=2000, max_steps=100000):\n        if step &lt; warmup_steps:\n            return step / warmup_steps\n        return 0.5 * (1 + np.cos(np.pi * (step - warmup_steps) / (max_steps - warmup_steps)))\n\n    scheduler = torch.optim.lr_scheduler.LambdaLR(\n        optimizer, lr_lambda=lambda step: get_lr(step)\n    )\n\n    step = 0\n    for epoch in range(num_epochs):\n        model.train()\n        total_loss = 0\n\n        for batch in dataloader:\n            input_ids = batch['input_ids'].to(device)\n            labels = input_ids.clone()\n\n            # å‰å‘ä¼ æ’­\n            logits, loss = model(input_ids, labels)\n\n            # åå‘ä¼ æ’­\n            optimizer.zero_grad()\n            loss.backward()\n\n            # æ¢¯åº¦è£å‰ª\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n\n            optimizer.step()\n            scheduler.step()\n\n            total_loss += loss.item()\n            step += 1\n\n            if step % 100 == 0:\n                print(f'Step {step}: Loss = {loss.item():.4f}, '\n                      f'LR = {scheduler.get_last_lr()[0]:.6f}')\n\n        avg_loss = total_loss / len(dataloader)\n        print(f'Epoch {epoch+1}: Avg Loss = {avg_loss:.4f}')\n\n    return model",
    "crumbs": [
      "ç¬¬ä¸‰éƒ¨åˆ†ï¼šå®è·µä¸åº”ç”¨",
      "<span class='chapter-number'>14</span>Â  <span class='chapter-title'>ç¬¬åä¸‰ç« ï¼šå¤§è¯­è¨€æ¨¡å‹æ—¶ä»£</span>"
    ]
  },
  {
    "objectID": "Chapter13.html#prompt-engineering",
    "href": "Chapter13.html#prompt-engineering",
    "title": "14Â  ç¬¬åä¸‰ç« ï¼šå¤§è¯­è¨€æ¨¡å‹æ—¶ä»£",
    "section": "14.5 13.4 Prompt Engineering",
    "text": "14.5 13.4 Prompt Engineering\n\n14.5.1 ğŸ¨ ä»€ä¹ˆæ˜¯ Promptï¼Ÿ\nPrompt = ç»™æ¨¡å‹çš„æŒ‡ä»¤/ç¤ºä¾‹\n\nä¾‹ï¼š\n  è¾“å…¥ï¼š\"å°†ä¸‹é¢çš„è‹±æ–‡ç¿»è¯‘æˆä¸­æ–‡ï¼š\\nHello, world!\"\n  æ¨¡å‹ï¼šæ ¹æ® prompt ç†è§£ä»»åŠ¡ï¼Œç”Ÿæˆç¿»è¯‘\n\n\n14.5.2 ğŸ“Š Prompt è®¾è®¡åŸåˆ™\n\n14.5.2.1 1. æ¸…æ™°æ˜ç¡®\nâŒ å·®çš„ Prompt:\n  \"å…³äº AI\"\n\nâœ… å¥½çš„ Prompt:\n  \"è¯·ç”¨ 200 å­—ä»‹ç»äººå·¥æ™ºèƒ½çš„å®šä¹‰ã€å‘å±•å†ç¨‹å’Œä¸»è¦åº”ç”¨é¢†åŸŸã€‚\"\n\n\n14.5.2.2 2. æä¾›ä¸Šä¸‹æ–‡\nâŒ æ— ä¸Šä¸‹æ–‡:\n  \"è¿™ä¸ªæ€ä¹ˆæ ·ï¼Ÿ\"\n\nâœ… æœ‰ä¸Šä¸‹æ–‡:\n  \"æˆ‘æ­£åœ¨å†™ä¸€ç¯‡å…³äºæ°”å€™å˜åŒ–çš„æ–‡ç« ã€‚ä»¥ä¸‹æ˜¯è‰ç¨¿çš„ç¬¬ä¸€æ®µï¼š\n  [æ®µè½å†…å®¹]\n  è¯·è¯„ä»·è¿™æ®µå†…å®¹çš„é€»è¾‘æ€§å’Œè¯´æœåŠ›ã€‚\"\n\n\n14.5.2.3 3. ä½¿ç”¨ç¤ºä¾‹ï¼ˆFew-shotï¼‰\nZero-shotï¼ˆæ— ç¤ºä¾‹ï¼‰:\n  \"æƒ…æ„Ÿåˆ†ç±»ï¼šè¿™éƒ¨ç”µå½±å¾ˆå¥½çœ‹\"\n\nFew-shotï¼ˆæœ‰ç¤ºä¾‹ï¼‰:\n  \"\"\"\n  æƒ…æ„Ÿåˆ†ç±»ä»»åŠ¡ï¼š\n\n  ç¤ºä¾‹ï¼š\n  æ–‡æœ¬: \"è¿™å®¶é¤å…å¤ªå·®äº†\" â†’ è´Ÿé¢\n  æ–‡æœ¬: \"æœåŠ¡æ€åº¦å¾ˆå¥½\" â†’ æ­£é¢\n  æ–‡æœ¬: \"è¿˜è¡Œå§\" â†’ ä¸­æ€§\n\n  ç°åœ¨åˆ†ç±»ï¼š\n  æ–‡æœ¬: \"è¿™éƒ¨ç”µå½±å¾ˆå¥½çœ‹\" â†’\n  \"\"\"\n\n\n14.5.2.4 4. æ€ç»´é“¾ï¼ˆChain-of-Thoughtï¼‰\næ™®é€š Prompt:\n  \"Roger æœ‰ 5 ä¸ªç½‘çƒã€‚ä»–åˆä¹°äº† 2 ç½ç½‘çƒï¼Œæ¯ç½ 3 ä¸ªçƒã€‚\n   ä»–ç°åœ¨æœ‰å¤šå°‘ä¸ªç½‘çƒï¼Ÿ\"\n\nCoT Prompt:\n  \"Roger æœ‰ 5 ä¸ªç½‘çƒã€‚ä»–åˆä¹°äº† 2 ç½ç½‘çƒï¼Œæ¯ç½ 3 ä¸ªçƒã€‚\n   ä»–ç°åœ¨æœ‰å¤šå°‘ä¸ªç½‘çƒï¼Ÿ\n\n   è®©æˆ‘ä»¬ä¸€æ­¥æ­¥æ€è€ƒï¼š\n   1. Roger æœ€åˆæœ‰ 5 ä¸ªç½‘çƒ\n   2. ä»–ä¹°äº† 2 ç½ï¼Œæ¯ç½ 3 ä¸ªï¼Œæ‰€ä»¥ä¹°äº† 2Ã—3=6 ä¸ª\n   3. æ€»å…±ï¼š5+6=11 ä¸ª\n\n   ç­”æ¡ˆï¼š11 ä¸ªç½‘çƒ\"\n\n\n\n\n14.5.3 ğŸ’» Prompt æ¨¡æ¿ç¤ºä¾‹\nclass PromptTemplate:\n    \"\"\"Prompt æ¨¡æ¿ç®¡ç†\"\"\"\n\n    def __init__(self):\n        self.templates = {\n            'translation': \"\"\"\nTranslate the following {source_lang} text to {target_lang}:\n\nText: {text}\n\nTranslation:\"\"\",\n\n            'summarization': \"\"\"\nSummarize the following text in {num_sentences} sentences:\n\n{text}\n\nSummary:\"\"\",\n\n            'classification': \"\"\"\nClassify the sentiment of the following text as Positive, Negative, or Neutral.\n\nExamples:\n{examples}\n\nText: {text}\nSentiment:\"\"\",\n\n            'qa': \"\"\"\nAnswer the following question based on the context.\n\nContext: {context}\n\nQuestion: {question}\n\nAnswer:\"\"\",\n\n            'cot_reasoning': \"\"\"\nQuestion: {question}\n\nLet's think step by step:\n\"\"\",\n        }\n\n    def format(self, template_name, **kwargs):\n        \"\"\"æ ¼å¼åŒ–æ¨¡æ¿\"\"\"\n        template = self.templates[template_name]\n        return template.format(**kwargs)\n\n# ==================== ä½¿ç”¨ç¤ºä¾‹ ====================\n\nprompt_template = PromptTemplate()\n\n# ç¿»è¯‘\ntranslation_prompt = prompt_template.format(\n    'translation',\n    source_lang='English',\n    target_lang='Chinese',\n    text='Hello, how are you?'\n)\n\n# Few-shot åˆ†ç±»\nexamples = \"\"\"\nText: \"This movie is amazing!\" â†’ Positive\nText: \"Waste of time.\" â†’ Negative\nText: \"It's okay.\" â†’ Neutral\"\"\"\n\nclassification_prompt = prompt_template.format(\n    'classification',\n    examples=examples,\n    text='I love this product!'\n)\n\nprint(classification_prompt)\n\n\n\n14.5.4 ğŸ”§ é«˜çº§ Prompt æŠ€å·§\n\n14.5.4.1 1. è§’è‰²æ‰®æ¼” (Role-Playing)\n\"ä½ æ˜¯ä¸€ä½èµ„æ·±çš„ Python ç¨‹åºå‘˜ã€‚è¯·å¸®æˆ‘ä¼˜åŒ–ä»¥ä¸‹ä»£ç ï¼š\n[ä»£ç ]\"\n\n\n14.5.4.2 2. çº¦æŸæ¡ä»¶\n\"ç”¨ç®€å•æ˜“æ‡‚çš„è¯­è¨€ï¼ˆé€‚åˆ 10 å²å„¿ç«¥ç†è§£ï¼‰è§£é‡Šä»€ä¹ˆæ˜¯ç¥ç»ç½‘ç»œã€‚\nè¦æ±‚ï¼š\n- ä¸è¶…è¿‡ 3 æ®µ\n- ä½¿ç”¨æ—¥å¸¸ç”Ÿæ´»çš„æ¯”å–»\n- é¿å…ä¸“ä¸šæœ¯è¯­\"\n\n\n14.5.4.3 3. è¾“å‡ºæ ¼å¼\n\"åˆ†æä»¥ä¸‹äº§å“è¯„è®ºï¼Œä»¥ JSON æ ¼å¼è¾“å‡ºï¼š\n{\n  \"sentiment\": \"positive/negative/neutral\",\n  \"key_points\": [\"point1\", \"point2\"],\n  \"rating\": 1-5\n}\n\nè¯„è®ºï¼š[è¯„è®ºå†…å®¹]\"\n\n\n14.5.4.4 4. è‡ªæˆ‘ä¸€è‡´æ€§ (Self-Consistency)\nå¤šæ¬¡ç”Ÿæˆç­”æ¡ˆï¼Œé€‰æ‹©æœ€ä¸€è‡´çš„ç»“æœ\ndef self_consistency_generate(model, tokenizer, prompt, n=5):\n    \"\"\"è‡ªæˆ‘ä¸€è‡´æ€§ç”Ÿæˆ\"\"\"\n\n    answers = []\n\n    for _ in range(n):\n        # ç”Ÿæˆç­”æ¡ˆï¼ˆå¸¦éšæœºæ€§ï¼‰\n        inputs = tokenizer(prompt, return_tensors='pt')\n        outputs = model.generate(\n            inputs['input_ids'],\n            max_new_tokens=100,\n            temperature=0.7,\n            do_sample=True\n        )\n\n        answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n        answers.append(answer)\n\n    # é€‰æ‹©æœ€å¸¸è§çš„ç­”æ¡ˆ\n    from collections import Counter\n    most_common = Counter(answers).most_common(1)[0][0]\n\n    return most_common, answers",
    "crumbs": [
      "ç¬¬ä¸‰éƒ¨åˆ†ï¼šå®è·µä¸åº”ç”¨",
      "<span class='chapter-number'>14</span>Â  <span class='chapter-title'>ç¬¬åä¸‰ç« ï¼šå¤§è¯­è¨€æ¨¡å‹æ—¶ä»£</span>"
    ]
  },
  {
    "objectID": "Chapter13.html#in-context-learning",
    "href": "Chapter13.html#in-context-learning",
    "title": "14Â  ç¬¬åä¸‰ç« ï¼šå¤§è¯­è¨€æ¨¡å‹æ—¶ä»£",
    "section": "14.6 13.5 In-Context Learning",
    "text": "14.6 13.5 In-Context Learning\n\n14.6.1 ğŸ¯ æ ¸å¿ƒæ¦‚å¿µ\nIn-Context Learning:\n  åœ¨è¾“å…¥ä¸­æä¾›ç¤ºä¾‹ï¼Œæ— éœ€æ›´æ–°å‚æ•°\n\nå…³é”®ç‰¹æ€§ï¼š\n  âœ“ æ— éœ€æ¢¯åº¦æ›´æ–°\n  âœ“ å³æ—¶é€‚åº”æ–°ä»»åŠ¡\n  âœ“ çµæ´»æ€§é«˜\n\n\n14.6.2 ğŸ“Š Few-Shot Learning ç¤ºä¾‹\nclass FewShotLearner:\n    \"\"\"Few-Shot Learning åŒ…è£…å™¨\"\"\"\n\n    def __init__(self, model, tokenizer):\n        self.model = model\n        self.tokenizer = tokenizer\n\n    def create_few_shot_prompt(self, task, examples, query):\n        \"\"\"\n        åˆ›å»º Few-Shot Prompt\n\n        å‚æ•°:\n            task: ä»»åŠ¡æè¿°\n            examples: [(input, output), ...]\n            query: æŸ¥è¯¢è¾“å…¥\n        \"\"\"\n        prompt = f\"{task}\\n\\n\"\n\n        # æ·»åŠ ç¤ºä¾‹\n        for i, (inp, out) in enumerate(examples, 1):\n            prompt += f\"Example {i}:\\n\"\n            prompt += f\"Input: {inp}\\n\"\n            prompt += f\"Output: {out}\\n\\n\"\n\n        # æ·»åŠ æŸ¥è¯¢\n        prompt += f\"Now solve:\\n\"\n        prompt += f\"Input: {query}\\n\"\n        prompt += f\"Output:\"\n\n        return prompt\n\n    def predict(self, task, examples, query, max_tokens=100):\n        \"\"\"Few-Shot é¢„æµ‹\"\"\"\n\n        # åˆ›å»º prompt\n        prompt = self.create_few_shot_prompt(task, examples, query)\n\n        # ç”Ÿæˆ\n        inputs = self.tokenizer(prompt, return_tensors='pt')\n        outputs = self.model.generate(\n            inputs['input_ids'],\n            max_new_tokens=max_tokens,\n            temperature=0.7,\n            top_p=0.9\n        )\n\n        # è§£ç \n        result = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n        # æå–è¾“å‡ºéƒ¨åˆ†\n        result = result.split(\"Output:\")[-1].strip()\n\n        return result\n\n# ==================== ä½¿ç”¨ç¤ºä¾‹ ====================\n\n# æƒ…æ„Ÿåˆ†ç±» Few-Shot\ntask = \"Classify the sentiment of the text as Positive or Negative.\"\n\nexamples = [\n    (\"I love this product!\", \"Positive\"),\n    (\"Terrible experience.\", \"Negative\"),\n    (\"Best purchase ever!\", \"Positive\"),\n    (\"Complete waste of money.\", \"Negative\"),\n]\n\nquery = \"This exceeded my expectations.\"\n\nlearner = FewShotLearner(model, tokenizer)\nresult = learner.predict(task, examples, query)\n\nprint(f\"Query: {query}\")\nprint(f\"Prediction: {result}\")\n\n\n\n14.6.3 ğŸ”¹ ç¤ºä¾‹é€‰æ‹©ç­–ç•¥\ndef select_diverse_examples(example_pool, query, n=5, method='semantic'):\n    \"\"\"\n    é€‰æ‹©å¤šæ ·åŒ–çš„ç¤ºä¾‹\n\n    æ–¹æ³•:\n        - random: éšæœºé€‰æ‹©\n        - semantic: åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦\n        - diverse: æœ€å¤§åŒ–å¤šæ ·æ€§\n    \"\"\"\n\n    if method == 'random':\n        return random.sample(example_pool, n)\n\n    elif method == 'semantic':\n        from sentence_transformers import SentenceTransformer\n\n        # åŠ è½½å¥å­ç¼–ç å™¨\n        encoder = SentenceTransformer('all-MiniLM-L6-v2')\n\n        # ç¼–ç æŸ¥è¯¢å’Œç¤ºä¾‹\n        query_emb = encoder.encode([query])[0]\n        example_embs = encoder.encode([ex[0] for ex in example_pool])\n\n        # è®¡ç®—ç›¸ä¼¼åº¦\n        from sklearn.metrics.pairwise import cosine_similarity\n        similarities = cosine_similarity([query_emb], example_embs)[0]\n\n        # é€‰æ‹©æœ€ç›¸ä¼¼çš„\n        top_indices = np.argsort(similarities)[-n:][::-1]\n        return [example_pool[i] for i in top_indices]\n\n    elif method == 'diverse':\n        # k-means èšç±»é€‰æ‹©å¤šæ ·åŒ–ç¤ºä¾‹\n        from sklearn.cluster import KMeans\n        from sentence_transformers import SentenceTransformer\n\n        encoder = SentenceTransformer('all-MiniLM-L6-v2')\n        example_embs = encoder.encode([ex[0] for ex in example_pool])\n\n        # èšç±»\n        kmeans = KMeans(n_clusters=n, random_state=42)\n        kmeans.fit(example_embs)\n\n        # ä»æ¯ä¸ªç°‡é€‰æ‹©æœ€æ¥è¿‘ä¸­å¿ƒçš„ç¤ºä¾‹\n        selected = []\n        for i in range(n):\n            cluster_indices = np.where(kmeans.labels_ == i)[0]\n            center = kmeans.cluster_centers_[i]\n\n            # æ‰¾æœ€æ¥è¿‘ä¸­å¿ƒçš„\n            distances = np.linalg.norm(example_embs[cluster_indices] - center, axis=1)\n            closest_idx = cluster_indices[np.argmin(distances)]\n            selected.append(example_pool[closest_idx])\n\n        return selected",
    "crumbs": [
      "ç¬¬ä¸‰éƒ¨åˆ†ï¼šå®è·µä¸åº”ç”¨",
      "<span class='chapter-number'>14</span>Â  <span class='chapter-title'>ç¬¬åä¸‰ç« ï¼šå¤§è¯­è¨€æ¨¡å‹æ—¶ä»£</span>"
    ]
  },
  {
    "objectID": "Chapter13.html#æŒ‡ä»¤å¾®è°ƒ-instruction-tuning",
    "href": "Chapter13.html#æŒ‡ä»¤å¾®è°ƒ-instruction-tuning",
    "title": "14Â  ç¬¬åä¸‰ç« ï¼šå¤§è¯­è¨€æ¨¡å‹æ—¶ä»£",
    "section": "14.7 13.6 æŒ‡ä»¤å¾®è°ƒ (Instruction Tuning)",
    "text": "14.7 13.6 æŒ‡ä»¤å¾®è°ƒ (Instruction Tuning)\n\n14.7.1 ğŸ¯ ä» GPT-3 åˆ° InstructGPT\né—®é¢˜ï¼š\n  GPT-3 è™½ç„¶å¼ºå¤§ï¼Œä½†ä¸æ€»æ˜¯éµå¾ªç”¨æˆ·æŒ‡ä»¤\n\nè§£å†³ï¼š\n  Instruction Tuning + RLHF\n\næµç¨‹ï¼š\n  1. æ”¶é›†æŒ‡ä»¤-å“åº”æ•°æ®\n  2. ç›‘ç£å¾®è°ƒ (SFT)\n  3. æ”¶é›†äººç±»åå¥½æ•°æ®\n  4. è®­ç»ƒå¥–åŠ±æ¨¡å‹ (RM)\n  5. å¼ºåŒ–å­¦ä¹ å¾®è°ƒ (PPO)\n\n\n14.7.2 ğŸ’» ç›‘ç£å¾®è°ƒ (SFT)\ndef instruction_tuning(model, instruction_dataset, num_epochs=3):\n    \"\"\"\n    æŒ‡ä»¤å¾®è°ƒ\n\n    æ•°æ®æ ¼å¼:\n    {\n        \"instruction\": \"å°†ä»¥ä¸‹è‹±æ–‡ç¿»è¯‘æˆä¸­æ–‡\",\n        \"input\": \"Hello, world!\",\n        \"output\": \"ä½ å¥½ï¼Œä¸–ç•Œï¼\"\n    }\n    \"\"\"\n\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model.to(device)\n\n    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n\n    for epoch in range(num_epochs):\n        model.train()\n        total_loss = 0\n\n        for batch in instruction_dataset:\n            # æ ¼å¼åŒ–ä¸º prompt\n            prompts = []\n            for item in batch:\n                prompt = f\"### Instruction:\\n{item['instruction']}\\n\\n\"\n                if item.get('input'):\n                    prompt += f\"### Input:\\n{item['input']}\\n\\n\"\n                prompt += f\"### Response:\\n{item['output']}\"\n                prompts.append(prompt)\n\n            # Tokenize\n            inputs = tokenizer(prompts, return_tensors='pt',\n                             padding=True, truncation=True)\n            inputs = {k: v.to(device) for k, v in inputs.items()}\n\n            # å‰å‘ä¼ æ’­\n            outputs = model(**inputs, labels=inputs['input_ids'])\n            loss = outputs.loss\n\n            # åå‘ä¼ æ’­\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n\n        print(f'Epoch {epoch+1}: Loss = {total_loss/len(instruction_dataset):.4f}')\n\n    return model\n\n\n\n14.7.3 ğŸ”¹ RLHF (Reinforcement Learning from Human Feedback)\nclass RewardModel(nn.Module):\n    \"\"\"å¥–åŠ±æ¨¡å‹\"\"\"\n\n    def __init__(self, base_model):\n        super().__init__()\n        self.base_model = base_model\n\n        # å¥–åŠ±å¤´\n        self.reward_head = nn.Linear(base_model.config.hidden_size, 1)\n\n    def forward(self, input_ids, attention_mask=None):\n        # è·å–æœ€åä¸€ä¸ª token çš„éšè—çŠ¶æ€\n        outputs = self.base_model(input_ids, attention_mask=attention_mask)\n        last_hidden = outputs.last_hidden_state[:, -1, :]\n\n        # é¢„æµ‹å¥–åŠ±\n        reward = self.reward_head(last_hidden)\n\n        return reward\n\ndef train_reward_model(model, comparison_dataset):\n    \"\"\"\n    è®­ç»ƒå¥–åŠ±æ¨¡å‹\n\n    æ•°æ®æ ¼å¼ï¼š(prompt, response_A, response_B, preference)\n    preference: 0 è¡¨ç¤º A æ›´å¥½ï¼Œ1 è¡¨ç¤º B æ›´å¥½\n    \"\"\"\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n\n    for epoch in range(num_epochs):\n        for batch in comparison_dataset:\n            prompts, responses_A, responses_B, preferences = batch\n\n            # è®¡ç®—å¥–åŠ±\n            rewards_A = model(responses_A)\n            rewards_B = model(responses_B)\n\n            # æŸå¤±ï¼šåå¥½çš„å“åº”åº”è¯¥æœ‰æ›´é«˜å¥–åŠ±\n            loss = -torch.log(torch.sigmoid(\n                (rewards_A - rewards_B) * (2 * preferences - 1)\n            )).mean()\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n    return model\n\ndef rlhf_training(policy_model, reward_model, prompts):\n    \"\"\"\n    ä½¿ç”¨ PPO è¿›è¡Œ RLHF\n\n    ç®€åŒ–ç‰ˆæœ¬ï¼ˆå®é™…å®ç°æ›´å¤æ‚ï¼‰\n    \"\"\"\n\n    optimizer = torch.optim.Adam(policy_model.parameters(), lr=1e-6)\n\n    for iteration in range(num_iterations):\n        for prompt in prompts:\n            # ç”Ÿæˆå“åº”\n            with torch.no_grad():\n                response = policy_model.generate(prompt)\n\n            # è®¡ç®—å¥–åŠ±\n            reward = reward_model(response)\n\n            # PPO æ›´æ–°ï¼ˆç®€åŒ–ï¼‰\n            # å®é™…éœ€è¦ï¼šold_log_probs, advantages, clip_epsilon ç­‰\n            log_probs = policy_model.compute_log_probs(prompt, response)\n            policy_loss = -(log_probs * reward).mean()\n\n            # KL æ•£åº¦æƒ©ç½šï¼ˆé˜²æ­¢åç¦»å¤ªè¿œï¼‰\n            ref_log_probs = ref_model.compute_log_probs(prompt, response)\n            kl_penalty = (log_probs - ref_log_probs).mean()\n\n            loss = policy_loss + 0.1 * kl_penalty\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()",
    "crumbs": [
      "ç¬¬ä¸‰éƒ¨åˆ†ï¼šå®è·µä¸åº”ç”¨",
      "<span class='chapter-number'>14</span>Â  <span class='chapter-title'>ç¬¬åä¸‰ç« ï¼šå¤§è¯­è¨€æ¨¡å‹æ—¶ä»£</span>"
    ]
  },
  {
    "objectID": "Chapter13.html#é«˜æ•ˆå¾®è°ƒlora-å’Œ-peft",
    "href": "Chapter13.html#é«˜æ•ˆå¾®è°ƒlora-å’Œ-peft",
    "title": "14Â  ç¬¬åä¸‰ç« ï¼šå¤§è¯­è¨€æ¨¡å‹æ—¶ä»£",
    "section": "14.8 13.7 é«˜æ•ˆå¾®è°ƒï¼šLoRA å’Œ PEFT",
    "text": "14.8 13.7 é«˜æ•ˆå¾®è°ƒï¼šLoRA å’Œ PEFT\n\n14.8.1 ğŸ¯ ä¸ºä»€ä¹ˆéœ€è¦é«˜æ•ˆå¾®è°ƒï¼Ÿ\né—®é¢˜ï¼š\n  å¤§æ¨¡å‹å…¨å‚æ•°å¾®è°ƒæˆæœ¬é«˜æ˜‚\n  - GPT-3 (175B å‚æ•°)\n  - éœ€è¦æ•°ç™¾ GB æ˜¾å­˜\n  - è®­ç»ƒæ—¶é—´é•¿\n\nè§£å†³ï¼š\n  Parameter-Efficient Fine-Tuning (PEFT)\n  - åªè®­ç»ƒå°‘é‡å‚æ•°\n  - ä¿æŒæ€§èƒ½\n\n\n\n14.8.2 ğŸ”¹ LoRA (Low-Rank Adaptation)\næ ¸å¿ƒæ€æƒ³ï¼šä½ç§©åˆ†è§£\nåŸå§‹æƒé‡æ›´æ–°ï¼š\n  W' = W + Î”W\n\nLoRAï¼š\n  W' = W + BA\n\n  å…¶ä¸­ B âˆˆ â„^(dÃ—r), A âˆˆ â„^(rÃ—k), r &lt;&lt; min(d, k)\n\nå‚æ•°é‡ï¼š\n  åŸå§‹ï¼šd Ã— k\n  LoRAï¼šr Ã— (d + k)  ï¼ˆå‡å°‘ &gt;90%ï¼‰\nclass LoRALayer(nn.Module):\n    \"\"\"LoRA å±‚\"\"\"\n\n    def __init__(self, in_features, out_features, rank=8, alpha=16):\n        super().__init__()\n\n        self.rank = rank\n        self.alpha = alpha\n\n        # ä½ç§©çŸ©é˜µ\n        self.lora_A = nn.Parameter(torch.zeros(rank, in_features))\n        self.lora_B = nn.Parameter(torch.zeros(out_features, rank))\n\n        # åˆå§‹åŒ–\n        nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n        nn.init.zeros_(self.lora_B)\n\n        # ç¼©æ”¾å› å­\n        self.scaling = alpha / rank\n\n    def forward(self, x):\n        # LoRA è·¯å¾„ï¼šx @ A^T @ B^T\n        lora_out = (x @ self.lora_A.T) @ self.lora_B.T\n        return lora_out * self.scaling\n\nclass LoRALinear(nn.Module):\n    \"\"\"å¸¦ LoRA çš„çº¿æ€§å±‚\"\"\"\n\n    def __init__(self, linear_layer, rank=8, alpha=16):\n        super().__init__()\n\n        # å†»ç»“åŸå§‹æƒé‡\n        self.linear = linear_layer\n        for param in self.linear.parameters():\n            param.requires_grad = False\n\n        # æ·»åŠ  LoRA\n        self.lora = LoRALayer(\n            linear_layer.in_features,\n            linear_layer.out_features,\n            rank, alpha\n        )\n\n    def forward(self, x):\n        # åŸå§‹è¾“å‡º + LoRA å¢é‡\n        return self.linear(x) + self.lora(x)\n\n# ==================== åº”ç”¨ LoRA ====================\n\ndef apply_lora_to_model(model, rank=8, alpha=16, target_modules=['q_proj', 'v_proj']):\n    \"\"\"\n    ä¸ºæ¨¡å‹æ·»åŠ  LoRA\n\n    é€šå¸¸åªå¯¹ attention çš„ Q, V çŸ©é˜µæ·»åŠ  LoRA\n    \"\"\"\n\n    for name, module in model.named_modules():\n        # æ£€æŸ¥æ˜¯å¦æ˜¯ç›®æ ‡æ¨¡å—\n        if any(target in name for target in target_modules):\n            if isinstance(module, nn.Linear):\n                # æ›¿æ¢ä¸º LoRA ç‰ˆæœ¬\n                parent_name = '.'.join(name.split('.')[:-1])\n                child_name = name.split('.')[-1]\n\n                parent_module = model.get_submodule(parent_name)\n                lora_linear = LoRALinear(module, rank, alpha)\n\n                setattr(parent_module, child_name, lora_linear)\n\n                print(f\"Applied LoRA to {name}\")\n\n    return model\n\n# ==================== è®­ç»ƒ LoRA ====================\n\ndef train_with_lora(model, dataloader, num_epochs=3):\n    \"\"\"ä½¿ç”¨ LoRA å¾®è°ƒ\"\"\"\n\n    # åº”ç”¨ LoRA\n    model = apply_lora_to_model(model)\n\n    # åªä¼˜åŒ– LoRA å‚æ•°\n    lora_params = [p for n, p in model.named_parameters() if 'lora' in n]\n    optimizer = torch.optim.AdamW(lora_params, lr=1e-4)\n\n    print(f\"Total params: {sum(p.numel() for p in model.parameters()):,}\")\n    print(f\"Trainable params: {sum(p.numel() for p in lora_params):,}\")\n\n    # è®­ç»ƒå¾ªç¯ï¼ˆä¸æ™®é€šå¾®è°ƒç›¸åŒï¼‰\n    for epoch in range(num_epochs):\n        # ... è®­ç»ƒä»£ç \n        pass\n\n    return model\n\n\n\n14.8.3 ğŸ”¹ å…¶ä»– PEFT æ–¹æ³•\n\n14.8.3.1 Adapter Tuning\nclass Adapter(nn.Module):\n    \"\"\"Adapter æ¨¡å—\"\"\"\n\n    def __init__(self, hidden_size, bottleneck_size=64):\n        super().__init__()\n\n        self.down_proj = nn.Linear(hidden_size, bottleneck_size)\n        self.up_proj = nn.Linear(bottleneck_size, hidden_size)\n        self.activation = nn.ReLU()\n\n    def forward(self, x):\n        residual = x\n        x = self.down_proj(x)\n        x = self.activation(x)\n        x = self.up_proj(x)\n        return x + residual  # æ®‹å·®è¿æ¥\n\n\n14.8.3.2 Prefix Tuning\nclass PrefixTuning(nn.Module):\n    \"\"\"Prefix Tuning\"\"\"\n\n    def __init__(self, num_prefix_tokens, d_model):\n        super().__init__()\n\n        # å¯å­¦ä¹ çš„ prefix embeddings\n        self.prefix_embeddings = nn.Parameter(\n            torch.randn(num_prefix_tokens, d_model)\n        )\n\n    def forward(self, input_embeddings):\n        batch_size = input_embeddings.size(0)\n\n        # æ‰©å±• prefix åˆ° batch\n        prefix = self.prefix_embeddings.unsqueeze(0).expand(batch_size, -1, -1)\n\n        # æ‹¼æ¥åˆ°è¾“å…¥å‰é¢\n        return torch.cat([prefix, input_embeddings], dim=1)",
    "crumbs": [
      "ç¬¬ä¸‰éƒ¨åˆ†ï¼šå®è·µä¸åº”ç”¨",
      "<span class='chapter-number'>14</span>Â  <span class='chapter-title'>ç¬¬åä¸‰ç« ï¼šå¤§è¯­è¨€æ¨¡å‹æ—¶ä»£</span>"
    ]
  },
  {
    "objectID": "Chapter13.html#llm-åº”ç”¨èŒƒå¼",
    "href": "Chapter13.html#llm-åº”ç”¨èŒƒå¼",
    "title": "14Â  ç¬¬åä¸‰ç« ï¼šå¤§è¯­è¨€æ¨¡å‹æ—¶ä»£",
    "section": "14.9 13.8 LLM åº”ç”¨èŒƒå¼",
    "text": "14.9 13.8 LLM åº”ç”¨èŒƒå¼\n\n14.9.1 ğŸ”¹ æ£€ç´¢å¢å¼ºç”Ÿæˆ (RAG)\nRAG = Retrieval + Generation\n\næµç¨‹ï¼š\n  1. æ£€ç´¢ç›¸å…³æ–‡æ¡£\n  2. å°†æ–‡æ¡£ä½œä¸ºä¸Šä¸‹æ–‡\n  3. ç”Ÿæˆç­”æ¡ˆ\nclass RAGSystem:\n    \"\"\"æ£€ç´¢å¢å¼ºç”Ÿæˆç³»ç»Ÿ\"\"\"\n\n    def __init__(self, llm, retriever, top_k=3):\n        self.llm = llm\n        self.retriever = retriever\n        self.top_k = top_k\n\n    def answer_question(self, question, knowledge_base):\n        \"\"\"\n        åŸºäºçŸ¥è¯†åº“å›ç­”é—®é¢˜\n\n        å‚æ•°:\n            question: ç”¨æˆ·é—®é¢˜\n            knowledge_base: æ–‡æ¡£åˆ—è¡¨\n        \"\"\"\n        # 1. æ£€ç´¢ç›¸å…³æ–‡æ¡£\n        relevant_docs = self.retriever.retrieve(\n            question, knowledge_base, top_k=self.top_k\n        )\n\n        # 2. æ„å»º prompt\n        context = \"\\n\\n\".join([\n            f\"Document {i+1}:\\n{doc}\"\n            for i, doc in enumerate(relevant_docs)\n        ])\n\n        prompt = f\"\"\"Answer the question based on the context below.\n\nContext:\n{context}\n\nQuestion: {question}\n\nAnswer:\"\"\"\n\n        # 3. ç”Ÿæˆç­”æ¡ˆ\n        answer = self.llm.generate(prompt)\n\n        return answer, relevant_docs\n\nclass SimpleRetriever:\n    \"\"\"ç®€å•çš„åŸºäºåµŒå…¥çš„æ£€ç´¢å™¨\"\"\"\n\n    def __init__(self, encoder_model='all-MiniLM-L6-v2'):\n        from sentence_transformers import SentenceTransformer\n        self.encoder = SentenceTransformer(encoder_model)\n\n    def retrieve(self, query, documents, top_k=3):\n        \"\"\"æ£€ç´¢æœ€ç›¸å…³çš„æ–‡æ¡£\"\"\"\n\n        # ç¼–ç \n        query_emb = self.encoder.encode([query])[0]\n        doc_embs = self.encoder.encode(documents)\n\n        # è®¡ç®—ç›¸ä¼¼åº¦\n        from sklearn.metrics.pairwise import cosine_similarity\n        similarities = cosine_similarity([query_emb], doc_embs)[0]\n\n        # è¿”å› top-k\n        top_indices = np.argsort(similarities)[-top_k:][::-1]\n\n        return [documents[i] for i in top_indices]\n\n\n\n14.9.2 ğŸ”¹ Agent ç³»ç»Ÿ\n```python class LLMAgent: â€œâ€œâ€œåŸºäº LLM çš„ Agentâ€â€œâ€\ndef __init__(self, llm, tools):\n    self.llm = llm\n    self.tools = {tool.name: tool for tool in tools}\n\ndef run(self, task, max_steps=5):\n    \"\"\"\n    æ‰§è¡Œä»»åŠ¡\n\n    æµç¨‹ï¼š\n    1. æ€è€ƒä¸‹ä¸€æ­¥\n    2. é€‰æ‹©å·¥å…·\n    3. æ‰§è¡ŒåŠ¨ä½œ\n    4. è§‚å¯Ÿç»“æœ\n    5. é‡å¤ç›´åˆ°å®Œæˆ\n    \"\"\"\n\n    history = []\n\n    for step in range(max_steps):\n        # æ„å»º prompt\n        prompt = self._build_agent_prompt(task, history)\n\n        # LLM å†³ç­–\n        response = self.llm.generate(prompt)\n\n        # è§£æåŠ¨ä½œ\n        action = self._parse_action(response)\n\n        if action['type'] == 'FINISH':\n            return action['answer']\n\n        # æ‰§è¡Œå·¥å…·\n        tool_name = action['tool']\n        tool_input = action['input']\n\n        if tool_name in self.tools:\n            observation = self.tools[tool_name].run(tool_input)\n        else:\n            observation = f\"Tool {tool_name} not found.\"\n\n        # è®°å½•\n        history.append({\n            'thought': action.get('tho",
    "crumbs": [
      "ç¬¬ä¸‰éƒ¨åˆ†ï¼šå®è·µä¸åº”ç”¨",
      "<span class='chapter-number'>14</span>Â  <span class='chapter-title'>ç¬¬åä¸‰ç« ï¼šå¤§è¯­è¨€æ¨¡å‹æ—¶ä»£</span>"
    ]
  }
]