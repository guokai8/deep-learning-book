# ç¬¬åä¸€ç« ï¼šæ— ç›‘ç£å­¦ä¹  (Unsupervised Learning)

## ğŸ“Œ ç« èŠ‚ç›®æ ‡
- ç†è§£æ— ç›‘ç£å­¦ä¹ çš„æ ¸å¿ƒæ€æƒ³
- æŒæ¡èšç±»ç®—æ³•ï¼ˆK-means, DBSCAN, å±‚æ¬¡èšç±»ï¼‰
- å­¦ä¹ é™ç»´æŠ€æœ¯ï¼ˆPCA, t-SNE, UMAPï¼‰
- äº†è§£è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•
- å®æˆ˜ï¼šæ•°æ®æ¢ç´¢ã€ç‰¹å¾å­¦ä¹ ã€å¼‚å¸¸æ£€æµ‹

---

## 11.1 æ— ç›‘ç£å­¦ä¹ æ¦‚è¿°

### ğŸ¯ ä»€ä¹ˆæ˜¯æ— ç›‘ç£å­¦ä¹ ï¼Ÿ

**å®šä¹‰**ï¼šä»æ— æ ‡ç­¾æ•°æ®ä¸­å­¦ä¹ æ•°æ®çš„å†…åœ¨ç»“æ„å’Œæ¨¡å¼

**ä¸ç›‘ç£å­¦ä¹ çš„åŒºåˆ«**ï¼š

```
ç›‘ç£å­¦ä¹ ï¼š
  è¾“å…¥: (X, y)  æœ‰æ ‡ç­¾
  ç›®æ ‡: å­¦ä¹  f: X â†’ y

æ— ç›‘ç£å­¦ä¹ ï¼š
  è¾“å…¥: X only  æ— æ ‡ç­¾
  ç›®æ ‡: å‘ç°æ•°æ®çš„éšè—ç»“æ„
```

### ğŸ“Š ä¸»è¦ä»»åŠ¡

#### **1. èšç±» (Clustering)**

```
ç›®æ ‡ï¼šå°†ç›¸ä¼¼æ ·æœ¬åˆ†ç»„

åº”ç”¨ï¼š
  - å®¢æˆ·åˆ†ç¾¤
  - å›¾åƒåˆ†å‰²
  - æ–‡æ¡£ç»„ç»‡
  - åŸºå› åˆ†æ
```

#### **2. é™ç»´ (Dimensionality Reduction)**

```
ç›®æ ‡ï¼šåœ¨ä½ç»´ç©ºé—´ä¿ç•™æ•°æ®ç‰¹æ€§

åº”ç”¨ï¼š
  - å¯è§†åŒ–
  - æ•°æ®å‹ç¼©
  - å™ªå£°æ¶ˆé™¤
  - ç‰¹å¾æå–
```

#### **3. å¯†åº¦ä¼°è®¡ (Density Estimation)**

```
ç›®æ ‡ï¼šä¼°è®¡æ•°æ®çš„æ¦‚ç‡åˆ†å¸ƒ

åº”ç”¨ï¼š
  - å¼‚å¸¸æ£€æµ‹
  - ç”Ÿæˆæ¨¡å‹
```

#### **4. è¡¨ç¤ºå­¦ä¹  (Representation Learning)**

```
ç›®æ ‡ï¼šå­¦ä¹ æœ‰ç”¨çš„æ•°æ®è¡¨ç¤º

åº”ç”¨ï¼š
  - è‡ªç›‘ç£å­¦ä¹ 
  - é¢„è®­ç»ƒæ¨¡å‹
```

---

## 11.2 èšç±»ç®—æ³•

### ğŸ”¹ K-Means èšç±»

#### **ç®—æ³•åŸç†**

```
ç›®æ ‡ï¼šæœ€å°åŒ–ç±»å†…è·ç¦»å¹³æ–¹å’Œ

J = âˆ‘_{k=1}^K âˆ‘_{xâˆˆC_k} ||x - Î¼_k||Â²

ç®—æ³•æ­¥éª¤ï¼š
1. éšæœºåˆå§‹åŒ– K ä¸ªä¸­å¿ƒ Î¼_k
2. åˆ†é…ï¼šæ¯ä¸ªç‚¹åˆ†é…åˆ°æœ€è¿‘çš„ä¸­å¿ƒ
3. æ›´æ–°ï¼šé‡æ–°è®¡ç®—æ¯ä¸ªç°‡çš„ä¸­å¿ƒ
4. é‡å¤ 2-3 ç›´åˆ°æ”¶æ•›
```

#### **å®ç°**

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs

class KMeans:
    """K-Means èšç±»"""

    def __init__(self, n_clusters=3, max_iters=100, random_state=None):
        self.n_clusters = n_clusters
        self.max_iters = max_iters
        self.random_state = random_state
        self.centers = None
        self.labels = None

    def fit(self, X):
        """
        è®­ç»ƒ K-Means

        å‚æ•°:
            X: (n_samples, n_features)
        """
        np.random.seed(self.random_state)
        n_samples = X.shape[0]

        # éšæœºåˆå§‹åŒ–ä¸­å¿ƒ
        random_indices = np.random.choice(n_samples, self.n_clusters, replace=False)
        self.centers = X[random_indices]

        for iteration in range(self.max_iters):
            # åˆ†é…æ ·æœ¬åˆ°æœ€è¿‘çš„ä¸­å¿ƒ
            labels = self._assign_clusters(X)

            # æ›´æ–°ä¸­å¿ƒ
            new_centers = self._update_centers(X, labels)

            # æ£€æŸ¥æ”¶æ•›
            if np.allclose(self.centers, new_centers):
                print(f"æ”¶æ•›äºç¬¬ {iteration+1} æ¬¡è¿­ä»£")
                break

            self.centers = new_centers

        self.labels = labels
        return self

    def _assign_clusters(self, X):
        """åˆ†é…æ ·æœ¬åˆ°æœ€è¿‘çš„ç°‡"""
        distances = np.sqrt(((X[:, np.newaxis] - self.centers) ** 2).sum(axis=2))
        return np.argmin(distances, axis=1)

    def _update_centers(self, X, labels):
        """æ›´æ–°ç°‡ä¸­å¿ƒ"""
        new_centers = np.zeros((self.n_clusters, X.shape[1]))

        for k in range(self.n_clusters):
            cluster_points = X[labels == k]
            if len(cluster_points) > 0:
                new_centers[k] = cluster_points.mean(axis=0)
            else:
                # å¦‚æœç°‡ä¸ºç©ºï¼Œé‡æ–°éšæœºåˆå§‹åŒ–
                new_centers[k] = X[np.random.randint(X.shape[0])]

        return new_centers

    def predict(self, X):
        """é¢„æµ‹æ–°æ ·æœ¬çš„ç°‡"""
        return self._assign_clusters(X)

    def fit_predict(self, X):
        """è®­ç»ƒå¹¶é¢„æµ‹"""
        self.fit(X)
        return self.labels

# ==================== å¯è§†åŒ– K-Means ====================

def visualize_kmeans(X, kmeans, title='K-Means Clustering'):
    """å¯è§†åŒ– K-Means ç»“æœ"""

    fig, ax = plt.subplots(figsize=(10, 8))

    # ç»˜åˆ¶æ•°æ®ç‚¹
    scatter = ax.scatter(X[:, 0], X[:, 1], c=kmeans.labels,
                        cmap='viridis', alpha=0.6, s=50)

    # ç»˜åˆ¶ä¸­å¿ƒ
    ax.scatter(kmeans.centers[:, 0], kmeans.centers[:, 1],
              c='red', marker='X', s=200, edgecolor='black',
              linewidth=2, label='Centroids')

    ax.set_xlabel('Feature 1')
    ax.set_ylabel('Feature 2')
    ax.set_title(title)
    ax.legend()
    plt.colorbar(scatter, label='Cluster')
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()

# ==================== è‚˜éƒ¨æ³•åˆ™ (Elbow Method) ====================

def elbow_method(X, max_k=10):
    """ä½¿ç”¨è‚˜éƒ¨æ³•åˆ™ç¡®å®šæœ€ä½³ K å€¼"""

    inertias = []
    K_range = range(1, max_k + 1)

    for k in K_range:
        kmeans = KMeans(n_clusters=k, random_state=42)
        kmeans.fit(X)

        # è®¡ç®—ç°‡å†…å¹³æ–¹å’Œ (inertia)
        labels = kmeans.labels
        inertia = 0
        for i in range(k):
            cluster_points = X[labels == i]
            if len(cluster_points) > 0:
                inertia += ((cluster_points - kmeans.centers[i]) ** 2).sum()

        inertias.append(inertia)

    # ç»˜åˆ¶è‚˜éƒ¨æ›²çº¿
    plt.figure(figsize=(10, 6))
    plt.plot(K_range, inertias, 'bo-', linewidth=2, markersize=8)
    plt.xlabel('Number of Clusters (K)')
    plt.ylabel('Inertia (Within-cluster Sum of Squares)')
    plt.title('Elbow Method for Optimal K')
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()

    return inertias

# ==================== ç¤ºä¾‹ ====================

if __name__ == '__main__':
    # ç”Ÿæˆåˆæˆæ•°æ®
    X, y_true = make_blobs(n_samples=300, centers=4,
                           n_features=2, random_state=42)

    # è‚˜éƒ¨æ³•åˆ™
    print("ä½¿ç”¨è‚˜éƒ¨æ³•åˆ™ç¡®å®šæœ€ä½³ K...")
    elbow_method(X, max_k=10)

    # K-Means èšç±»
    kmeans = KMeans(n_clusters=4, random_state=42)
    kmeans.fit(X)

    print(f"\nç°‡ä¸­å¿ƒ:\n{kmeans.centers}")
    print(f"ç°‡åˆ†é…: {kmeans.labels}")

    # å¯è§†åŒ–
    visualize_kmeans(X, kmeans)
```

---

### ğŸ”¹ DBSCAN (Density-Based Spatial Clustering)

#### **ç®—æ³•åŸç†**

```
åŸºäºå¯†åº¦çš„èšç±»ï¼š
  - æ ¸å¿ƒç‚¹ï¼šÎµ é‚»åŸŸå†…è‡³å°‘æœ‰ MinPts ä¸ªç‚¹
  - è¾¹ç•Œç‚¹ï¼šåœ¨æ ¸å¿ƒç‚¹çš„ Îµ é‚»åŸŸå†…ï¼Œä½†è‡ªå·±ä¸æ˜¯æ ¸å¿ƒç‚¹
  - å™ªå£°ç‚¹ï¼šæ—¢ä¸æ˜¯æ ¸å¿ƒç‚¹ä¹Ÿä¸æ˜¯è¾¹ç•Œç‚¹

ä¼˜åŠ¿ï¼š
  âœ“ å¯ä»¥å‘ç°ä»»æ„å½¢çŠ¶çš„ç°‡
  âœ“ ä¸éœ€è¦é¢„å…ˆæŒ‡å®šç°‡æ•°
  âœ“ èƒ½è¯†åˆ«å™ªå£°ç‚¹

å‚æ•°ï¼š
  - Îµ (epsilon): é‚»åŸŸåŠå¾„
  - MinPts: æœ€å°ç‚¹æ•°
```

#### **å®ç°**

```python
from sklearn.neighbors import NearestNeighbors

class DBSCAN:
    """DBSCAN èšç±»"""

    def __init__(self, eps=0.5, min_samples=5):
        self.eps = eps
        self.min_samples = min_samples
        self.labels = None

    def fit(self, X):
        """è®­ç»ƒ DBSCAN"""
        n_samples = X.shape[0]

        # è®¡ç®—æ‰€æœ‰ç‚¹çš„é‚»åŸŸ
        neighbors_model = NearestNeighbors(radius=self.eps)
        neighbors_model.fit(X)
        neighborhoods = neighbors_model.radius_neighbors(X, return_distance=False)

        # åˆå§‹åŒ–æ ‡ç­¾ï¼ˆ-1 è¡¨ç¤ºæœªåˆ†ç±»ï¼‰
        labels = np.full(n_samples, -1)

        # å½“å‰ç°‡ ID
        cluster_id = 0

        for i in range(n_samples):
            # å¦‚æœå·²åˆ†ç±»ï¼Œè·³è¿‡
            if labels[i] != -1:
                continue

            # è·å–é‚»åŸŸ
            neighbors = neighborhoods[i]

            # å¦‚æœä¸æ˜¯æ ¸å¿ƒç‚¹ï¼Œæ ‡è®°ä¸ºå™ªå£°ï¼ˆæš‚æ—¶ï¼‰
            if len(neighbors) < self.min_samples:
                labels[i] = -1
                continue

            # å¼€å§‹æ–°ç°‡
            labels[i] = cluster_id

            # ç§å­é›†åˆï¼ˆå¾…æ‰©å±•çš„ç‚¹ï¼‰
            seeds = set(neighbors) - {i}

            while seeds:
                q = seeds.pop()

                # å¦‚æœæ˜¯å™ªå£°ç‚¹ï¼Œæ”¹ä¸ºè¾¹ç•Œç‚¹
                if labels[q] == -1:
                    labels[q] = cluster_id

                # å¦‚æœå·²åˆ†ç±»åˆ°å…¶ä»–ç°‡ï¼Œè·³è¿‡
                if labels[q] != -1:
                    continue

                labels[q] = cluster_id

                # å¦‚æœ q ä¹Ÿæ˜¯æ ¸å¿ƒç‚¹ï¼Œæ‰©å±•ç§å­é›†
                q_neighbors = neighborhoods[q]
                if len(q_neighbors) >= self.min_samples:
                    seeds.update(q_neighbors)

            cluster_id += 1

        self.labels = labels
        return self

    def fit_predict(self, X):
        """è®­ç»ƒå¹¶è¿”å›æ ‡ç­¾"""
        self.fit(X)
        return self.labels

# ==================== å¯è§†åŒ– DBSCAN ====================

def visualize_dbscan(X, dbscan):
    """å¯è§†åŒ– DBSCAN ç»“æœ"""

    fig, ax = plt.subplots(figsize=(10, 8))

    # æ ¸å¿ƒæ ·æœ¬ mask
    unique_labels = set(dbscan.labels)
    colors = plt.cm.Spectral(np.linspace(0, 1, len(unique_labels)))

    for k, col in zip(unique_labels, colors):
        if k == -1:
            # å™ªå£°ç‚¹ç”¨é»‘è‰²è¡¨ç¤º
            col = [0, 0, 0, 1]

        class_member_mask = (dbscan.labels == k)

        xy = X[class_member_mask]
        ax.scatter(xy[:, 0], xy[:, 1], c=[col],
                  s=50, alpha=0.6,
                  label=f'Cluster {k}' if k != -1 else 'Noise')

    ax.set_xlabel('Feature 1')
    ax.set_ylabel('Feature 2')
    ax.set_title('DBSCAN Clustering')
    ax.legend()
    ax.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()

# ==================== ç¤ºä¾‹ ====================

if __name__ == '__main__':
    # ç”Ÿæˆæœˆç‰™å½¢æ•°æ®
    from sklearn.datasets import make_moons
    X, _ = make_moons(n_samples=300, noise=0.05, random_state=42)

    # DBSCAN èšç±»
    dbscan = DBSCAN(eps=0.2, min_samples=5)
    dbscan.fit(X)

    print(f"å‘ç° {len(set(dbscan.labels)) - (1 if -1 in dbscan.labels else 0)} ä¸ªç°‡")
    print(f"å™ªå£°ç‚¹æ•°: {list(dbscan.labels).count(-1)}")

    # å¯è§†åŒ–
    visualize_dbscan(X, dbscan)
```

---

### ğŸ”¹ å±‚æ¬¡èšç±» (Hierarchical Clustering)

#### **ç®—æ³•åŸç†**

```
ä¸¤ç§ç­–ç•¥ï¼š

1. å‡èš (Agglomerative)ï¼šè‡ªåº•å‘ä¸Š
   - æ¯ä¸ªç‚¹åˆå§‹åŒ–ä¸ºä¸€ä¸ªç°‡
   - é€æ­¥åˆå¹¶æœ€ç›¸ä¼¼çš„ç°‡
   - ç›´åˆ°å½¢æˆä¸€ä¸ªå¤§ç°‡æˆ–è¾¾åˆ°åœæ­¢æ¡ä»¶

2. åˆ†è£‚ (Divisive)ï¼šè‡ªé¡¶å‘ä¸‹
   - æ‰€æœ‰ç‚¹åˆå§‹åŒ–ä¸ºä¸€ä¸ªç°‡
   - é€æ­¥åˆ†è£‚ç°‡

é“¾æ¥æ–¹æ³• (Linkage):
  - Single: æœ€è¿‘ç‚¹è·ç¦»
  - Complete: æœ€è¿œç‚¹è·ç¦»
  - Average: å¹³å‡è·ç¦»
  - Ward: æœ€å°åŒ–æ–¹å·®
```

#### **å®ç°**

```python
from scipy.cluster.hierarchy import dendrogram, linkage
from scipy.spatial.distance import pdist, squareform

class AgglomerativeClustering:
    """å‡èšå±‚æ¬¡èšç±»"""

    def __init__(self, n_clusters=2, linkage='ward'):
        self.n_clusters = n_clusters
        self.linkage_method = linkage
        self.labels = None
        self.linkage_matrix = None

    def fit(self, X):
        """è®­ç»ƒå±‚æ¬¡èšç±»"""
        # è®¡ç®—é“¾æ¥çŸ©é˜µ
        self.linkage_matrix = linkage(X, method=self.linkage_method)

        # æ ¹æ® n_clusters åˆ‡å‰²æ ‘çŠ¶å›¾
        from scipy.cluster.hierarchy import fcluster
        self

-----

> continue

.labels = fcluster(self.linkage_matrix,
                                     self.n_clusters,
                                     criterion='maxclust') - 1  # è½¬ä¸º 0-based

        return self

    def fit_predict(self, X):
        """è®­ç»ƒå¹¶è¿”å›æ ‡ç­¾"""
        self.fit(X)
        return self.labels

# ==================== æ ‘çŠ¶å›¾å¯è§†åŒ– ====================

def plot_dendrogram(X, method='ward', title='Hierarchical Clustering Dendrogram'):
    """ç»˜åˆ¶æ ‘çŠ¶å›¾"""

    plt.figure(figsize=(12, 6))

    # è®¡ç®—é“¾æ¥çŸ©é˜µ
    Z = linkage(X, method=method)

    # ç»˜åˆ¶æ ‘çŠ¶å›¾
    dendrogram(Z)

    plt.xlabel('Sample Index')
    plt.ylabel('Distance')
    plt.title(title)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()

# ==================== ç¤ºä¾‹ ====================

if __name__ == '__main__':
    # ç”Ÿæˆæ•°æ®
    X, _ = make_blobs(n_samples=100, centers=3,
                      n_features=2, random_state=42)

    # ç»˜åˆ¶æ ‘çŠ¶å›¾
    plot_dendrogram(X, method='ward')

    # å±‚æ¬¡èšç±»
    hc = AgglomerativeClustering(n_clusters=3, linkage='ward')
    labels = hc.fit_predict(X)

    # å¯è§†åŒ–ç»“æœ
    plt.figure(figsize=(10, 8))
    plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=50)
    plt.xlabel('Feature 1')
    plt.ylabel('Feature 2')
    plt.title('Hierarchical Clustering Result')
    plt.colorbar(label='Cluster')
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()
```

---

## 11.3 é™ç»´æŠ€æœ¯

### ğŸ”¹ ä¸»æˆåˆ†åˆ†æ (PCA)

#### **ç®—æ³•åŸç†**

```
ç›®æ ‡ï¼šæ‰¾åˆ°æ–¹å·®æœ€å¤§çš„æ–¹å‘

æ•°å­¦è¡¨è¾¾ï¼š
1. ä¸­å¿ƒåŒ–æ•°æ®ï¼šX_centered = X - mean(X)
2. è®¡ç®—åæ–¹å·®çŸ©é˜µï¼šC = (1/n)Â·X^TÂ·X
3. ç‰¹å¾å€¼åˆ†è§£ï¼šC = VÂ·Î›Â·V^T
4. é€‰æ‹©å‰ k ä¸ªä¸»æˆåˆ†

æŠ•å½±ï¼š
  Z = XÂ·V_k  (é™ç»´åçš„æ•°æ®)

é‡æ„ï¼š
  X_reconstructed = ZÂ·V_k^T
```

#### **å®ç°**

```python
class PCA:
    """ä¸»æˆåˆ†åˆ†æ"""

    def __init__(self, n_components=2):
        self.n_components = n_components
        self.components = None
        self.mean = None
        self.explained_variance = None
        self.explained_variance_ratio = None

    def fit(self, X):
        """è®­ç»ƒ PCA"""
        # ä¸­å¿ƒåŒ–
        self.mean = np.mean(X, axis=0)
        X_centered = X - self.mean

        # è®¡ç®—åæ–¹å·®çŸ©é˜µ
        cov_matrix = np.cov(X_centered.T)

        # ç‰¹å¾å€¼åˆ†è§£
        eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)

        # æŒ‰ç‰¹å¾å€¼é™åºæ’åº
        idx = eigenvalues.argsort()[::-1]
        eigenvalues = eigenvalues[idx]
        eigenvectors = eigenvectors[:, idx]

        # é€‰æ‹©å‰ n_components ä¸ªä¸»æˆåˆ†
        self.components = eigenvectors[:, :self.n_components]
        self.explained_variance = eigenvalues[:self.n_components]
        self.explained_variance_ratio = (
            self.explained_variance / eigenvalues.sum()
        )

        return self

    def transform(self, X):
        """é™ç»´"""
        X_centered = X - self.mean
        return np.dot(X_centered, self.components)

    def fit_transform(self, X):
        """è®­ç»ƒå¹¶é™ç»´"""
        self.fit(X)
        return self.transform(X)

    def inverse_transform(self, Z):
        """é‡æ„"""
        return np.dot(Z, self.components.T) + self.mean

# ==================== å¯è§†åŒ– PCA ====================

def visualize_pca(X, y=None, title='PCA Visualization'):
    """å¯è§†åŒ– PCA é™ç»´ç»“æœ"""

    # PCA é™åˆ° 2D
    pca = PCA(n_components=2)
    X_pca = pca.fit_transform(X)

    plt.figure(figsize=(12, 5))

    # é™ç»´åçš„æ•°æ®
    plt.subplot(1, 2, 1)
    if y is not None:
        scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1],
                            c=y, cmap='viridis', alpha=0.6)
        plt.colorbar(scatter, label='Class')
    else:
        plt.scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.6)

    plt.xlabel(f'PC1 ({pca.explained_variance_ratio[0]:.2%})')
    plt.ylabel(f'PC2 ({pca.explained_variance_ratio[1]:.2%})')
    plt.title('PCA Projection')
    plt.grid(True, alpha=0.3)

    # æ–¹å·®è§£é‡Šæ¯”ä¾‹
    plt.subplot(1, 2, 2)
    n_components = min(10, X.shape[1])
    pca_full = PCA(n_components=n_components)
    pca_full.fit(X)

    cumsum = np.cumsum(pca_full.explained_variance_ratio)

    plt.plot(range(1, n_components+1),
            pca_full.explained_variance_ratio,
            'bo-', label='Individual')
    plt.plot(range(1, n_components+1),
            cumsum,
            'rs-', label='Cumulative')

    plt.xlabel('Principal Component')
    plt.ylabel('Explained Variance Ratio')
    plt.title('Variance Explained')
    plt.legend()
    plt.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

# ==================== ç¤ºä¾‹ï¼šæ‰‹å†™æ•°å­— ====================

if __name__ == '__main__':
    from sklearn.datasets import load_digits

    # åŠ è½½ MNIST æ•°å­—æ•°æ®é›†
    digits = load_digits()
    X, y = digits.data, digits.target

    print(f"åŸå§‹ç»´åº¦: {X.shape}")

    # PCA é™ç»´
    pca = PCA(n_components=2)
    X_pca = pca.fit_transform(X)

    print(f"é™ç»´å: {X_pca.shape}")
    print(f"æ–¹å·®è§£é‡Šæ¯”ä¾‹: {pca.explained_variance_ratio}")

    # å¯è§†åŒ–
    visualize_pca(X, y, title='PCA on MNIST Digits')
```

---

### ğŸ”¹ t-SNE (t-Distributed Stochastic Neighbor Embedding)

#### **ç®—æ³•åŸç†**

```
ç›®æ ‡ï¼šä¿æŒå±€éƒ¨ç»“æ„

æ­¥éª¤ï¼š
1. è®¡ç®—é«˜ç»´ç©ºé—´ä¸­ç‚¹å¯¹çš„ç›¸ä¼¼åº¦ p_ij
2. åœ¨ä½ç»´ç©ºé—´éšæœºåˆå§‹åŒ–
3. è®¡ç®—ä½ç»´ç©ºé—´ä¸­çš„ç›¸ä¼¼åº¦ q_ij
4. æœ€å°åŒ– KL æ•£åº¦ï¼šKL(P||Q)

ç‰¹ç‚¹ï¼š
  âœ“ æ“…é•¿å¯è§†åŒ–èšç±»ç»“æ„
  âœ“ ä¿ç•™å±€éƒ¨é‚»åŸŸ
  âœ— è®¡ç®—å¤æ‚åº¦é«˜ O(nÂ²)
  âœ— å…¨å±€ç»“æ„ä¸ä¿è¯
  âœ— æ¯æ¬¡è¿è¡Œç»“æœä¸åŒ
```

#### **ä½¿ç”¨ sklearn**

```python
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt

def visualize_tsne(X, y, perplexity=30, title='t-SNE Visualization'):
    """t-SNE å¯è§†åŒ–"""

    print("è¿è¡Œ t-SNE...")
    tsne = TSNE(n_components=2, perplexity=perplexity,
                random_state=42, n_iter=1000)
    X_tsne = tsne.fit_transform(X)

    plt.figure(figsize=(10, 8))

    if y is not None:
        scatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1],
                            c=y, cmap='tab10', alpha=0.6, s=20)
        plt.colorbar(scatter, label='Class')
    else:
        plt.scatter(X_tsne[:, 0], X_tsne[:, 1], alpha=0.6, s=20)

    plt.xlabel('t-SNE Component 1')
    plt.ylabel('t-SNE Component 2')
    plt.title(title)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()

# ==================== å¯¹æ¯” PCA å’Œ t-SNE ====================

def compare_pca_tsne(X, y):
    """å¯¹æ¯” PCA å’Œ t-SNE"""

    fig, axes = plt.subplots(1, 2, figsize=(16, 6))

    # PCA
    pca = PCA(n_components=2)
    X_pca = pca.fit_transform(X)

    scatter1 = axes[0].scatter(X_pca[:, 0], X_pca[:, 1],
                              c=y, cmap='tab10', alpha=0.6, s=20)
    axes[0].set_xlabel(f'PC1 ({pca.explained_variance_ratio[0]:.2%})')
    axes[0].set_ylabel(f'PC2 ({pca.explained_variance_ratio[1]:.2%})')
    axes[0].set_title('PCA')
    axes[0].grid(True, alpha=0.3)
    plt.colorbar(scatter1, ax=axes[0], label='Class')

    # t-SNE
    print("è¿è¡Œ t-SNE...")
    tsne = TSNE(n_components=2, random_state=42)
    X_tsne = tsne.fit_transform(X)

    scatter2 = axes[1].scatter(X_tsne[:, 0], X_tsne[:, 1],
                              c=y, cmap='tab10', alpha=0.6, s=20)
    axes[1].set_xlabel('t-SNE 1')
    axes[1].set_ylabel('t-SNE 2')
    axes[1].set_title('t-SNE')
    axes[1].grid(True, alpha=0.3)
    plt.colorbar(scatter2, ax=axes[1], label='Class')

    plt.tight_layout()
    plt.show()

# ==================== ç¤ºä¾‹ ====================

if __name__ == '__main__':
    from sklearn.datasets import load_digits

    digits = load_digits()
    X, y = digits.data, digits.target

    # é™é‡‡æ ·ï¼ˆt-SNE å¾ˆæ…¢ï¼‰
    from sklearn.model_selection import train_test_split
    X_sample, _, y_sample, _ = train_test_split(
        X, y, train_size=500, stratify=y, random_state=42
    )

    # å¯¹æ¯”
    compare_pca_tsne(X_sample, y_sample)
```

---

### ğŸ”¹ UMAP (Uniform Manifold Approximation and Projection)

```
ä¼˜åŠ¿ï¼š
  âœ“ æ¯” t-SNE å¿«
  âœ“ æ›´å¥½åœ°ä¿ç•™å…¨å±€ç»“æ„
  âœ“ æ”¯æŒæ–°æ•°æ®çš„ transform

ä½¿ç”¨ï¼š
```

```python
import umap

def visualize_umap(X, y, title='UMAP Visualization'):
    """UMAP å¯è§†åŒ–"""

    print("è¿è¡Œ UMAP...")
    reducer = umap.UMAP(n_components=2, random_state=42)
    X_umap = reducer.fit_transform(X)

    plt.figure(figsize=(10, 8))
    scatter = plt.scatter(X_umap[:, 0], X_umap[:, 1],
                         c=y, cmap='tab10', alpha=0.6, s=20)
    plt.colorbar(scatter, label='Class')
    plt.xlabel('UMAP 1')
    plt.ylabel('UMAP 2')
    plt.title(title)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()
```

---

## 11.4 è‡ªç›‘ç£å­¦ä¹  (Self-Supervised Learning)

### ğŸ¯ æ ¸å¿ƒæ€æƒ³

```
ä»æ— æ ‡ç­¾æ•°æ®ä¸­è‡ªåŠ¨ç”Ÿæˆç›‘ç£ä¿¡å·

å¸¸è§é¢„è®­ç»ƒä»»åŠ¡ï¼š

å›¾åƒï¼š
  - æ—‹è½¬é¢„æµ‹
  - æ‹¼å›¾æ±‚è§£
  - å›¾åƒä¿®å¤
  - å¯¹æ¯”å­¦ä¹ 

æ–‡æœ¬ï¼š
  - æ©ç è¯­è¨€æ¨¡å‹ (MLM)
  - ä¸‹ä¸€å¥é¢„æµ‹ (NSP)
  - è‡ªå›å½’è¯­è¨€æ¨¡å‹
```

### ğŸ”¹ å›¾åƒå¯¹æ¯”å­¦ä¹ ï¼šSimCLR

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class SimCLR(nn.Module):
    """SimCLR å¯¹æ¯”å­¦ä¹ æ¡†æ¶"""

    def __init__(self, base_encoder, projection_dim=128):
        super(SimCLR, self).__init__()

        # ç¼–ç å™¨ï¼ˆå¦‚ ResNetï¼‰
        self.encoder = base_encoder

        # æŠ•å½±å¤´
        self.projector = nn.Sequential(
            nn.Linear(base_encoder.output_dim, 2048),
            nn.ReLU(),
            nn.Linear(2048, projection_dim)
        )

    def forward(self, x):
        # æå–ç‰¹å¾
        h = self.encoder(x)

        # æŠ•å½±
        z = self.projector(h)

        # L2 å½’ä¸€åŒ–
        z = F.normalize(z, dim=1)

        return z

class NTXentLoss(nn.Module):
    """å½’ä¸€åŒ–æ¸©åº¦äº¤å‰ç†µæŸå¤±"""

    def __init__(self, temperature=0.5):
        super(NTXentLoss, self).__init__()
        self.temperature = temperature

    def forward(self, z_i, z_j):
        """
        å‚æ•°:
            z_i, z_j: ä¸¤ä¸ªå¢å¼ºè§†å›¾çš„è¡¨ç¤º
        """
        batch_size = z_i.size(0)

        # æ‹¼æ¥
        z = torch.cat([z_i, z_j], dim=0)  # (2B, D)

        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        sim = torch.mm(z, z.T) / self.temperature  # (2B, 2B)

        # æ©ç ï¼šå»æ‰å¯¹è§’çº¿
        mask = torch.eye(2 * batch_size, dtype=torch.bool).to(z.device)
        sim = sim.masked_fill(mask, -1e9)

        # æ­£æ ·æœ¬ï¼šå¯¹è§’å—å¤–çš„å¯¹åº”ä½ç½®
        positive_pairs = torch.arange(batch_size).to(z.device)
        positive_pairs = torch.cat([
            positive_pairs + batch_size,  # z_i çš„æ­£æ ·æœ¬æ˜¯ z_j
            positive_pairs                 # z_j çš„æ­£æ ·æœ¬æ˜¯ z_i
        ])

        # äº¤å‰ç†µæŸå¤±
        loss = F.cross_entropy(sim, positive_pairs)

        return loss

# ==================== æ•°æ®å¢å¼º ====================

from torchvision import transforms

def get_simclr_augmentation():
    """SimCLR æ•°æ®å¢å¼º"""

    color_jitter = transforms.ColorJitter(
        brightness=0.8, contrast=0.8,
        saturation=0.8, hue=0.2
    )

    return transforms.Compose([
        transforms.RandomResizedCrop(224),
        transforms.RandomHorizontalFlip(),
        transforms.RandomApply([color_jitter], p=0.8),
        transforms.RandomGrayscale(p=0.2),
        transforms.ToTensor(),
        transforms.Normalize(
            mean=[0.485, 0.456, 0.406],
            std=[0.229, 0.224, 0.225]
        )
    ])

# ==================== è®­ç»ƒ SimCLR ====================

def train_simclr(model, dataloader, num_epochs=100):
    """è®­ç»ƒ SimCLR"""

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.to(device)

    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
    criterion = NTXentLoss(temperature=0.5)

    for epoch in range(num_epochs):
        model.train()
        total_loss = 0

        for (x_i, x_j), _ in dataloader:
            x_i, x_j = x_i.to(device), x_j.to(device)

            # å‰å‘ä¼ æ’­
            z_i = model(x_i)
            z_j = model(x_j)

            # è®¡ç®—æŸå¤±
            loss = criterion(z_i, z_j)

            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            total_loss += loss.item()

        if (epoch + 1) % 10 == 0:
            print(f'Epoch {epoch+1}: Loss = {total_loss/len(dataloader):.4f}')

    return model
```

---

### ğŸ”¹ è‡ªç¼–ç å™¨ (Autoencoder)

```python
class Autoencoder(nn.Module):
    """è‡ªç¼–ç å™¨"""

    def __init__(self, input_dim, encoding_dim):
        super(Autoencoder, self).__init__()

        # ç¼–ç å™¨
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, 512),
            nn.ReLU(),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Linear(256, encoding_dim)
        )

        # è§£ç å™¨
        self.decoder = nn.Sequential(
            nn.Linear(encoding_dim, 256),
            nn.ReLU(),
            nn.Linear(256, 512),
            nn.ReLU(),
            nn.Linear(512, input_dim),
            nn.Sigmoid()  # è¾“å‡º [0, 1]
        )

    def forward(self, x):
        # ç¼–ç 
        encoded = self.encoder(x)

        # è§£ç 
        decoded = self.decoder(encoded)

        return decoded

    def encode(self, x):
        """ä»…ç¼–ç """
        return self.encoder(x)

# ==================== è®­ç»ƒè‡ªç¼–ç å™¨ ====================

def train_autoencoder(model, dataloader, num_epochs=50):
    """è®­ç»ƒè‡ªç¼–ç å™¨"""

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.to(device)

    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
    criterion = nn.MSELoss()

    for epoch in range(num_epochs):
        model.train()
        total_loss = 0

        for data, _ in dataloader:
            data = data.to(device)
            data = data.view(data.size(0), -1)  # Flatten

            # å‰å‘ä¼ æ’­
            reconstructed = model(data)

            # é‡æ„æŸå¤±
            loss = criterion(reconstructed, data)

            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            total_loss += loss.item()

        if (epoch + 1) % 10 == 0:
            print(f'Epoch {epoch+1}: Loss = {total_loss/len(dataloader):.4f}')

    return model

# ==================== å¯è§†åŒ–é‡æ„ ====================

def visualize_reconstruction(model, dataloader, num_images=10):
    """å¯è§†åŒ–é‡æ„ç»“æœ"""

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.eval()

    # è·å–ä¸€æ‰¹æ•°æ®
    data, _ = next(iter(dataloader))
    data = data[:num_images].to(device)

    # é‡æ„
    with torch.no_grad():
        data_flat = data.view(data.size(0), -1)
        reconstructed = model(data_flat)
        reconstructed = reconstructed.view_as(data)

    # ç»˜å›¾
    fig, axes = plt.subplots(2, num_images, figsize=(num_images*2, 4))

    for i in range(num_images):
        # åŸå›¾
        axes[0, i].imshow(data[i].cpu().squeeze(), cmap='gray')
        axes[0, i].axis('off')
        if i == 0:
            axes[0, i].set_title('Original', fontsize=12)

        # é‡æ„
        axes[1, i].imshow(reconstructed[i].cpu().squeeze(), cmap='gray')
        axes[1, i].axis('off')
        if i == 0:
            axes[1, i].set_title('Reconstructed', fontsize=12)

    plt.tight_layout()
    plt.show()

# ==================== ç¤ºä¾‹ ====================

if __name__ == '__main__':
    from torchvision.datasets import MNIST
    from torch.utils.data import DataLoader

    # åŠ è½½ MNIST
    transform = transforms.Compose([
        transforms.ToTensor()
    ])

    train_dataset = MNIST(root='./data', train=True,
                         download=True, transform=transform)
    train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)

    # åˆ›å»ºè‡ªç¼–ç å™¨
    autoencoder = Autoencoder(input_dim=784, encoding_dim=32)

    # è®­ç»ƒ
    print("è®­ç»ƒè‡ªç¼–ç å™¨...")
    train_autoencoder(autoencoder, train_loader, num_epochs=20)

    # å¯è§†åŒ–
    visualize_reconstruction(autoencoder, train_loader)
```

---

### ğŸ”¹ å˜åˆ†è‡ªç¼–ç å™¨ (VAE)

```python
class VAE(nn.Module):
    """å˜åˆ†è‡ªç¼–ç å™¨"""

    def __init__(self, input_dim, latent_dim):
        super(VAE, self).__init__()

        # ç¼–ç å™¨
        self.fc1 = nn.Linear(input_dim, 512)
        self.fc21 = nn.Linear(512, latent_dim)  # å‡å€¼
        self.fc22 = nn.Linear(512, latent_dim)  # å¯¹æ•°æ–¹å·®

        # è§£ç å™¨
        self.fc3 = nn.Linear(latent_dim, 512)
        self.fc4 = nn.Linear(512, input_dim)

    def encode(self, x):
        """ç¼–ç ä¸ºå‡å€¼å’Œæ–¹å·®"""
        h = F.relu(self.fc1(x))
        mu = self.fc21(h)
        logvar = self.fc22(h)
        return mu, logvar

    def reparameterize(self, mu, logvar):
        """é‡å‚æ•°åŒ–æŠ€å·§"""
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std

    def decode(self, z):
        """è§£ç """
        h = F.relu(self.fc3(z))
        return torch.sigmoid(self.fc4(h))

    def forward(self, x):
        mu, logvar = self.encode(x)
        z = self.reparameterize(mu, logvar)
        return self.decode(z), mu, logvar

def vae_loss(recon_x, x, mu, logvar):
    """VAE æŸå¤±å‡½æ•°"""
    # é‡æ„æŸå¤±
    recon_loss = F.binary_cross_entropy(recon_x, x, reduction='sum')

    # KL æ•£åº¦
    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())

    return recon_loss + kl_loss

# è®­ç»ƒç±»ä¼¼è‡ªç¼–ç å™¨ï¼Œä½†ä½¿ç”¨ vae_loss
```

---

## 11.5 å®æˆ˜ï¼šå®¢æˆ·åˆ†ç¾¤

```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
import seaborn as sns

# ==================== ç”Ÿæˆæ¨¡æ‹Ÿå®¢æˆ·æ•°æ® ====================

def generate_customer_data(n_samples=1000):
    """ç”Ÿæˆæ¨¡æ‹Ÿå®¢æˆ·æ•°æ®"""

    np.random.seed(42)

    data = {
        'customer_id': range(n_samples),
        'age': np.random.randint(18, 70, n_samples),
        'income': np.random.lognormal(10, 1, n_samples),
        'spending_score': np.random.randint(1, 100, n_samples),
        'num_purchases': np.random.poisson(5, n_samples),
        'avg_purchase_value': np.random.gamma(50, 2, n_samples),
        'days_since_last_purchase': np.random.exponential(30, n_samples)
    }

    df = pd.DataFrame(data)
    return df

# ==================== å®¢æˆ·åˆ†ç¾¤æµç¨‹ ====================

class CustomerSegmentation:
    """å®¢æˆ·åˆ†ç¾¤åˆ†æ"""

    def __init__(self, n_clusters=4):
        self.n_clusters = n_clusters
        self.scaler = StandardScaler()
        self.pca = PCA(n_components=2)
        self.kmeans = KMeans(n_clusters=n_clusters, random_state=42)

    def preprocess(self, df):
        """æ•°æ®é¢„å¤„ç†"""
        # é€‰æ‹©æ•°å€¼ç‰¹å¾
        features = ['age', 'income', 'spending_score',
                   'num_purchases', 'avg_purchase_value',
                   'days_since_last_purchase']

        X = df[features].values

        # æ ‡å‡†åŒ–
        X_scaled = self.scaler.fit_transform(X)

        return X_scaled, features

    def fit(self, df):
        """è®­ç»ƒåˆ†ç¾¤æ¨¡å‹"""
        X_scaled, features = self.preprocess(df)

        # K-Means èšç±»
        labels = self.kmeans.fit_predict(X_scaled)

        # PCA é™ç»´ç”¨äºå¯è§†åŒ–
        X_pca = self.pca.fit_transform(X_scaled)

        # æ·»åŠ åˆ° DataFrame
        df['cluster'] = labels
        df['pca1'] = X_pca[:, 0]
        df['pca2'] = X_pca[:, 1]

        return df

    def analyze_clusters(self, df):
        """åˆ†æç°‡ç‰¹å¾"""
        features = ['age', 'income', 'spending_score',
                   'num_purchases', 'avg_purchase_value',
                   'days_since_last_purchase']

        print("\nå„ç°‡ç»Ÿè®¡ä¿¡æ¯:")
        print("="*80)

        for cluster_id in range(self.n_clusters):
            cluster_data = df[df['cluster'] == cluster_id]

            print(f"\nç°‡ {cluster_id} (n={len(cluster_data)}):")
            print(cluster_data[features].describe().T[['mean', 'std']])

        return df.groupby('cluster')[features].mean()

    def visualize(self, df):
        """å¯è§†åŒ–åˆ†ç¾¤ç»“æœ"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))

        # 1. PCA ç©ºé—´ä¸­çš„ç°‡
        axes[0, 0].scatter(df['pca1'], df['pca2'],
                          c=df['cluster'], cmap='viridis',
                          alpha=0.6, s=50)
        axes[0, 0].set_xlabel('PC1')
        axes[0, 0].set_ylabel('PC2')
        axes[0, 0].set_title('Customer Segments in PCA Space')
        axes[0, 0].grid(True, alpha=0.3)

        # 2. å¹´é¾„ vs æ”¶å…¥
        for cluster_id in range(self.n_clusters):
            cluster_data = df[df['cluster'] == cluster_id]
            axes[0, 1].scatter(cluster_data['age'],
                             cluster_data['income'],
                             label=f'Cluster {cluster_id}',
                             alpha=0.6, s=30)

        axes[0, 1].set_xlabel('Age')
        axes[0, 1].set_ylabel('Income')
        axes[0, 1].set_title('Age vs Income by Cluster')
        axes[0, 1].legend()
        axes[0, 1].grid(True, alpha=0.3)

        # 3. æ¶ˆè´¹åˆ†æ•°åˆ†å¸ƒ
        for cluster_id in range(self.n_clusters):
            cluster_data = df[df['cluster'] == cluster_id]
            axes[1, 0].hist(cluster_data['spending_score'],
                          alpha=0.5, bins=20,
                          label=f'Cluster {cluster_id}')

        axes[1, 0].set_xlabel('Spending Score')
        axes[1, 0].set_ylabel('Frequency')
        axes[1, 0].set_title('Spending Score Distribution')
        axes[1, 0].legend()
        axes[1, 0].grid(True, alpha=0.3)

        # 4. ç°‡å¤§å°
        cluster_sizes = df['cluster'].value_counts().sort_index()
        axes[1, 1].bar(cluster_sizes.index, cluster_sizes.values,
                      color='steelblue', alpha=0.7)
        axes[1, 1].set_xlabel('Cluster')
        axes[1, 1].set_ylabel('Number of Customers')
        axes[1, 1].set_title('Cluster Sizes')
        axes[1, 1].grid(True, alpha=0.3, axis='y')

        plt.tight_layout()
        plt.show()

    def recommend_actions(self, cluster_stats):
        """ä¸ºæ¯ä¸ªç°‡æ¨èè¥é”€ç­–ç•¥"""
        print("\nè¥é”€ç­–ç•¥æ¨è:")
        print("="*80)

        for cluster_id in range(self.n_clusters):
            stats = cluster_stats.loc[cluster_id]

            print(f"\nç°‡ {cluster_id}:")

            if stats['income'] > cluster_stats['income'].median():
                if stats['spending_score'] > cluster_stats['spending_score'].median():
                    print("  ç±»å‹: é«˜ä»·å€¼å®¢æˆ· ğŸ’")
                    print("  ç­–ç•¥: VIP æœåŠ¡ã€é«˜ç«¯äº§å“æ¨è")
                else:
                    print("  ç±»å‹: æ½œåŠ›å®¢æˆ· ğŸ“ˆ")
                    print("  ç­–ç•¥: ä¸ªæ€§åŒ–æ¨èã€ä¿ƒé”€æ´»åŠ¨")
            else:
                if stats['spending_score'] > cluster_stats['spending_score'].median():
                    print("  ç±»å‹: æ´»è·ƒå®¢æˆ· â­")
                    print("  ç­–ç•¥: å¿ è¯šåº¦è®¡åˆ’ã€ä¼šå‘˜ä¼˜æƒ ")
                else:
                    print("  ç±»å‹: ä½æ´»è·ƒå®¢æˆ· ğŸ’¤")
                    print("  ç­–ç•¥: æ¿€æ´»campaignã€æŠ˜æ‰£ä¼˜æƒ ")

# ==================== ä¸»ç¨‹åº ====================

if __name__ == '__main__':
    # ç”Ÿæˆæ•°æ®
    df = generate_customer_data(n_samples=1000)

    print("å®¢æˆ·æ•°æ®é¢„è§ˆ:")
    print(df.head())

    # å®¢æˆ·åˆ†ç¾¤
    segmentation = CustomerSegmentation(n_clusters=4)
    df = segmentation.fit(df)

    # åˆ†æç°‡
    cluster_stats = segmentation.analyze_clusters(df)

    # å¯è§†åŒ–
    segmentation.visualize(df)

    # æ¨èç­–ç•¥
    segmentation.recommend_actions(cluster_stats)
```

---

## ğŸ“ æœ¬ç« ä½œä¸š

### ä½œä¸š 1ï¼šèšç±»å¯¹æ¯”

```python
# åœ¨åŒä¸€æ•°æ®é›†ä¸Šå¯¹æ¯”ï¼š
# 1. K-Means
# 2. DBSCAN
# 3. å±‚æ¬¡èšç±»

# è¯„ä¼°æŒ‡æ ‡ï¼š
#   - Silhouette Score
#   - Davies-Bouldin Index
#   - Calinski-Harabasz Index

# åˆ†æï¼š
#   - å“ªç§ç®—æ³•æœ€é€‚åˆä½ çš„æ•°æ®ï¼Ÿ
#   - ä¸åŒå‚æ•°çš„å½±å“
```

### ä½œä¸š 2ï¼šé™ç»´æŠ€æœ¯å¯¹æ¯”

```python
# åœ¨ MNIST æˆ– Fashion-MNIST ä¸Šå¯¹æ¯”ï¼š
# 1. PCA
# 2. t-SNE
# 3. UMAP

# è¯„ä¼°ï¼š
#   - å¯è§†åŒ–æ•ˆæœ
#   - è¿è¡Œæ—¶é—´
#   - ä¿ç•™çš„ä¿¡æ¯é‡
#   - åœ¨é™ç»´åæ•°æ®ä¸Šè®­ç»ƒåˆ†ç±»å™¨çš„æ€§èƒ½
```

### ä½œä¸š 3ï¼šå¼‚å¸¸æ£€æµ‹

```python
# å®ç°å¼‚å¸¸æ£€æµ‹ç³»ç»Ÿï¼š
# 1. ä½¿ç”¨ Autoencoder
# 2. ä½¿ç”¨ Isolation Forest
# 3. ä½¿ç”¨ One-Class SVM

# æ•°æ®é›†ï¼šä¿¡ç”¨å¡æ¬ºè¯ˆæ£€æµ‹
# è¯„ä¼°ï¼šROC-AUC, Precision-Recall
```

### ä½œä¸š 4ï¼šè‡ªç›‘ç£å­¦ä¹ 

```python
# å®ç°ä¸€ä¸ªè‡ªç›‘ç£å­¦ä¹ pipelineï¼š
# 1. é€‰æ‹©é¢„è®­ç»ƒä»»åŠ¡ï¼ˆå¦‚ SimCLRï¼‰
# 2. åœ¨æ— æ ‡ç­¾æ•°æ®ä¸Šé¢„è®­ç»ƒ
# 3. åœ¨å°é‡æ ‡æ³¨æ•°æ®ä¸Šå¾®è°ƒ
# 4. å¯¹æ¯”ï¼šä»é›¶è®­ç»ƒ vs è‡ªç›‘ç£é¢„è®­ç»ƒ
```

---

## ğŸ”‘ æœ¬ç« å…³é”®æ¦‚å¿µ

| æ¦‚å¿µ | è¯´æ˜ |
|------|------|
| æ— ç›‘ç£å­¦ä¹  | ä»æ— æ ‡ç­¾æ•°æ®å­¦ä¹ ç»“æ„ |
| èšç±» | å°†ç›¸ä¼¼æ ·æœ¬åˆ†ç»„ |
| K-Means | åŸºäºä¸­å¿ƒçš„èšç±» |
| DBSCAN | åŸºäºå¯†åº¦çš„èšç±» |
| å±‚æ¬¡èšç±» | æ„å»ºèšç±»æ ‘ |
| PCA | çº¿æ€§é™ç»´ |
| t-SNE | éçº¿æ€§é™ç»´ï¼ˆå¯è§†åŒ–ï¼‰ |
| UMAP | å¿«é€Ÿéçº¿æ€§é™ç»´ |
| è‡ªç›‘ç£å­¦ä¹  | è‡ªåŠ¨ç”Ÿæˆç›‘ç£ä¿¡å· |
| å¯¹æ¯”å­¦ä¹  | å­¦ä¹ ç›¸ä¼¼å’Œä¸ç›¸ä¼¼ |
| è‡ªç¼–ç å™¨ | é‡æ„å­¦ä¹ è¡¨ç¤º |

---

