# ç¬¬å…«ç« ï¼šAttention ä¸ Transformer

## ğŸ“Œ ç« èŠ‚ç›®æ ‡
- ç†è§£æ³¨æ„åŠ›æœºåˆ¶çš„åŠ¨æœºå’ŒåŸç†
- æŒæ¡ Self-Attention çš„è®¡ç®—
- æ·±å…¥å­¦ä¹  Transformer æ¶æ„
- äº†è§£ BERT å’Œ GPT çš„è®¾è®¡
- å®æˆ˜ï¼šæ–‡æœ¬åˆ†ç±»ã€ç¿»è¯‘

---

## 8.1 ä¸ºä»€ä¹ˆéœ€è¦ Attentionï¼Ÿ

### ğŸš¨ Seq2Seq çš„ç“¶é¢ˆ

**ä¼ ç»Ÿ Seq2Seq**ï¼š

```
ç¼–ç å™¨ï¼šæºå¥å­ â†’ å›ºå®šé•¿åº¦å‘é‡ h
          â†“
è§£ç å™¨ï¼šh â†’ ç›®æ ‡å¥å­

é—®é¢˜ï¼šæ‰€æœ‰ä¿¡æ¯å‹ç¼©åˆ°å›ºå®šå‘é‡ h
     é•¿å¥å­ä¿¡æ¯ä¸¢å¤±ï¼
```

**ä¾‹å­**ï¼šç¿»è¯‘é•¿å¥

```
è‹±æ–‡ï¼šThe quick brown fox jumps over the lazy dog.
     (9ä¸ªè¯)

ç¼–ç æˆå•ä¸ªå‘é‡ h (512ç»´)
     â†“
è§£ç æ—¶ï¼Œæ—©æœŸè¯çš„ä¿¡æ¯å·²ç»æ¨¡ç³Š
```

### âœ… Attention è§£å†³æ–¹æ¡ˆ

**æ ¸å¿ƒæ€æƒ³**ï¼šè§£ç æ¯ä¸ªè¯æ—¶ï¼ŒåŠ¨æ€å…³æ³¨æºå¥å­çš„ä¸åŒéƒ¨åˆ†

```
ç¼–ç å™¨ï¼šäº§ç”Ÿæ‰€æœ‰æ—¶åˆ»çš„éšè—çŠ¶æ€ hâ‚, hâ‚‚, ..., hâ‚™

è§£ç ç¬¬ t ä¸ªè¯æ—¶ï¼š
  1. è®¡ç®—ä¸æ¯ä¸ª háµ¢ çš„ç›¸å…³æ€§
  2. åŠ æƒæ±‚å’Œå¾—åˆ° context vector c_t
  3. ä½¿ç”¨ c_t ç”Ÿæˆè¾“å‡º
```

---

## 8.2 Attention æœºåˆ¶

### ğŸ“ è®¡ç®—è¿‡ç¨‹

**1. è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°**

```
score(h_t, h_s) = h_t^T Â· W Â· h_s

æˆ–ç®€åŒ–ç‰ˆï¼š
score(h_t, h_s) = h_t^T Â· h_s  (ç‚¹ç§¯)
```

**2. å½’ä¸€åŒ–ï¼ˆSoftmaxï¼‰**

```
Î±_t,s = exp(score(h_t, h_s)) / Î£_i exp(score(h_t, h_i))

Î±_t = [Î±_t,1, Î±_t,2, ..., Î±_t,n]  (æ³¨æ„åŠ›æƒé‡)
```

**3. åŠ æƒæ±‚å’Œ**

```
c_t = Î£_s Î±_t,s Â· h_s

c_t: context vectorï¼ˆä¸Šä¸‹æ–‡å‘é‡ï¼‰
```

**4. ç”Ÿæˆè¾“å‡º**

```
output_t = f(h_t, c_t)
```

### ğŸ’» å®ç°

```python
class BahdanauAttention(nn.Module):
    """Bahdanau (Additive) Attention"""

    def __init__(self, hidden_size):
        super(BahdanauAttention, self).__init__()
        self.W_h = nn.Linear(hidden_size, hidden_size)
        self.W_s = nn.Linear(hidden_size, hidden_size)
        self.v = nn.Linear(hidden_size, 1)

    def forward(self, query, keys, values):
        """
        å‚æ•°ï¼š
            query: (batch, hidden_size) è§£ç å™¨å½“å‰çŠ¶æ€
            keys: (batch, seq_len, hidden_size) ç¼–ç å™¨æ‰€æœ‰çŠ¶æ€
            values: (batch, seq_len, hidden_size) åŒ keys
        """
        # query: (batch, 1, hidden_size)
        query = query.unsqueeze(1)

        # è®¡ç®—åˆ†æ•°
        score = self.v(torch.tanh(
            self.W_h(keys) + self.W_s(query)
        ))  # (batch, seq_len, 1)

        # æ³¨æ„åŠ›æƒé‡
        attention_weights = torch.softmax(score, dim=1)

        # åŠ æƒæ±‚å’Œ
        context = torch.sum(attention_weights * values, dim=1)
        # (batch, hidden_size)

        return context, attention_weights

class LuongAttention(nn.Module):
    """Luong (Multiplicative) Attention"""

    def __init__(self, hidden_size, method='dot'):
        super(LuongAttention, self).__init__()
        self.method = method

        if method == 'general':
            self.W = nn.Linear(hidden_size, hidden_size, bias=False)
        elif method == 'concat':
            self.W = nn.Linear(hidden_size * 2, hidden_size)
            self.v = nn.Linear(hidden_size, 1)

    def forward(self, query, keys, values):
        """
        å‚æ•°åŒä¸Š
        """
        query = query.unsqueeze(1)  # (batch, 1, hidden)

        if self.method == 'dot':
            # ç‚¹ç§¯æ³¨æ„åŠ›
            score = torch.bmm(query, keys.transpose(1, 2))
        elif self.method == 'general':
            # ä¸€èˆ¬æ³¨æ„åŠ›
            score = torch.bmm(self.W(query), keys.transpose(1, 2))
        elif self.method == 'concat':
            # æ‹¼æ¥æ³¨æ„åŠ›
            query_expanded = query.expand(-1, keys.size(1), -1)
            score = self.v(torch.tanh(
                self.W(torch.cat([query_expanded, keys], dim=2))
            ))

        # (batch, 1, seq_len)
        attention_weights = torch.softmax(score, dim=2)

        # åŠ æƒæ±‚å’Œ
        context = torch.bmm(attention_weights, values)
        # (batch, 1, hidden_size)

        return context.squeeze(1), attention_weights.squeeze(1)
```

---

## 8.3 Self-Attention â­

### ğŸ¯ åŠ¨æœº

**æ™®é€š Attention**ï¼šæŸ¥è¯¢å’Œé”®æ¥è‡ªä¸åŒåºåˆ—ï¼ˆç¼–ç å™¨å’Œè§£ç å™¨ï¼‰

**Self-Attention**ï¼šæŸ¥è¯¢ã€é”®ã€å€¼éƒ½æ¥è‡ªåŒä¸€åºåˆ—

**ä½œç”¨**ï¼š
- æ•æ‰åºåˆ—å†…éƒ¨çš„ä¾èµ–å…³ç³»
- å¹¶è¡Œè®¡ç®—ï¼ˆä¸åƒ RNN éœ€è¦ä¸²è¡Œï¼‰
- å…¨å±€æ„Ÿå—é‡ï¼ˆæ¯ä¸ªä½ç½®éƒ½èƒ½çœ‹åˆ°æ‰€æœ‰ä½ç½®ï¼‰

### ğŸ“ è®¡ç®—æœºåˆ¶

**Query, Key, Value (Q, K, V)**

```
ç»™å®šè¾“å…¥åºåˆ— X = [xâ‚, xâ‚‚, ..., xâ‚™]

é€šè¿‡çº¿æ€§å˜æ¢å¾—åˆ°ï¼š
Q = X Â· W_Q  (queries)
K = X Â· W_K  (keys)
V = X Â· W_V  (values)

æ¯ä¸ªéƒ½æ˜¯ (seq_len, d_k) çŸ©é˜µ
```

**Scaled Dot-Product Attention**

```
Attention(Q, K, V) = softmax(QÂ·K^T / âˆšd_k) Â· V

æ­¥éª¤ï¼š
1. è®¡ç®— Q å’Œ K çš„ç‚¹ç§¯ï¼šQÂ·K^T
   ç»“æœï¼š(seq_len, seq_len) æ³¨æ„åŠ›çŸ©é˜µ

2. ç¼©æ”¾ï¼šé™¤ä»¥ âˆšd_k
   é˜²æ­¢ç‚¹ç§¯è¿‡å¤§å¯¼è‡´ softmax é¥±å’Œ

3. Softmaxï¼šå¯¹æ¯ä¸€è¡Œå½’ä¸€åŒ–
   å¾—åˆ°æ³¨æ„åŠ›æƒé‡

4. åŠ æƒæ±‚å’Œï¼šä¹˜ä»¥ V
   å¾—åˆ°è¾“å‡º
```

### ğŸ’» å®ç°

```python
class ScaledDotProductAttention(nn.Module):
    def __init__(self, dropout=0.1):
        super().__init__()
        self.dropout = nn.Dropout(dropout)

    def forward(self, Q, K, V, mask=None):
        """
        å‚æ•°ï¼š
            Q: (batch, n_heads, seq_len, d_k)
            K: (batch, n_heads, seq_len, d_k)
            V: (batch, n_heads, seq_len, d_v)
            mask: (batch, 1, seq_len, seq_len) å¯é€‰
        """
        d_k = Q.size(-1)

        # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
        scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(d_k)
        # (batch, n_heads, seq_len, seq_len)

        # åº”ç”¨ maskï¼ˆå¯é€‰ï¼Œç”¨äº padding æˆ–æœªæ¥ä¿¡æ¯ï¼‰
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)

        # Softmax
        attention_weights = torch.softmax(scores, dim=-1)
        attention_weights = self.dropout(attention_weights)

        # åŠ æƒæ±‚å’Œ
        output = torch.matmul(attention_weights, V)
        # (batch, n_heads, seq_len, d_v)

        return output, attention_weights
```

---

## 8.4 Multi-Head Attention

### ğŸ¯ ä¸ºä»€ä¹ˆéœ€è¦å¤šå¤´ï¼Ÿ

**å•å¤´æ³¨æ„åŠ›**ï¼šåªæœ‰ä¸€ç»„ Q, K, V
- å¯èƒ½åªå…³æ³¨æŸä¸€ç§æ¨¡å¼

**å¤šå¤´æ³¨æ„åŠ›**ï¼šå¤šç»„ Q, K, V å¹¶è¡Œ
- ä¸åŒçš„å¤´å¯ä»¥å…³æ³¨ä¸åŒçš„æ¨¡å¼
- ç±»ä¼¼ CNN çš„å¤šä¸ªå·ç§¯æ ¸

### ğŸ“ è®¡ç®—

```
Multi-Head Attention(Q, K, V) = Concat(headâ‚, ..., head_h) Â· W_O

å…¶ä¸­ï¼š
head_i = Attention(QÂ·W_Q^i, KÂ·W_K^i, VÂ·W_V^i)

å‚æ•°ï¼š
  W_Q^i âˆˆ â„^(d_model Ã— d_k)
  W_K^i âˆˆ â„^(d_model Ã— d_k)
  W_V^i âˆˆ â„^(d_model Ã— d_v)
  W_O âˆˆ â„^(hÂ·d_v Ã— d_model)

é€šå¸¸ï¼šd_k = d_v = d_model / h
```

### ğŸ’» å®ç°

```python
class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, n_heads, dropout=0.1):
        super().__init__()
        assert d_model % n_heads == 0

        self.d_model = d_model
        self.n_heads = n_heads
        self.d_k = d_model // n_heads

        # çº¿æ€§å˜æ¢
        self.W_Q = nn.Linear(d_model, d_model)
        self.W_K = nn.Linear(d_model, d_model)
        self.W_V = nn.Linear(d_model, d_model)
        self.W_O = nn.Linear(d_model, d_model)

        self.attention = ScaledDotProductAttention(dropout)
        self.dropout = nn.Dropout(dropout)

    def split_heads(self, x):
        """
        (batch, seq_len, d_model)
        â†’ (batch, n_heads, seq_len, d_k)
        """
        batch_size, seq_len, d_model = x.size()
        return x.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)

    def combine_heads(self, x):
        """
        (batch, n_heads, seq_len, d_k)
        â†’ (batch, seq_len, d_model)
        """
        batch_size, n_heads, seq_len, d_k = x.size()
        return x.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)

    def forward(self, Q, K, V, mask=None):
        """
        å‚æ•°ï¼š
            Q, K, V: (batch, seq_len, d_model)
            mask: å¯é€‰
        """
        # çº¿æ€§å˜æ¢
        Q = self.W_Q(Q)  # (batch, seq_len, d_model)
        K = self.W_K(K)
        V = self.W_V(V)

        # åˆ†æˆå¤šä¸ªå¤´
        Q = self.split_heads(Q)  # (batch, n_heads, seq_len, d_k)
        K = self.split_heads(K)
        V = self.split_heads(V)

        # è®¡ç®—æ³¨æ„åŠ›
        attn_output, attn_weights = self.attention(Q, K, V, mask)
        # (batch, n_heads, seq_len, d_k)

        # åˆå¹¶å¤šä¸ªå¤´
        output = self.combine_heads(attn_output)
        # (batch, seq_len, d_model)

        # æœ€åçš„çº¿æ€§å±‚
        output = self.W_O(output)
        output = self.dropout(output)

        return output, attn_weights
```

---

## 8.5 Transformer æ¶æ„ ğŸŒŸ

### ğŸ—ï¸ æ•´ä½“ç»“æ„

```
è¾“å…¥åºåˆ—
    â†“
[Positional Encoding]
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Encoder (Ã—Nå±‚)    â”‚
â”‚  - Multi-Head Attn  â”‚
â”‚  - Feed Forward     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Decoder (Ã—Nå±‚)    â”‚
â”‚  - Masked Attn      â”‚
â”‚  - Cross Attn       â”‚
â”‚  - Feed Forward     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
  è¾“å‡ºæ¦‚ç‡
```

### ğŸ”¹ ä½ç½®ç¼–ç  (Positional Encoding)

**é—®é¢˜**ï¼šSelf-Attention æ²¡æœ‰é¡ºåºä¿¡æ¯

**è§£å†³**ï¼šæ·»åŠ ä½ç½®ç¼–ç 

```
PE(pos, 2i)   = sin(pos / 10000^(2i/d_model))
PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))

å…¶ä¸­ï¼š
  pos: ä½ç½®ï¼ˆ0, 1, 2, ...ï¼‰
  i: ç»´åº¦ç´¢å¼•ï¼ˆ0, 1, ..., d_model/2ï¼‰
```

**å®ç°**ï¼š

```python
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super().__init__()

        # åˆ›å»ºä½ç½®ç¼–ç çŸ©é˜µ
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len).unsqueeze(1).float()
        div_term = torch.exp(
            torch.arange(0, d_model, 2).float() *
            (-np.log(10000.0) / d_model)
        )

        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)

        pe = pe.unsqueeze(0)  # (1, max_len, d_model)
        self.register_buffer('pe', pe)

    def forward(self, x):
        """
        å‚æ•°ï¼š
            x: (batch, seq_len, d_model)
        """
        seq_len = x.size(1)
        x = x + self.pe[:, :seq_len, :]
        return x
```

---

### ğŸ”¹ Encoder å±‚

```python
class TransformerEncoderLayer(nn.Module):
    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):
        super().__init__()

        # Multi-Head Self-Attention
        self.self_attn = MultiHeadAttention(d_model, n_heads, dropout)

        # Feed Forward
        self.ff = nn.Sequential(
            nn.Linear(d_model, d_ff),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(d_ff, d_model)
        )

        # Layer Normalization
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)

        self.dropout = nn.Dropout(dropout)

    def forward(self, x, mask=None):
        """
        å‚æ•°ï¼š
            x: (batch, seq_len, d_model)
        """
        # Self-Attention + Residual + Norm
        attn_output, _ = self.self_attn(x, x, x, mask)
        x = self.norm1(x + self.dropout(attn_output))

        # Feed Forward + Residual + Norm
        ff_output = self.ff(x)
        x = self.norm2(x + self.dropout(ff_output))

        return x
```

---

### ğŸ”¹ Decoder å±‚

```python
class TransformerDecoderLayer(nn.Module):
    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):
        super().__init__()

        # Masked Self-Attention
        self.self_attn = MultiHeadAttention(d_model, n_heads, dropout)

        # Cross-Attention (Encoder-Decoder Attention)
        self.cross_attn = MultiHeadAttention(d_model, n_heads, dropout)

        # Feed Forward
        self.ff = nn.Sequential(
            nn.Linear(d_model, d_ff),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(d_ff, d_model)
        )

        # Layer Normalization
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.norm3 = nn.LayerNorm(d_model)

        self.dropout = nn.Dropout(dropout)

    def forward(self, x, encoder_output, src_mask=None, tgt_mask=None):
        """
        å‚æ•°ï¼š
            x: (batch, tgt_len, d_model) ç›®æ ‡åºåˆ—
            encoder_output: (batch, src_len, d_model) ç¼–ç å™¨è¾“å‡º
            src_mask: source mask
            tgt_mask: target maskï¼ˆé˜²æ­¢çœ‹åˆ°æœªæ¥ä¿¡æ¯ï¼‰
        """
        # Masked Self-Attention
        attn_output, _ = self.self_attn(x, x, x, tgt_mask)
        x = self.norm1(x + self.dropout(attn_output))

        # Cross-Attention
        attn_output, _ = self.cross_attn(x, encoder_output, encoder_output, src_mask)
        x = self.norm2(x + self.dropout(attn_output))

        # Feed Forward
        ff_output = self.ff(x)
        x = self.norm3(x + self.dropout(ff_output))

        return x
```

---

### ğŸ”¹ å®Œæ•´ Transformer

```python
class Transformer(nn.Module):
    def __init__(self, src_vocab_size, tgt_vocab_size,
                 d_model=512, n_heads=8, n_layers=6,
                 d_ff=2048, dropout=0.1, max_len=5000):
        super().__init__()

        self.d_model = d_model

        # Embedding
        self.src_embedding = nn.Embedding(src_vocab_size, d_model)
        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)

        # Positional Encoding
        self.pos_encoding = PositionalEncoding(d_model, max_len)

        # Encoder
        self.encoder_layers = nn.ModuleList([
            TransformerEncoderLayer(d_model, n_heads, d_ff, dropout)
            for _ in range(n_layers)
        ])

        # Decoder
        self.decoder_layers = nn.ModuleList([
            TransformerDecoderLayer(d_model, n_heads, d_ff, dropout)
            for _ in range(n_layers)
        ])

        # Output
        self.fc_out = nn.Linear(d_model, tgt_vocab_size)
        self.dropout = nn.Dropout(dropout)

    def make_src_mask(self, src):
        """åˆ›å»º source mask (padding)"""
        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)
        # (batch, 1, 1, src_len)
        return src_mask

    def make_tgt_mask(self, tgt):
        """åˆ›å»º target mask (padding + future)"""
        tgt_len = tgt.size(1)

        # Padding mask
        tgt_pad_mask = (tgt != 0).unsqueeze(1).unsqueeze(2)
        # (batch, 1, 1, tgt_len)

        # Future mask
        tgt_sub_mask = torch.tril(
            torch.ones((tgt_len, tgt_len), device=tgt.device)
        ).bool()
        # (tgt_len, tgt_len)

        tgt_mask = tgt_pad_mask & tgt_sub_mask
        return tgt_mask

    def encode(self, src, src_mask):
        """Encoder"""
        x = self.src_embedding(src) * np.sqrt(self.d_model)
        x = self.pos_encoding(x)
        x = self.dropout(x)

        for layer in self.encoder_layers:
            x = layer(x, src_mask)

        return x

    def decode(self, tgt, encoder_output, src_mask, tgt_mask):
        """Decoder"""
        x = self.tgt_embedding(tgt) * np.sqrt(self.d_model)
        x = self.pos_encoding(x)
        x = self.dropout(x)

        for layer in self.decoder_layers:
            x = layer(x, encoder_output, src_mask, tgt_mask)

        return x

    def forward(self, src, tgt):
        """
        å‚æ•°ï¼š
            src: (batch, src_len)
            tgt: (batch, tgt_len)
        """
        src_mask = self.make_src_mask(src)
        tgt_mask = self.make_tgt_mask(tgt)

        encoder_output = self.encode(src, src_mask)
        decoder_output = self.decode(tgt, encoder_output, src_mask, tgt_mask)

        output = self.fc_out(decoder_output)
        return output
```

---

## 8.6 BERT (Bidirectional Encoder Representations from Transformers)

### ğŸ¯ æ ¸å¿ƒæ€æƒ³

**åªä½¿ç”¨ Transformer çš„ Encoder**

**è®­ç»ƒä»»åŠ¡**ï¼š
1. **Masked Language Model (MLM)**
   - éšæœº mask 15% çš„è¯
   - è®©æ¨¡å‹é¢„æµ‹è¢« mask çš„è¯

2. **Next Sentence Prediction (NSP)**
   - åˆ¤æ–­ä¸¤ä¸ªå¥å­æ˜¯å¦è¿ç»­

### ğŸ“ æ¶æ„

```
è¾“å…¥ï¼š[CLS] å¥å­1 [SEP] å¥å­2 [SEP]

Embedding = Token Emb + Segment Emb + Position Emb
    â†“
Transformer Encoder (Ã—12 or 24å±‚)
    â†“
è¾“å‡ºï¼šæ¯ä¸ª token çš„è¡¨ç¤º

[CLS] çš„è¾“å‡ºç”¨äºåˆ†ç±»ä»»åŠ¡
```

### ğŸ’» ä½¿ç”¨ BERTï¼ˆHugging Faceï¼‰

```python
from transformers import BertTokenizer, BertModel, BertForSequenceClassification
import torch

# ==================== åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ ====================

# Tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# æ¨¡å‹
model = BertModel.from_pretrained('bert-base-uncased')
model.eval()

# ==================== æ–‡æœ¬ç¼–ç  ====================

text = "Hello, my name is BERT."
inputs = tokenizer(text, return_tensors='pt')

# inputs['input_ids']: token IDs
# inputs['attention_mask']: mask (1=real, 0=padding)

# ==================== è·å–è¡¨ç¤º ====================

with torch.no_grad():
    outputs = model(**inputs)

# outputs.last_hidden_state: (1, seq_len, 768)
# outputs.pooler_output: (1, 768) [CLS] token

# ==================== å¾®è°ƒç”¨äºåˆ†ç±» ====================

class BERTClassifier(nn.Module):
    def __init__(self, n_classes):
        super().__init__()
        self.bert = BertModel.from_pretrained('bert-base-uncased')
        self.dropout = nn.Dropout(0.3)
        self.fc = nn.Linear(768, n_classes)

    def forward(self, input_ids, attention_mask):
        outputs = self.bert(
            input_ids=input_ids,
            attention_mask=attention_mask
        )

        # ä½¿ç”¨ [CLS] token
        pooled_output = outputs.pooler_output
        output = self.dropout(pooled_output)
        output = self.fc(output)

        return output

# ä½¿ç”¨
model = BERTClassifier(n_classes=2)

# è®­ç»ƒ
optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)
```

---

## 8.7 GPT (Generative Pre-trained Transformer)

### ğŸ¯ æ ¸å¿ƒæ€æƒ³

**åªä½¿ç”¨ Transformer çš„ Decoder**

**è®­ç»ƒä»»åŠ¡**ï¼š**è‡ªå›å½’è¯­è¨€æ¨¡å‹**
- ç»™å®šå‰é¢çš„è¯ï¼Œé¢„æµ‹ä¸‹ä¸€ä¸ªè¯
- P(w_t | w_1, ..., w_{t-1})

### ğŸ“ æ¶æ„

```
è¾“å…¥ï¼šw_1, w_2, ..., w_{t-1}
    â†“
Token Embedding + Position Embedding
    â†“
Transformer Decoder (ä»…è‡ªæ³¨æ„åŠ›ï¼ŒÃ—12/24/48å±‚)
    â†“
é¢„æµ‹ï¼šw_t
```

**ä¸ BERT çš„åŒºåˆ«**ï¼š

| ç‰¹æ€§ | BERT | GPT |
|------|------|-----|
| æ¶æ„ | Encoder only | Decoder only |
| æ³¨æ„åŠ› | åŒå‘ | å•å‘ï¼ˆcausalï¼‰ |
| è®­ç»ƒ | MLM + NSP | è¯­è¨€å»ºæ¨¡ |
| åº”ç”¨ | ç†è§£ä»»åŠ¡ | ç”Ÿæˆä»»åŠ¡ |

### ğŸ’» ä½¿ç”¨ GPT-2

```python
from transformers import GPT2Tokenizer, GPT2LMHeadModel

# åŠ è½½æ¨¡å‹
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
model = GPT2LMHeadModel.from_pretrained('gpt2')

# æ–‡æœ¬ç”Ÿæˆ
def generate_text(prompt, max_length=50):
    inputs = tokenizer(prompt, return_tensors='pt')

    with torch.no_grad():
        outputs = model.generate(
            inputs['input_ids'],
            max_length=max_length,
            num_return_sequences=1,
            no_repeat_ngram_size=2,
            temperature=0.7,
            top_k=50,
            top_p=0.95
        )

    text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return text

# ä½¿ç”¨
prompt = "Once upon a time,"
generated_text = generate_text(prompt)
print(generated_text)
```

---

## ğŸ“ æœ¬ç« ä½œä¸š

### ä½œä¸š 1ï¼šä»é›¶å®ç° Transformer

```python
# TODO:
# 1. å®ç°å®Œæ•´çš„ Transformer
# 2. åœ¨æœºå™¨ç¿»è¯‘ä»»åŠ¡ä¸Šè®­ç»ƒ
# 3. å¯è§†åŒ–æ³¨æ„åŠ›æƒé‡
# 4. å¯¹æ¯”ä¸åŒå±‚æ•°å’Œå¤´æ•°çš„æ•ˆæœ
```

### ä½œä¸š 2ï¼šBERT å¾®è°ƒ

```python
# ä½¿ç”¨ Hugging Face Transformers
# ä»»åŠ¡ï¼šæ–‡æœ¬åˆ†ç±»ï¼ˆIMDB æˆ– AG Newsï¼‰
# 1. åŠ è½½é¢„è®­ç»ƒ BERT
# 2. æ·»åŠ åˆ†ç±»å¤´
# 3. å¾®è°ƒ
# 4. è¯„ä¼°æ€§èƒ½
# 5. å¯¹æ¯”ä»å¤´è®­ç»ƒ vs å¾®è°ƒ
```

### ä½œä¸š 3ï¼šæ–‡æœ¬ç”Ÿæˆ

```python
# ä½¿ç”¨ GPT-2 æˆ–è®­ç»ƒè‡ªå·±çš„æ¨¡å‹
# 1. å®ç°ä¸åŒçš„é‡‡æ ·ç­–ç•¥
#    - Greedy
#    - Beam Search
#    - Top-K
#    - Nucleus (Top-P)
# 2. å¯¹æ¯”ç”Ÿæˆè´¨é‡
# 3. å®ç°æ¡ä»¶ç”Ÿæˆ
```

---

## ğŸ”‘ æœ¬ç« å…³é”®æ¦‚å¿µ

| æ¦‚å¿µ | è¯´æ˜ |
|------|------|
| Attention | åŠ¨æ€å…³æ³¨ç›¸å…³ä¿¡æ¯ |
| Self-Attention | åºåˆ—å†…éƒ¨çš„æ³¨æ„åŠ› |
| Multi-Head | å¤šç»„æ³¨æ„åŠ›å¹¶è¡Œ |
| Positional Encoding | ä½ç½®ä¿¡æ¯ç¼–ç  |
| Transformer | å®Œå…¨åŸºäºæ³¨æ„åŠ›çš„æ¶æ„ |
| Encoder-Decoder | åºåˆ—åˆ°åºåˆ—è½¬æ¢ |
| BERT | é¢„è®­ç»ƒåŒå‘ç¼–ç å™¨ |
| GPT | é¢„è®­ç»ƒè‡ªå›å½’è§£ç å™¨ |
| MLM | æ©ç è¯­è¨€æ¨¡å‹ |
| Fine-tuning | å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹ |

---

