# ç¬¬åäºŒç« ï¼šå¯è§£é‡Šæ€§ä¸å¯¹æŠ—æ”»å‡»

## ğŸ“Œ ç« èŠ‚ç›®æ ‡
- ç†è§£æ·±åº¦å­¦ä¹ çš„é»‘ç›’é—®é¢˜
- æŒæ¡æ¨¡å‹å¯è§£é‡Šæ€§æŠ€æœ¯
- å­¦ä¹ å¯¹æŠ—æ”»å‡»å’Œé˜²å¾¡æ–¹æ³•
- äº†è§£é²æ£’æ€§å’Œå®‰å…¨æ€§
- å®æˆ˜ï¼šå¯è§†åŒ–æ¨¡å‹å†³ç­–ã€ç”Ÿæˆå¯¹æŠ—æ ·æœ¬

---

## 12.1 ä¸ºä»€ä¹ˆéœ€è¦å¯è§£é‡Šæ€§ï¼Ÿ

### ğŸ¯ é»‘ç›’é—®é¢˜

```
æ·±åº¦å­¦ä¹ æ¨¡å‹ = é»‘ç›’ï¼Ÿ

è¾“å…¥ â†’ [ç¥ç»ç½‘ç»œ] â†’ è¾“å‡º
        ï¼Ÿï¼Ÿï¼Ÿ

é—®é¢˜ï¼š
  - ä¸ºä»€ä¹ˆåšå‡ºè¿™ä¸ªé¢„æµ‹ï¼Ÿ
  - æ¨¡å‹å­¦åˆ°äº†ä»€ä¹ˆç‰¹å¾ï¼Ÿ
  - å¦‚ä½•è°ƒè¯•é”™è¯¯ï¼Ÿ
  - å¦‚ä½•å»ºç«‹ä¿¡ä»»ï¼Ÿ
```

### ğŸ“Š åº”ç”¨åœºæ™¯éœ€æ±‚

```
åŒ»ç–—è¯Šæ–­ï¼š
  "ä¸ºä»€ä¹ˆè¯Šæ–­ä¸ºç™Œç—‡ï¼Ÿ"
  â†’ éœ€è¦æŒ‡å‡ºå…³é”®åŒºåŸŸ

é‡‘èé£æ§ï¼š
  "ä¸ºä»€ä¹ˆæ‹’ç»è´·æ¬¾ï¼Ÿ"
  â†’ æ³•å¾‹è¦æ±‚å¯è§£é‡Š

è‡ªåŠ¨é©¾é©¶ï¼š
  "ä¸ºä»€ä¹ˆåšå‡ºè¿™ä¸ªå†³ç­–ï¼Ÿ"
  â†’ å®‰å…¨æ€§è¦æ±‚
```

---

## 12.2 å¯è§£é‡Šæ€§æ–¹æ³•åˆ†ç±»

### ğŸ“ åˆ†ç±»ç»´åº¦

#### **1. å…¨å±€ vs å±€éƒ¨**

```
å…¨å±€è§£é‡Š (Global Interpretation):
  - æ¨¡å‹æ•´ä½“å¦‚ä½•å·¥ä½œ
  - å“ªäº›ç‰¹å¾æœ€é‡è¦
  - ä¾‹ï¼šç‰¹å¾é‡è¦æ€§

å±€éƒ¨è§£é‡Š (Local Interpretation):
  - å•ä¸ªé¢„æµ‹å¦‚ä½•äº§ç”Ÿ
  - ä¸ºä»€ä¹ˆè¿™ä¸ªæ ·æœ¬è¢«åˆ†ç±»ä¸ºX
  - ä¾‹ï¼šLIME, SHAP
```

#### **2. æ¨¡å‹ç‰¹å®š vs æ¨¡å‹æ— å…³**

```
æ¨¡å‹ç‰¹å®š (Model-Specific):
  - é’ˆå¯¹ç‰¹å®šæ¨¡å‹æ¶æ„
  - ä¾‹ï¼šç¥ç»ç½‘ç»œçš„æ¢¯åº¦å¯è§†åŒ–

æ¨¡å‹æ— å…³ (Model-Agnostic):
  - é€‚ç”¨äºä»»ä½•æ¨¡å‹
  - ä¾‹ï¼šLIME, SHAP
```

---

## 12.3 ç‰¹å¾é‡è¦æ€§

### ğŸ”¹ æ’åˆ—é‡è¦æ€§ (Permutation Importance)

**åŸç†**ï¼šæ‰“ä¹±æŸä¸ªç‰¹å¾ï¼Œçœ‹æ€§èƒ½ä¸‹é™å¤šå°‘

```python
import numpy as np
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt

def permutation_importance(model, X, y, metric=accuracy_score, n_repeats=10):
    """
    è®¡ç®—æ’åˆ—é‡è¦æ€§

    å‚æ•°:
        model: è®­ç»ƒå¥½çš„æ¨¡å‹
        X: ç‰¹å¾çŸ©é˜µ
        y: æ ‡ç­¾
        metric: è¯„ä¼°æŒ‡æ ‡
        n_repeats: é‡å¤æ¬¡æ•°
    """
    # åŸºçº¿æ€§èƒ½
    baseline_score = metric(y, model.predict(X))

    importances = []
    n_features = X.shape[1]

    for feature_idx in range(n_features):
        scores = []

        for _ in range(n_repeats):
            X_permuted = X.copy()

            # æ‰“ä¹±è¯¥ç‰¹å¾
            np.random.shuffle(X_permuted[:, feature_idx])

            # è®¡ç®—æ€§èƒ½ä¸‹é™
            permuted_score = metric(y, model.predict(X_permuted))
            score_decrease = baseline_score - permuted_score
            scores.append(score_decrease)

        # å¹³å‡é‡è¦æ€§
        importances.append({
            'feature': feature_idx,
            'importance': np.mean(scores),
            'std': np.std(scores)
        })

    return sorted(importances, key=lambda x: x['importance'], reverse=True)

# ==================== å¯è§†åŒ– ====================

def plot_feature_importance(importances, feature_names=None):
    """ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§"""

    indices = [imp['feature'] for imp in importances]
    values = [imp['importance'] for imp in importances]
    stds = [imp['std'] for imp in importances]

    if feature_names is None:
        feature_names = [f'Feature {i}' for i in indices]
    else:
        feature_names = [feature_names[i] for i in indices]

    plt.figure(figsize=(10, 6))
    plt.barh(range(len(values)), values, xerr=stds,
            color='steelblue', alpha=0.7)
    plt.yticks(range(len(values)), feature_names)
    plt.xlabel('Importance')
    plt.title('Feature Importance (Permutation)')
    plt.grid(True, alpha=0.3, axis='x')
    plt.tight_layout()
    plt.show()

# ==================== ç¤ºä¾‹ ====================

if __name__ == '__main__':
    from sklearn.datasets import load_iris
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.model_selection import train_test_split

    # åŠ è½½æ•°æ®
    iris = load_iris()
    X, y = iris.data, iris.target
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42
    )

    # è®­ç»ƒæ¨¡å‹
    model = RandomForestClassifier(random_state=42)
    model.fit(X_train, y_train)

    # è®¡ç®—é‡è¦æ€§
    importances = permutation_importance(model, X_test, y_test)

    # å¯è§†åŒ–
    plot_feature_importance(importances, iris.feature_names)
```

---

## 12.4 æ¢¯åº¦å¯è§†åŒ–æŠ€æœ¯

### ğŸ”¹ Saliency Mapsï¼ˆæ˜¾è‘—å›¾ï¼‰

**åŸç†**ï¼šè®¡ç®—è¾“å‡ºå¯¹è¾“å…¥çš„æ¢¯åº¦

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import matplotlib.pyplot as plt
import numpy as np

class SaliencyMap:
    """æ˜¾è‘—å›¾ç”Ÿæˆå™¨"""

    def __init__(self, model):
        self.model = model
        self.model.eval()

    def generate(self, image, target_class=None):
        """
        ç”Ÿæˆæ˜¾è‘—å›¾

        å‚æ•°:
            image: è¾“å…¥å›¾åƒ (C, H, W)
            target_class: ç›®æ ‡ç±»åˆ«ï¼ˆNone åˆ™ä½¿ç”¨é¢„æµ‹ç±»åˆ«ï¼‰
        """
        # ç¡®ä¿éœ€è¦æ¢¯åº¦
        image = image.unsqueeze(0).requires_grad_(True)

        # å‰å‘ä¼ æ’­
        output = self.model(image)

        # é€‰æ‹©ç›®æ ‡ç±»åˆ«
        if target_class is None:
            target_class = output.argmax(dim=1).item()

        # åå‘ä¼ æ’­
        self.model.zero_grad()
        output[0, target_class].backward()

        # è·å–æ¢¯åº¦
        saliency = image.grad.data.abs()

        # å–æœ€å¤§å€¼ä½œä¸ºæ˜¾è‘—æ€§
        saliency = saliency.max(dim=1)[0]  # (1, H, W)

        return saliency.squeeze().cpu().numpy()

# ==================== å¯è§†åŒ– ====================

def visualize_saliency(image, saliency, title='Saliency Map'):
    """å¯è§†åŒ–æ˜¾è‘—å›¾"""

    fig, axes = plt.subplots(1, 3, figsize=(15, 5))

    # åŸå›¾
    if image.shape[0] == 3:  # RGB
        img_display = image.permute(1, 2, 0).cpu().numpy()
    else:  # ç°åº¦
        img_display = image.squeeze().cpu().numpy()

    axes[0].imshow(img_display, cmap='gray' if len(img_display.shape)==2 else None)
    axes[0].set_title('Original Image')
    axes[0].axis('off')

    # æ˜¾è‘—å›¾
    axes[1].imshow(saliency, cmap='hot')
    axes[1].set_title('Saliency Map')
    axes[1].axis('off')

    # å åŠ 
    axes[2].imshow(img_display, cmap='gray' if len(img_display.shape)==2 else None)
    axes[2].imshow(saliency, cmap='hot', alpha=0.5)
    axes[2].set_title('Overlay')
    axes[2].axis('off')

    plt.suptitle(title)
    plt.tight_layout()
    plt.show()
```

---

### ğŸ”¹ Grad-CAM (Gradient-weighted Class Activation Mapping)

**åŸç†**ï¼šç»“åˆæ¢¯åº¦å’Œç‰¹å¾å›¾

```python
class GradCAM:
    """Grad-CAM ç±»æ¿€æ´»æ˜ å°„"""

    def __init__(self, model, target_layer):
        self.model = model
        self.target_layer = target_layer

        self.gradients = None
        self.activations = None

        # æ³¨å†Œé’©å­
        self.target_layer.register_forward_hook(self._save_activation)
        self.target_layer.register_backward_hook(self._save_gradient)

    def _save_activation(self, module, input, output):
        """ä¿å­˜æ¿€æ´»"""
        self.activations = output.detach()

    def _save_gradient(self, module, grad_input, grad_output):
        """ä¿å­˜æ¢¯åº¦"""
        self.gradients = grad_output[0].detach()

    def generate(self, image, target_class=None):
        """
        ç”Ÿæˆ Grad-CAM

        è¿”å›: CAM çƒ­åŠ›å›¾
        """
        # å‰å‘ä¼ æ’­
        output = self.model(image)

        if target_class is None:
            target_class = output.argmax(dim=1).item()

        # åå‘ä¼ æ’­
        self.model.zero_grad()
        output[0, target_class].backward()

        # è®¡ç®—æƒé‡ï¼ˆå…¨å±€å¹³å‡æ± åŒ–æ¢¯åº¦ï¼‰
        weights = self.gradients.mean(dim=(2, 3), keepdim=True)

        # åŠ æƒæ±‚å’Œ
        cam = (weights * self.activations).sum(dim=1, keepdim=True)

        # ReLU
        cam = F.relu(cam)

        # å½’ä¸€åŒ–
        cam = cam - cam.min()
        cam = cam / cam.max()

        return cam.squeeze().cpu().numpy()

# ==================== å¯è§†åŒ– Grad-CAM ====================

def visualize_gradcam(image, cam, title='Grad-CAM'):
    """å¯è§†åŒ– Grad-CAM"""

    import cv2

    fig, axes = plt.subplots(1, 3, figsize=(15, 5))

    # åŸå›¾
    if image.shape[0] == 3:
        img_display = image.permute(1, 2, 0).cpu().numpy()
    else:
        img_display = image.squeeze().cpu().numpy()

    # å½’ä¸€åŒ–åˆ° [0, 1]
    img_display = (img_display - img_display.min()) / (img_display.max() - img_display.min())

    axes[0].imshow(img_display)
    axes[0].set_title('Original Image')
    axes[0].axis('off')

    # CAM çƒ­åŠ›å›¾
    axes[1].imshow(cam, cmap='jet')
    axes[1].set_title('Grad-CAM')
    axes[1].axis('off')

    # å åŠ 
    # è°ƒæ•´ CAM å¤§å°åˆ°å›¾åƒå¤§å°
    cam_resized = cv2.resize(cam, (img_display.shape[1], img_display.shape[0]))

    axes[2].imshow(img_display)
    axes[2].imshow(cam_resized, cmap='jet', alpha=0.5)
    axes[2].set_title('Overlay')
    axes[2].axis('off')

    plt.suptitle(title)
    plt.tight_layout()
    plt.show()

# ==================== ä½¿ç”¨ç¤ºä¾‹ ====================

if __name__ == '__main__':
    from torchvision import models, transforms
    from PIL import Image

    # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
    model = models.resnet50(pretrained=True)
    model.eval()

    # ç›®æ ‡å±‚ï¼ˆResNet çš„æœ€åä¸€ä¸ªå·ç§¯å±‚ï¼‰
    target_layer = model.layer4[-1]

    # åˆ›å»º Grad-CAM
    gradcam = GradCAM(model, target_layer)

    # åŠ è½½å›¾åƒ
    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ])

    image = Image.open('example.jpg')
    image_tensor = transform(image).unsqueeze(0)

    # ç”Ÿæˆ Grad-CAM
    cam = gradcam.generate(image_tensor)

    # å¯è§†åŒ–
    visualize_gradcam(image_tensor[0], cam)
```

---

## 12.5 LIME (Local Interpretable Model-agnostic Explanations)

### ğŸ¯ æ ¸å¿ƒæ€æƒ³

```
ä¸ºå•ä¸ªé¢„æµ‹æä¾›å±€éƒ¨çº¿æ€§è§£é‡Šï¼š

1. åœ¨é¢„æµ‹ç‚¹é™„è¿‘é‡‡æ ·
2. ç”¨é»‘ç›’æ¨¡å‹é¢„æµ‹è¿™äº›æ ·æœ¬
3. è®­ç»ƒç®€å•æ¨¡å‹ï¼ˆå¦‚çº¿æ€§æ¨¡å‹ï¼‰æ‹Ÿåˆ
4. ç”¨ç®€å•æ¨¡å‹è§£é‡Š
```

### ğŸ’» å®ç°

```python
import numpy as np
from sklearn.linear_model import Ridge
from sklearn.metrics.pairwise import euclidean_distances

class LIME:
    """LIME è§£é‡Šå™¨"""

    def __init__(self, kernel_width=0.25, n_samples=5000):
        self.kernel_width = kernel_width
        self.n_samples = n_samples

    def explain_instance(self, instance, predict_fn, num_features=10):
        """
        è§£é‡Šå•ä¸ªå®ä¾‹

        å‚æ•°:
            instance: è¦è§£é‡Šçš„å®ä¾‹
            predict_fn: é¢„æµ‹å‡½æ•°
            num_features: è¿”å›æœ€é‡è¦çš„ç‰¹å¾æ•°
        """
        # åœ¨å®ä¾‹é™„è¿‘é‡‡æ ·
        samples = self._generate_samples(instance)

        # ç”¨é»‘ç›’æ¨¡å‹é¢„æµ‹
        predictions = predict_fn(samples)

        # è®¡ç®—æƒé‡ï¼ˆè·ç¦»è¶Šè¿‘æƒé‡è¶Šå¤§ï¼‰
        distances = euclidean_distances(samples, instance.reshape(1, -1)).ravel()
        weights = self._kernel(distances)

        # è®­ç»ƒçº¿æ€§æ¨¡å‹
        linear_model = Ridge(alpha=1.0)
        linear_model.fit(samples, predictions, sample_weight=weights)

        # è·å–ç‰¹å¾é‡è¦æ€§
        feature_importance = linear_model.coef_

        # è¿”å›æœ€é‡è¦çš„ç‰¹å¾
        top_features = np.argsort(np.abs(feature_importance))[-num_features:][::-1]

        explanation = [
            (feature_idx, feature_importance[feature_idx])
            for feature_idx in top_features
        ]

        return explanation

    def _generate_samples(self, instance):
        """åœ¨å®ä¾‹é™„è¿‘ç”Ÿæˆæ ·æœ¬"""
        n_features = len(instance)

        # é«˜æ–¯æ‰°åŠ¨
        samples = np.random.normal(
            loc=instance,
            scale=1.0,
            size=(self.n_samples, n_features)
        )

        return samples

    def _kernel(self, distances):
        """æ ¸å‡½æ•°ï¼ˆè·ç¦» â†’ æƒé‡ï¼‰"""
        return np.exp(-(distances ** 2) / (self.kernel_width ** 2))

# ==================== å›¾åƒ LIME ====================

class ImageLIME:
    """å›¾åƒ LIME è§£é‡Šå™¨"""

    def __init__(self, n_samples=1000, n_segments=50):
        self.n_samples = n_samples
        self.n_segments = n_segments

    def explain_instance(self, image, predict_fn, top_labels=1):
        """
        è§£é‡Šå›¾åƒåˆ†ç±»

        å‚æ•°:
            image: è¾“å…¥å›¾åƒ (H, W, C)
            predict_fn: é¢„æµ‹å‡½æ•°
            top_labels: è§£é‡Šçš„ç±»åˆ«æ•°
        """
        from skimage.segmentation import quickshift

        # è¶…åƒç´ åˆ†å‰²
        segments = quickshift(image, kernel_size=4, max_dist=200, ratio=0.2)
        n_segments = len(np.unique(segments))

        # ç”Ÿæˆæ‰°åŠ¨æ ·æœ¬
        samples = np.zeros((self.n_samples, n_segments))
        perturbed_images = []

        for i in range(self.n_samples):
            # éšæœºmaskä¸€äº›è¶…åƒç´ 
            active_segments = np.random.choice(
                [0, 1], size=n_segments, p=[0.5, 0.5]
            )
            samples[i] = active_segments

            # ç”Ÿæˆæ‰°åŠ¨å›¾åƒ
            perturbed_image = image.copy()
            for seg_id in range(n_segments):
                if active_segments[seg_id] == 0:
                    perturbed_image[segments == seg_id] = 0

            perturbed_images.append(perturbed_image)

        # é¢„æµ‹
        perturbed_images = np.array(perturbed_images)
        predictions = predict_fn(perturbed_images)

        # è®­ç»ƒçº¿æ€§æ¨¡å‹
        from sklearn.linear_model import Ridge

        explanations = []
        for label in range(top_labels):
            linear_model = Ridge(alpha=1.0)
            linear_model.fit(samples, predictions[:, label])

            # è·å–è¶…åƒç´ é‡è¦æ€§
            segment_importance = linear_model.coef_

            explanations.append({
                'label': label,
                'segments': segments,
                'importance': segment_importance
            })

        return explanations

    def visualize_explanation(self, image, explanation, threshold=0.1):
        """å¯è§†åŒ–è§£é‡Š"""
        segments = explanation['segments']
        importance = explanation['importance']

        # å½’ä¸€åŒ–é‡è¦æ€§
        importance = (importance - importance.min()) / (importance.max() - importance.min())

        # åˆ›å»ºæ©ç 
        mask = np.zeros(image.shape[:2])
        for seg_id in range(len(importance)):
            if importance[seg_id] > threshold:
                mask[segments == seg_id] = importance[seg_id]

        # å¯è§†åŒ–
        fig, axes = plt.subplots(1, 3, figsize=(15, 5))

        axes[0].imshow(image)
        axes[0].set_title('Original Image')
        axes[0].axis('off')

        axes[1].imshow(mask, cmap='hot')
        axes[1].set_title('Importance Map')
        axes[1].axis('off')

        axes[2].imshow(image)
        axes[2].imshow(mask, cmap='hot', alpha=0.5)
        axes[2].set_title('Overlay')
        axes[2].axis('off')

        plt.tight_layout()
        plt.show()
```

---

## 12.6 SHAP (SHapley Additive exPlanations)

### ğŸ¯ æ ¸å¿ƒæ€æƒ³

```
åŸºäºåšå¼ˆè®ºçš„ Shapley å€¼ï¼š

æ¯ä¸ªç‰¹å¾å¯¹é¢„æµ‹çš„è¾¹é™…è´¡çŒ®

SHAP å€¼æ€§è´¨ï¼š
  1. å±€éƒ¨å‡†ç¡®æ€§
  2. ç¼ºå¤±æ€§
  3. ä¸€è‡´æ€§
```

### ğŸ’» ä½¿ç”¨ SHAP åº“

```python
import shap
import numpy as np
import matplotlib.pyplot as plt

# ==================== æ ‘æ¨¡å‹ SHAP ====================

def explain_tree_model(model, X, feature_names=None):
    """è§£é‡Šæ ‘æ¨¡å‹"""

    # åˆ›å»º SHAP è§£é‡Šå™¨
    explainer = shap.TreeExplainer(model)

    # è®¡ç®— SHAP å€¼
    shap_values = explainer.shap_values(X)

    # å¯è§†åŒ–
    # 1. æ‘˜è¦å›¾
    shap.summary_plot(shap_values, X, feature_names=feature_names)

    # 2. å•ä¸ªæ ·æœ¬è§£é‡Š
    shap.force_plot(
        explainer.expected_value,
        shap_values[0],
        X[0],
        feature_names=feature_names,
        matplotlib=True
    )
    plt.show()

    # 3. ä¾èµ–å›¾
    if feature_names:
        shap.dependence_plot(
            feature_names[0],
            shap_values,
            X,
            feature_names=feature_names
        )

    return shap_values

# ==================== æ·±åº¦å­¦ä¹  SHAP ====================

def explain_deep_model(model, X, background_data):
    """è§£é‡Šæ·±åº¦å­¦ä¹ æ¨¡å‹"""

    # åˆ›å»º DeepExplainer
    explainer = shap.DeepExplainer(model, background_data)

    # è®¡ç®— SHAP å€¼
    shap_values = explainer.shap_values(X)

    # å›¾åƒå¯è§†åŒ–
    if len(X.shape) == 4:  # å›¾åƒæ•°æ®
        shap.image_plot(shap_values, X)

    return shap_values

# ==================== ç¤ºä¾‹ ====================

if __name__ == '__main__':
    from sklearn.datasets import load_boston
    from sklearn.ensemble import RandomForestRegressor

    # åŠ è½½æ•°æ®
    boston = load_boston()
    X, y = boston.data, boston.target

    # è®­ç»ƒæ¨¡å‹
    model = RandomForestRegressor(random_state=42)
    model.fit(X, y)

    # SHAP è§£é‡Š
    shap_values = explain_tree_model(
        model, X[:100], feature_names=boston.feature_names
    )
```

---

## 12.7 å¯¹æŠ—æ”»å‡» (Adversarial Attacks)

### ğŸ¯ ä»€ä¹ˆæ˜¯å¯¹æŠ—æ ·æœ¬ï¼Ÿ

```
å¯¹æŠ—æ ·æœ¬ï¼šæ•…æ„è®¾è®¡çš„è¾“å…¥ï¼Œä½¿æ¨¡å‹äº§ç”Ÿé”™è¯¯é¢„æµ‹

x_adv = x + Î´

å…¶ä¸­ Î´ æ˜¯ç²¾å¿ƒè®¾è®¡çš„å¾®å°æ‰°åŠ¨ï¼Œäººçœ¼å‡ ä¹æ— æ³•å¯Ÿè§‰
```

**ç¤ºä¾‹**ï¼š

```
åŸå›¾ï¼šç†ŠçŒ« â†’ é¢„æµ‹ï¼šç†ŠçŒ« (99% ç½®ä¿¡åº¦)
     â†“ + å¾®å°å™ªå£°
å¯¹æŠ—æ ·æœ¬ï¼šç†ŠçŒ«? â†’ é¢„æµ‹ï¼šé•¿è‡‚çŒ¿ (99% ç½®ä¿¡åº¦)
```

---

### ğŸ”¹ FGSM (Fast Gradient Sign Method)

**åŸç†**ï¼šæ²¿ç€æ¢¯åº¦æ–¹å‘æ·»åŠ æ‰°åŠ¨

```
x_adv = x + Îµ Â· sign(âˆ‡_x L(Î¸, x, y))

Îµ: æ‰°åŠ¨å¹…åº¦
L: æŸå¤±å‡½æ•°
```

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class FGSM:
    """Fast Gradient Sign Method æ”»å‡»"""

    def __init__(self, model, epsilon=0.03):
        self.model = model
        self.epsilon = epsilon
        self.model.eval()

    def attack(self, images, labels):
        """
        ç”Ÿæˆå¯¹æŠ—æ ·æœ¬

        å‚æ•°:
            images: åŸå§‹å›¾åƒ (batch, C, H, W)
            labels: çœŸå®æ ‡ç­¾
        """
        images = images.clone().detach().requires_grad_(True)

        # å‰å‘ä¼ æ’­
        outputs = self.model(images)
        loss = F.cross_entropy(outputs, labels)

        # åå‘ä¼ æ’­
        self.model.zero_grad()
        loss.backward()

        # è·å–æ¢¯åº¦ç¬¦å·
        data_grad = images.grad.data
        sign_data_grad = data_grad.sign()

        # ç”Ÿæˆå¯¹æŠ—æ ·æœ¬
        perturbed_images = images + self.epsilon * sign_data_grad

        # è£å‰ªåˆ°åˆæ³•èŒƒå›´ [0, 1]
        perturbed_images = torch.clamp(perturbed_images, 0, 1)

        return perturbed_images.detach()

# ==================== å¯è§†åŒ–å¯¹æŠ—æ”»å‡» ====================

def visualize_adversarial_attack(model, image, label, epsilon=0.03):
    """å¯è§†åŒ–å¯¹æŠ—æ”»å‡»æ•ˆæœ"""

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.to(device)
    image = image.to(device)
    label = label.to(device)

    # åŸå§‹é¢„æµ‹
    with torch.no_grad():
        output_orig = model(image.unsqueeze(0))
        pred_orig = output_orig.argmax(dim=1).item()
        conf_orig = F.softmax(output_orig, dim=1)[0, pred_orig].item()

    # ç”Ÿæˆå¯¹æŠ—æ ·æœ¬
    fgsm = FGSM(model, epsilon=epsilon)
    adv_image = fgsm.attack(image.unsqueeze(0), label.unsqueeze(0))

    # å¯¹æŠ—æ ·æœ¬é¢„æµ‹
    with torch.no_grad():
        output_adv = model(adv_image)
        pred_adv = output_adv.argmax(dim=1).item()
        conf_adv = F.softmax(output_adv, dim=1)[0, pred_adv].item()

    # æ‰°åŠ¨
    perturbation = (adv_image - image.unsqueeze(0)).squeeze()

    # å¯è§†åŒ–
    fig, axes = plt.subplots(1, 4, figsize=(16, 4))

    # åŸå›¾
    img_orig = image.permute(1, 2, 0).cpu().numpy()
    axes[0].imshow(img_orig)
    axes[0].set_title(f'Original\nPred: {pred_orig} ({conf_orig:.2%})')
    axes[0].axis('off')

    # æ‰°åŠ¨ï¼ˆæ”¾å¤§æ˜¾ç¤ºï¼‰
    pert_display = perturbation.permute(1, 2, 0).cpu().numpy()
    pert_display = (pert_display - pert_display.min()) / (pert_display.max() - pert_display.min())
    axes[1].imshow(pert_display)
    axes[1].set_title(f'Perturbation (Ã—10)')
    axes[1].axis('off')

    # å¯¹æŠ—æ ·æœ¬
    img_adv = adv_image.squeeze().permute(1, 2, 0).cpu().numpy()
    axes[2].imshow(img_adv)
    axes[2].set_title(f'Adversarial\nPred: {pred_adv} ({conf_adv:.2%})')
    axes[2].axis('off')

    # å·®å¼‚
    diff = np.abs(img_adv - img_orig)
    axes[3].imshow(diff)
    axes[3].set_title('Absolute Difference')
    axes[3].axis('off')

    plt.tight_layout()
    plt.show()

    return pred_orig == pred_adv  # æ”»å‡»æ˜¯å¦æˆåŠŸ
```

---

### ğŸ”¹ PGD (Projected Gradient Descent)

**æ›´å¼ºçš„æ”»å‡»**ï¼šè¿­ä»£ç‰ˆ FGSM

```python
class PGD:
    """Projected Gradient Descent æ”»å‡»"""

    def __init__(self, model, epsilon=0.03, alpha=0.01, num_iter=40):
        self.model = model
        self.epsilon = epsilon
        self.alpha = alpha
        self.num_iter = num_iter
        self.model.eval()

    def attack(self, images, labels):
        """ç”Ÿæˆå¯¹æŠ—æ ·æœ¬"""

        # éšæœºåˆå§‹åŒ–
        delta = torch.zeros_like(images).uniform_(-self.epsilon, self.epsilon)
        delta.requires_grad = True

        for _ in range(self.num_iter):
            # å‰å‘ä¼ æ’­
            outputs = self.model(images + delta)
            loss = F.cross_entropy(outputs, labels)

            # åå‘ä¼ æ’­
            loss.backward()

            # æ›´æ–°æ‰°åŠ¨
            delta.data = delta.data + self.alpha * delta.grad.sign()

            # æŠ•å½±åˆ° Îµ-çƒå†…
            delta.data = torch.clamp(delta.data, -self.epsilon, self.epsilon)

            # ç¡®ä¿åœ¨åˆæ³•èŒƒå›´å†…
            delta.data = torch.clamp(images.data + delta.data, 0, 1) - images.data

            # æ¸…ç©ºæ¢¯åº¦
            delta.grad.zero_()

        return (images + delta).detach()
```

---

### ğŸ”¹ C&W Attack (Carlini & Wagner)

**æœ€ä¼˜åŒ–æ”»å‡»**ï¼š

```
æœ€å°åŒ–: ||Î´||_2 + cÂ·loss(x+Î´, t)

å…¶ä¸­ t æ˜¯ç›®æ ‡ç±»åˆ«ï¼ˆå®šå‘æ”»å‡»ï¼‰
```

```python
class CWAttack:
    """Carlini & Wagner L2 æ”»å‡»"""

    def __init__(self, model, c=1.0, kappa=0, learning_rate=0.01, num_iter=1000):
        self.model = model
        self.c = c
        self.kappa = kappa
        self.learning_rate = learning_rate
        self.num_iter = num_iter
        self.model.eval()

    def attack(self, images, labels, targeted=False, target_labels=None):
        """
        C&W æ”»å‡»

        å‚æ•°:
            targeted: æ˜¯å¦ä¸ºå®šå‘æ”»å‡»
            target_labels: ç›®æ ‡ç±»åˆ«ï¼ˆå®šå‘æ”»å‡»æ—¶ä½¿ç”¨ï¼‰
        """
        batch_size = images.size(0)

        # ä½¿ç”¨ tanh ç©ºé—´
        w = torch.zeros_like(images, requires_grad=True)
        optimizer = torch.optim.Adam([w], lr=self.learning_rate)

        best_adv = images.clone()
        best_l2 = float('inf') * torch.ones(batch_size)

        for iteration in range(self.num_iter):
            # è½¬æ¢å›å›¾åƒç©ºé—´
            adv_images = 0.5 * (torch.tanh(w) + 1)

            # é¢„æµ‹
            outputs = self.model(adv_images)

            # C&W æŸå¤±
            if targeted:
                # å®šå‘æ”»å‡»ï¼šæœ€å¤§åŒ–ç›®æ ‡ç±»åˆ«
                loss_adv = self._cw_loss(outputs, target_labels, targeted=True)
            else:
                # éå®šå‘æ”»å‡»ï¼šæœ€å°åŒ–çœŸå®ç±»åˆ«
                loss_adv = self._cw_loss(outputs, labels, targeted=False)

            # L2 è·ç¦»
            l2_dist = torch.norm((adv_images - images).view(batch_size, -1), p=2, dim=1)

            # æ€»æŸå¤±
            loss = l2_dist.sum() + self.c * loss_adv.sum()

            # ä¼˜åŒ–
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            # æ›´æ–°æœ€ä½³æ ·æœ¬
            for i in range(batch_size):
                if l2_dist[i] < best_l2[i]:
                    pred = outputs[i].argmax().item()
                    if targeted:
                        if pred == target_labels[i].item():
                            best_l2[i] = l2_dist[i]
                            best_adv[i] = adv_images[i]
                    else:
                        if pred != labels[i].item():
                            best_l2[i] = l2_dist[i]
                            best_adv[i] = adv_images[i]

        return best_adv.detach()

    def _cw_loss(self, outputs, labels, targeted):
        """C&W æŸå¤±å‡½æ•°"""
        real = outputs.gather(1, labels.unsqueeze(1)).squeeze(1)

        # è·å–é™¤çœŸå®ç±»åˆ«å¤–çš„æœ€å¤§ logit
        other, _ = torch.max(outputs - 1e9 * F.one_hot(labels, outputs.size(1)), dim=1)

        if targeted:
            # å®šå‘ï¼šmax(other - real, -kappa)
            loss = torch.clamp(other - real, min=-self.kappa)
        else:
            # éå®šå‘ï¼šmax(real - other, -kappa)
            loss = torch.clamp(real - other, min=-self.kappa)

        return loss
```

---

## 12.8 å¯¹æŠ—é˜²å¾¡

### ğŸ”¹ å¯¹æŠ—è®­ç»ƒ (Adversarial Training)

**æœ€æœ‰æ•ˆçš„é˜²å¾¡æ–¹æ³•**

```python
def adversarial_training(model, train_loader, num_epochs=10, epsilon=0.03):
    """å¯¹æŠ—è®­ç»ƒ"""

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.to(device)

    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
    criterion = nn.CrossEntropyLoss()

    # åˆ›å»ºæ”»å‡»å™¨
    fgsm = FGSM(model, epsilon=epsilon)

    for epoch in range(num_epochs):
        model.train()
        total_loss = 0
        correct_clean = 0
        correct_adv = 0
        total = 0

        for images, labels in train_loader:
            images, labels = images.to(device), labels.to(device)

            # ç”Ÿæˆå¯¹æŠ—æ ·æœ¬
            adv_images = fgsm.attack(images, labels)

            # åˆå¹¶å¹²å‡€æ ·æœ¬å’Œå¯¹æŠ—æ ·æœ¬
            all_images = torch.cat([images, adv_images])
            all_labels = torch.cat([labels, labels])

            # å‰å‘ä¼ æ’­
            outputs = model(all_images)
            loss = criterion(outputs, all_labels)

            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            # ç»Ÿè®¡
            total_loss += loss.item()

            # å‡†ç¡®ç‡ï¼ˆåˆ†åˆ«ç»Ÿè®¡ï¼‰
            outputs_clean = outputs[:len(images)]
            outputs_adv = outputs[len(images):]

            _, pred_clean = outputs_clean.max(1)
            _, pred_adv = outputs_adv.max(1)

            correct_clean += pred_clean.eq(labels).sum().item()
            correct_adv += pred_adv.eq(labels).sum().item()
            total += labels.size(0)

        print(f'Epoch {epoch+1}: Loss={total_loss/len(train_loader):.4f}, '
              f'Clean Acc={100.*correct_clean/total:.2f}%, '
              f'Adv Acc={100.*correct_adv/total:.2f}%')

    return model
```

---

### ğŸ”¹ è¾“å…¥å˜æ¢é˜²å¾¡

```python
class InputTransformDefense:
    """è¾“å…¥å˜æ¢é˜²å¾¡"""

    def __init__(self, model):
        self.model = model
        self.model.eval()

    def predict_with_defense(self, images):
        """å¸¦é˜²å¾¡çš„é¢„æµ‹"""

        # 1. JPEG å‹ç¼©
        images_jpeg = self._jpeg_compression(images)

        # 2. éšæœºè°ƒæ•´å¤§å°å’Œå¡«å……
        images_resized = self._random_resize_pad(images_jpeg)

        # 3. ä½æ·±åº¦é™ä½
        images_quantized = self._bit_depth_reduction(images_resized)

        # é¢„æµ‹
        with torch.no_grad():
            outputs = self.model(images_quantized)

        return outputs

    def _jpeg_compression(self, images, quality=75):
        """JPEG å‹ç¼©"""
        from PIL import Image
        from io import BytesIO

        compressed = []
        for img in images:
            img_pil = transforms.ToPILImage()(img.cpu())
            buffer = BytesIO()
            img_pil.save(buffer, format='JPEG', quality=quality)
            buffer.seek(0)
            img_compressed = Image.open(buffer)
            compressed.append(transforms.ToTensor()(img_compressed))

        return torch.stack(compressed).to(images.device)

    def _random_resize_pad(self, images, resize_factor=0.9):
        """éšæœºè°ƒæ•´å¤§å°å’Œå¡«å……"""
        B, C, H, W = images.shape
        new_size = int(H * resize_factor)

        resized = F.interpolate(images, size=new_size, mode='bilinear')

        # éšæœºä½ç½®å¡«å……
        pad_size = H - new_size
        pad_top = torch.randint(0, pad_size + 1, (1,)).item()
        pad_left = torch.randint(0, pad_size + 1, (1,)).item()

        padded = F.pad(resized,
                      (pad_left, pad_size - pad_left,
                       pad_top, pad_size - pad_top))

        return padded

    def _bit_depth_reduction(self, images, bits=4):
        """ä½æ·±åº¦é™ä½"""
        levels = 2 ** bits
        images_quantized = torch.round(images * (levels - 1)) / (levels - 1)
        return images_quantized
```

---

### ğŸ”¹ é›†æˆé˜²å¾¡

```python
class EnsembleDefense:
    """é›†æˆé˜²å¾¡"""

    def __init__(self, models):
        self.models = models
        for model in self.models:
            model.eval()

    def predict(self, images):
        """é›†æˆé¢„æµ‹"""

        all_outputs = []

        with torch.no_grad():
            for model in self.models:
                outputs = model(images)
                all_outputs.append(F.softmax(outputs, dim=1))

        # å¹³å‡æ¦‚ç‡
        ensemble_output = torch.stack(all_outputs).mean(dim=0)

        return ensemble_output
```

---

## 12.9 é²æ£’æ€§è¯„ä¼°

```python
class RobustnessEvaluator:
    """é²æ£’æ€§è¯„ä¼°å™¨"""

    def __init__(self, model, test_loader, device):
        self.model = model
        self.test_loader = test_loader
        self.device = device
        self.model.eval()

    def evaluate_clean_accuracy(self):
        """è¯„ä¼°å¹²å‡€æ ·æœ¬å‡†ç¡®ç‡"""
        correct = 0
        total = 0

        with torch.no_grad():
            for images, labels in self.test_loader:
                images, labels = images.to(self.device), labels.to(self.device)

                outputs = self.model(images)
                _, predicted = outputs.max(1)

                total += labels.size(0)
                correct += predicted.eq(labels).sum().item()

        accuracy = 100. * correct / total
        print(f"Clean Accuracy: {accuracy:.2f}%")
        return accuracy

    def evaluate_adversarial_robustness(self, attack, epsilons=[0.01, 0.03, 0.1]):
        """è¯„ä¼°å¯¹æŠ—é²æ£’æ€§"""

        results = {}

        for epsilon in epsilons:
            attack.epsilon = epsilon

            correct = 0
            total = 0

            for images, labels in self.test_loader:
                images, labels = images.to(self.device), labels.to(self.device)

                # ç”Ÿæˆå¯¹æŠ—æ ·æœ¬
                adv_images = attack.attack(images, labels)

                # é¢„æµ‹
                with torch.no_grad():
                    outputs = self.model(adv_images)
                    _, predicted = outputs.max(1)

                total += labels.size(0)
                correct += predicted.eq(labels).sum().item()

            accuracy = 100. * correct / total
            results[epsilon] = accuracy
            print(f"Adversarial Accuracy (Îµ={epsilon}): {accuracy:.2f}%")

        return results

    def plot_robustness_curve(self, results):
        """ç»˜åˆ¶é²æ£’æ€§æ›²çº¿"""
        epsilons = sorted(results.keys())
        accuracies = [results[eps] for eps in epsilons]

        plt.figure(figsize=(10, 6))
        plt.plot(epsilons, accuracies, 'bo-', linewidth=2, markersize=8)
        plt.xlabel('Perturbation Magnitude (Îµ)')
        plt.ylabel('Accuracy (%)')
        plt.title('Model Robustness to Adversarial Attacks')
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.show()

# ==================== ä½¿ç”¨ç¤ºä¾‹ ====================

if __name__ == '__main__':
    # è¯„ä¼°æ¨¡å‹é²æ£’æ€§
    evaluator = RobustnessEvaluator(model, test_loader, device)

    # å¹²å‡€å‡†ç¡®ç‡
    evaluator.evaluate_clean_accuracy()

    # å¯¹æŠ—é²æ£’æ€§
    fgsm = FGSM(model)
    results = evaluator.evaluate_adversarial_robustness(
        fgsm, epsilons=[0.0, 0.01, 0.03, 0.05, 0.1, 0.2]
    )

    # ç»˜åˆ¶æ›²çº¿
    evaluator.plot_robustness_curve(results)
```

---

## ğŸ“ æœ¬ç« ä½œä¸š

### ä½œä¸š 1ï¼šæ¨¡å‹å¯è§£é‡Šæ€§

```python
# åœ¨å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸Šï¼š
# 1. å®ç° Saliency Map
# 2. å®ç° Grad-CAM
# 3. ä½¿ç”¨ LIME è§£é‡Šé¢„æµ‹
# 4. å¯¹æ¯”ä¸‰ç§æ–¹æ³•çš„ç»“æœ
# 5. åˆ†ææ¨¡å‹å…³æ³¨çš„åŒºåŸŸæ˜¯å¦åˆç†
```

### ä½œä¸š 2ï¼šå¯¹æŠ—æ”»å‡»å®éªŒ

```python
# å®ç°å¹¶å¯¹æ¯”ï¼š
# 1. FGSM
# 2. PGD
# 3. C&W
#
# è¯„ä¼°ï¼š
#   - æ”»å‡»æˆåŠŸç‡
#   - æ‰°åŠ¨å¤§å°ï¼ˆL2, Lâˆï¼‰
#   - å¯æ„ŸçŸ¥æ€§
#   - è®¡ç®—æ—¶é—´
```

### ä½œä¸š 3ï¼šå¯¹æŠ—é˜²å¾¡

```python
# å®ç°å¹¶è¯„ä¼°é˜²å¾¡æ–¹æ³•ï¼š
# 1. å¯¹æŠ—è®­ç»ƒ
# 2. è¾“å…¥å˜æ¢
# 3. é›†æˆé˜²å¾¡
#
# å¯¹æ¯”ï¼š
#   - å¹²å‡€æ ·æœ¬å‡†ç¡®ç‡
#   - å¯¹æŠ—æ ·æœ¬å‡†ç¡®ç‡
#   - é²æ£’æ€§ vs å‡†ç¡®æ€§ trade-off
```

### ä½œä¸š 4ï¼šå¯ä¿¡ AI ç³»ç»Ÿ

```python
# è®¾è®¡ä¸€ä¸ªå¯ä¿¡ AI ç³»ç»Ÿï¼š
# 1. æä¾›é¢„æµ‹è§£é‡Š
# 2. è¯„ä¼°é¢„æµ‹ç½®ä¿¡åº¦
# 3. æ£€æµ‹å¯¹æŠ—æ ·æœ¬
# 4. æä¾›ä¸ç¡®å®šæ€§ä¼°è®¡
#
# åœ¨åŒ»ç–—æˆ–é‡‘èåœºæ™¯ä¸­æµ‹è¯•
```

---

## ğŸ”‘ æœ¬ç« å…³é”®æ¦‚å¿µ

| æ¦‚å¿µ | è¯´æ˜ |
|------|------|
| å¯è§£é‡Šæ€§ | ç†è§£æ¨¡å‹å†³ç­–è¿‡ç¨‹ |
| Saliency Map | åŸºäºæ¢¯åº¦çš„æ˜¾è‘—æ€§ |
| Grad-CAM | ç±»æ¿€æ´»æ˜ å°„ |
| LIME | å±€éƒ¨çº¿æ€§è§£é‡Š |
| SHAP | Shapley å€¼è§£é‡Š |
| å¯¹æŠ—æ ·æœ¬ | ç²¾å¿ƒè®¾è®¡çš„æ‰°åŠ¨è¾“å…¥ |
| FGSM | å¿«é€Ÿæ¢¯åº¦ç¬¦å·æ”»å‡» |
| PGD | æŠ•å½±æ¢¯åº¦ä¸‹é™æ”»å‡» |
| C&W | ä¼˜åŒ–æ”»å‡» |
| å¯¹æŠ—è®­ç»ƒ | æœ€æœ‰æ•ˆçš„é˜²å¾¡ |
| é²æ£’æ€§ | å¯¹æ‰°åŠ¨çš„æŠµæŠ—èƒ½åŠ› |

---

éœ€è¦æˆ‘ç»§ç»­å†™**ç¬¬åä¸‰ç« ï¼šå¤§è¯­è¨€æ¨¡å‹æ—¶ä»£**å—ï¼Ÿè¿™å°†æ˜¯æœ€åä¸€ç« ï¼Œæ¶µç›– LLMã€Prompt Engineeringã€In-Context Learning ç­‰å‰æ²¿è¯é¢˜ã€‚

-----
