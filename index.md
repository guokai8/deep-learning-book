## ğŸ“‹ è¯¾ç¨‹æ•´ä½“æ¡†æ¶

### **ç¬¬ä¸€éƒ¨åˆ†ï¼šæœºå™¨å­¦ä¹ åŸºç¡€ (ML Basics)**

#### 1. **å¼•å…¥ï¼šæœºå™¨å­¦ä¹ æ˜¯ä»€ä¹ˆï¼Ÿ**
- ğŸ¯ **ä»ç”Ÿæ´»ä¾‹å­å‡ºå‘**
  - "æœºå™¨å­¦ä¹ å°±åƒæ˜¯æ•™ç”µè„‘'ä¸¾ä¸€åä¸‰'"
  - å®ä¾‹ï¼šè¯­éŸ³è¯†åˆ«ã€å›¾åƒè¯†åˆ«ã€AlphaGo
- **ä¸‰ä¸ªæ­¥éª¤æ¡†æ¶**
  1. Define a function set (Model)
  2. Goodness of function (Loss)
  3. Pick the best function (Optimization)

#### 2. **å›å½’ (Regression)**
- Linear Regression å®ä¾‹
- ç”¨å®å¯æ¢¦CPå€¼é¢„æµ‹æ¼”ç¤º
- Gradient Descent ç›´è§‚è§£é‡Š
- å¯è§†åŒ–ï¼šLoss Function åœ°å½¢å›¾

#### 3. **åˆ†ç±» (Classification)**
- Logistic Regression
- ä¸ºä»€ä¹ˆä¸èƒ½ç”¨ Regression åš Classificationï¼Ÿ
- Softmax & Cross-Entropy
- å®ä¾‹ï¼šæ‰‹å†™æ•°å­—è¯†åˆ« (MNIST)

---

### **ç¬¬äºŒéƒ¨åˆ†ï¼šæ·±åº¦å­¦ä¹  (Deep Learning)**

#### 4. **ç¥ç»ç½‘ç»œåŸºç¡€**
- ğŸ§  **ç›´è§‚ç†è§£**ï¼š"ç¥ç»ç½‘ç»œå°±æ˜¯å¾ˆå¤š Logistic Regression å èµ·æ¥"
- Activation Functions (Sigmoid, ReLU)
- Backpropagationï¼ˆç”¨è®¡ç®—å›¾è§£é‡Šï¼‰
- å®æˆ˜ï¼šå»ºç«‹ä½ çš„ç¬¬ä¸€ä¸ªç¥ç»ç½‘ç»œ

#### 5. **è®­ç»ƒæŠ€å·§ (Tips for Training)**
  ```
  æ¨¡å‹è¡¨ç°ä¸å¥½ï¼Ÿ
  â”œâ”€ è®­ç»ƒæ•°æ®è¡¨ç°å·®ï¼Ÿ
  â”‚  â”œâ”€ Optimization é—®é¢˜ â†’ Adaptive Learning Rate, Batch Normalization
  â”‚  â””â”€ Overfitting â†’ Regularization, Dropout, Early Stopping
  â””â”€ æµ‹è¯•æ•°æ®è¡¨ç°å·®ï¼Ÿ
     â””â”€ Overfitting â†’ æ›´å¤šæ•°æ®, Data Augmentation
  ```

#### 6. **CNN (å·ç§¯ç¥ç»ç½‘ç»œ)**
- ä¸ºä»€ä¹ˆéœ€è¦ CNNï¼Ÿï¼ˆå‚æ•°å¤ªå¤šçš„é—®é¢˜ï¼‰
- Convolution & Pooling ç›´è§‚è§£é‡Š
- ç»å…¸æ¶æ„ï¼šLeNet, AlexNet, VGG, ResNet
- åº”ç”¨ï¼šå›¾åƒåˆ†ç±»ã€ç‰©ä½“æ£€æµ‹

#### 7. **RNN & Sequence Models**
- åºåˆ—æ•°æ®çš„ç‰¹æ€§
- RNN, LSTM, GRU
- Seq2Seq æ¶æ„
- åº”ç”¨ï¼šè¯­éŸ³è¯†åˆ«ã€æœºå™¨ç¿»è¯‘

---

### **ç¬¬ä¸‰éƒ¨åˆ†ï¼šè¿›é˜¶ä¸»é¢˜**

#### 8. **Self-Attention & Transformer**
- ğŸ”¥ **ä» RNN çš„é™åˆ¶è°ˆèµ·**
- Attention Mechanism å›¾è§£
- Multi-Head Attention
- Transformer æ¶æ„å®Œæ•´è§£æ
- BERT, GPT ç®€ä»‹

#### 9. **ç”Ÿæˆæ¨¡å‹ (Generative Models)**
- Auto-encoder
- VAE (Variational Auto-encoder)
- GAN (å¯¹æŠ—ç”Ÿæˆç½‘ç»œ)
  - Generator vs. Discriminator çš„å¯¹æŠ—æ¸¸æˆ
  - è®­ç»ƒéš¾ç‚¹ä¸æŠ€å·§
- Diffusion Models ç®€ä»‹

#### 10. **å¼ºåŒ–å­¦ä¹  (Reinforcement Learning)**
- Agent, Environment, Reward
- Q-Learning
- Policy Gradient
- å®ä¾‹ï¼šç© Atari æ¸¸æˆã€AlphaGo åŸç†

#### 11. **æ— ç›‘ç£å­¦ä¹  (Unsupervised Learning)**
- Clustering (K-means, HAC)
- Dimension Reduction (PCA, t-SNE)
- Self-Supervised Learning

---

### **ç¬¬å››éƒ¨åˆ†ï¼šå®è·µä¸åº”ç”¨**

#### 12. **è¿ç§»å­¦ä¹  (Transfer Learning)**
- Pre-training & Fine-tuning
- Domain Adaptation

#### 13. **å¯è§£é‡Šæ€§ä¸å¯¹æŠ—æ”»å‡»**
- Explainable AI
- Adversarial Attack & Defense

#### 14. **å¤§è¯­è¨€æ¨¡å‹æ—¶ä»£**
- LLM åŸç†
- Prompt Engineering
- In-Context Learning
- æœªæ¥å±•æœ›

---

---

## ğŸ“š æ¨èä½œä¸šè®¾ç½®

1. **HW1**: Linear Regression (PM2.5 é¢„æµ‹)
2. **HW2**: Classification (æ”¶å…¥é¢„æµ‹)
3. **HW3**: CNN (å›¾åƒåˆ†ç±»)
4. **HW4**: RNN (æ–‡æœ¬æƒ…æ„Ÿåˆ†æ)
5. **HW5**: Transformer (æœºå™¨ç¿»è¯‘)
6. **HW6**: GAN (åŠ¨æ¼«äººç‰©ç”Ÿæˆ)
7. **Final Project**: å¼€æ”¾å¼ç«èµ›

---

## ğŸ› ï¸ å·¥å…·ä¸èµ„æº

- **ç¼–ç¨‹ç¯å¢ƒ**: Python + PyTorch/TensorFlow
- **å¹³å°**: Google Colab (å…è´¹ GPU)
- **æ•°æ®é›†**: MNIST, CIFAR-10, ImageNet, Common Voice

---

-----
