# ç¬¬äºŒç« ï¼šå›å½’ (Regression)

## ğŸ“Œ ç« èŠ‚ç›®æ ‡
- æŒæ¡çº¿æ€§å›å½’çš„åŸç†å’Œå®ç°
- æ·±å…¥ç†è§£æ¢¯åº¦ä¸‹é™ç®—æ³•
- å­¦ä¼šè¯„ä¼°å›å½’æ¨¡å‹çš„æ€§èƒ½
- å¤„ç†å®é™…æ•°æ®é›†çš„æŠ€å·§
- äº†è§£å›å½’çš„å˜ç§å’Œæ”¹è¿›æ–¹æ³•

---

## 2.1 ä»€ä¹ˆæ˜¯å›å½’ï¼Ÿ

### ğŸ¯ å®šä¹‰

> **Regressionï¼ˆå›å½’ï¼‰**ï¼šé¢„æµ‹ä¸€ä¸ª**è¿ç»­çš„æ•°å€¼**

**ä¾‹å­**ï¼š
- âœ… é¢„æµ‹æˆ¿ä»·ï¼š$350,000
- âœ… é¢„æµ‹æ¸©åº¦ï¼š25.3Â°C
- âœ… é¢„æµ‹è‚¡ç¥¨ä»·æ ¼ï¼š$152.50
- âŒ é¢„æµ‹ç±»åˆ«ï¼šçŒ«/ç‹—ï¼ˆè¿™æ˜¯åˆ†ç±»é—®é¢˜ï¼‰

### ğŸ“Š å›å½’ vs åˆ†ç±»

```
å›å½’ (Regression):
  è¾“å‡º = è¿ç»­æ•°å€¼
  ä¾‹ï¼š0, 0.5, 1.23, 100, -5.7, ...

åˆ†ç±» (Classification):
  è¾“å‡º = ç¦»æ•£ç±»åˆ«
  ä¾‹ï¼šçŒ«, ç‹—, çŒª / æ˜¯, å¦ / Class 1, 2, 3
```

---

## 2.2 æ¡ˆä¾‹ï¼šå®å¯æ¢¦ CP å€¼é¢„æµ‹

### ğŸ® é—®é¢˜è®¾å®š

**èƒŒæ™¯**ï¼šPokemon GO æ¸¸æˆä¸­ï¼Œå®å¯æ¢¦è¿›åŒ–åçš„ CP å€¼ï¼ˆæˆ˜æ–—åŠ›ï¼‰æ˜¯å¤šå°‘ï¼Ÿ

**æ•°æ®**ï¼š
| è¿›åŒ–å‰ CP (x) | è¿›åŒ–å CP (y) |
|--------------|--------------|
| 10 | 28 |
| 20 | 55 |
| 30 | 82 |
| 40 | 109 |
| 50 | 136 |
| ... | ... |

**ç›®æ ‡**ï¼šç»™å®šè¿›åŒ–å‰çš„ CP å€¼ï¼Œé¢„æµ‹è¿›åŒ–åçš„ CP å€¼

---

## 2.3 Step 1: å®šä¹‰æ¨¡å‹ (Model)

### ğŸ”¹ Linear Modelï¼ˆçº¿æ€§æ¨¡å‹ï¼‰

æœ€ç®€å•çš„æ¨¡å‹ï¼šä¸€æ¡ç›´çº¿

```
y = b + wÂ·x
```

- **x**: è¾“å…¥ç‰¹å¾ï¼ˆè¿›åŒ–å‰ CPï¼‰
- **y**: è¾“å‡ºé¢„æµ‹ï¼ˆè¿›åŒ–å CPï¼‰
- **w**: æƒé‡ (weight)
- **b**: åå·® (bias)

### ğŸ“ˆ å¯è§†åŒ–

```
y (è¿›åŒ–åCP)
â†‘
|         â—
|       â—   y = b + wÂ·x
|     â—
|   â—
| â—
|____________â†’ x (è¿›åŒ–å‰CP)
```

ä¸åŒçš„ w å’Œ b ä¼šäº§ç”Ÿä¸åŒçš„ç›´çº¿ï¼š

```
w = 2, b = 10:  y = 10 + 2x  (é™¡å³­)
w = 3, b = 0:   y = 3x       (æ›´é™¡)
w = 1, b = 20:  y = 20 + x   (å¹³ç¼“ï¼Œèµ·ç‚¹é«˜)
```

### ğŸ”¸ æ›´å¤æ‚çš„æ¨¡å‹

æœ‰æ—¶å€™æ•°æ®ä¸æ˜¯ç›´çº¿ï¼Œå¯ä»¥ç”¨å¤šé¡¹å¼ï¼š

```
y = b + wâ‚Â·x + wâ‚‚Â·xÂ²
y = b + wâ‚Â·x + wâ‚‚Â·xÂ² + wâ‚ƒÂ·xÂ³
```

**æ€è€ƒ**ï¼šæ¨¡å‹è¶Šå¤æ‚è¶Šå¥½å—ï¼Ÿ

---

## 2.4 Step 2: æŸå¤±å‡½æ•° (Loss Function)

### ğŸ¯ å¦‚ä½•è¯„ä¼°ä¸€ä¸ª function çš„å¥½åï¼Ÿ

**ç­”æ¡ˆ**ï¼šçœ‹é¢„æµ‹å€¼å’ŒçœŸå®å€¼çš„å·®è·ï¼

### ğŸ“ Mean Squared Error (MSE)

æœ€å¸¸ç”¨çš„æŸå¤±å‡½æ•°ï¼š

```
L(w, b) = (1/N) Î£(Å·â¿ - yâ¿)Â²

å…¶ä¸­ï¼š
- N: è®­ç»ƒæ•°æ®æ•°é‡
- yâ¿: ç¬¬ n ç¬”æ•°æ®çš„çœŸå®å€¼
- Å·â¿ = b + wÂ·xâ¿: ç¬¬ n ç¬”æ•°æ®çš„é¢„æµ‹å€¼
```

### ğŸ“Š ä¾‹å­è®¡ç®—

å‡è®¾ w=2, b=10ï¼Œæœ‰ 3 ç¬”æ•°æ®ï¼š

| x | y (çœŸå®) | Å· = 10+2x (é¢„æµ‹) | è¯¯å·® (Å·-y) | å¹³æ–¹è¯¯å·® |
|---|---------|-----------------|-----------|---------|
| 10 | 28 | 30 | 2 | 4 |
| 20 | 55 | 50 | -5 | 25 |
| 30 | 82 | 70 | -12 | 144 |

```
Loss = (4 + 25 + 144) / 3 = 57.67
```

### ğŸ—ºï¸ Loss Function æ˜¯ä¸€ä¸ªåœ°å½¢å›¾

æŠŠ Loss çœ‹æˆ w å’Œ b çš„å‡½æ•°ï¼š`L(w, b)`

```
Loss
 â†‘
 |      å±±å³°
 |    /    \
 |   /  â—   \    â— æŸç»„å‚æ•°çš„ Loss
 |  /        \
 | /    â˜…     \  â˜… æœ€ä½³å‚æ•°ï¼ˆLoss æœ€å°ï¼‰
 |/_____è°·åº•___\
   w, b çš„ç»„åˆ
```

**ç›®æ ‡**ï¼šæ‰¾åˆ°è®© Loss æœ€å°çš„ w å’Œ b

---

## 2.5 Step 3: æ¢¯åº¦ä¸‹é™ (Gradient Descent)

### ğŸ”ï¸ ç›´è§‚ç†è§£ï¼šç›²äººä¸‹å±±

æƒ³è±¡ä½ æ˜¯ç›²äººï¼Œç«™åœ¨å±±ä¸Šï¼Œæƒ³æ‰¾åˆ°æœ€ä½ç‚¹ï¼š

1. **ç«™åœ¨æŸä¸ªä½ç½®**ï¼ˆåˆå§‹å‚æ•° wâ°, bâ°ï¼‰
2. **æ„Ÿå—å‘¨å›´çš„å¡åº¦**ï¼ˆè®¡ç®—æ¢¯åº¦ âˆ‚L/âˆ‚w, âˆ‚L/âˆ‚bï¼‰
3. **å¾€ä¸‹å¡æ–¹å‘èµ°ä¸€å°æ­¥**ï¼ˆæ›´æ–°å‚æ•°ï¼‰
4. **é‡å¤ 2-3 æ­¥**ï¼Œç›´åˆ°åˆ°è¾¾è°·åº•

### ğŸ“ æ•°å­¦å…¬å¼

```
é‡å¤ä»¥ä¸‹æ­¥éª¤ï¼Œç›´åˆ°æ”¶æ•›ï¼š

w â† w - Î· Â· âˆ‚L/âˆ‚w
b â† b - Î· Â· âˆ‚L/âˆ‚b
```

**ç¬¦å·è¯´æ˜**ï¼š
- `Î·` (eta): **Learning Rate**ï¼ˆå­¦ä¹ ç‡ï¼‰
  - æ§åˆ¶æ¯æ¬¡æ›´æ–°çš„æ­¥é•¿
  - å¤ªå¤§ï¼šå¯èƒ½è·³è¿‡æœ€ä½ç‚¹ï¼Œç”šè‡³å‘æ•£
  - å¤ªå°ï¼šæ”¶æ•›å¤ªæ…¢
  - å…¸å‹å€¼ï¼š0.001, 0.01, 0.1

- `âˆ‚L/âˆ‚w`: Loss å¯¹ w çš„**åå¯¼æ•°**ï¼ˆæ¢¯åº¦ï¼‰
  - è¡¨ç¤º w å¢åŠ ä¸€ç‚¹ï¼ŒLoss ä¼šå¢åŠ /å‡å°‘å¤šå°‘
  - æ­£å€¼ï¼šw å¢åŠ  â†’ Loss å¢åŠ  â†’ åº”è¯¥å‡å° w
  - è´Ÿå€¼ï¼šw å¢åŠ  â†’ Loss å‡å°‘ â†’ åº”è¯¥å¢åŠ  w

### ğŸ§® æ¨å¯¼æ¢¯åº¦å…¬å¼

å¯¹äºçº¿æ€§æ¨¡å‹ `y = b + wÂ·x`ï¼ŒLoss ä¸ºï¼š

```
L(w, b) = (1/N) Î£(b + wÂ·xâ¿ - yâ¿)Â²
```

è®¡ç®—åå¯¼æ•°ï¼š

```
âˆ‚L/âˆ‚w = (2/N) Î£(b + wÂ·xâ¿ - yâ¿)Â·xâ¿
âˆ‚L/âˆ‚b = (2/N) Î£(b + wÂ·xâ¿ - yâ¿)
```

### ğŸ’» Python å®ç°

```python
import numpy as np

# è®­ç»ƒæ•°æ®
X = np.array([10, 20, 30, 40, 50])
y = np.array([28, 55, 82, 109, 136])

# åˆå§‹åŒ–å‚æ•°
w = 0.0
b = 0.0
learning_rate = 0.0001
epochs = 1000

# æ¢¯åº¦ä¸‹é™
for epoch in range(epochs):
    # å‰å‘ä¼ æ’­ï¼šè®¡ç®—é¢„æµ‹å€¼
    y_pred = b + w * X

    # è®¡ç®— Loss
    loss = np.mean((y_pred - y) ** 2)

    # è®¡ç®—æ¢¯åº¦
    grad_w = 2 * np.mean((y_pred - y) * X)
    grad_b = 2 * np.mean(y_pred - y)

    # æ›´æ–°å‚æ•°
    w = w - learning_rate * grad_w
    b = b - learning_rate * grad_b

    # æ¯ 100 ä¸ª epoch æ‰“å°ä¸€æ¬¡
    if epoch % 100 == 0:
        print(f"Epoch {epoch}: Loss={loss:.2f}, w={w:.2f}, b={b:.2f}")

print(f"\næœ€ç»ˆå‚æ•°: w={w:.2f}, b={b:.2f}")
print(f"é¢„æµ‹ x=25: y={b + w*25:.2f}")
```

**è¾“å‡ºç¤ºä¾‹**ï¼š
```
Epoch 0: Loss=7289.00, w=3.51, b=1.47
Epoch 100: Loss=138.24, w=2.54, b=6.89
Epoch 200: Loss=11.17, w=2.63, b=4.12
...
Epoch 900: Loss=0.24, w=2.70, b=1.20

æœ€ç»ˆå‚æ•°: w=2.70, b=1.00
é¢„æµ‹ x=25: y=68.50
```

---

## 2.6 å¯è§†åŒ–æ¢¯åº¦ä¸‹é™è¿‡ç¨‹

### ğŸ“‰ Loss éšè®­ç»ƒå˜åŒ–

```
Loss
 â†‘
 |  â—
 |   â—
 |     â—
 |       â—
 |         â—____
 |              â—â—â—â—
 |__________________â†’ Epoch
```

**è§‚å¯Ÿ**ï¼š
- å¼€å§‹æ—¶ Loss å¿«é€Ÿä¸‹é™
- åæœŸä¸‹é™å˜æ…¢
- æœ€ç»ˆè¶‹äºå¹³ç¨³ï¼ˆæ”¶æ•›ï¼‰

### ğŸ¯ å‚æ•°åœ¨å‚æ•°ç©ºé—´ä¸­çš„ç§»åŠ¨

```
b
â†‘
|     èµ·ç‚¹â—
|        â†˜
|         â†˜
|          â†˜
|           â†˜
|            â—ç»ˆç‚¹
|_____________â†’ w
```

æ¯ä¸€æ­¥éƒ½æœç€ Loss å‡å°çš„æ–¹å‘ç§»åŠ¨

---

## 2.7 Learning Rate çš„å½±å“

### âš™ï¸ ä¸åŒå­¦ä¹ ç‡çš„è¡¨ç°

#### **Learning Rate å¤ªå°ï¼ˆÎ· = 0.00001ï¼‰**

```python
learning_rate = 0.00001
epochs = 10000  # éœ€è¦æ›´å¤šè½®æ¬¡
```

**ç°è±¡**ï¼š
- âœ— æ”¶æ•›éå¸¸æ…¢
- âœ— å¯èƒ½éœ€è¦è®­ç»ƒå¾ˆä¹…
- âœ“ ä½†æ¯”è¾ƒç¨³å®š

```
Loss
 â†‘
 |  â—
 |   â—
 |    â—
 |     â—    ä¸‹é™å¤ªæ…¢ï¼
 |      â—
 |       â—
 |________â—________â†’ Epoch
```

#### **Learning Rate é€‚ä¸­ï¼ˆÎ· = 0.0001ï¼‰**

```python
learning_rate = 0.0001
epochs = 1000
```

**ç°è±¡**ï¼š
- âœ“ æ”¶æ•›é€Ÿåº¦åˆé€‚
- âœ“ ç¨³å®šä¸‹é™
- âœ“ æ•ˆæœæœ€å¥½

```
Loss
 â†‘
 |  â—
 |    â—
 |      â—
 |        â—___     æ°åˆ°å¥½å¤„
 |            â—â—
 |______________â†’ Epoch
```

#### **Learning Rate å¤ªå¤§ï¼ˆÎ· = 0.01ï¼‰**

```python
learning_rate = 0.01
epochs = 1000
```

**ç°è±¡**ï¼š
- âœ— å‚æ•°éœ‡è¡
- âœ— å¯èƒ½è·³è¿‡æœ€ä¼˜ç‚¹
- âœ— ç”šè‡³å‘æ•£ï¼ˆLoss è¶Šæ¥è¶Šå¤§ï¼‰

```
Loss
 â†‘
 |    â—
 |  â—   â—
 | â—     â—    éœ‡è¡ï¼
 |â—       â—
 |_________â†’ Epoch

æˆ–è€…ï¼š

Loss
 â†‘          â—
 |        â—
 |      â—      å‘æ•£ï¼
 |    â—
 |  â—
 |_________â†’ Epoch
```

### ğŸ’¡ å¦‚ä½•é€‰æ‹© Learning Rateï¼Ÿ

**æ–¹æ³• 1ï¼šè¯•éªŒæ³•**
```python
# å°è¯•ä¸åŒçš„å­¦ä¹ ç‡
for lr in [0.1, 0.01, 0.001, 0.0001, 0.00001]:
    # è®­ç»ƒæ¨¡å‹ï¼Œè§‚å¯Ÿ Loss æ›²çº¿
```

**æ–¹æ³• 2ï¼šLearning Rate Schedule**
```python
# å¼€å§‹ç”¨å¤§çš„å­¦ä¹ ç‡ï¼Œé€æ¸å‡å°
initial_lr = 0.1
for epoch in range(epochs):
    lr = initial_lr / (1 + epoch * 0.001)
    # ä½¿ç”¨å½“å‰çš„ lr æ›´æ–°å‚æ•°
```

**æ–¹æ³• 3ï¼šAdaptive Learning Rate**ï¼ˆåç»­ç« èŠ‚ä¼šè®²ï¼‰
- Adam
- RMSprop
- AdaGrad

---

## 2.8 å¤šä¸ªç‰¹å¾çš„çº¿æ€§å›å½’

### ğŸ”¢ ä»ä¸€ä¸ªç‰¹å¾åˆ°å¤šä¸ªç‰¹å¾

**ä¹‹å‰**ï¼šåªç”¨è¿›åŒ–å‰çš„ CP å€¼
```
y = b + wÂ·x
```

**ç°åœ¨**ï¼šè€ƒè™‘å¤šä¸ªå› ç´ 
- xâ‚: è¿›åŒ–å‰ CP å€¼
- xâ‚‚: ç‰©ç§ï¼ˆç¼–å·ï¼‰
- xâ‚ƒ: è¡€é‡ (HP)
- xâ‚„: é‡é‡
- ...

```
y = b + wâ‚Â·xâ‚ + wâ‚‚Â·xâ‚‚ + wâ‚ƒÂ·xâ‚ƒ + wâ‚„Â·xâ‚„
```

### ğŸ“ å‘é‡åŒ–è¡¨ç¤º

```
y = b + wâ‚Â·xâ‚ + wâ‚‚Â·xâ‚‚ + ... + wâ‚™Â·xâ‚™

å†™æˆå‘é‡å½¢å¼ï¼š
y = b + wáµ€x

å…¶ä¸­ï¼š
w = [wâ‚, wâ‚‚, ..., wâ‚™]áµ€  (æƒé‡å‘é‡)
x = [xâ‚, xâ‚‚, ..., xâ‚™]áµ€  (ç‰¹å¾å‘é‡)
```

### ğŸ’» å®ç°

```python
import numpy as np

# æ•°æ®ï¼ˆæ¯è¡Œä¸€ä¸ªæ ·æœ¬ï¼Œæ¯åˆ—ä¸€ä¸ªç‰¹å¾ï¼‰
X = np.array([
    [10, 1, 50, 5.0],  # CP=10, ç§ç±»=1, HP=50, é‡é‡=5.0
    [20, 1, 60, 5.5],
    [30, 2, 70, 6.0],
    [40, 2, 80, 6.5],
    [50, 3, 90, 7.0],
])
y = np.array([28, 55, 82, 109, 136])

# åˆå§‹åŒ–å‚æ•°
n_features = X.shape[1]  # ç‰¹å¾æ•°é‡
w = np.zeros(n_features)  # [0, 0, 0, 0]
b = 0.0
learning_rate = 0.00001
epochs = 1000

# æ¢¯åº¦ä¸‹é™
for epoch in range(epochs):
    # é¢„æµ‹ï¼šy_pred = b + X @ w
    y_pred = b + np.dot(X, w)

    # Loss
    loss = np.mean((y_pred - y) ** 2)

    # æ¢¯åº¦
    grad_w = 2 * np.dot(X.T, (y_pred - y)) / len(y)
    grad_b = 2 * np.mean(y_pred - y)

    # æ›´æ–°
    w = w - learning_rate * grad_w
    b = b - learning_rate * grad_b

    if epoch % 200 == 0:
        print(f"Epoch {epoch}: Loss={loss:.2f}")

print(f"\næƒé‡: {w}")
print(f"åå·®: {b:.2f}")
```

---

## 2.9 è¯„ä¼°å›å½’æ¨¡å‹

### ğŸ“Š è¯„ä¼°æŒ‡æ ‡

#### **1. Mean Squared Error (MSE)**

```
MSE = (1/N) Î£(Å·â¿ - yâ¿)Â²
```

- å¹³å‡å¹³æ–¹è¯¯å·®
- å•ä½æ˜¯åŸå§‹å•ä½çš„å¹³æ–¹
- å¯¹å¼‚å¸¸å€¼æ•æ„Ÿï¼ˆå› ä¸ºå¹³æ–¹ï¼‰

#### **2. Root Mean Squared Error (RMSE)**

```
RMSE = âˆšMSE = âˆš[(1/N) Î£(Å·â¿ - yâ¿)Â²]
```

- å¯¹ MSE å¼€æ ¹å·
- å•ä½å’ŒåŸå§‹æ•°æ®ç›¸åŒ
- æ›´ç›´è§‚

#### **3. Mean Absolute Error (MAE)**

```
MAE = (1/N) Î£|Å·â¿ - yâ¿|
```

- å¹³å‡ç»å¯¹è¯¯å·®
- å¯¹å¼‚å¸¸å€¼ä¸æ•æ„Ÿ
- ä¹Ÿå¾ˆç›´è§‚

#### **4. RÂ² Score (å†³å®šç³»æ•°)**

```
RÂ² = 1 - (SS_res / SS_tot)

SS_res = Î£(yâ¿ - Å·â¿)Â²  (æ®‹å·®å¹³æ–¹å’Œ)
SS_tot = Î£(yâ¿ - È³)Â²   (æ€»å¹³æ–¹å’Œï¼ŒÈ³æ˜¯å¹³å‡å€¼)
```

- èŒƒå›´ï¼š-âˆ åˆ° 1
- RÂ² = 1: å®Œç¾é¢„æµ‹
- RÂ² = 0: å’Œé¢„æµ‹å¹³å‡å€¼ä¸€æ ·å¥½
- RÂ² < 0: æ¯”é¢„æµ‹å¹³å‡å€¼è¿˜å·®

### ğŸ’» è®¡ç®—è¯„ä¼°æŒ‡æ ‡

```python
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import numpy as np

# çœŸå®å€¼å’Œé¢„æµ‹å€¼
y_true = np.array([28, 55, 82, 109, 136])
y_pred = np.array([30, 52, 80, 110, 135])

# è®¡ç®—æŒ‡æ ‡
mse = mean_squared_error(y_true, y_pred)
rmse = np.sqrt(mse)
mae = mean_absolute_error(y_true, y_pred)
r2 = r2_score(y_true, y_pred)

print(f"MSE:  {mse:.2f}")
print(f"RMSE: {rmse:.2f}")
print(f"MAE:  {mae:.2f}")
print(f"RÂ²:   {r2:.4f}")
```

**è¾“å‡º**ï¼š
```
MSE:  5.40
RMSE: 2.32
MAE:  1.80
RÂ²:   0.9996
```

---

## 2.10 è®­ç»ƒé›† vs æµ‹è¯•é›†

### ğŸ¯ ä¸ºä»€ä¹ˆè¦åˆ†å¼€ï¼Ÿ

**é—®é¢˜**ï¼šå¦‚æœåªåœ¨è®­ç»ƒæ•°æ®ä¸Šè¯„ä¼°ï¼Œå¯èƒ½äº§ç”Ÿ**è¿‡æ‹Ÿåˆ**ï¼

```
å­¦ç”ŸAï¼šè€ƒè¯•å‰çœ‹è¿‡æ‰€æœ‰é¢˜ç›®å’Œç­”æ¡ˆ
      â†’ è€ƒè¯•100åˆ†
      â†’ ä½†çœŸçš„ç†è§£äº†å—ï¼Ÿ

å­¦ç”ŸBï¼šè®¤çœŸå­¦ä¹ æ¦‚å¿µå’Œæ–¹æ³•
      â†’ è€ƒè¯•95åˆ†
      â†’ ä½†èƒ½è§£å†³æ–°é—®é¢˜
```

### ğŸ“‚ æ•°æ®åˆ†å‰²

```
åŸå§‹æ•°æ® (100%)
    â†“
    â”œâ”€ è®­ç»ƒé›† (Training Set) - 80%
    â”‚    ç”¨æ¥è®­ç»ƒæ¨¡å‹ï¼ˆå­¦ä¹ å‚æ•°ï¼‰
    â”‚
    â””â”€ æµ‹è¯•é›† (Test Set) - 20%
         ç”¨æ¥è¯„ä¼°æ¨¡å‹ï¼ˆæ¨¡æ‹ŸçœŸå®åœºæ™¯ï¼‰
```

**é‡è¦**ï¼šæµ‹è¯•é›†åœ¨è®­ç»ƒæ—¶**å®Œå…¨ä¸èƒ½çœ‹**ï¼

### ğŸ’» å®ç°

```python
from sklearn.model_selection import train_test_split

# åˆ†å‰²æ•°æ®
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,      # 20% ä½œä¸ºæµ‹è¯•é›†
    random_state=42     # å›ºå®šéšæœºç§å­ï¼Œä¿è¯å¯é‡å¤
)

print(f"è®­ç»ƒé›†å¤§å°: {len(X_train)}")
print(f"æµ‹è¯•é›†å¤§å°: {len(X_test)}")

# åªåœ¨è®­ç»ƒé›†ä¸Šè®­ç»ƒ
model.fit(X_train, y_train)

# åœ¨ä¸¤ä¸ªé›†åˆä¸Šåˆ†åˆ«è¯„ä¼°
train_score = model.score(X_train, y_train)
test_score = model.score(X_test, y_test)

print(f"è®­ç»ƒé›† RÂ²: {train_score:.4f}")
print(f"æµ‹è¯•é›† RÂ²: {test_score:.4f}")
```

### ğŸš¨ å¸¸è§æƒ…å†µåˆ†æ

| è®­ç»ƒé›†è¡¨ç° | æµ‹è¯•é›†è¡¨ç° | è¯Šæ–­ | è§£å†³æ–¹æ¡ˆ |
|----------|----------|------|---------|
| å¥½ | å¥½ | âœ“ æ­£å¸¸ | ç»§ç»­ä¼˜åŒ– |
| å·® | å·® | Underfitting | æ›´å¤æ‚æ¨¡å‹ |
| å¥½ | å·® | Overfitting | æ­£åˆ™åŒ–/æ›´å¤šæ•°æ® |
| å·® | å¥½ | ç½•è§ï¼Œæ•°æ®é—®é¢˜ | æ£€æŸ¥æ•°æ®åˆ†å‰² |

---

## 2.11 æ­£åˆ™åŒ– (Regularization)

### ğŸ¯ ä»€ä¹ˆæ˜¯æ­£åˆ™åŒ–ï¼Ÿ

**é—®é¢˜**ï¼šæ¨¡å‹å¤ªå¤æ‚ï¼Œè¿‡æ‹Ÿåˆè®­ç»ƒæ•°æ®

```
y = b + wâ‚x + wâ‚‚xÂ² + wâ‚ƒxÂ³ + ... + wâ‚â‚€â‚€xÂ¹â°â°
```

è¿™ä¸ªæ¨¡å‹æœ‰ 100 ä¸ªå‚æ•°ï¼Œå¯ä»¥å®Œç¾æ‹Ÿåˆè®­ç»ƒæ•°æ®ï¼Œä½†åœ¨æ–°æ•°æ®ä¸Šè¡¨ç°å¾ˆå·®ï¼

**è§£å†³**ï¼šæƒ©ç½šè¿‡å¤§çš„å‚æ•°ï¼Œè®©æ¨¡å‹æ›´"å¹³æ»‘"

### ğŸ“ L2 Regularization (Ridge)

ä¿®æ”¹ Loss Functionï¼š

```
L(w, b) = (1/N) Î£(Å·â¿ - yâ¿)Â² + Î»Â·Î£wáµ¢Â²
          \_____________/   \______/
           åŸå§‹ Loss      æ­£åˆ™åŒ–é¡¹
```

- `Î»` (lambda): æ­£åˆ™åŒ–å¼ºåº¦
  - Î» = 0: æ²¡æœ‰æ­£åˆ™åŒ–
  - Î» å¾ˆå¤§: å¼ºçƒˆæƒ©ç½šå¤§å‚æ•°
  - å…¸å‹å€¼: 0.01, 0.1, 1, 10

**æ•ˆæœ**ï¼šå‚æ•°å€¾å‘äºå˜å°ï¼Œæ¨¡å‹æ›´ç®€å•

### ğŸ“ L1 Regularization (Lasso)

```
L(w, b) = (1/N) Î£(Å·â¿ - yâ¿)Â² + Î»Â·Î£|wáµ¢|
```

**æ•ˆæœ**ï¼šä¸€äº›å‚æ•°ä¼šå˜æˆ 0ï¼ˆç‰¹å¾é€‰æ‹©ï¼‰

### ğŸ’» ä½¿ç”¨ Scikit-learn

```python
from sklearn.linear_model import Ridge, Lasso

# Ridge Regression (L2)
ridge_model = Ridge(alpha=1.0)  # alpha å°±æ˜¯ Î»
ridge_model.fit(X_train, y_train)

# Lasso Regression (L1)
lasso_model = Lasso(alpha=1.0)
lasso_model.fit(X_train, y_train)

# æ¯”è¾ƒå‚æ•°
print("Ridge æƒé‡:", ridge_model.coef_)
print("Lasso æƒé‡:", lasso_model.coef_)
# Lasso çš„ä¸€äº›æƒé‡å¯èƒ½æ˜¯ 0
```

---

## 2.12 ç‰¹å¾å·¥ç¨‹ (Feature Engineering)

### ğŸ› ï¸ ä»€ä¹ˆæ˜¯ç‰¹å¾å·¥ç¨‹ï¼Ÿ

> è®¾è®¡å’Œé€‰æ‹©å¥½çš„ç‰¹å¾ï¼Œæ¯”é€‰æ‹©ç®—æ³•æ›´é‡è¦ï¼

**åƒåœ¾è¿›ï¼Œåƒåœ¾å‡º (Garbage In, Garbage Out)**

### ğŸ”¹ å¸¸è§æŠ€å·§

#### **1. ç‰¹å¾ç¼©æ”¾ (Feature Scaling)**

**é—®é¢˜**ï¼šä¸åŒç‰¹å¾çš„å°ºåº¦å·®å¼‚å¾ˆå¤§

```
ç‰¹å¾1ï¼šæˆ¿å±‹é¢ç§¯ (50-200 mÂ²)
ç‰¹å¾2ï¼šæˆ¿é—´æ•°é‡ (1-5 ä¸ª)
ç‰¹å¾3ï¼šè·ç¦»å¸‚ä¸­å¿ƒ (0-50 km)
ç‰¹å¾4ï¼šå»ºé€ å¹´ä»½ (1950-2023)
```

**æ–¹æ³•**ï¼š

**æ ‡å‡†åŒ– (Standardization)**
```
x' = (x - Î¼) / Ïƒ

Î¼: å¹³å‡å€¼
Ïƒ: æ ‡å‡†å·®
ç»“æœï¼šå¹³å‡å€¼=0ï¼Œæ ‡å‡†å·®=1
```

**å½’ä¸€åŒ– (Normalization)**
```
x' = (x - min) / (max - min)

ç»“æœï¼šèŒƒå›´ [0, 1]
```

```python
from sklearn.preprocessing import StandardScaler, MinMaxScaler

# æ ‡å‡†åŒ–
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_train)

# å½’ä¸€åŒ–
normalizer = MinMaxScaler()
X_normalized = normalizer.fit_transform(X_train)
```

#### **2. å¤šé¡¹å¼ç‰¹å¾ (Polynomial Features)**

æŠŠçº¿æ€§æ¨¡å‹å˜æˆéçº¿æ€§ï¼š

```
åŸå§‹ç‰¹å¾: [xâ‚, xâ‚‚]
â†“
å¤šé¡¹å¼ç‰¹å¾: [1, xâ‚, xâ‚‚, xâ‚Â², xâ‚xâ‚‚, xâ‚‚Â²]
```

```python
from sklearn.preprocessing import PolynomialFeatures

poly = PolynomialFeatures(degree=2)
X_poly = poly.fit_transform(X)

# ä¾‹å­
# è¾“å…¥: [[2, 3]]
# è¾“å‡º: [[1, 2, 3, 4, 6, 9]]
#        1  xâ‚ xâ‚‚ xâ‚Â² xâ‚xâ‚‚ xâ‚‚Â²
```

#### **3. ç±»åˆ«ç‰¹å¾ç¼–ç **

**é—®é¢˜**ï¼šç‰¹å¾æ˜¯ç±»åˆ«ï¼Œä¸æ˜¯æ•°å­—

```
åŸå¸‚: ['åŒ—äº¬', 'ä¸Šæµ·', 'æ·±åœ³']
```

**æ–¹æ³• 1ï¼šLabel Encoding**
```
åŒ—äº¬ â†’ 0
ä¸Šæµ· â†’ 1
æ·±åœ³ â†’ 2
```

**é—®é¢˜**ï¼šæš—ç¤ºäº†é¡ºåºå…³ç³»ï¼ˆæ·±åœ³ > ä¸Šæµ· > åŒ—äº¬ï¼Ÿï¼‰

**æ–¹æ³• 2ï¼šOne-Hot Encoding**
```
åŒ—äº¬ â†’ [1, 0, 0]
ä¸Šæµ· â†’ [0, 1, 0]
æ·±åœ³ â†’ [0, 0, 1]
```

```python
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
import pandas as pd

# One-Hot Encoding
df = pd.DataFrame({'city': ['åŒ—äº¬', 'ä¸Šæµ·', 'æ·±åœ³', 'åŒ—äº¬']})
df_encoded = pd.get_dummies(df, columns=['city'])
print(df_encoded)
```

**è¾“å‡º**ï¼š
```
   city_åŒ—äº¬  city_ä¸Šæµ·  city_æ·±åœ³
0         1         0         0
1         0         1         0
2         0         0         1
3         1         0         0
```

#### **4. ç‰¹å¾äº¤å‰ (Feature Crossing)**

åˆ›å»ºç‰¹å¾ä¹‹é—´çš„äº¤äº’é¡¹ï¼š

```
ç‰¹å¾ï¼šé¢ç§¯(xâ‚), æˆ¿é—´æ•°(xâ‚‚)

æ–°ç‰¹å¾ï¼šæ¯ä¸ªæˆ¿é—´çš„å¹³å‡é¢ç§¯ = xâ‚ / xâ‚‚
```

#### **5. å¤„ç†ç¼ºå¤±å€¼**

```python
from sklearn.impute import SimpleImputer

# ç”¨å¹³å‡å€¼å¡«å……
imputer = SimpleImputer(strategy='mean')
X_filled = imputer.fit_transform(X)

# å…¶ä»–ç­–ç•¥: 'median', 'most_frequent', 'constant'
```

---

## 2.13 å®æˆ˜ï¼šPM2.5 é¢„æµ‹

### ğŸ“‹ é—®é¢˜æè¿°

**æ•°æ®**ï¼šä¸°åŸæ°”è±¡ç«™çš„è§‚æµ‹æ•°æ®
- 18 ä¸ªç‰¹å¾ï¼šPM2.5, PM10, æ¸©åº¦, æ¹¿åº¦, é£é€Ÿ...
- æ¯å°æ—¶è®°å½•ä¸€æ¬¡
- **ç›®æ ‡**ï¼šæ ¹æ®å‰ 9 å°æ—¶çš„æ•°æ®ï¼Œé¢„æµ‹ç¬¬ 10 å°æ—¶çš„ PM2.5

### ğŸ“‚ æ•°æ®æ ¼å¼

```
æ—¥æœŸ        AMB_TEMP  CH4    CO    ...  PM2.5
2017-01  0     14      1.8   0.37       35
2017-01  1     14      1.8   0.37       26
2017-01  2     14      1.8   0.36       18
...
```

### ğŸ’» å®Œæ•´ä»£ç 

```python
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt

# 1. è¯»å–æ•°æ®
data = pd.read_csv('train.csv', encoding='big5')

# 2. æ•°æ®é¢„å¤„ç†
# é€‰æ‹©ç‰¹å¾
features = ['PM2.5', 'PM10', 'SO2', 'CO', 'O3', 'TEMP', 'WIND_SPEED']
data = data[data['è§‚æµ‹é¡¹ç›®'].isin(features)]

# è½¬æ¢ä¸ºæ•°å€¼
data = data.iloc[:, 3:]  # å»æ‰å‰3åˆ—ï¼ˆæ—¥æœŸã€æµ‹ç«™ã€è§‚æµ‹é¡¹ç›®ï¼‰
data[data == 'NR'] = 0    # NR (No Rain) æ”¹ä¸º 0
data = data.astype(float)

# 3. æ„é€ è®­ç»ƒæ ·æœ¬
# æ¯9å°æ—¶çš„æ•°æ®é¢„æµ‹ç¬¬10å°æ—¶çš„ PM2.5
def create_dataset(data, look_back=9):
    X, y = [], []
    for i in range(0, len(data) - look_back - 1, 18):  # æ¯18è¡Œæ˜¯ä¸€å°æ—¶çš„æ‰€æœ‰ç‰¹å¾
        # å–9å°æ—¶çš„æ•°æ®
        sample = []
        for j in range(look_back):
            sample.extend(data.iloc[i+j*18:(i+1)*18, :].values.flatten())
        X.append(sample)

        # ç¬¬10å°æ—¶çš„ PM2.5ï¼ˆå‡è®¾ PM2.5 æ˜¯ç¬¬ä¸€ä¸ªç‰¹å¾ï¼‰
        y.append(data.iloc[i + look_back*18, 0])

    return np.array(X), np.array(y)

X, y = create_dataset(data)

# 4. åˆ†å‰²æ•°æ®é›†
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# 5. ç‰¹å¾ç¼©æ”¾
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# 6. è®­ç»ƒæ¨¡å‹
model = LinearRegression()
model.fit(X_train_scaled, y_train)

# 7. è¯„ä¼°
y_train_pred = model.predict(X_train_scaled)
y_test_pred = model.predict(X_test_scaled)

train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))
test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))

print(f"è®­ç»ƒé›† RMSE: {train_rmse:.2f}")
print(f"æµ‹è¯•é›† RMSE: {test_rmse:.2f}")

# 8. å¯è§†åŒ–
plt.figure(figsize=(10, 5))
plt.scatter(y_test, y_test_pred, alpha=0.5)
plt.plot([0, max(y_test)], [0, max(y_test)], 'r--', lw=2)
plt.xlabel('çœŸå® PM2.5')
plt.ylabel('é¢„æµ‹ PM2.5')
plt.title('PM2.5 é¢„æµ‹ç»“æœ')
plt.show()
```

---

## 2.14 å›å½’çš„å±€é™æ€§

### âš ï¸ ä»€ä¹ˆæ—¶å€™å›å½’ä¸é€‚ç”¨ï¼Ÿ

#### **1. è¾“å‡ºæ˜¯ç±»åˆ«**

```
âœ— å›å½’: é¢„æµ‹ "çŒ«" = 1.7ï¼Ÿ
âœ“ åˆ†ç±»: é¢„æµ‹ "çŒ«" çš„æ¦‚ç‡ = 0.9
```

#### **2. æ•°æ®éçº¿æ€§ä¸”å¤æ‚**

```
ç®€å•éçº¿æ€§ï¼šå¯ä»¥ç”¨å¤šé¡¹å¼å›å½’
å¤æ‚éçº¿æ€§ï¼šéœ€è¦ç¥ç»ç½‘ç»œ
```

#### **3. æœ‰æ˜æ˜¾çš„å¼‚å¸¸å€¼**

å›å½’å¯¹å¼‚å¸¸å€¼å¾ˆæ•æ„Ÿï¼ˆå› ä¸ºå¹³æ–¹è¯¯å·®ï¼‰

#### **4. ç‰¹å¾ä¹‹é—´é«˜åº¦ç›¸å…³ï¼ˆå¤šé‡å…±çº¿æ€§ï¼‰**

```
ç‰¹å¾1ï¼šæˆ¿å±‹é¢ç§¯ (mÂ²)
ç‰¹å¾2ï¼šæˆ¿å±‹é¢ç§¯ (ftÂ²)
â†’ å®Œå…¨çº¿æ€§ç›¸å…³ï¼Œæ¨¡å‹ä¸ç¨³å®š
```

### ğŸ’¡ è§£å†³æ–¹æ¡ˆ

- åˆ†ç±»é—®é¢˜ â†’ ç”¨ Logistic Regression
- å¤æ‚éçº¿æ€§ â†’ ç”¨ç¥ç»ç½‘ç»œ
- å¼‚å¸¸å€¼é—®é¢˜ â†’ ç”¨ Robust Regression æˆ–å»é™¤å¼‚å¸¸å€¼
- å¤šé‡å…±çº¿æ€§ â†’ ç”¨ Ridge/Lasso æˆ–ç§»é™¤ç›¸å…³ç‰¹å¾

---

## ğŸ“ æœ¬ç« ä½œä¸š

### ä½œä¸š 1ï¼šç†è®ºé¢˜

1. **è§£é‡Šæ¢¯åº¦ä¸‹é™**
   - ç”¨è‡ªå·±çš„è¯è§£é‡Šæ¢¯åº¦ä¸‹é™çš„åŸç†
   - ç”»å‡º Loss ä¸‹é™çš„æ›²çº¿
   - è¯´æ˜å­¦ä¹ ç‡çš„ä½œç”¨

2. **è¿‡æ‹Ÿåˆ vs æ¬ æ‹Ÿåˆ**
   - ä»€ä¹ˆæ˜¯è¿‡æ‹Ÿåˆï¼Ÿç»™å‡ºä¾‹å­
   - å¦‚ä½•æ£€æµ‹è¿‡æ‹Ÿåˆï¼Ÿ
   - åˆ—å‡º 3 ç§é˜²æ­¢è¿‡æ‹Ÿåˆçš„æ–¹æ³•

3. **ç‰¹å¾å·¥ç¨‹**
   - ä¸ºä»€ä¹ˆéœ€è¦ç‰¹å¾ç¼©æ”¾ï¼Ÿ
   - One-Hot Encoding å’Œ Label Encoding çš„åŒºåˆ«ï¼Ÿ
   - ç»™æˆ¿ä»·é¢„æµ‹é—®é¢˜è®¾è®¡ 3 ä¸ªæœ‰ç”¨çš„ç‰¹å¾

### ä½œä¸š 2ï¼šç¼–ç¨‹å®è·µ

**ä»»åŠ¡**ï¼šæ³¢å£«é¡¿æˆ¿ä»·é¢„æµ‹

```python
from sklearn.datasets import load_boston
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression, Ridge
from sklearn.metrics import mean_squared_error
import numpy as np

# 1. åŠ è½½æ•°æ®
boston = load_boston()
X, y = boston.data, boston.target

# 2. æ•°æ®åˆ†å‰²
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# 3. è®­ç»ƒçº¿æ€§å›å½’æ¨¡å‹
model = LinearRegression()
model.fit(X_train, y_train)

# 4. è¯„ä¼°
y_pred = model.predict(X_test)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
print(f"RMSE: {rmse:.2f}")

# TODO: å®Œæˆä»¥ä¸‹ä»»åŠ¡
# a) ç‰¹å¾ç¼©æ”¾ï¼Œè§‚å¯Ÿæ€§èƒ½å˜åŒ–
# b) å°è¯•å¤šé¡¹å¼ç‰¹å¾ (degree=2)
# c) ä½¿ç”¨ Ridge Regressionï¼Œè°ƒæ•´ alpha å‚æ•°
# d) ç”»å‡ºçœŸå®å€¼ vs é¢„æµ‹å€¼çš„æ•£ç‚¹å›¾
# e) åˆ†æå“ªäº›ç‰¹å¾æœ€é‡è¦ï¼ˆæŸ¥çœ‹ model.coef_ï¼‰
```

**æç¤º**ï¼š
- ä½¿ç”¨ `StandardScaler` è¿›è¡Œç‰¹å¾ç¼©æ”¾
- ä½¿ç”¨ `PolynomialFeatures` åˆ›å»ºå¤šé¡¹å¼ç‰¹å¾
- å°è¯• alpha = [0.01, 0.1, 1, 10, 100]

### ä½œä¸š 3ï¼šKaggle å®æˆ˜

å‰å¾€ Kaggleï¼Œå‚åŠ  "House Prices - Advanced Regression Techniques" ç«èµ›

**è¦æ±‚**ï¼š
1. ä¸‹è½½æ•°æ®é›†
2. è¿›è¡Œ EDA (Exploratory Data Analysis)
3. ç‰¹å¾å·¥ç¨‹
4. è®­ç»ƒæ¨¡å‹
5. æäº¤é¢„æµ‹ç»“æœ
6. å†™ä¸€ä»½æŠ¥å‘Šè¯´æ˜ä½ çš„æ–¹æ³•

---

## ğŸ”‘ æœ¬ç« å…³é”®æ¦‚å¿µæ€»ç»“

| æ¦‚å¿µ | è¯´æ˜ |
|------|------|
| å›å½’ | é¢„æµ‹è¿ç»­æ•°å€¼ |
| çº¿æ€§å›å½’ | y = b + wÂ·x |
| MSE | å‡æ–¹è¯¯å·®ï¼Œå¸¸ç”¨æŸå¤±å‡½æ•° |
| æ¢¯åº¦ä¸‹é™ | æ‰¾æœ€ä¼˜å‚æ•°çš„æ–¹æ³• |
| å­¦ä¹ ç‡ | æ§åˆ¶å‚æ•°æ›´æ–°æ­¥é•¿ |
| è¿‡æ‹Ÿåˆ | è®­ç»ƒé›†å¥½ï¼Œæµ‹è¯•é›†å·® |
| æ¬ æ‹Ÿåˆ | è®­ç»ƒé›†å’Œæµ‹è¯•é›†éƒ½å·® |
| æ­£åˆ™åŒ– | æƒ©ç½šè¿‡å¤§å‚æ•°ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ |
| ç‰¹å¾ç¼©æ”¾ | ç»Ÿä¸€ç‰¹å¾å°ºåº¦ |
| ç‰¹å¾å·¥ç¨‹ | è®¾è®¡å¥½çš„ç‰¹å¾ |

---

## ğŸ¯ ä¸‹ä¸€ç« é¢„å‘Š

**ç¬¬ä¸‰ç« ï¼šé€»è¾‘å›å½’ä¸åˆ†ç±» (Logistic Regression & Classification)**
- ä¸ºä»€ä¹ˆä¸èƒ½ç”¨çº¿æ€§å›å½’åšåˆ†ç±»ï¼Ÿ
- Sigmoid å‡½æ•°çš„ä½œç”¨
- Cross Entropy Loss
- å¤šåˆ†ç±»é—®é¢˜ (Softmax)
- å®æˆ˜ï¼šæ‰‹å†™æ•°å­—è¯†åˆ« (MNIST)

---

-----
